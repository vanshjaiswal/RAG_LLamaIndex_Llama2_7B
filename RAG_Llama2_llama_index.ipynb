{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cce75581b9442e484f1f467c4483117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1c7eab3edba46f9a1094f6641050521",
              "IPY_MODEL_a83e939c1c46409ead9611d498bc4bcc",
              "IPY_MODEL_a52036de001543c7bfe20c4bcd47f90e"
            ],
            "layout": "IPY_MODEL_4ff36f6a3cbe4a86bd28204641409b8b"
          }
        },
        "b1c7eab3edba46f9a1094f6641050521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d3eeec391c41d7826f63248d599ab7",
            "placeholder": "​",
            "style": "IPY_MODEL_9a869d1b49344d25a5fa61c6d63be7aa",
            "value": "config.json: 100%"
          }
        },
        "a83e939c1c46409ead9611d498bc4bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4577344977a34c4db0a0976ed5c9cdae",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3953a4ab9e3d4816a0a3cba28e009335",
            "value": 614
          }
        },
        "a52036de001543c7bfe20c4bcd47f90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12de6f56f8c43709d9e5a861b58862b",
            "placeholder": "​",
            "style": "IPY_MODEL_1ed535f109504beea9fe5e059e50177d",
            "value": " 614/614 [00:00&lt;00:00, 25.3kB/s]"
          }
        },
        "4ff36f6a3cbe4a86bd28204641409b8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d3eeec391c41d7826f63248d599ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a869d1b49344d25a5fa61c6d63be7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4577344977a34c4db0a0976ed5c9cdae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3953a4ab9e3d4816a0a3cba28e009335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c12de6f56f8c43709d9e5a861b58862b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed535f109504beea9fe5e059e50177d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3783b2a7e5874a88b22dbe25734cb163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6772f12a121b4b23bea4f93141124f52",
              "IPY_MODEL_8d28b142986a4607b9240e34504dca84",
              "IPY_MODEL_4abc3bd1530f40659320c51199e3f89c"
            ],
            "layout": "IPY_MODEL_64d009628d164de78ce7d34c7551739c"
          }
        },
        "6772f12a121b4b23bea4f93141124f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162a6d5553d54beb9628e69c79b20666",
            "placeholder": "​",
            "style": "IPY_MODEL_ee25138d2c02403785253a6a94cb1bc5",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "8d28b142986a4607b9240e34504dca84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d11a3ba7ab94f7b9fdfd1c0bf8e393f",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca54a6d8057b41b1896526af1d506424",
            "value": 26788
          }
        },
        "4abc3bd1530f40659320c51199e3f89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d54e5ca30874d0f8007a2e789aab186",
            "placeholder": "​",
            "style": "IPY_MODEL_7bdb4bce9f8a4acb890111c5d45f6999",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 2.01MB/s]"
          }
        },
        "64d009628d164de78ce7d34c7551739c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162a6d5553d54beb9628e69c79b20666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee25138d2c02403785253a6a94cb1bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d11a3ba7ab94f7b9fdfd1c0bf8e393f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca54a6d8057b41b1896526af1d506424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d54e5ca30874d0f8007a2e789aab186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bdb4bce9f8a4acb890111c5d45f6999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82e66c4e9e784d5d9599c5e345608c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50b948efe86f4a25abedafe3e6350bae",
              "IPY_MODEL_1a0bec2554eb4ef49ec41e171d1e5232",
              "IPY_MODEL_8ad82d8892214941a7efbabb0bee97a3"
            ],
            "layout": "IPY_MODEL_74b1fc00dffb4c949aa22a8076d7e190"
          }
        },
        "50b948efe86f4a25abedafe3e6350bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8387499768e24d11bee94b7fae479f6f",
            "placeholder": "​",
            "style": "IPY_MODEL_d5fbbd23a9364c94ab37ee232ba886f3",
            "value": "Downloading shards: 100%"
          }
        },
        "1a0bec2554eb4ef49ec41e171d1e5232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f35cb5a2a91f4cd68e63013aa61f8100",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5fc7017c3daa4a44b1eb384e64a17cac",
            "value": 2
          }
        },
        "8ad82d8892214941a7efbabb0bee97a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fa58ce7a3c84e9f82e759d24e5efbeb",
            "placeholder": "​",
            "style": "IPY_MODEL_3a5a2ea12e4e45e282911ccde71ec1ce",
            "value": " 2/2 [01:07&lt;00:00, 30.95s/it]"
          }
        },
        "74b1fc00dffb4c949aa22a8076d7e190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8387499768e24d11bee94b7fae479f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fbbd23a9364c94ab37ee232ba886f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35cb5a2a91f4cd68e63013aa61f8100": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc7017c3daa4a44b1eb384e64a17cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fa58ce7a3c84e9f82e759d24e5efbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a5a2ea12e4e45e282911ccde71ec1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d85722344dc4e40858191462a9fced4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37f9764f7a194b2cb49c44ca0a8ff7c5",
              "IPY_MODEL_68262dcf352f441284fbd28fc5582a94",
              "IPY_MODEL_14c7bf42e30547cfad7e423536d0a9e3"
            ],
            "layout": "IPY_MODEL_da8c6495b3424503b0519e6586103cb5"
          }
        },
        "37f9764f7a194b2cb49c44ca0a8ff7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a758a4649314334afebd13411921a84",
            "placeholder": "​",
            "style": "IPY_MODEL_64fedf40dc4e44bcac734474c69e3622",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "68262dcf352f441284fbd28fc5582a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_213f96af7d8b434a81226b4327ba31fb",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb18f061e25c48b681394c0a5f3beaa6",
            "value": 9976576152
          }
        },
        "14c7bf42e30547cfad7e423536d0a9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6098bb4cedb04156878b96b1a8469b89",
            "placeholder": "​",
            "style": "IPY_MODEL_76cca771a8a54eb784218ce71877594a",
            "value": " 9.98G/9.98G [00:50&lt;00:00, 250MB/s]"
          }
        },
        "da8c6495b3424503b0519e6586103cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a758a4649314334afebd13411921a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64fedf40dc4e44bcac734474c69e3622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "213f96af7d8b434a81226b4327ba31fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb18f061e25c48b681394c0a5f3beaa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6098bb4cedb04156878b96b1a8469b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76cca771a8a54eb784218ce71877594a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a0b293e50634131ae58c4089bef56fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2a1978144f64b5f9e19a5c41617f15f",
              "IPY_MODEL_7795cde88bc8468d9633a9e47bca52bd",
              "IPY_MODEL_97adbc9a0d7f46ca95b88983300c0436"
            ],
            "layout": "IPY_MODEL_944790a28a3c443ead377c010ccaef17"
          }
        },
        "c2a1978144f64b5f9e19a5c41617f15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c91902d3d4d1489c95290d9b949335f4",
            "placeholder": "​",
            "style": "IPY_MODEL_7875996a298f4dc78925c5c6bb558371",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "7795cde88bc8468d9633a9e47bca52bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e438239089ee4d1e85ab135bf614f8a6",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73ef4e8d9d614f72bf0dac3e875f948c",
            "value": 3500296424
          }
        },
        "97adbc9a0d7f46ca95b88983300c0436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2314f3b9170b461faa0850ec7b4b5db1",
            "placeholder": "​",
            "style": "IPY_MODEL_8bf7404cf96d472388ce64cc293a0728",
            "value": " 3.50G/3.50G [00:16&lt;00:00, 217MB/s]"
          }
        },
        "944790a28a3c443ead377c010ccaef17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91902d3d4d1489c95290d9b949335f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7875996a298f4dc78925c5c6bb558371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e438239089ee4d1e85ab135bf614f8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ef4e8d9d614f72bf0dac3e875f948c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2314f3b9170b461faa0850ec7b4b5db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf7404cf96d472388ce64cc293a0728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5dea3ad0c074123aa0daff377f12339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70d4f8e073d7437889694729bb9147ee",
              "IPY_MODEL_35fee16af81646bb94f7509ec4932074",
              "IPY_MODEL_012263ffad3d4a6a94339ff01ab1f0e2"
            ],
            "layout": "IPY_MODEL_b9136f5fe4cc4a4f969c5441badda5c3"
          }
        },
        "70d4f8e073d7437889694729bb9147ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d59c8956a1a841d3b90b71fc324dfd1e",
            "placeholder": "​",
            "style": "IPY_MODEL_618cebc6b3854f28b0a38af78b2c1cfd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "35fee16af81646bb94f7509ec4932074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c1a2e40d64a48ea8f3e9a28ac4a99dc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f6a9af688de4f7485de5fdbb131173e",
            "value": 2
          }
        },
        "012263ffad3d4a6a94339ff01ab1f0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf0337dc39c6499c953cb8c46c5ecbc5",
            "placeholder": "​",
            "style": "IPY_MODEL_7d99a3d6734a43e39ef3b5f1f641756d",
            "value": " 2/2 [01:02&lt;00:00, 28.58s/it]"
          }
        },
        "b9136f5fe4cc4a4f969c5441badda5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59c8956a1a841d3b90b71fc324dfd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618cebc6b3854f28b0a38af78b2c1cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c1a2e40d64a48ea8f3e9a28ac4a99dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6a9af688de4f7485de5fdbb131173e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf0337dc39c6499c953cb8c46c5ecbc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d99a3d6734a43e39ef3b5f1f641756d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c39a27f49a4e45a69e12c93b34e2d5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4e6f6550d8641478eb04a7a46255804",
              "IPY_MODEL_f4f3782e18b04220bdae397d3d8dbb43",
              "IPY_MODEL_cbfdfd695ec0445fb00440b8d19c9c2b"
            ],
            "layout": "IPY_MODEL_7fd1e94ecbab4fde853d5fcf2e43a017"
          }
        },
        "d4e6f6550d8641478eb04a7a46255804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dd94460b6d24cd184656f2f812c4509",
            "placeholder": "​",
            "style": "IPY_MODEL_574480f7eca941318a27feec2a3e7083",
            "value": "generation_config.json: 100%"
          }
        },
        "f4f3782e18b04220bdae397d3d8dbb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a1cc6ea33cf4a269574aad929db4a13",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_946db45e6da545da8db8e6f8e2ec17ac",
            "value": 188
          }
        },
        "cbfdfd695ec0445fb00440b8d19c9c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a132d1f2f904a6080c5d36aee40c009",
            "placeholder": "​",
            "style": "IPY_MODEL_58172ee191db45b2880e1ceb80ca95a6",
            "value": " 188/188 [00:00&lt;00:00, 8.29kB/s]"
          }
        },
        "7fd1e94ecbab4fde853d5fcf2e43a017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd94460b6d24cd184656f2f812c4509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "574480f7eca941318a27feec2a3e7083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a1cc6ea33cf4a269574aad929db4a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946db45e6da545da8db8e6f8e2ec17ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a132d1f2f904a6080c5d36aee40c009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58172ee191db45b2880e1ceb80ca95a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a03260f8db53424b95e5e0f8838560ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53ef9c1c1cf74c83b9b4f74752f84a49",
              "IPY_MODEL_ddb696912dfb4ff2a0b6aaeb359f8ee7",
              "IPY_MODEL_6f1e59f341bf43d490b184dc7507d186"
            ],
            "layout": "IPY_MODEL_a844938a59f74935a93a5c1bacb8e020"
          }
        },
        "53ef9c1c1cf74c83b9b4f74752f84a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1e4077a5eca495aa0df047adac510d2",
            "placeholder": "​",
            "style": "IPY_MODEL_fa78cd882c2b40eab55698981968299c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ddb696912dfb4ff2a0b6aaeb359f8ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bd0feeae414c188416fe5a75fe8e99",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_986a2e3084f5445597ac22ae45d3562b",
            "value": 1618
          }
        },
        "6f1e59f341bf43d490b184dc7507d186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1f1e15d48e44aeb0ca69523706afdb",
            "placeholder": "​",
            "style": "IPY_MODEL_f28849387d3c4866822dc182c2361b73",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 66.9kB/s]"
          }
        },
        "a844938a59f74935a93a5c1bacb8e020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e4077a5eca495aa0df047adac510d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa78cd882c2b40eab55698981968299c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26bd0feeae414c188416fe5a75fe8e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986a2e3084f5445597ac22ae45d3562b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b1f1e15d48e44aeb0ca69523706afdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28849387d3c4866822dc182c2361b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8a78f75fb645a9bf4297f09a8883ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8ded96fa7ad4ac8983daadf2866f266",
              "IPY_MODEL_4b72d2984b0d4a2e93fc426524e69c2b",
              "IPY_MODEL_e9ec9527cf59485baf07ce867eeb4cf5"
            ],
            "layout": "IPY_MODEL_c124637edd7f48b5ae0d0b3a93bd8418"
          }
        },
        "c8ded96fa7ad4ac8983daadf2866f266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a34a026afe0644c49fc50e1850f9afa2",
            "placeholder": "​",
            "style": "IPY_MODEL_3db996a7f93f47e4b1d1b1072282846d",
            "value": "tokenizer.model: 100%"
          }
        },
        "4b72d2984b0d4a2e93fc426524e69c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f71309562344458a50e72fc2c00711",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b55766ec80aa4f6fa285276e438a5db8",
            "value": 499723
          }
        },
        "e9ec9527cf59485baf07ce867eeb4cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75257e71a7e346398b7093d707fececd",
            "placeholder": "​",
            "style": "IPY_MODEL_e8527b5c6ac84a919653c716120faf7b",
            "value": " 500k/500k [00:00&lt;00:00, 19.0MB/s]"
          }
        },
        "c124637edd7f48b5ae0d0b3a93bd8418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34a026afe0644c49fc50e1850f9afa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db996a7f93f47e4b1d1b1072282846d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71f71309562344458a50e72fc2c00711": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55766ec80aa4f6fa285276e438a5db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75257e71a7e346398b7093d707fececd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8527b5c6ac84a919653c716120faf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b08f3f1dd20475582404a9c15ac9588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb297a0215c643a9806d1819e6f311b1",
              "IPY_MODEL_0a0c04287a6f4c42b1a5a181c3d1e055",
              "IPY_MODEL_20706b2e97b144fcbb050f8dd74ce618"
            ],
            "layout": "IPY_MODEL_dd7f872a94fc478abf5036a7cdb9999f"
          }
        },
        "eb297a0215c643a9806d1819e6f311b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b190cf64cd0247208bfdb15ad5440dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_c6322b4ce7fd4ac09680c3943bec0152",
            "value": "tokenizer.json: 100%"
          }
        },
        "0a0c04287a6f4c42b1a5a181c3d1e055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddb0631060ea484faf18199a228879c8",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d332b725a834a4ba3487743b46659a0",
            "value": 1842767
          }
        },
        "20706b2e97b144fcbb050f8dd74ce618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc997d0e48a4aed8ab38ecfcb18d642",
            "placeholder": "​",
            "style": "IPY_MODEL_ecc9970f033d46feb0fff9ebd717bddf",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 7.09MB/s]"
          }
        },
        "dd7f872a94fc478abf5036a7cdb9999f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b190cf64cd0247208bfdb15ad5440dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6322b4ce7fd4ac09680c3943bec0152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddb0631060ea484faf18199a228879c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d332b725a834a4ba3487743b46659a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bc997d0e48a4aed8ab38ecfcb18d642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc9970f033d46feb0fff9ebd717bddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cb52f5bdfe544c4998336eee9d3844c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82a3fc45c004c95b062dbf610c1ddd6",
              "IPY_MODEL_b88555f787014e8589187e60dc04b98d",
              "IPY_MODEL_8cd85dfa6cd7465596f2809cc2130676"
            ],
            "layout": "IPY_MODEL_eed131eba1dc44cd810f6399e23539e6"
          }
        },
        "f82a3fc45c004c95b062dbf610c1ddd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_603c578828d54f4f87be550b5bf17994",
            "placeholder": "​",
            "style": "IPY_MODEL_e8d6fd5015d448a1b55191d8386cb8c2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "b88555f787014e8589187e60dc04b98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2855a8ccf34d1ab27c2c6823e0dcec",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6372e5ca4f44272820e4bf772c2a3ae",
            "value": 414
          }
        },
        "8cd85dfa6cd7465596f2809cc2130676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c4c74a4a2d414e8b996711fdddc9c1",
            "placeholder": "​",
            "style": "IPY_MODEL_7de8a9827f8a4dc1b054bbd063dc3298",
            "value": " 414/414 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "eed131eba1dc44cd810f6399e23539e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "603c578828d54f4f87be550b5bf17994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d6fd5015d448a1b55191d8386cb8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af2855a8ccf34d1ab27c2c6823e0dcec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6372e5ca4f44272820e4bf772c2a3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69c4c74a4a2d414e8b996711fdddc9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de8a9827f8a4dc1b054bbd063dc3298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "335258a2365340a9aa540d353c92d355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_304138af4b874e91976875c168ad0342",
              "IPY_MODEL_89859ed4d7cf4c8abcc5d3b3829dacca",
              "IPY_MODEL_a04fb3d1a4b14f0e8c133c64f122783b"
            ],
            "layout": "IPY_MODEL_b6d9c4ee969145cda249d16139070a80"
          }
        },
        "304138af4b874e91976875c168ad0342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a0af0c729a44128b74c6eb9f0e9284",
            "placeholder": "​",
            "style": "IPY_MODEL_75b370cca5214751878db98e020e49d6",
            "value": "modules.json: 100%"
          }
        },
        "89859ed4d7cf4c8abcc5d3b3829dacca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237be1a331bc4bff85619f271acfa002",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89abd83007924204a89e09d7a28d2b30",
            "value": 349
          }
        },
        "a04fb3d1a4b14f0e8c133c64f122783b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6547b1a65acf465fb6ef560f561c5a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_143c16cbc77f44bd9140ab6a6d2c5d67",
            "value": " 349/349 [00:00&lt;00:00, 5.12kB/s]"
          }
        },
        "b6d9c4ee969145cda249d16139070a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a0af0c729a44128b74c6eb9f0e9284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b370cca5214751878db98e020e49d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237be1a331bc4bff85619f271acfa002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89abd83007924204a89e09d7a28d2b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6547b1a65acf465fb6ef560f561c5a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "143c16cbc77f44bd9140ab6a6d2c5d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a5f18a5ddcd48afa9d51e8a3abd2269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_687042e2bde44b528da342369460b4f2",
              "IPY_MODEL_36ef1f1d15ee4728b185607c70fc3774",
              "IPY_MODEL_456ef102fd1e4b4a9fead89faa67368e"
            ],
            "layout": "IPY_MODEL_5487b2527c4b4ac08c6a835fbc88b147"
          }
        },
        "687042e2bde44b528da342369460b4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e025ac6355046f9a744058c4edb1adb",
            "placeholder": "​",
            "style": "IPY_MODEL_af2bff5e5d29425ab6293c78caca7676",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "36ef1f1d15ee4728b185607c70fc3774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e7df71550644a9bb2f22e4b45442931",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17f08c6d5ec647829e223b4740d49a95",
            "value": 116
          }
        },
        "456ef102fd1e4b4a9fead89faa67368e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdaf66f454b349698a550b25a241fb02",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fe5827f5f64d27b8c0a2d07ffbfcf9",
            "value": " 116/116 [00:00&lt;00:00, 4.43kB/s]"
          }
        },
        "5487b2527c4b4ac08c6a835fbc88b147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e025ac6355046f9a744058c4edb1adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2bff5e5d29425ab6293c78caca7676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e7df71550644a9bb2f22e4b45442931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17f08c6d5ec647829e223b4740d49a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdaf66f454b349698a550b25a241fb02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7fe5827f5f64d27b8c0a2d07ffbfcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15100addfdec4b72b4275ef08f488c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c24c00186cca41f885727f0be6bd7806",
              "IPY_MODEL_8aab4586c7db407fbd268dd89133c2d4",
              "IPY_MODEL_3e5c32e1901441eab45a0627c30e7bca"
            ],
            "layout": "IPY_MODEL_4202b757c305496cac860d8fda7f71bc"
          }
        },
        "c24c00186cca41f885727f0be6bd7806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc24ef422cd408abbbd28d87fc30e0e",
            "placeholder": "​",
            "style": "IPY_MODEL_df53dc8b46554622bbe90523a95bba5a",
            "value": "README.md: 100%"
          }
        },
        "8aab4586c7db407fbd268dd89133c2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5154f3cb62294f9a8d0c9592c9d68b4b",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d476df15783446d5b46795388eae1620",
            "value": 10621
          }
        },
        "3e5c32e1901441eab45a0627c30e7bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24646d0961c34537be104fe3b9b70a18",
            "placeholder": "​",
            "style": "IPY_MODEL_f28c5d46db6b4602ab94917e505eae81",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 239kB/s]"
          }
        },
        "4202b757c305496cac860d8fda7f71bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc24ef422cd408abbbd28d87fc30e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df53dc8b46554622bbe90523a95bba5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5154f3cb62294f9a8d0c9592c9d68b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d476df15783446d5b46795388eae1620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24646d0961c34537be104fe3b9b70a18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28c5d46db6b4602ab94917e505eae81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da3c923801334fa98506609efff84106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48a06348493e4fd0804430afd055acdc",
              "IPY_MODEL_840210d43ea347ccb69f3239c05dcf68",
              "IPY_MODEL_1f43faf2d868481fbc9da7d16c20772a"
            ],
            "layout": "IPY_MODEL_8d3fb82852a14ec9bc04b9eb82722da1"
          }
        },
        "48a06348493e4fd0804430afd055acdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51ec5e5e1e3c40a7beb16e57c6f81bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_30ed7beeebfb4365bcd9f3efeebe6b5b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "840210d43ea347ccb69f3239c05dcf68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ebab556dea4eb9919cbca593b39012",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65a79ebd80a340cc87ea41fa005ff799",
            "value": 53
          }
        },
        "1f43faf2d868481fbc9da7d16c20772a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a1c36225cf4cbfab35fce25636b56c",
            "placeholder": "​",
            "style": "IPY_MODEL_fdbc6992d8d743da98f8c249421926a6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 508B/s]"
          }
        },
        "8d3fb82852a14ec9bc04b9eb82722da1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ec5e5e1e3c40a7beb16e57c6f81bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ed7beeebfb4365bcd9f3efeebe6b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56ebab556dea4eb9919cbca593b39012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a79ebd80a340cc87ea41fa005ff799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10a1c36225cf4cbfab35fce25636b56c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdbc6992d8d743da98f8c249421926a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b29537128a304b11a630351f5f9ee514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16670f57b1bc4b05a445a99fd0d264bd",
              "IPY_MODEL_df53ae5397874b0c8f2695c78301b033",
              "IPY_MODEL_da9d498977f742e8a98c50c717e37cc1"
            ],
            "layout": "IPY_MODEL_eb82fd0010e946889643ab8e2cb2619a"
          }
        },
        "16670f57b1bc4b05a445a99fd0d264bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dfcaa8596ca44d0ac157ff1671b4ece",
            "placeholder": "​",
            "style": "IPY_MODEL_e5c852952c834c7b835cbf98abe32a87",
            "value": "config.json: 100%"
          }
        },
        "df53ae5397874b0c8f2695c78301b033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c82d78a6a43141da9232f021aedbe0a1",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eeb64e3f64341d0b7499f009688d100",
            "value": 571
          }
        },
        "da9d498977f742e8a98c50c717e37cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca0b6c390bf4879ba44f492cb17506e",
            "placeholder": "​",
            "style": "IPY_MODEL_0a92c678e0a243f495da29ca97f74b59",
            "value": " 571/571 [00:00&lt;00:00, 9.03kB/s]"
          }
        },
        "eb82fd0010e946889643ab8e2cb2619a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfcaa8596ca44d0ac157ff1671b4ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c852952c834c7b835cbf98abe32a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c82d78a6a43141da9232f021aedbe0a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eeb64e3f64341d0b7499f009688d100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eca0b6c390bf4879ba44f492cb17506e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a92c678e0a243f495da29ca97f74b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "103c2dc08b78468c866943f6f06d0722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f2b00b86f094ab49e78324731c40b94",
              "IPY_MODEL_bab255a370fa44be8ec40417f8122d33",
              "IPY_MODEL_cab1c8a52c894d968558e332e8918b8f"
            ],
            "layout": "IPY_MODEL_c7ba0da5e8ff4e4282f937e3f99c727c"
          }
        },
        "1f2b00b86f094ab49e78324731c40b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_092d52ebbf594dcfb16b15fbebdeb240",
            "placeholder": "​",
            "style": "IPY_MODEL_db490f4208bb4ecfb4c5ea9595dac6fa",
            "value": "model.safetensors: 100%"
          }
        },
        "bab255a370fa44be8ec40417f8122d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808945c0f329415fb3cdd96d5e5efe56",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5931beb4fd5a473d8a128ace4b87f7d9",
            "value": 437971872
          }
        },
        "cab1c8a52c894d968558e332e8918b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b76aa3dba51459da9ecf31ba0224e33",
            "placeholder": "​",
            "style": "IPY_MODEL_16c76cfc091240cc8045a786c9d78d9a",
            "value": " 438M/438M [00:01&lt;00:00, 250MB/s]"
          }
        },
        "c7ba0da5e8ff4e4282f937e3f99c727c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "092d52ebbf594dcfb16b15fbebdeb240": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db490f4208bb4ecfb4c5ea9595dac6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "808945c0f329415fb3cdd96d5e5efe56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5931beb4fd5a473d8a128ace4b87f7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b76aa3dba51459da9ecf31ba0224e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c76cfc091240cc8045a786c9d78d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e5b90b6b67045d2be02aaeb4f53263f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf62bb2c97974e46b988a7b03f59b297",
              "IPY_MODEL_0b65e6c4e4fb4023b5c00bc20d8377c7",
              "IPY_MODEL_4f1725a0abf04e499819d59df4bd7182"
            ],
            "layout": "IPY_MODEL_f0fe78705f2f4dc8a90baf1ac5363b6d"
          }
        },
        "cf62bb2c97974e46b988a7b03f59b297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c0109e77a7b4399ad10e34a046bb5bf",
            "placeholder": "​",
            "style": "IPY_MODEL_1a0cf724d3e049069e300e15e8b09241",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0b65e6c4e4fb4023b5c00bc20d8377c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b00faf51ba044b0be55d6f70964f042",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dbc1cf4b90d44ffa8c4d13129884cda",
            "value": 363
          }
        },
        "4f1725a0abf04e499819d59df4bd7182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97f5f00ef85c45e69881e61df56ce86b",
            "placeholder": "​",
            "style": "IPY_MODEL_a7a4f346396347b39c6c6b33a4c73010",
            "value": " 363/363 [00:00&lt;00:00, 26.5kB/s]"
          }
        },
        "f0fe78705f2f4dc8a90baf1ac5363b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0109e77a7b4399ad10e34a046bb5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0cf724d3e049069e300e15e8b09241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b00faf51ba044b0be55d6f70964f042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dbc1cf4b90d44ffa8c4d13129884cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97f5f00ef85c45e69881e61df56ce86b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7a4f346396347b39c6c6b33a4c73010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990bc0042c1a4f9f8e15fb98be347090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_622e3b10d8aa4759b55c7f41d49f8039",
              "IPY_MODEL_92f3cf98e9224063a154f063a1bca6c0",
              "IPY_MODEL_b9c9e625fbaa4bfeb46995557d0bf713"
            ],
            "layout": "IPY_MODEL_624ec952dabb450684d617b5502364cb"
          }
        },
        "622e3b10d8aa4759b55c7f41d49f8039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badbd1d7647f414cbd8fae47c817897d",
            "placeholder": "​",
            "style": "IPY_MODEL_74953d868bb347ca9128fad4e499e1b3",
            "value": "vocab.txt: 100%"
          }
        },
        "92f3cf98e9224063a154f063a1bca6c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bad666995c047e797607d207c9278b0",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f58c096ebdf645eaa35f0b33cccc1eed",
            "value": 231536
          }
        },
        "b9c9e625fbaa4bfeb46995557d0bf713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b058abfaade0433186ed69b1d1d82bee",
            "placeholder": "​",
            "style": "IPY_MODEL_db175c591dc34c29bace049696642393",
            "value": " 232k/232k [00:00&lt;00:00, 12.9MB/s]"
          }
        },
        "624ec952dabb450684d617b5502364cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "badbd1d7647f414cbd8fae47c817897d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74953d868bb347ca9128fad4e499e1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bad666995c047e797607d207c9278b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58c096ebdf645eaa35f0b33cccc1eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b058abfaade0433186ed69b1d1d82bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db175c591dc34c29bace049696642393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf54f0c242974975bb3ab621ed679d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db269263495d4fbfb5019f41e1e90465",
              "IPY_MODEL_ea7c600f06574155a449a92bcf098558",
              "IPY_MODEL_bcaaa94a39c1454b9ca78a0f0ba80473"
            ],
            "layout": "IPY_MODEL_9e2672f471564acd9de4c0b6b87a768f"
          }
        },
        "db269263495d4fbfb5019f41e1e90465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a708ff82f1b4ca08734bdc2eee65fc1",
            "placeholder": "​",
            "style": "IPY_MODEL_afe88b8e33704495ba4a4421a5a04206",
            "value": "tokenizer.json: 100%"
          }
        },
        "ea7c600f06574155a449a92bcf098558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8430c90db4423b8ebe564c70876e29",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_139280f7856e4ffab7720b11d7d9e62c",
            "value": 466021
          }
        },
        "bcaaa94a39c1454b9ca78a0f0ba80473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a674bec15464d419f43846c8c7dc32b",
            "placeholder": "​",
            "style": "IPY_MODEL_65d14c4843d140caac9985f819a1360e",
            "value": " 466k/466k [00:00&lt;00:00, 2.41MB/s]"
          }
        },
        "9e2672f471564acd9de4c0b6b87a768f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a708ff82f1b4ca08734bdc2eee65fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe88b8e33704495ba4a4421a5a04206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d8430c90db4423b8ebe564c70876e29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "139280f7856e4ffab7720b11d7d9e62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a674bec15464d419f43846c8c7dc32b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d14c4843d140caac9985f819a1360e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d2d3ba2174e4942909464f099b8580e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c22077e1c3a64f7f84494978e7341194",
              "IPY_MODEL_355776f78d164837bb2a3355150971c4",
              "IPY_MODEL_c87cbe993be34ced905ba44ab74eb29c"
            ],
            "layout": "IPY_MODEL_66aa2a60919e43868f6f46cf0e5c5ab2"
          }
        },
        "c22077e1c3a64f7f84494978e7341194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f244c1654b48328985451913b047f2",
            "placeholder": "​",
            "style": "IPY_MODEL_6d302273b1934a1fabfd32fdfc31bcd2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "355776f78d164837bb2a3355150971c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_842541c522e547db8caf61533f225a47",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca17ea1970574d9793109eae5fcb3ba8",
            "value": 239
          }
        },
        "c87cbe993be34ced905ba44ab74eb29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d5285f92e94ff882921adef87d6564",
            "placeholder": "​",
            "style": "IPY_MODEL_6fd79eff17534d328f66cd0b44e986cf",
            "value": " 239/239 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "66aa2a60919e43868f6f46cf0e5c5ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f244c1654b48328985451913b047f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d302273b1934a1fabfd32fdfc31bcd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "842541c522e547db8caf61533f225a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca17ea1970574d9793109eae5fcb3ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47d5285f92e94ff882921adef87d6564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd79eff17534d328f66cd0b44e986cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92d2b908b55747f4ac8871240149008e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2989a2d728c452db3640a51e8c969e6",
              "IPY_MODEL_121417b5d3bb46e49459762dac4c8bef",
              "IPY_MODEL_a8be24b38613463f977d56fa4e7ba2a6"
            ],
            "layout": "IPY_MODEL_cb04593fb9404e26ab113aafa0cbf160"
          }
        },
        "e2989a2d728c452db3640a51e8c969e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7160d0700e439cbd56e6351f4ddff5",
            "placeholder": "​",
            "style": "IPY_MODEL_492634a0e764462b9381565446643ee3",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "121417b5d3bb46e49459762dac4c8bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8a889d501147c1b704294ee9c2beb1",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2074921a754045e3972ededfa0b2f08c",
            "value": 190
          }
        },
        "a8be24b38613463f977d56fa4e7ba2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9cacbdb06c44db8cafc91ff5105283",
            "placeholder": "​",
            "style": "IPY_MODEL_545f0015725d47deb1e62db74c724976",
            "value": " 190/190 [00:00&lt;00:00, 10.6kB/s]"
          }
        },
        "cb04593fb9404e26ab113aafa0cbf160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7160d0700e439cbd56e6351f4ddff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492634a0e764462b9381565446643ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca8a889d501147c1b704294ee9c2beb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2074921a754045e3972ededfa0b2f08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db9cacbdb06c44db8cafc91ff5105283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545f0015725d47deb1e62db74c724976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#RAG System using Llama2 with Hugging Face"
      ],
      "metadata": {
        "id": "p4mJQdDruiGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we are going to use Llama index and llama 2 with RAG. We are going to read PDF then Index it and then using Llama 2 for inferencing"
      ],
      "metadata": {
        "id": "q_HYGW_Yw5eR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCiKL9G_uHdX",
        "outputId": "924f9bbb-77a8-4dd8-a69d-eb2ab61c5d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m235.5/290.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ],
      "source": [
        "#Installing the pypdf package for reading the pdf\n",
        "\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcon5XTX1EIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the transformers, einops, accelerate, langchain, bitsbytes for the quantization since the model is on 16bit and we want to load it on 4 bit and work on it.\n",
        "\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDwwDGlFw4Un",
        "outputId": "d5906210-cb57-4f84-c412-b24f7346e1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Embedding\n",
        "# For Embedding we are going to use the sentence transformers\n",
        "\n",
        "\"\"\"Embeddings are numerical representations of real-world objects that machine learning (ML) and artificial intelligence (AI) systems\n",
        " use to understand complex knowledge domains like humans do. As an example, computing algorithms understand that the difference between 2 and 3 is 1,\n",
        "indicating a close relationship between 2 and 3 as compared to 2 and 100. However, real-world data includes more complex relationships.\n",
        "\n",
        "Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art text and image embedding models.\n",
        "It can be used to compute embeddings using Sentence Transformer models or to calculate similarity scores using Cross-Encoder models\n",
        " This unlocks a wide range of applications, including semantic search, semantic textual similarity, and paraphrase mining.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "!pip install install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TkGbo801E54",
        "outputId": "f80e8a35-cbf6-4fef-e604-1e9e24d7328e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting install\n",
            "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m204.8/227.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: install, sentence_transformers\n",
            "Successfully installed install-1.3.5 sentence_transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing Llama Index\n",
        "\"\"\"\n",
        "LlamaIndex is the leading data framework for building LLM applications\n",
        "\n",
        "LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data,\n",
        "but they are not trained on your data. Your data may be private or specific to the problem you're trying to solve.\n",
        "It's behind APIs, in SQL databases, or trapped in PDFs and slide decks.\n",
        "\n",
        "Context augmentation makes your data available to the LLM to solve the problem at hand.\n",
        "LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production.\n",
        "Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF_BTnl62cJB",
        "outputId": "c8c7c552-4559-49d1-977b-cbf4a98b7c44",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.10.54.post1-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.2.8-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core==0.10.54 (from llama_index)\n",
            "  Downloading llama_index_core-0.10.54-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.4-py3-none-any.whl (9.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.1.30-py3-none-any.whl (38 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (8.4.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.54->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.54->llama_index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.2.0)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama_index)\n",
            "  Downloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.54->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.54->llama_index) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.54->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.54->llama_index) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.54->llama_index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.54->llama_index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.54->llama_index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.54->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.54->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.54->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.54->llama_index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.54->llama_index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.54->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.54->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.54->llama_index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.54->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.54->llama_index)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.54->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.54->llama_index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.54->llama_index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.54->llama_index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.54->llama_index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.54->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.54->llama_index) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.54->llama_index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-cloud-0.0.6 llama-index-agent-openai-0.2.8 llama-index-cli-0.1.12 llama-index-core-0.10.54 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.4 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.30 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.6 llama_index-0.10.54.post1 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.35.13 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "# from llama_index.llms import HuggingFaceLLM\n",
        "# from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "VFA-38Kk2wRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGCmR3Yx3hmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGIzSJYd4cVm",
        "outputId": "b5640786-3705-48aa-ad0f-f5b98a3dcc6c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-huggingface\n",
            "  Downloading llama_index_llms_huggingface-0.2.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.4)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.41 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.54)\n",
            "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
            "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.35.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.32.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (3.21.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-huggingface) (1.16.0)\n",
            "Installing collected packages: text-generation, llama-index-llms-huggingface\n",
            "Successfully installed llama-index-llms-huggingface-0.2.4 text-generation-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n"
      ],
      "metadata": {
        "id": "CnLrm3q737j2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ed9485-2fae-4223-91bc-d85c739699b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents=SimpleDirectoryReader(\"/content/pdf\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O7eXgjgA4K5N",
        "outputId": "83825abd-8ce4-46fd-aac0-9781a316e6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 30 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 34 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 42 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 44 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 46 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 48 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 65 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 67 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 126 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 128 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 131 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 133 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 196 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 219 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 237 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 243 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 258 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 492 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 494 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 865 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1059 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1061 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1298 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1300 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1599 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='9c1c9644-a48f-4c5e-a304-1ee2b92a27e8', embedding=None, metadata={'page_label': '1', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 1            Approaching (Almost) Any Machine Learning Problem   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59579aac-7f29-4436-b99c-84ee4bf3d547', embedding=None, metadata={'page_label': '2', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 2       It would not have been possible for me to write this book without the support of my family and friends. I would also like to thank the reviewers who selflessly devoted their time in reviewing this book (names in alphabetical order).   Aakash Nain Aditya Soni Andreas Müller Andrey Lukyanenko Ayon Roy Bojan Tunguz Gilberto Titericz Jr. Konrad Banachewicz Luca Massaron Nabajeet Barman Parul Pandey Ram Ramrakhya Sanyam Bhutani Sudalai Rajkumar Tanishq Abraham Walter Reade Yuval Reina   I hope I did not miss anyone.        ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='acb1a850-a3c6-4d56-8e97-e7c5140af04b', embedding=None, metadata={'page_label': '3', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 3 Before you start, there are a few things that you must be aware of while going through this book.   This is not a traditional book.  The book expects you to have basic knowledge of machine learning and deep learning.   Important terms are bold.   Variable names and function/class names are italic.   ═════════════════════════════════════════════════════════════════════════ All the code is between these two lines ═════════════════════════════════════════════════════════════════════════  Most of the times, the output is provided right after the code blocks.  Figures are locally defined. For example, figure 1 is the first figure   Code is very important in this book and there is a lot of it. You must go through the code carefully and implement it on your own if you want to understand what’s going on.  Comments in Python begin with a hash (#). All the code in this book is explained line-by-line only using comments. Thus, these comments must not be ignored.  Bash commands start with $ or ❯.  If you find a pirated copy of this book (print or e-book or pdf), contact me directly with the details so that I can take necessary actions.       If you didn’t code, you didn’t learn.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d34f02b-8a1a-452d-8616-636b565efa87', embedding=None, metadata={'page_label': '4', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 4  Table of Contents Setting up your working environment ..................................................... 5 Supervised vs unsupervised learning ...................................................... 7 Cross-validation ................................................................................... 14 Evaluation metrics ............................................................................... 30 Arranging machine learning projects ................................................... 73 Approaching categorical variables ....................................................... 85 Feature engineering ........................................................................... 142 Feature selection ................................................................................ 155 Hyperparameter optimization ............................................................. 167 Approaching image classification & segmentation ............................. 185 Approaching text classification/regression ......................................... 225 Approaching ensembling and stacking ............................................... 272 Approaching reproducible code & model serving ................................ 283     ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0fe02ca-8136-403b-82d3-61a3bd767dd0', embedding=None, metadata={'page_label': '5', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 5 Setting up your working environment  Before we begin with coding, it’s essential to get everything set-up on your machine. Throughout this book, we will be using Ubuntu 18.04 and Python 3.7.6. If you are a Windows user, you can install Ubuntu in multiple ways. On a virtual machine, for example, Virtual Box which is provided by Oracle and is free software. Alongside Windows as a dual boot system. I prefer dual boot as it is native. If you are not an Ubuntu user, you might face problems with some of the bash scripts in this book. To circumvent that you can install Ubuntu in a VM or go for Linux shell on Windows.  Setting up Python on any machine is quite easy with Anaconda. I particularly like Miniconda, which is a minimal installer for conda. It is available for Linux, OSX and Windows. Since Python 2 support ended at the end of 2019, we will be using the Python 3 distribution. You should keep in mind that miniconda does not come with all the packages as regular Anaconda. We will, thus, be installing packages as we go. Installing miniconda is quite easy.  The first thing that you need to do is download Miniconda3 to your system.  $ cd ~/Downloads $ wget https://repo.anaconda.com/miniconda/...  where the URL after wget command is the URL from miniconda3 webpage. For 64-bit Linux systems, the URL at the time of writing this book was:  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  Once you have downloaded miniconda3, you can run the following command:  $ sh Miniconda3-latest-Linux-x86_64.sh  Next, please read and follow the instructions on your screen. If you installed everything correctly, you should be able to start the conda environment by typing conda init the terminal. We will create a conda environment that we will be using throughout this book. To create a conda environment, you can type:  $ conda create -n environment_name python=3.7.6 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16729557-f255-48de-876d-a7a79cbf90ce', embedding=None, metadata={'page_label': '6', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 6 This command will create a conda environment named environment_name which can be activated using:   $ conda activate environment_name  And we are all set-up with the environment. Now it’s time to install some packages that we would be using. A package can be installed in two different ways when you are in a conda environment. You can either install the package from conda repository or the official PyPi repository.  $ conda/pip install package_name  Note: It might be possible that some packages are not available in the conda repo. Thus, installing using pip would be the most preferred way in this book. I have already created a list of packages used while writing this book which is saved in the environment.yml. You can find it in extra material available in my GitHub repository. You can create the environment using the following command:  $ conda env create -f environment.yml  This command will create an environment called ml. To activate this environment and start using it, you should run:  $ conda activate ml  And we are all set and ready to do some applied machine learning!   Always remember to be in the “ml” environment when coding along with this book.   Let’s start with our real first chapter now.     ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='35f6406b-8675-468f-b56e-b786e2e506fc', embedding=None, metadata={'page_label': '7', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 7 Supervised vs unsupervised learning  When dealing with machine learning problems, there are generally two types of data (and machine learning models): • Supervised data: always has one or multiple targets associated with it. • Unsupervised data: does not have any target variable.  A supervised problem is considerably easier to tackle than an unsupervised one. A problem in which we are required to predict a value is known as a supervised problem. For example, if the problem is to predict house prices given historical house prices, with features like presence of a hospital, school or supermarket, distance to nearest public transport, etc. is a supervised problem. Similarly, when we are provided with images of cats and dogs, and we know beforehand which ones are cats and which ones are dogs, and if the task is to create a model which predicts whether a provided image is of a cat or a dog, the problem is considered to be supervised.  \\n Figure 1: A supervised dataset.  As we see in figure 1, every row of the data is associated with a target or label. The columns are different features and rows represent different data points which are usually called samples. The example shows ten samples with ten features and a target variable which can be either a number or a category. If the target is categorical, the problem becomes a classification problem. And if the target is a real \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ed1970b6-e6c3-4298-9a60-8a4e08414153', embedding=None, metadata={'page_label': '8', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 8 number, the problem is defined as a regression problem. Thus, supervised problems can be divided into two sub-classes:  • Classification: predicting a category, e.g. dog or cat. • Regression: predicting a value, e.g. house prices.  It must be noted that sometimes we might use regression in a classification setting depending on the metric used for evaluation. But we will come to that later.  Another type of machine learning problem is the unsupervised type. Unsupervised datasets do not have a target associated with them and in general, are more challenging to deal with when compared to supervised problems.  Let’s say you work in a financial firm which deals with credit card transactions. There is a lot of data that comes in every second. The only problem is that it is difficult to find humans who will mark each and every transaction either as a valid or genuine transaction or a fraud. When we do not have any information about a transaction being fraud or genuine, the problem becomes an unsupervised problem. To tackle these kinds of problems we have to think about how many clusters can data be divided into. Clustering is one of the approaches that you can use for problems like this, but it must be noted that there are several other approaches available that can be applied to unsupervised problems. For a fraud detection problem, we can say that data can be divided into two classes (fraud or genuine).  When we know the number of clusters, we can use a clustering algorithm for unsupervised problems. In figure 2, the data is assumed to have two classes, dark colour represents fraud, and light colour represents genuine transactions. These classes, however, are not known to us before the clustering approach. After a clustering algorithm is applied, we should be able to distinguish between the two assumed targets. To make sense of unsupervised problems, we can also use numerous decomposition techniques such as Principal Component Analysis (PCA), t-distributed Stochastic Neighbour Embedding (t-SNE) etc.   Supervised problems are easier to tackle in the sense that they can be evaluated easily. We will read more about evaluation techniques in the following chapters. However, it is challenging to assess the results of unsupervised algorithms and a lot of human interference or heuristics are required. In this book, we will majorly be focusing on supervised data and models, but it does not mean that we will be ignoring the unsupervised data problems. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09433cd9-77a2-41b7-8dab-aef7e8c452d0', embedding=None, metadata={'page_label': '9', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 9  \\n Figure 2: An unsupervised dataset.  Most of the time, when people start with data science or machine learning, they begin with very well-known datasets, for example, Titanic dataset, or Iris dataset which are supervised problems. In the Titanic dataset, you have to predict the survival of people aboard Titanic based on factors like their ticket class, gender, age, etc. Similarly, in the iris dataset, you have to predict the species of flower based on factors like sepal width, petal length, sepal length and petal width.   Unsupervised datasets may include datasets for customer segmentation. For example, you have data for the customers visiting your e-commerce website or the data for customers visiting a store or a mall, and you would like to segment them or cluster them in different categories. Another example of unsupervised datasets may include things like credit card fraud detection or just clustering several images.  Most of the time, it’s also possible to convert a supervised dataset to unsupervised to see how they look like when plotted.  For example, let’s take a look at the dataset in figure 3. Figure 3 shows MNIST dataset which is a very popular dataset of handwritten digits, and it is a supervised problem in which you are given the images of the numbers and the correct label associated with them. You have to build a model that can identify which digit is it when provided only with the image.   This dataset can easily be converted to an unsupervised setting for basic visualization.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5248aca-89e9-4e41-b1ff-5c0c96112eca', embedding=None, metadata={'page_label': '10', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 10  Figure 3: MNIST dataset1  If we do a t-Distributed Stochastic Neighbour Embedding (t-SNE) decomposition of this dataset, we can see that we can separate the images to some extent just by doing with two components on the image pixels. This is shown in figure 4. \\n Figure 4: t-SNE visualization of the MNIST dataset. 3000 images were used.  Let’s take a look at how this was done. First and foremost is importing all the required libraries.   1 Image source: By Josef Steppan - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=64810040 \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bfcff54-3690-4352-b3ad-c586abce2803', embedding=None, metadata={'page_label': '11', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 11 ═════════════════════════════════════════════════════════════════════════ import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns  from sklearn import datasets from sklearn import manifold  %matplotlib inline ═════════════════════════════════════════════════════════════════════════  We use matplotlib and seaborn for plotting, numpy to handle the numerical arrays, pandas to create dataframes from the numerical arrays and scikit-learn (sklearn) to get the data and perform t-SNE.  After the imports, we need to either download the data and read it separately or use sklearn’s built-in function that provides us with the MNIST dataset.  ═════════════════════════════════════════════════════════════════════════ data = datasets.fetch_openml(                   'mnist_784',                    version=1,                    return_X_y=True ) pixel_values, targets = data targets = targets.astype(int) ═════════════════════════════════════════════════════════════════════════  In this part of the code, we have fetched the data using sklearn datasets, and we have an array of pixel values and another array of targets. Since the targets are of string type, we convert them to integers.  pixel_values is a 2-dimensional array of shape 70000x784. There are 70000 different images, each of size 28x28 pixels. Flattening 28x28 gives 784 data points.  We can visualize the samples in this dataset by reshaping them to their original shape and then plotting them using matplotlib.  ═════════════════════════════════════════════════════════════════════════ single_image = pixel_values[1, :].reshape(28, 28)  plt.imshow(single_image, cmap='gray') ═════════════════════════════════════════════════════════════════════════ \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='92e1acfe-6184-4481-87db-72340b56ab25', embedding=None, metadata={'page_label': '12', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 12 This code will plot an image like the following:  \\n Figure 5: Plotting a single image from MNIST dataset.  The most important step comes after we have grabbed the data.  ═════════════════════════════════════════════════════════════════════════ tsne = manifold.TSNE(n_components=2, random_state=42)  transformed_data = tsne.fit_transform(pixel_values[:3000, :]) ═════════════════════════════════════════════════════════════════════════  This step creates the t-SNE transformation of the data. We use only two components as we can visualize them well in a two-dimensional setting. The transformed_data, in this case, is an array of shape 3000x2 (3000 rows and 2 columns). A data like this can be converted to a pandas dataframe by calling pd.DataFrame on the array.  ═════════════════════════════════════════════════════════════════════════ tsne_df = pd.DataFrame(     np.column_stack((transformed_data, targets[:3000])),      columns=[\"x\", \"y\", \"targets\"] )  tsne_df.loc[:, \"targets\"] = tsne_df.targets.astype(int) ═════════════════════════════════════════════════════════════════════════ Here we are creating a pandas dataframe from a numpy array. There are three columns: x, y and targets. x and y are the two components from t-SNE decomposition and targets is the actual number. This gives us a dataframe which looks like the one shown in figure 6. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f3edcec-ac9e-4c84-9a1a-34a9e765e01a', embedding=None, metadata={'page_label': '13', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 13  Figure 6: First 10 rows of pandas dataframe with t-SNE components and targets.  And finally, we can plot it using seaborn and matplotlib.  ═════════════════════════════════════════════════════════════════════════ grid = sns.FacetGrid(tsne_df, hue=\"targets\", size=8)  grid.map(plt.scatter, \"x\", \"y\").add_legend() ═════════════════════════════════════════════════════════════════════════  This is one way of visualizing unsupervised datasets. We can also do k-means clustering on the same dataset and see how it performs in an unsupervised setting. One question that arises all the time is how to find the optimal number of clusters in k-means clustering. Well, there is no right answer. You have to find the number by cross-validation. Cross-validation will be discussed later in this book. Please note that the above code was run in a jupyter notebook.   In this book, we will use jupyter for simple things like the example above and for plotting. For most of the stuff in this book, we will be using python scripts. You can choose what you want to use since the results are going to be the same.   MNIST is a supervised classification problem, and we converted it to an unsupervised problem only to check if it gives any kind of good results and it is apparent that we do get good results with decomposition with t-SNE. The results would be even better if we use classification algorithms. What are they and how to use them? Let’s look at them in the next chapters.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b30ab971-44e6-41db-8a5c-201317f645e9', embedding=None, metadata={'page_label': '14', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 14 Cross-validation  We did not build any models in the previous chapter. The reason for that is simple. Before creating any kind of machine learning model, we must know what cross-validation is and how to choose the best cross-validation depending on your datasets.  So, what is cross-validation, and why should we care about it?  We can find multiple definitions as to what cross-validation is. Mine is a one-liner: cross-validation is a step in the process of building a machine learning model which helps us ensure that our models fit the data accurately and also ensures that we do not overfit. But this leads to another term: overfitting.   To explain overfitting, I think it’s best if we look at a dataset. There is a red wine-quality dataset2 which is quite famous. This dataset has 11 different attributes that decide the quality of red wine.   These attributes include: • fixed acidity • volatile acidity • citric acid • residual sugar • chlorides • free sulfur dioxide • total sulfur dioxide • density • pH • sulphates • alcohol  Based on these different attributes, we are required to predict the quality of red wine which is a value between 0 and 10.   2 P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis; Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10f1c73f-bd07-42a0-93a0-f0d2a78714d7', embedding=None, metadata={'page_label': '15', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 15 Let’s see how this data looks like.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd df = pd.read_csv(\"winequality-red.csv\") ═════════════════════════════════════════════════════════════════════════  This dataset looks something like this:  \\n Figure 1: A snapshot of the red wine quality dataset.  We can treat this problem either as a classification problem or as a regression problem since wine quality is nothing but a real number between 0 and 10. For simplicity, let’s choose classification. This dataset, however, consists of only six types of quality values. We will thus map all quality values from 0 to 5.  ═════════════════════════════════════════════════════════════════════════ # a mapping dictionary that maps the quality values from 0 to 5 quality_mapping = {     3: 0,     4: 1,     5: 2,     6: 3,     7: 4,     8: 5 }  # you can use the map function of pandas with # any dictionary to convert the values in a given # column to values in the dictionary df.loc[:, \"quality\"] = df.quality.map(quality_mapping) ═════════════════════════════════════════════════════════════════════════ \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='208f30bf-d774-41e5-8198-089b817c5f48', embedding=None, metadata={'page_label': '16', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 16 When we look at this data and consider it a classification problem, a lot of algorithms come to our mind that we can apply to it, probably, we can use neural networks. But it would be a bit of a stretch if we dive into neural networks from the beginning. So, let’s start with something simple that we can visualize too: decision trees.  Before we begin to understand what overfitting is, let’s divide the data into two parts. This dataset has 1599 samples. We keep 1000 samples for training and 599 as a separate set.  Splitting can be done easily by the following chunk of code:  ═════════════════════════════════════════════════════════════════════════ # use sample with frac=1 to shuffle the dataframe # we reset the indices since they change after # shuffling the dataframe df = df.sample(frac=1).reset_index(drop=True)  # top 1000 rows are selected # for training df_train = df.head(1000)  # bottom 599 values are selected # for testing/validation df_test = df.tail(599) ═════════════════════════════════════════════════════════════════════════  We will now train a decision tree model on the training set. For the decision tree model, I am going to use scikit-learn.  ═════════════════════════════════════════════════════════════════════════ # import from scikit-learn from sklearn import tree from sklearn import metrics  # initialize decision tree classifier class # with a max_depth of 3 clf = tree.DecisionTreeClassifier(max_depth=3)  # choose the columns you want to train on # these are the features for the model cols = ['fixed acidity',          'volatile acidity',          'citric acid', \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d151c2b-39d9-4a27-872e-5dbee794ef32', embedding=None, metadata={'page_label': '17', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 17         'residual sugar',         'chlorides',         'free sulfur dioxide',         'total sulfur dioxide',         'density',         'pH',         'sulphates',         'alcohol']  # train the model on the provided features # and mapped quality from before clf.fit(df_train[cols], df_train.quality) ═════════════════════════════════════════════════════════════════════════  Note that I have used a max_depth of 3 for the decision tree classifier. I have left all other parameters of this model to its default value.  Now, we test the accuracy of this model on the training set and the test set:  ═════════════════════════════════════════════════════════════════════════ # generate predictions on the training set train_predictions = clf.predict(df_train[cols])  # generate predictions on the test set test_predictions = clf.predict(df_test[cols])  # calculate the accuracy of predictions on # training data set train_accuracy = metrics.accuracy_score(     df_train.quality, train_predictions )  # calculate the accuracy of predictions on # test data set test_accuracy = metrics.accuracy_score(     df_test.quality, test_predictions ) ═════════════════════════════════════════════════════════════════════════  The training and test accuracies are found to be 58.9% and 54.25%. Now we increase the max_depth to 7 and repeat the process. This gives training accuracy of 76.6% and test accuracy of 57.3%. Here, we have used accuracy, mainly because it is the most straightforward metric. It might not be the best metric for this problem. What about we calculate these accuracies for different values of max_depth and make a plot? \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a54871f-e6e2-40b8-88d4-32f8ee6bc16d', embedding=None, metadata={'page_label': '18', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 18 ═════════════════════════════════════════════════════════════════════════ # NOTE: this code is written in a jupyter notebook  # import scikit-learn tree and metrics from sklearn import tree from sklearn import metrics  # import matplotlib and seaborn # for plotting import matplotlib import matplotlib.pyplot as plt import seaborn as sns  # this is our global size of label text # on the plots matplotlib.rc('xtick', labelsize=20)  matplotlib.rc('ytick', labelsize=20)   # This line ensures that the plot is displayed # inside the notebook %matplotlib inline   # initialize lists to store accuracies # for training and test data # we start with 50% accuracy train_accuracies = [0.5] test_accuracies = [0.5]  # iterate over a few depth values for depth in range(1, 25):     # init the model     clf = tree.DecisionTreeClassifier(max_depth=depth)      # columns/features for training     # note that, this can be done outside      # the loop     cols = [         'fixed acidity',          'volatile acidity',         'citric acid',          'residual sugar',         'chlorides',         'free sulfur dioxide',          'total sulfur dioxide',         'density',         'pH',          'sulphates', \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4461c974-e878-4bfb-9d1b-f8d2a4b42016', embedding=None, metadata={'page_label': '19', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 19         \\'alcohol\\'         ]      # fit the model on given features     clf.fit(df_train[cols], df_train.quality)      # create training & test predictions     train_predictions = clf.predict(df_train[cols])     test_predictions = clf.predict(df_test[cols])      # calculate training & test accuracies     train_accuracy = metrics.accuracy_score(         df_train.quality, train_predictions     )     test_accuracy = metrics.accuracy_score(         df_test.quality, test_predictions     )          # append accuracies     train_accuracies.append(train_accuracy)     test_accuracies.append(test_accuracy)    # create two plots using matplotlib # and seaborn plt.figure(figsize=(10, 5)) sns.set_style(\"whitegrid\") plt.plot(train_accuracies, label=\"train accuracy\") plt.plot(test_accuracies, label=\"test accuracy\") plt.legend(loc=\"upper left\", prop={\\'size\\': 15}) plt.xticks(range(0, 26, 5)) plt.xlabel(\"max_depth\", size=20) plt.ylabel(\"accuracy\", size=20) plt.show() ═════════════════════════════════════════════════════════════════════════  This generates a plot, as shown in figure 2.   We see that the best score for test data is obtained when max_depth has a value of 14. As we keep increasing the value of this parameter, test accuracy remains the same or gets worse, but the training accuracy keeps increasing. It means that our simple decision tree model keeps learning about the training data better and better with an increase in max_depth, but the performance on test data does not improve at all.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e85b788e-a440-4945-b9a4-dfbb4848f5e9', embedding=None, metadata={'page_label': '20', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 20 This is called overfitting.   The model fits perfectly on the training set and performs poorly when it comes to the test set. This means that the model will learn the training data well but will not generalize on unseen samples. In the dataset above, one can build a model with very high max_depth which will have outstanding results on training data, but that kind of model is not useful as it will not provide a similar result on the real-world samples or live data. \\n Figure 2: Training and test accuracies for different values of max_depth.  One might argue that this approach isn’t overfitting as the accuracy of the test set more or less remains the same. Another definition of overfitting would be when the test loss increases as we keep improving training loss. This is very common when it comes to neural networks.   Whenever we train a neural network, we must monitor loss during the training time for both training and test set. If we have a very large network for a dataset which is quite small (i.e. very less number of samples), we will observe that the loss for both training and test set will decrease as we keep training. However, at some point, test loss will reach its minima, and after that, it will start increasing even though training loss decreases further. We must stop training where the validation loss reaches its minimum value.   This is the most common explanation of overfitting. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdfc83a1-680d-4411-becc-8afdd601f5e6', embedding=None, metadata={'page_label': '21', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 21 Occam’s razor in simple words states that one should not try to complicate things that can be solved in a much simpler manner. In other words, the simplest solutions are the most generalizable solutions. In general, whenever your model does not obey Occam’s razor, it is probably overfitting.  \\n Figure 3: Most general definition of overfitting.  Now we can go back to cross-validation.  While explaining about overfitting, I decided to divide the data into two parts. I trained the model on one part and checked its performance on the other part. Well, this is also a kind of cross-validation commonly known as a hold-out set. We use this kind of (cross-) validation when we have a large amount of data and model inference is a time-consuming process.  There are many different ways one can do cross-validation, and it is the most critical step when it comes to building a good machine learning model which is generalizable when it comes to unseen data. Choosing the right cross-validation depends on the dataset you are dealing with, and one’s choice of cross-validation on one dataset may or may not apply to other datasets. However, there are a few types of cross-validation techniques which are the most popular and widely used.   These include:  • k-fold cross-validation • stratified k-fold cross-validation \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6a8cf51-f6c6-47ac-98ba-23159b145825', embedding=None, metadata={'page_label': '22', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 22 • hold-out based validation • leave-one-out cross-validation • group k-fold cross-validation  Cross-validation is dividing training data into a few parts. We train the model on some of these parts and test on the remaining parts. Take a look at figure 4.  \\n Figure 4: Splitting a dataset into training and validation sets  Figure 4 & 5 say that when you get a dataset to build machine learning models, you separate them into two different sets: training and validation. Many people also split it into a third set and call it a test set. We will, however, be using only two sets. As you can see, we divide the samples and the targets associated with them. We can divide the data into k different sets which are exclusive of each other. This is known as k-fold cross-validation.   Figure 5: K-fold cross-validation  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb17f7d7-b636-43ee-b3a2-bbf1e95dab25', embedding=None, metadata={'page_label': '23', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 23 We can split any data into k-equal parts using KFold from scikit-learn. Each sample is assigned a value from 0 to k-1 when using k-fold cross validation.   ═════════════════════════════════════════════════════════════════════════ # import pandas and model_selection module of scikit-learn import pandas as pd from sklearn import model_selection   if __name__ == \"__main__\":     # Training data is in a CSV file called train.csv     df = pd.read_csv(\"train.csv\")          # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1      # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)      # initiate the kfold class from model_selection module     kf = model_selection.KFold(n_splits=5)      # fill the new kfold column     for fold, (trn_, val_) in enumerate(kf.split(X=df)):         df.loc[val_, \\'kfold\\'] = fold      # save the new csv with kfold column      df.to_csv(\"train_folds.csv\", index=False) ═════════════════════════════════════════════════════════════════════════  You can use this process with almost all kinds of datasets. For example, when you have images, you can create a CSV with image id, image location and image label and use the process above.  The next important type of cross-validation is stratified k-fold. If you have a skewed dataset for binary classification with 90% positive samples and only 10% negative samples, you don\\'t want to use random k-fold cross-validation. Using simple k-fold cross-validation for a dataset like this can result in folds with all negative samples. In these cases, we prefer using stratified k-fold cross-validation. Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So, in each fold, you will have the same 90% positive and 10% negative samples. Thus, whatever metric you choose to evaluate, it will give similar results across all folds.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea9fc482-3387-4d42-b8ac-ee7c1286fce4', embedding=None, metadata={'page_label': '24', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 24 It’s easy to modify the code for creating k-fold cross-validation to create stratified k-folds. We are only changing from model_selection.KFold to model_selection.StratifiedKFold and in the kf.split(...) function, we specify the target column on which we want to stratify. We assume that our CSV dataset has a column called “target” and it is a classification problem!  ═════════════════════════════════════════════════════════════════════════ # import pandas and model_selection module of scikit-learn import pandas as pd from sklearn import model_selection  if __name__ == \"__main__\":     # Training data is in a csv file called train.csv     df = pd.read_csv(\"train.csv\")      # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1      # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)      # fetch targets     y = df.target.values      # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)      # fill the new kfold column     for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):         df.loc[v_, \\'kfold\\'] = f      # save the new csv with kfold column     df.to_csv(\"train_folds.csv\", index=False) ═════════════════════════════════════════════════════════════════════════  For the wine dataset, let’s look at the distribution of labels.  ═════════════════════════════════════════════════════════════════════════ b = sns.countplot(x=\\'quality\\', data=df) b.set_xlabel(\"quality\", fontsize=20) b.set_ylabel(\"count\", fontsize=20) ═════════════════════════════════════════════════════════════════════════  Note that we continue on the code above. So, we have converted the target values. Looking at figure 6 we can say that the quality is very much skewed. Some classes ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b1ed51b-9517-44d4-975a-761ca03bb0bc', embedding=None, metadata={'page_label': '25', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 25 have a lot of samples, and some don’t have that many. If we do a simple k-fold, we won’t have an equal distribution of targets in every fold. Thus, we choose stratified k-fold in this case. \\n Figure 6: Distribution of “quality” in wine dataset  The rule is simple. If it’s a standard classification problem, choose stratified k-fold blindly.  But what should we do if we have a large amount of data? Suppose we have 1 million samples. A 5 fold cross-validation would mean training on 800k samples and validating on 200k. Depending on which algorithm we choose, training and even validation can be very expensive for a dataset which is of this size. In these cases, we can opt for a hold-out based validation.  The process for creating the hold-out remains the same as stratified k-fold. For a dataset which has 1 million samples, we can create ten folds instead of 5 and keep one of those folds as hold-out. This means we will have 100k samples in the hold-out, and we will always calculate loss, accuracy and other metrics on this set and train on 900k samples.  Hold-out is also used very frequently with time-series data. Let’s assume the problem we are provided with is predicting sales of a store for 2020, and you are provided all the data from 2015-2019. In this case, you can select all the data for 2019 as a hold-out and train your model on all the data from 2015 to 2018.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac27adef-b039-49f7-a1aa-e6bfb35d0ba0', embedding=None, metadata={'page_label': '26', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 26  Figure 7: Example of a time-series data  In the example presented in figure 7, let’s say our job is to predict the sales from time step 31 to 40. We can then keep 21 to 30 as hold-out and train our model from step 0 to step 20. You should note that when you are predicting from 31 to 40, you should include the data from 21 to 30 in your model; otherwise, performance will be sub-par.  In many cases, we have to deal with small datasets and creating big validation sets means losing a lot of data for the model to learn. In those cases, we can opt for a type of k-fold cross-validation where k=N, where N is the number of samples in the dataset. This means that in all folds of training, we will be training on all data samples except 1. The number of folds for this type of cross-validation is the same as the number of samples that we have in the dataset.   One should note that this type of cross-validation can be costly in terms of the time it takes if the model is not fast enough, but since it’s only preferable to use this cross-validation for small datasets, it doesn’t matter much.  Now we can move to regression. The good thing about regression problems is that we can use all the cross-validation techniques mentioned above for regression problems except for stratified k-fold. That is we cannot use stratified k-fold directly, but there are ways to change the problem a bit so that we can use stratified k-fold for regression problems. Mostly, simple k-fold cross-validation works for any regression problem. However, if you see that the distribution of targets is not consistent, you can use stratified k-fold.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbf3a9ee-c7ca-4ef8-b764-c1d6aa6953ff', embedding=None, metadata={'page_label': '27', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 27 To use stratified k-fold for a regression problem, we have first to divide the target into bins, and then we can use stratified k-fold in the same way as for classification problems. There are several choices for selecting the appropriate number of bins. If you have a lot of samples( > 10k, > 100k), then you don’t need to care about the number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate number of bins.  Sturge’s rule: Number of Bins = 1 + log2(N)  Where N is the number of samples you have in your dataset. This function is plotted in Figure 8. \\n Figure 8: Plotting samples vs the number of bins by Sturge’s Rule  Let’s make a sample regression dataset and try to apply stratified k-fold as shown in the following python snippet.  ═════════════════════════════════════════════════════════════════════════ # stratified-kfold for regression import numpy as np import pandas as pd  from sklearn import datasets from sklearn import model_selection \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f27ef11-2d94-4aea-ba38-d823b83ad692', embedding=None, metadata={'page_label': '28', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 28 def create_folds(data):     # we create a new column called kfold and fill it with -1     data[\"kfold\"] = -1          # the next step is to randomize the rows of the data     data = data.sample(frac=1).reset_index(drop=True)      # calculate the number of bins by Sturge\\'s rule     # I take the floor of the value, you can also     # just round it     num_bins = int(np.floor(1 + np.log2(len(data))))      # bin targets     data.loc[:, \"bins\"] = pd.cut(         data[\"target\"], bins=num_bins, labels=False     )          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     # note that, instead of targets, we use bins!     for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):         data.loc[v_, \\'kfold\\'] = f          # drop the bins column     data = data.drop(\"bins\", axis=1)     # return dataframe with folds     return data  if __name__ == \"__main__\":     # we create a sample dataset with 15000 samples      # and 100 features and 1 target     X, y = datasets.make_regression(         n_samples=15000, n_features=100, n_targets=1     )      # create a dataframe out of our numpy arrays     df = pd.DataFrame(         X,         columns=[f\"f_{i}\" for i in range(X.shape[1])]     )     df.loc[:, \"target\"] = y      # create folds     df = create_folds(df) ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e12d63a-aca1-47aa-83e6-0f14631f2d4f', embedding=None, metadata={'page_label': '29', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 29 Cross-validation is the first and most essential step when it comes to building machine learning models. If you want to do feature engineering, split your data first. If you're going to build models, split your data first. If you have a good cross-validation scheme in which validation data is representative of training and real-world data, you will be able to build a good machine learning model which is highly generalizable.   The types of cross-validation presented in this chapter can be applied to almost any machine learning problem. Still, you must keep in mind that cross-validation also depends a lot on the data and you might need to adopt new forms of cross-validation depending on your problem and data.   For example, let’s say we have a problem in which we would like to build a model to detect skin cancer from skin images of patients. Our task is to build a binary classifier which takes an input image and predicts the probability for it being benign or malignant.   In these kinds of datasets, you might have multiple images for the same patient in the training dataset. So, to build a good cross-validation system here, you must have stratified k-folds, but you must also make sure that patients in training data do not appear in validation data. Fortunately, scikit-learn offers a type of cross-validation known as GroupKFold. Here the patients can be considered as groups. But unfortunately, there is no way to combine GroupKFold with StratifiedKFold in scikit-learn. So you need to do that yourself. I’ll leave it as an exercise for the reader.  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2aea2a8-f6fe-47cb-a3db-58e457263865', embedding=None, metadata={'page_label': '30', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 30 Evaluation metrics  When it comes to machine learning problems, you will encounter a lot of different types of metrics in the real world. Sometimes, people even end up creating metrics that suit the business problem. It’s out of the scope of this book to introduce and explain each and every type of metric. Instead, we will see some of the most common metrics that you can use when starting with your very first few projects.  At the start of the book, we introduced supervised and unsupervised learning. Although there are some kinds of metrics that you can use for unsupervised learning, we will only focus on supervised. The reason for this is because supervised problems are in abundance compared to un-supervised, and evaluation of unsupervised methods is quite subjective.  If we talk about classification problems, the most common metrics used are: - Accuracy - Precision (P) - Recall (R) - F1 score (F1) - Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC) - Log loss - Precision at k (P@k) - Average precision at k (AP@k) - Mean average precision at k (MAP@k)  When it comes to regression, the most commonly used evaluation metrics are: - Mean absolute error (MAE) - Mean squared error (MSE) - Root mean squared error (RMSE) - Root mean squared logarithmic error (RMSLE) - Mean percentage error (MPE) - Mean absolute percentage error (MAPE) - R2  Knowing about how the aforementioned metrics work is not the only thing we have to understand. We must also know when to use which metrics, and that depends on ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2306fadd-b88a-4351-b0ef-45fc600cd544', embedding=None, metadata={'page_label': '31', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 31 what kind of data and targets you have. I think it’s more about the targets and less about the data.   To learn more about these metrics, let’s start with a simple problem. Suppose we have a binary classification problem, i.e. a problem in which there are only two targets. Let’s suppose it’s a problem of classifying chest x-ray images. There are chest x-ray images with no problem, and some of the chest x-ray images have collapsed lung which is also known as pneumothorax. So, our task is to build a classifier that given a chest x-ray image can detect if it has pneumothorax.  \\n Figure 1: A lung image showing pneumothorax. Image is taken from SIIM-ACR Pneumothorax Segmentation Competition3  We also assume that we have an equal number of pneumothorax and non- pneumothorax chest x-ray images; let’s say 100 each. Thus, we have 100 positive samples and 100 negative samples with a total of 200 images.   The first step is to divide the data described above into two equal sets of 100 images each, i.e. training and validation set. In both the sets, we have 50 positive and 50 negative samples.  3 https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='054f7c6b-331c-45d4-9410-ab4e3b0924d1', embedding=None, metadata={'page_label': '32', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 32 When we have an equal number of positive and negative samples in a binary classification metric, we generally use accuracy, precision, recall and f1.  Accuracy: It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For the problem described above, if you build a model that classifies 90 images accurately, your accuracy is 90% or 0.90. If only 83 images are classified correctly, the accuracy of your model is 83% or 0.83. Simple.  Python code for calculating accuracy is also quite simple.  ═════════════════════════════════════════════════════════════════════════ def accuracy(y_true, y_pred):     \"\"\"     Function to calculate accuracy     :param y_true: list of true values     :param y_pred: list of predicted values     :return: accuracy score     \"\"\"     # initialize a simple counter for correct predictions     correct_counter = 0     # loop over all elements of y_true     # and y_pred \"together\"     for yt, yp in zip(y_true, y_pred):         if yt == yp:             # if prediction is equal to truth, increase the counter             correct_counter += 1      # return accuracy     # which is correct predictions over the number of samples     return correct_counter / len(y_true)  ═════════════════════════════════════════════════════════════════════════  We can also calculate accuracy using scikit-learn.  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics    ...: l1 = [0,1,1,1,0,0,0,1]    ...: l2 = [0,1,0,1,0,1,0,0]    ...: metrics.accuracy_score(l1, l2)  Out[X]: 0.625 ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c6cc53d-ac24-4e74-8fc0-7fff2ea974ea', embedding=None, metadata={'page_label': '33', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 33 Now, let’s say we change the dataset a bit such that there are 180 chest x-ray images which do not have pneumothorax and only 20 with pneumothorax. Even in this case, we will create the training and validation sets with the same ratio of positive to negative (pneumothorax to non- pneumothorax) targets. In each set, we have 90 non- pneumothorax and 10 pneumothorax images. If you say that all images in the validation set are non-pneumothorax, what would your accuracy be? Let’s see; you classified 90% of the images correctly. So, your accuracy is 90%.   But look at it one more time.   You didn’t even build a model and got an accuracy of 90%. That seems kind of useless. If we look carefully, we will see that the dataset is skewed, i.e., the number of samples in one class outnumber the number of samples in other class by a lot. In these kinds of cases, it is not advisable to use accuracy as an evaluation metric as it is not representative of the data. So, you might get high accuracy, but your model will probably not perform that well when it comes to real-world samples, and you won’t be able to explain to your managers why.  In these cases, it’s better to look at other metrics such as precision.   Before learning about precision, we need to know a few terms. Here we have assumed that chest x-ray images with pneumothorax are positive class (1) and without pneumothorax are negative class (0).  True positive (TP): Given an image, if your model predicts the image has pneumothorax, and the actual target for that image has pneumothorax, it is considered a true positive.  True negative (TN): Given an image, if your model predicts that the image does not have pneumothorax and the actual target says that it is a non-pneumothorax image, it is considered a true negative.  In simple words, if your model correctly predicts positive class, it is true positive, and if your model accurately predicts negative class, it is a true negative.  False positive (FP): Given an image, if your model predicts pneumothorax and the actual target for that image is non- pneumothorax, it a false positive.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa24b389-68e7-4172-9447-88abe25f1773', embedding=None, metadata={'page_label': '34', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 34 False negative (FN): Given an image, if your model predicts non-pneumothorax and the actual target for that image is pneumothorax, it is a false negative.  In simple words, if your model incorrectly (or falsely) predicts positive class, it is a false positive. If your model incorrectly (or falsely) predicts negative class, it is a false negative.  Let’s look at implementations of these, one at a time.  ═════════════════════════════════════════════════════════════════════════ def true_positive(y_true, y_pred):     \"\"\"     Function to calculate True Positives     :param y_true: list of true values     :param y_pred: list of predicted values     :return: number of true positives     \"\"\"     # initialize     tp = 0     for yt, yp in zip(y_true, y_pred):         if yt == 1 and yp == 1:             tp += 1     return tp  def true_negative(y_true, y_pred):     \"\"\"     Function to calculate True Negatives     :param y_true: list of true values     :param y_pred: list of predicted values     :return: number of true negatives     \"\"\"     # initialize     tn = 0     for yt, yp in zip(y_true, y_pred):         if yt == 0 and yp == 0:             tn += 1     return tn  def false_positive(y_true, y_pred):     \"\"\"     Function to calculate False Positives     :param y_true: list of true values     :param y_pred: list of predicted values     :return: number of false positives     \"\"\"     # initialize ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a88e934-fd3e-4ed6-a166-5a009bcaa125', embedding=None, metadata={'page_label': '35', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 35     fp = 0     for yt, yp in zip(y_true, y_pred):         if yt == 0 and yp == 1:             fp += 1     return fp  def false_negative(y_true, y_pred):     \"\"\"     Function to calculate False Negatives     :param y_true: list of true values     :param y_pred: list of predicted values     :return: number of false negatives     \"\"\"     # initialize     fn = 0     for yt, yp in zip(y_true, y_pred):         if yt == 1 and yp == 0:             fn += 1     return fn ═════════════════════════════════════════════════════════════════════════  The way I have implemented these here is quite simple and works only for binary classification. Let’s check these functions.  ═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1]    ...: l2 = [0,1,0,1,0,1,0,0]  In [X]: true_positive(l1, l2) Out[X]: 2  In [X]: false_positive(l1, l2) Out[X]: 1  In [X]: false_negative(l1, l2) Out[X]: 2  In [X]: true_negative(l1, l2) Out[X]: 3 ═════════════════════════════════════════════════════════════════════════  If we have to define accuracy using the terms described above, we can write:  Accuracy Score = (TP + TN) / (TP + TN + FP + FN)  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12d7109c-ec8b-4141-ac81-114af381837e', embedding=None, metadata={'page_label': '36', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 36 We can now quickly implement accuracy score using TP, TN, FP and FN in python. Let’s call it accuracy_v2.  ═════════════════════════════════════════════════════════════════════════ def accuracy_v2(y_true, y_pred):     \"\"\"     Function to calculate accuracy using tp/tn/fp/fn     :param y_true: list of true values     :param y_pred: list of predicted values     :return: accuracy score     \"\"\"     tp = true_positive(y_true, y_pred)     fp = false_positive(y_true, y_pred)     fn = false_negative(y_true, y_pred)     tn = true_negative(y_true, y_pred)     accuracy_score = (tp + tn) / (tp + tn + fp + fn)     return accuracy_score ═════════════════════════════════════════════════════════════════════════  We can quickly check the correctness of this function by comparing it to our previous implementation and scikit-learn version.  ═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1]    ...: l2 = [0,1,0,1,0,1,0,0]  In [X]: accuracy(l1, l2) Out[X]: 0.625  In [X]: accuracy_v2(l1, l2) Out[X]: 0.625  In [X]: metrics.accuracy_score(l1, l2) Out[X]: 0.625 ═════════════════════════════════════════════════════════════════════════  Please note that in this code, metrics.accuracy_score comes from scikit-learn.  Great. All values match. This means we have not made any mistakes in the implementation.  Now, we can move to other important metrics.  First one is precision. Precision is defined as: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='375804ad-2ac1-4c76-9628-0ea5ad98befa', embedding=None, metadata={'page_label': '37', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 37 Precision = TP / (TP + FP)  Let’s say we make a new model on the new skewed dataset and our model correctly identified 80 non-pneumothorax out of 90 and 8 pneumothorax out of 10. Thus, we identify 88 images out of 100 successfully. The accuracy is, therefore, 0.88 or 88%.   But, out of these 100 samples, 10 non-pneumothorax images are misclassified as having pneumothorax and 2 pneumothorax are misclassified as not having pneumothorax.  Thus, we have:  - TP : 8 - TN: 80 - FP: 10 - FN: 2  So, our precision is 8 / (8 + 10) = 0.444. This means our model is correct 44.4% times when it’s trying to identify positive samples (pneumothorax).  Now, since we have implemented TP, TN, FP and FN, we can easily implement precision in python.  ═════════════════════════════════════════════════════════════════════════ def precision(y_true, y_pred):     \"\"\"     Function to calculate precision     :param y_true: list of true values     :param y_pred: list of predicted values     :return: precision score     \"\"\"     tp = true_positive(y_true, y_pred)     fp = false_positive(y_true, y_pred)     precision = tp / (tp + fp)     return precision ═════════════════════════════════════════════════════════════════════════  Let’s try this implementation of precision.  ═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1]    ...: l2 = [0,1,0,1,0,1,0,0] ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1750cbc4-42ed-4d87-b7f1-020a641ece6e', embedding=None, metadata={'page_label': '38', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 38 In [X]: precision(l1, l2) Out[X]: 0.6666666666666666 ═════════════════════════════════════════════════════════════════════════  This seems fine.  Next, we come to recall. Recall is defined as:  Recall = TP / (TP + FN)  In the above case recall is 8 / (8 + 2) = 0.80. This means our model identified 80% of positive samples correctly.   ═════════════════════════════════════════════════════════════════════════ def recall(y_true, y_pred):     \"\"\"     Function to calculate recall     :param y_true: list of true values     :param y_pred: list of predicted values     :return: recall score     \"\"\"     tp = true_positive(y_true, y_pred)     fn = false_negative(y_true, y_pred)     recall = tp / (tp + fn)     return recall ═════════════════════════════════════════════════════════════════════════  In the case of our two small lists, we should have a recall of 0.5. Let’s check.  ═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1]    ...: l2 = [0,1,0,1,0,1,0,0]  In [X]: recall(l1, l2) Out[X]: 0.5 ═════════════════════════════════════════════════════════════════════════  And that matches our calculated value!  For a “good” model, our precision and recall values should be high. We see that in the above example, the recall value is quite high. However, precision is very low! Our model produces quite a lot of false positives but less false negatives. Fewer false negatives are good in this type of problem because you don’t want to say that ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62e5762b-b5a6-491a-8c3f-16994dc81966', embedding=None, metadata={'page_label': '39', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 39 patients do not have pneumothorax when they do. That is going to be more harmful. But we do have a lot of false positives, and that’s not good either.  Most of the models predict a probability, and when we predict, we usually choose this threshold to be 0.5. This threshold is not always ideal, and depending on this threshold, your value of precision and recall can change drastically. If for every threshold we choose, we calculate the precision and recall values, we can create a plot between these sets of values. This plot or curve is known as the precision-recall curve.   Before looking into the precision-recall curve, let’s assume two lists.  ═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,    ...:           1, 0, 0, 0, 0, 0, 0, 0, 1, 0]  In [X]: y_pred = [0.02638412, 0.11114267, 0.31620708,    ...:           0.0490937,  0.0191491,  0.17554844,    ...:           0.15952202, 0.03819563, 0.11639273,    ...:           0.079377,   0.08584789, 0.39095342,    ...:           0.27259048, 0.03447096, 0.04644807,    ...:           0.03543574, 0.18521942, 0.05934905,    ...:           0.61977213, 0.33056815] ═════════════════════════════════════════════════════════════════════════  So, y_true is our targets, and y_pred is the probability values for a sample being assigned a value of 1. So, now, we look at probabilities in prediction instead of the predicted value (which is most of the time calculated with a threshold at 0.5).  ═════════════════════════════════════════════════════════════════════════ precisions = [] recalls = [] # how we assumed these thresholds is a long story thresholds = [0.0490937 , 0.05934905, 0.079377,                0.08584789, 0.11114267, 0.11639273,                0.15952202, 0.17554844, 0.18521942,                0.27259048, 0.31620708, 0.33056815,                0.39095342, 0.61977213]  # for every threshold, calculate predictions in binary # and append calculated precisions and recalls # to their respective lists for i in thresholds:     temp_prediction = [1 if x >= i else 0 for x in y_pred] ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d02d5f4-b47f-49de-b46a-34525bfd4ba9', embedding=None, metadata={'page_label': '40', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 40     p = precision(y_true, temp_prediction)     r = recall(y_true, temp_prediction)     precisions.append(p)     recalls.append(r) ═════════════════════════════════════════════════════════════════════════  Now, we can plot these values of precisions and recalls.  ═════════════════════════════════════════════════════════════════════════ plt.figure(figsize=(7, 7)) plt.plot(recalls, precisions) plt.xlabel('Recall', fontsize=15) plt.ylabel('Precision', fontsize=15) ═════════════════════════════════════════════════════════════════════════  Figure 2 shows the precision-recall curve we get this way. \\n Figure 2: precision-recall curve  This precision-recall curve looks very different from what you might have seen on the internet. It’s because we had only 20 samples, and only 3 of them were positive samples. But there’s nothing to worry. It’s the same old precision-recall curve.  \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2afcc91-18dd-409f-9d4c-4533e9b483c6', embedding=None, metadata={'page_label': '41', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 41 You will notice that it’s challenging to choose a value of threshold that gives both good precision and recall values. If the threshold is too high, you have a smaller number of true positives and a high number of false negatives. This decreases your recall; however, your precision score will be high. If you reduce the threshold too low, false positives will increase a lot, and precision will be less.  Both precision and recall range from 0 to 1 and a value closer to 1 is better.  F1 score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean) of precision and recall. If we denote precision using P and recall using R, we can represent the F1 score as:  F1 = 2PR / (P + R)  A little bit of mathematics will lead you to the following equation of F1 based on TP, FP and FN  F1 = 2TP / (2TP + FP + FN)  A Python implementation is simple because we have already implemented these.  ═════════════════════════════════════════════════════════════════════════ def f1(y_true, y_pred):     \"\"\"     Function to calculate f1 score     :param y_true: list of true values     :param y_pred: list of predicted values     :return: f1 score     \"\"\"     p = precision(y_true, y_pred)     r = recall(y_true, y_pred)      score = 2 * p * r / (p + r)      return score ═════════════════════════════════════════════════════════════════════════  Let’s see the results of this and compare it with scikit-learn.  ═════════════════════════════════════════════════════════════════════════  In [X]: y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,    ...:           1, 0, 0, 0, 0, 0, 0, 0, 1, 0]  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8295de32-81be-4ff2-bbf1-c051b6ab6b2c', embedding=None, metadata={'page_label': '42', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 42 In [X]: y_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0,    ...:           1, 0, 0, 0, 0, 0, 0, 0, 1, 0]  In [X]: f1(y_true, y_pred) Out[X]: 0.5714285714285715 ═════════════════════════════════════════════════════════════════════════  And from scikit learn for the same lists, we get:  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  In [X]: metrics.f1_score(y_true, y_pred) Out[X]: 0.5714285714285715 ═════════════════════════════════════════════════════════════════════════  Instead of looking at precision and recall individually, you can also just look at F1 score. Same as for precision, recall and accuracy, F1 score also ranges from 0 to 1, and a perfect prediction model has an F1 of 1. When dealing with datasets that have skewed targets, we should look at F1 (or precision and recall) instead of accuracy.  Then there are other crucial terms that we should know about.   The first one is TPR or True Positive Rate, which is the same as recall.  TPR = TP / (TP + FN)  Even though it is same as recall, we will make a python function for it for further use with this name.  ═════════════════════════════════════════════════════════════════════════ def tpr(y_true, y_pred):     \"\"\"     Function to calculate tpr     :param y_true: list of true values     :param y_pred: list of predicted values     :return: tpr/recall     \"\"\"     return recall(y_true, y_pred) ═════════════════════════════════════════════════════════════════════════  TPR or recall is also known as sensitivity.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca2f5df0-eccf-4dc8-bb51-2d7654f93ab2', embedding=None, metadata={'page_label': '43', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 43 And FPR or False Positive Rate, which is defined as:  FPR = FP / (TN + FP)  ═════════════════════════════════════════════════════════════════════════ def fpr(y_true, y_pred):     \"\"\"     Function to calculate fpr     :param y_true: list of true values     :param y_pred: list of predicted values     :return: fpr     \"\"\"     fp = false_positive(y_true, y_pred)     tn = true_negative(y_true, y_pred)     return fp / (tn + fp) ═════════════════════════════════════════════════════════════════════════  And 1 - FPR is known as specificity or True Negative Rate or TNR.  These are a lot of terms, but the most important ones out of these are only TPR and FPR.   Let’s assume that we have only 15 samples and their target values are binary:  Actual targets : [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]   We train a model like the random forest, and we can get the probability of when a sample is positive.  Predicted probabilities for 1: [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]  For a typical threshold of  >= 0.5, we can evaluate all the above values of precision, recall/TPR, F1 and FPR. But we can do the same if we choose the value of the threshold to be 0.4 or 0.6. In fact, we can choose any value between 0 and 1 and calculate all the metrics described above.  Let’s calculate only two values, though: TPR and FPR.  ═════════════════════════════════════════════════════════════════════════ # empty lists to store tpr  # and fpr values ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d3f5813-7858-4c4f-a055-6910a2fd8dbc', embedding=None, metadata={'page_label': '44', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 44 tpr_list = [] fpr_list = []  # actual targets y_true = [0, 0, 0, 0, 1, 0, 1,            0, 0, 1, 0, 1, 0, 0, 1]  # predicted probabilities of a sample being 1 y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,            0.9, 0.5, 0.3, 0.66, 0.3, 0.2,            0.85, 0.15, 0.99]  # handmade thresholds thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,               0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]  # loop over all thresholds for thresh in thresholds:      # calculate predictions for a given threshold     temp_pred = [1 if x >= thresh else 0 for x in y_pred]     # calculate tpr     temp_tpr = tpr(y_true, temp_pred)     # calculate fpr     temp_fpr = fpr(y_true, temp_pred)     # append tpr and fpr to lists     tpr_list.append(temp_tpr)     fpr_list.append(temp_fpr)  ═════════════════════════════════════════════════════════════════════════  We can thus get a tpr and fpr value for each threshold.  \\n Figure 3: Table for threshold, TPR and FPR values  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dd8cc05-e502-4253-8db5-b5fd3bcfe618', embedding=None, metadata={'page_label': '45', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 45 If we plot the table as shown in figure 3, i.e. if we have TPR on the y-axis and FPR on the x-axis, we will get a curve as shown in figure 4.  ═════════════════════════════════════════════════════════════════════════ plt.figure(figsize=(7, 7)) plt.fill_between(fpr_list, tpr_list, alpha=0.4) plt.plot(fpr_list, tpr_list, lw=3) plt.xlim(0, 1.0) plt.ylim(0, 1.0) plt.xlabel('FPR', fontsize=15) plt.ylabel('TPR', fontsize=15) plt.show() ═════════════════════════════════════════════════════════════════════════ \\n Figure 4: Receiver operating characteristic (ROC) curve  This curve is also known as the Receiver Operating Characteristic (ROC). And if we calculate the area under this ROC curve, we are calculating another metric which is used very often when you have a dataset which has skewed binary targets.   This metric is known as the Area Under ROC Curve or Area Under Curve or just simply AUC. There are many ways to calculate the area under the ROC curve. For this particular purpose, we will stick to the fantastic implementation by scikit-learn.  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe847a3b-c9df-445e-8fb7-48b2ed022313', embedding=None, metadata={'page_label': '46', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 46 In [X]: y_true = [0, 0, 0, 0, 1, 0, 1,    ...:           0, 0, 1, 0, 1, 0, 0, 1]  In [X]: y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,    ...:           0.9, 0.5, 0.3, 0.66, 0.3, 0.2,    ...:           0.85, 0.15, 0.99]  In [X]: metrics.roc_auc_score(y_true, y_pred) Out[X]: 0.8300000000000001 ═════════════════════════════════════════════════════════════════════════  AUC values range from 0 to 1.  - AUC = 1 implies you have a perfect model. Most of the time, it means that you made some mistake with validation and should revisit data processing and validation pipeline of yours. If you didn’t make any mistakes, then congratulations, you have the best model one can have for the dataset you built it on.  - AUC = 0 implies that your model is very bad (or very good!). Try inverting the probabilities for the predictions, for example, if your probability for the positive class is p, try substituting it with 1-p. This kind of AUC may also mean that there is some problem with your validation or data processing.  - AUC = 0.5 implies that your predictions are random. So, for any binary classification problem, if I predict all targets as 0.5, I will get an AUC of 0.5.  AUC values between 0 and 0.5 imply that your model is worse than random. Most of the time, it’s because you inverted the classes. If you try to invert your predictions, your AUC might become more than 0.5. AUC values closer to 1 are considered good.  But what does AUC say about our model?   Suppose you get an AUC of 0.85 when you build a model to detect pneumothorax from chest x-ray images. This means that if you select a random image from your dataset with pneumothorax (positive sample) and another random image without pneumothorax (negative sample), then the pneumothorax image will rank higher than a non-pneumothorax image with a probability of 0.85.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='05c71711-6e51-440d-8b48-2aef509da736', embedding=None, metadata={'page_label': '47', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 47 After calculating probabilities and AUC, you would want to make predictions on the test set. Depending on the problem and use-case, you might want to either have probabilities or actual classes. If you want to have probabilities, it’s effortless. You already have them. If you want to have classes, you need to select a threshold. In the case of binary classification, you can do something like the following.  Prediction = Probability >= Threshold  Which means, that prediction is a new list which contains only binary variables. An item in prediction is 1 if the probability is greater than or equal to a given threshold else the value is 0.  And guess what, you can use the ROC curve to choose this threshold! The ROC curve will tell you how the threshold impacts false positive rate and true positive rate and thus, in turn, false positives and true positives. You should choose the threshold that is best suited for your problem and datasets.   For example, if you don’t want to have too many false positives, you should have a high threshold value. This will, however, also give you a lot more false negatives. Observe the trade-off and select the best threshold. Let’s see how these thresholds impact true positive and false positive values.  ═════════════════════════════════════════════════════════════════════════ # empty lists to store true positive  # and false positive values tp_list = [] fp_list = []  # actual targets y_true = [0, 0, 0, 0, 1, 0, 1,            0, 0, 1, 0, 1, 0, 0, 1]  # predicted probabilities of a sample being 1 y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,            0.9, 0.5, 0.3, 0.66, 0.3, 0.2,            0.85, 0.15, 0.99]  # some handmade thresholds thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,               0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]  # loop over all thresholds for thresh in thresholds:  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43a45624-bc35-43f1-afb3-436dd5f5977a', embedding=None, metadata={'page_label': '48', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 48     # calculate predictions for a given threshold     temp_pred = [1 if x >= thresh else 0 for x in y_pred]     # calculate tp     temp_tp = true_positive(y_true, temp_pred)     # calculate fp     temp_fp = false_positive(y_true, temp_pred)     # append tp and fp to lists     tp_list.append(temp_tp)     fp_list.append(temp_fp) ═══════════════════════════════════════════════════════════════════════  Using this, we can create a table, as shown in Figure 5.  \\n Figure 5: TP and FP values for different thresholds  Most of the time, the top-left value on ROC curve should give you a quite good threshold, as shown in figure 6.  Comparing the table and the ROC curve, we see that a threshold of around 0.6 is quite good where we do not lose a lot of true positives and neither we have a lot of false positives. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3da39996-913e-467e-a115-938703cd4f39', embedding=None, metadata={'page_label': '49', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 49  Figure 6: Select the best threshold from the leftmost top point in the ROC curve  AUC is a widely used metric for skewed binary classification tasks in the industry, and a metric everyone should know about. Once you understand the idea behind AUC, as explained in the paragraphs above, it is also easy to explain it to non-technical people who would probably be assessing your models in the industry.  Another important metric you should learn after learning AUC is log loss. In case of a binary classification problem, we define log loss as:  Log Loss = - 1.0 * ( target * log(prediction) + (1 - target) * log(1 - prediction) )  Where target is either 0 or 1 and prediction is a probability of a sample belonging to class 1.  For multiple samples in the dataset, the log-loss over all samples is a mere average of all individual log losses. One thing to remember is that log loss penalizes quite high for an incorrect or a far-off prediction, i.e. log loss punishes you for being very sure and very wrong.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  def log_loss(y_true, y_proba): \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a385da2-26b7-44f9-8d06-1f1c25339f89', embedding=None, metadata={'page_label': '50', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 50     \"\"\"     Function to calculate log loss     :param y_true: list of true values     :param y_proba: list of probabilities for 1     :return: overall log loss     \"\"\"     # define an epsilon value     # this can also be an input     # this value is used to clip probabilities     epsilon = 1e-15     # initialize empty list to store     # individual losses     loss = []     # loop over all true and predicted probability values     for yt, yp in zip(y_true, y_proba):         # adjust probability         # 0 gets converted to 1e-15         # 1 gets converted to 1-1e-15         # Why? Think about it!         yp = np.clip(yp, epsilon, 1 - epsilon)         # calculate loss for one sample         temp_loss = - 1.0 * (             yt * np.log(yp)              + (1 - yt) * np.log(1 - yp)         )         # add to loss list         loss.append(temp_loss)     # return mean loss over all samples     return np.mean(loss) ═════════════════════════════════════════════════════════════════════════  Let’s test our implementation:  ═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [0, 0, 0, 0, 1, 0, 1,    ...:           0, 0, 1, 0, 1, 0, 0, 1]  In [X]: y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,    ...:           0.9, 0.5, 0.3, 0.66, 0.3, 0.2,    ...:           0.85, 0.15, 0.99]  In [X]: log_loss(y_true, y_proba) Out[X]: 0.49882711861432294 ═════════════════════════════════════════════════════════════════════════  We can compare this with scikit-learn: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8906c18c-3b0b-45ef-a40a-7c03105573b1', embedding=None, metadata={'page_label': '51', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 51 ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  In [X]: metrics.log_loss(y_true, y_proba) Out[X]: 0.49882711861432294 ═════════════════════════════════════════════════════════════════════════  Thus, our implementation is correct. Implementation of log loss is easy. Interpretation may seem a bit difficult. You must remember that log loss penalizes a lot more than other metrics.   For example, if you are 51% sure about a sample belonging to class 1, log loss would be:  - 1.0 * ( 1 * log(0.51) + (1 - 1) * log(1 – 0.51) ) = 0.67  And if you are 49% sure for a sample belonging to class 0, log loss would be:  - 1.0 * ( 0 * log(0.49) + (1 - 0) * log(1 – 0.49) ) = 0.67  So, even though we can choose a cut off at 0.5 and get perfect predictions, we will still have a very high log loss. So, when dealing with log loss, you need to be very careful; any non-confident prediction will have a very high log loss.  Most of the metrics that we discussed until now can be converted to a multi-class version. The idea is quite simple. Let’s take precision and recall. We can calculate precision and recall for each class in a multi-class classification problem.  There are three different ways to calculate this which might get confusing from time to time. Let’s assume we are interested in precision first. We know that precision depends on true positives and false positives.  - Macro averaged precision: calculate precision for all classes individually and then average them  - Micro averaged precision: calculate class wise true positive and false positive and then use that to calculate overall precision  - Weighted precision: same as macro but in this case, it is weighted average depending on the number of items in each class ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85102e5f-df4d-4343-8b0c-d53101913d3d', embedding=None, metadata={'page_label': '52', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 52 This seems complicated but is easy to understand by python implementations. Let’s see how macro-averaged precision is implemented.  ═════════════════════════════════════════════════════════════════════════ import numpy as np   def macro_precision(y_true, y_pred):     \"\"\"     Function to calculate macro averaged precision     :param y_true: list of true values     :param y_pred: list of predicted values     :return: macro precision score     \"\"\"          # find the number of classes by taking     # length of unique values in true list     num_classes = len(np.unique(y_true))          # initialize precision to 0     precision = 0          # loop over all classes     for class_ in range(num_classes):                  # all classes except current are considered negative         temp_true = [1 if p == class_ else 0 for p in y_true]         temp_pred = [1 if p == class_ else 0 for p in y_pred]                  # calculate true positive for current class         tp = true_positive(temp_true, temp_pred)                  # calculate false positive for current class         fp = false_positive(temp_true, temp_pred)                  # calculate precision for current class         temp_precision = tp / (tp + fp)                  # keep adding precision for all classes         precision += temp_precision      # calculate and return average precision over all classes     precision /= num_classes     return precision ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a24c705-7191-4276-aa9e-6153cb9f11f6', embedding=None, metadata={'page_label': '53', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 53 You will notice that it wasn’t so difficult. Similarly, we have micro-averaged precision score.  ═════════════════════════════════════════════════════════════════════════ import numpy as np   def micro_precision(y_true, y_pred):     \"\"\"     Function to calculate micro averaged precision     :param y_true: list of true values     :param y_pred: list of predicted values     :return: micro precision score     \"\"\"          # find the number of classes by taking     # length of unique values in true list     num_classes = len(np.unique(y_true))          # initialize tp and fp to 0     tp = 0     fp = 0          # loop over all classes     for class_ in range(num_classes):         # all classes except current are considered negative         temp_true = [1 if p == class_ else 0 for p in y_true]         temp_pred = [1 if p == class_ else 0 for p in y_pred]                  # calculate true positive for current class         # and update overall tp         tp += true_positive(temp_true, temp_pred)                  # calculate false positive for current class         # and update overall tp         fp += false_positive(temp_true, temp_pred)              # calculate and return overall precision     precision = tp / (tp + fp)     return precision ═════════════════════════════════════════════════════════════════════════  This isn’t difficult, either. Then what is? Nothing. Machine learning is easy.   Now, let’s look at the implementation of weighted precision.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a26d7baa-8e97-402f-9728-6ff395e5103a', embedding=None, metadata={'page_label': '54', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 54 ═════════════════════════════════════════════════════════════════════════ from collections import Counter import numpy as np   def weighted_precision(y_true, y_pred):     \"\"\"     Function to calculate weighted averaged precision     :param y_true: list of true values     :param y_pred: list of predicted values     :return: weighted precision score     \"\"\"          # find the number of classes by taking     # length of unique values in true list     num_classes = len(np.unique(y_true))          # create class:sample count dictionary     # it looks something like this:     # {0: 20, 1:15, 2:21}     class_counts = Counter(y_true)          # initialize precision to 0     precision = 0          # loop over all classes     for class_ in range(num_classes):         # all classes except current are considered negative         temp_true = [1 if p == class_ else 0 for p in y_true]         temp_pred = [1 if p == class_ else 0 for p in y_pred]                  # calculate tp and fp for class         tp = true_positive(temp_true, temp_pred)         fp = false_positive(temp_true, temp_pred)                  # calculate precision of class         temp_precision = tp / (tp + fp)                  # multiply precision with count of samples in class         weighted_precision = class_counts[class_] * temp_precision                  # add to overall precision         precision += weighted_precision     # calculate overall precision by dividing by     # total number of samples     overall_precision = precision / len(y_true) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdd76288-c436-4a34-83e9-8559bd15f223', embedding=None, metadata={'page_label': '55', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 55     return overall_precision ═════════════════════════════════════════════════════════════════════════  Let’s compare our implementations with scikit-learn to know if we implemented it right.  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  In [X]: y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]  In [X]: y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]  In [X]: macro_precision(y_true, y_pred) Out[X]: 0.3611111111111111  In [X]: metrics.precision_score(y_true, y_pred, average=\"macro\") Out[X]: 0.3611111111111111  In [X]: micro_precision(y_true, y_pred) Out[X]: 0.4444444444444444  In [X]: metrics.precision_score(y_true, y_pred, average=\"micro\") Out[X]: 0.4444444444444444  In [X]: weighted_precision(y_true, y_pred) Out[X]: 0.39814814814814814  In [X]: metrics.precision_score(y_true, y_pred, average=\"weighted\") Out[X]: 0.39814814814814814 ═════════════════════════════════════════════════════════════════════════  It seems like we implemented everything correctly. Please note that the implementations shown here may not be the most efficient, but they are the easiest to understand.  Similarly, we can implement the recall metric for multi-class. Precision and recall depend on true positive, false positive and false negative while F1 depends on precision and recall.   Implementation for recall is left as an exercise for the reader and one version of F1 for multi-class, i.e., weighted average is implemented here.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27bd9619-64b9-4be2-8787-3861d3479ca2', embedding=None, metadata={'page_label': '56', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 56 ═════════════════════════════════════════════════════════════════════════ from collections import Counter import numpy as np   def weighted_f1(y_true, y_pred):     \"\"\"     Function to calculate weighted f1 score     :param y_true: list of true values     :param y_proba: list of predicted values     :return: weighted f1 score     \"\"\"          # find the number of classes by taking     # length of unique values in true list     num_classes = len(np.unique(y_true))          # create class:sample count dictionary     # it looks something like this:     # {0: 20, 1:15, 2:21}     class_counts = Counter(y_true)          # initialize f1 to 0     f1 = 0          # loop over all classes     for class_ in range(num_classes):         # all classes except current are considered negative         temp_true = [1 if p == class_ else 0 for p in y_true]         temp_pred = [1 if p == class_ else 0 for p in y_pred]                  # calculate precision and recall for class         p = precision(temp_true, temp_pred)         r = recall(temp_true, temp_pred)                  # calculate f1 of class         if p + r != 0:             temp_f1 = 2 * p * r / (p + r)         else:             temp_f1 = 0                  # multiply f1 with count of samples in class         weighted_f1 = class_counts[class_] * temp_f1                  # add to f1 precision         f1 += weighted_f1      ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f4ea93b-d9a0-4554-adea-52f8b84a34fe', embedding=None, metadata={'page_label': '57', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 57     # calculate overall F1 by dividing by     # total number of samples     overall_f1 = f1 / len(y_true)     return overall_f1 ═════════════════════════════════════════════════════════════════════════  Note that there are a few lines of code above which are new. And that’s why you should read the code carefully.  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  In [X]: y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]  In [X]: y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]  In [X]: weighted_f1(y_true, y_pred) Out[X]: 0.41269841269841273  In [X]: metrics.f1_score(y_true, y_pred, average=\"weighted\") Out[X]: 0.41269841269841273 ═════════════════════════════════════════════════════════════════════════  Thus, we have precision, recall and F1 implemented for multi-class problems. You can similarly convert AUC and log loss to multi-class formats too. This format of conversion is known as one-vs-all. I’m not going to implement them here as the implementation is quite similar to what we have already discussed.  In binary or multi-class classification, it is also quite popular to take a look at confusion matrix. Don’t be confused; it’s quite easy. A confusion matrix is nothing but a table of TP, FP, TN and FN. Using the confusion matrix, you can quickly see how many samples were misclassified and how many were classified correctly.  One might argue that the confusion matrix should be covered quite early in this chapter, but I chose not to do it. If you understand TP, FP, TN, FN, precision, recall and AUC, it becomes quite easy to understand and interpret confusion matrix. Let’s see what confusion matrix looks like for a binary classification problem in figure 7.  We see that the confusion matrix is made up of TP, FP, FN and TN. These are the only values we need to calculate precision, recall, F1 score and AUC. Sometimes, people also prefer calling FP as Type-I error and FN as Type-II error.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='968c8f86-fd8e-4531-80b6-e42d42f86cd3', embedding=None, metadata={'page_label': '58', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 58  Figure 7: Confusion matrix for a binary classification task  We can also expand the binary confusion matrix to a multi-class confusion matrix. How would that look like? If we have N classes, it will be a matrix of size NxN. For every class, we calculate the total number of samples that went to the class in concern and other classes. This can be best understood by an example.  Suppose we have the following actual classes:  [0, 1, 2, 0, 1, 2, 0, 2, 2]  And our predictions are:  [0, 2, 1, 0, 2, 1, 0, 0, 2]  Then our confusion matrix will look as shown in figure 8.  What does figure 8 tell us?   Let’s look at class 0. We see that there are 3 instances of class 0 in the actual target. However, in prediction, we have 3 instances that belong to class 0 and 1 instance that belongs to class 1. Ideally, for class 0 in the actual label, predicted labels 1 and 2 shouldn’t have any instance. Let’s see class 2. In actual labels, this count adds up to 4 while in predicted it adds up to 3. Only 1 instance has a perfect prediction for class 2 and 2 instances go to class 1.   A perfect confusion matrix should only be filled diagonally from left to right.   \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e2791e3-97d4-498a-b5fd-ec95e66f75b2', embedding=None, metadata={'page_label': '59', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 59  Figure 8: Confusion matrix for a multi-class problem  Confusion matrix gives an easy way to calculate different metrics that we have discussed before. Scikit-learn offers an easy and straightforward way to generate a confusion matrix. Please note that the confusion matrix that I have shown in figure 8 is a transpose of scikit-learn’s confusion matrix and an original version can be plotted by the following code.  ═════════════════════════════════════════════════════════════════════════ import matplotlib.pyplot as plt import seaborn as sns from sklearn import metrics  # some targets y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]  #some predictions y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]  # get confusion matrix from sklearn cm = metrics.confusion_matrix(y_true, y_pred)  # plot using matplotlib and seaborn plt.figure(figsize=(10, 10)) cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True) sns.set(font_scale=2.5) sns.heatmap(cm, annot=True, cmap=cmap, cbar=False) \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e179ad0-a94b-4a81-8a06-e381abdd1377', embedding=None, metadata={'page_label': '60', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 60 plt.ylabel('Actual Labels', fontsize=20) plt.xlabel('Predicted Labels', fontsize=20) ═════════════════════════════════════════════════════════════════════════  So, until now, we have tackled metrics for binary and multi-class classification. Then comes another type of classification problem called multi-label classification. In multi-label classification, each sample can have one or more classes associated with it. One simple example of this type of problem would be a task in which you are asked to predict different objects in a given image.  \\n Figure 9: Different objects in an image4  Figure 9 shows an example image from a well-known dataset. Note that this dataset’s objective is something different but let’s not go there. Let’s assume that the aim is only to predict if an object is present in an image or not. For figure 9, we have a chair, flower-pot, window, but we don’t have other objects such as computer, bed, tv, etc. So, one image can have multiple targets associated with it. This type of problem is the multi-label classification problem.  The metrics for this type of classification problem are a bit different. Some suitable and most common metrics are:  - Precision at k (P@k) - Average precision at k (AP@k)  4 https://www.flickr.com/photos/krakluski/2950388100 License: CC BY 2.0 \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37f96367-0aab-42cc-9d18-b4b858fad544', embedding=None, metadata={'page_label': '61', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 61 - Mean average precision at k (MAP@k) - Log loss  Let’s start with precision at k or P@k. One must not confuse this precision with the precision discussed earlier. If you have a list of original classes for a given sample and list of predicted classes for the same, precision is defined as the number of hits in the predicted list considering only top-k predictions, divided by k.  If that’s confusing, it will become apparent with python code.  ═════════════════════════════════════════════════════════════════════════ def pk(y_true, y_pred, k):     \"\"\"     This function calculates precision at k      for a single sample     :param y_true: list of values, actual classes     :param y_pred: list of values, predicted classes     :param k: the value for k     :return: precision at a given value k     \"\"\"     # if k is 0, return 0. we should never have this     # as k is always >= 1     if k == 0:         return 0     # we are interested only in top-k predictions     y_pred = y_pred[:k]     # convert predictions to set     pred_set = set(y_pred)     # convert actual values to set     true_set = set(y_true)     # find common values     common_values = pred_set.intersection(true_set)     # return length of common values over k     return len(common_values) / len(y_pred[:k]) ═════════════════════════════════════════════════════════════════════════  With code, everything becomes much easier to understand.  Now, we have average precision at k or AP@k. AP@k is calculated using P@k. For example, if we have to calculate AP@3, we calculate P@1, P@2 and P@3 and then divide the sum by 3.   Let’s see its implementation.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6aae851-a30e-431d-82a8-cf554aabed74', embedding=None, metadata={'page_label': '62', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 62 ═════════════════════════════════════════════════════════════════════════ def apk(y_true, y_pred, k):     \"\"\"     This function calculates average precision at k      for a single sample     :param y_true: list of values, actual classes     :param y_pred: list of values, predicted classes     :return: average precision at a given value k     \"\"\"     # initialize p@k list of values     pk_values = []     # loop over all k. from 1 to k + 1     for i in range(1, k + 1):         # calculate p@i and append to list         pk_values.append(pk(y_true, y_pred, i))      # if we have no values in the list, return 0     if len(pk_values) == 0:         return 0     # else, we return the sum of list over length of list     return sum(pk_values) / len(pk_values) ═════════════════════════════════════════════════════════════════════════  These two functions can be used to calculate average precision at k (AP@k) for two given lists; let’s see how.  In [X]: y_true = [    ...:     [1, 2, 3],    ...:     [0, 2],    ...:     [1],    ...:     [2, 3],    ...:     [1, 0],    ...:     []    ...: ]  In [X]: y_pred = [    ...:     [0, 1, 2],    ...:     [1],    ...:     [0, 2, 3],    ...:     [2, 3, 4, 0],    ...:     [0, 1, 2],    ...:     [0]    ...: ]  In [X]: for i in range(len(y_true)):    ...:     for j in range(1, 4):    ...:         print( ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27fd9046-b02d-4913-9633-54f9345c4ef7', embedding=None, metadata={'page_label': '63', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 63    ...:             f\"\"\"    ...:             y_true={y_true[i]},    ...:             y_pred={y_pred[i]},    ...:             AP@{j}={apk(y_true[i], y_pred[i], k=j)}    ...:             \"\"\"    ...:         )    ...:              y_true=[1, 2, 3],             y_pred=[0, 1, 2],             AP@1=0.0               y_true=[1, 2, 3],             y_pred=[0, 1, 2],             AP@2=0.25               y_true=[1, 2, 3],             y_pred=[0, 1, 2],             AP@3=0.38888888888888884      .      . ═════════════════════════════════════════════════════════════════════════  Please note that I have omitted many values from the output, but you get the point. So, this is how we can calculate AP@k which is per sample. In machine learning, we are interested in all samples, and that’s why we have mean average precision at k or MAP@k. MAP@k is just an average of AP@k and can be calculated easily by the following python code.  ═════════════════════════════════════════════════════════════════════════ def mapk(y_true, y_pred, k):     \"\"\"     This function calculates mean avg precision at k      for a single sample     :param y_true: list of values, actual classes     :param y_pred: list of values, predicted classes     :return: mean avg precision at a given value k     \"\"\"     # initialize empty list for apk values     apk_values = []     # loop over all samples     for i in range(len(y_true)):         # store apk values for every sample         apk_values.append( ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2a50422f-bb2c-4da4-a19c-10f5fede9330', embedding=None, metadata={'page_label': '64', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 64             apk(y_true[i], y_pred[i], k=k)         )     # return mean of apk values list     return sum(apk_values) / len(apk_values) ═════════════════════════════════════════════════════════════════════════  Now, we can calculate MAP@k for k=1, 2, 3 and 4 for the same list of lists.  ═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [    ...:     [1, 2, 3],    ...:     [0, 2],    ...:     [1],    ...:     [2, 3],    ...:     [1, 0],    ...:     []    ...: ]  In [X]: y_pred = [    ...:     [0, 1, 2],    ...:     [1],    ...:     [0, 2, 3],    ...:     [2, 3, 4, 0],    ...:     [0, 1, 2],    ...:     [0]    ...: ]  In [X]: mapk(y_true, y_pred, k=1) Out[X]: 0.3333333333333333  In [X]: mapk(y_true, y_pred, k=2) Out[X]: 0.375  In [X]: mapk(y_true, y_pred, k=3) Out[X]: 0.3611111111111111  In [X]: mapk(y_true, y_pred, k=4) Out[X]: 0.34722222222222215  ═════════════════════════════════════════════════════════════════════════  P@k, AP@k and MAP@k all range from 0 to 1 with 1 being the best.  Please note that sometimes you might see different implementations of P@k and AP@k on the internet. For example, let’s take a look at one of these implementations. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='71cebb70-868d-4a07-9125-215353f2b60d', embedding=None, metadata={'page_label': '65', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 65 ═════════════════════════════════════════════════════════════════════════ # taken from: # https://github.com/benhamner/Metrics/blob/ # master/Python/ml_metrics/average_precision.py import numpy as np  def apk(actual, predicted, k=10):     \"\"\"     Computes the average precision at k.     This function computes the AP at k between two lists of     items.     Parameters     ----------     actual : list              A list of elements to be predicted (order doesn\\'t matter)     predicted : list              A list of predicted elements (order does matter)     k : int, optional              The maximum number of predicted elements     Returns     -------     score : double              The average precision at k over the input lists     \"\"\"     if len(predicted)>k:         predicted = predicted[:k]      score = 0.0     num_hits = 0.0      for i,p in enumerate(predicted):         if p in actual and p not in predicted[:i]:             num_hits += 1.0             score += num_hits / (i+1.0)      if not actual:         return 0.0      return score / min(len(actual), k) ═════════════════════════════════════════════════════════════════════════  This implementation is another version of AP@k where order matters and we weigh the predictions. This implementation will have slightly different results from what I have presented.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='023a487c-db24-43ea-9126-bc403c8181d0', embedding=None, metadata={'page_label': '66', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 66 Now, we come to log loss for multi-label classification. This is quite easy. You can convert the targets to binary format and then use a log loss for each column. In the end, you can take the average of log loss in each column. This is also known as mean column-wise log loss. Of course, there are other ways you can implement this, and you should explore it as you come across it.  We have now reached a stage where we can say that we now know all binary, multi-class and multi-label classification metrics, and now we can move to regression metrics.  The most common metric in regression is error. Error is simple and very easy to understand.  Error = True Value – Predicted Value  Absolute error is just absolute of the above.  Absolute Error = Abs ( True Value – Predicted Value )  Then we have mean absolute error (MAE). It’s just mean of all absolute errors.  ═════════════════════════════════════════════════════════════════════════ import numpy as np   def mean_absolute_error(y_true, y_pred):     \"\"\"     This function calculates mae     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: mean absolute error     \"\"\"     # initialize error at 0     error = 0     # loop over all samples in the true and predicted list     for yt, yp in zip(y_true, y_pred):         # calculate absolute error          # and add to error         error += np.abs(yt - yp)     # return mean error     return error / len(y_true) ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='774fa207-cbf9-4b97-8ff2-68ee200eef6c', embedding=None, metadata={'page_label': '67', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 67 Similarly, we have squared error and mean squared error (MSE).  Squared Error = ( True Value – Predicted Value )2  And mean squared error (MSE) can be implemented as follows.  ═════════════════════════════════════════════════════════════════════════ def mean_squared_error(y_true, y_pred):     \"\"\"     This function calculates mse     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: mean squared error     \"\"\"     # initialize error at 0     error = 0     # loop over all samples in the true and predicted list     for yt, yp in zip(y_true, y_pred):         # calculate squared error          # and add to error         error += (yt - yp) ** 2     # return mean error     return error / len(y_true) ═════════════════════════════════════════════════════════════════════════  MSE and RMSE (root mean squared error) are the most popular metrics used in evaluating regression models.  RMSE = SQRT ( MSE )  Another type of error in same class is squared logarithmic error. Some people call it SLE, and when we take mean of this error across all samples, it is known as MSLE (mean squared logarithmic error) and implemented as follows.  ═════════════════════════════════════════════════════════════════════════ import numpy as np   def mean_squared_log_error(y_true, y_pred):     \"\"\"     This function calculates msle     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: mean squared logarithmic error ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97a6a8f2-5b7e-44c7-bcf7-ec0becd26e55', embedding=None, metadata={'page_label': '68', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 68     \"\"\"     # initialize error at 0     error = 0     # loop over all samples in true and predicted list     for yt, yp in zip(y_true, y_pred):         # calculate squared log error          # and add to error         error += (np.log(1 + yt) - np.log(1 + yp)) ** 2     # return mean error     return error / len(y_true) ═════════════════════════════════════════════════════════════════════════  Root mean squared logarithmic error is just a square root of this. It is also known as RMSLE.   Then we have the percentage error:  Percentage Error = ( ( True Value – Predicted Value ) / True Value ) * 100  Same can be converted to mean percentage error for all samples.  ═════════════════════════════════════════════════════════════════════════ def mean_percentage_error(y_true, y_pred):     \"\"\"     This function calculates mpe     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: mean percentage error     \"\"\"     # initialize error at 0     error = 0      # loop over all samples in true and predicted list     for yt, yp in zip(y_true, y_pred):         # calculate percentage error          # and add to error         error += (yt - yp) / yt      # return mean percentage error     return error / len(y_true) ═════════════════════════════════════════════════════════════════════════  And an absolute version of the same (and more common version) is known as mean absolute percentage error or MAPE.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a39e310-6f41-4b96-9937-3f1539b8885c', embedding=None, metadata={'page_label': '69', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 69 ═════════════════════════════════════════════════════════════════════════ import numpy as np   def mean_abs_percentage_error(y_true, y_pred):     \"\"\"     This function calculates MAPE     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: mean absolute percentage error     \"\"\"     # initialize error at 0     error = 0     # loop over all samples in true and predicted list     for yt, yp in zip(y_true, y_pred):         # calculate percentage error          # and add to error         error += np.abs(yt - yp) / yt     # return mean percentage error     return error / len(y_true) ═════════════════════════════════════════════════════════════════════════  The best thing about regression is that there are only a few most popular metrics that can be applied to almost every regression problem. And it is much easier to understand when we compare it to classification metrics.   Let’s talk about another regression metric known as R2 (R-squared), also known as the coefficient of determination.   In simple words, R-squared says how good your model fits the data. R-squared closer to 1.0 says that the model fits the data quite well, whereas closer 0 means that model isn’t that good. R-squared can also be negative when the model just makes absurd predictions.   The formula for R-squared is shown in figure 10, but as always a python implementation makes things more clear.     Figure 10: Formula for R-squared \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7840479b-9eec-4d98-9126-dfb67e18292e', embedding=None, metadata={'page_label': '70', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 70 ═════════════════════════════════════════════════════════════════════════ import numpy as np   def r2(y_true, y_pred):     \"\"\"     This function calculates r-squared score     :param y_true: list of real numbers, true values     :param y_pred: list of real numbers, predicted values     :return: r2 score     \"\"\"          # calculate the mean value of true values     mean_true_value = np.mean(y_true)          # initialize numerator with 0     numerator = 0     # initialize denominator with 0     denominator = 0          # loop over all true and predicted values     for yt, yp in zip(y_true, y_pred):         # update numerator         numerator += (yt - yp) ** 2         # update denominator         denominator += (yt - mean_true_value) ** 2     # calculate the ratio     ratio = numerator / denominator     # return 1 - ratio     return 1 – ratio ═════════════════════════════════════════════════════════════════════════  There are many more evaluation metrics, and this list is never-ending. I can write a book which is only about different evaluation metrics. Maybe I will. For now, these evaluations metrics will fit almost every problem you want to attempt. Please note that I have implemented these metrics in the most straightforward manner, and that means they are not efficient enough. You can make most of them in a very efficient way by properly using numpy. For example, take a look at the implementation of mean absolute error without any loops.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  def mae_np(y_true, y_pred):     return np.mean(np.abs(y_true - y_pred)) ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe1ce2d9-2533-4258-892f-74feb9652cd1', embedding=None, metadata={'page_label': '71', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 71 I could have implemented all the metrics this way but to learn it’s better to look at low-level implementation. Once you learn the low-level implementation in pure python, and without using a lot of numpy, you can easily convert it to numpy and make it much faster.  Then, there are some advanced metrics.  One of them which is quite widely used is quadratic weighted kappa, also known as QWK. It is also known as Cohen’s kappa. QWK measures the “agreement” between two “ratings”. The ratings can be any real numbers in 0 to N. And predictions are also in the same range. An agreement can be defined as how close these ratings are to each other. So, it’s suitable for a classification problem with N different categories/classes. If the agreement is high, the score is closer towards 1.0. In the case of low agreement, the score is close to 0. Cohen’s kappa has a good implementation in scikit-learn, and detailed discussion of this metric is beyond the scope of this book.  ═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics  In [X]: y_true = [1, 2, 3, 1, 2, 3, 1, 2, 3]  In [X]: y_pred = [2, 1, 3, 1, 2, 3, 3, 1, 2]  In [X]: metrics.cohen_kappa_score(y_true, y_pred, weights=\"quadratic\") Out[X]: 0.33333333333333337  In [X]: metrics.accuracy_score(y_true, y_pred) Out[X]: 0.4444444444444444 ═════════════════════════════════════════════════════════════════════════  You can see that even though accuracy is high, QWK is less. A QWK greater than 0.85 is considered to be very good!  An important metric is Matthew’s Correlation Coefficient (MCC). MCC ranges from -1 to 1. 1 is perfect prediction, -1 is imperfect prediction, and 0 is random prediction. The formula for MCC is quite simple.                TP * TN - FP * FN MCC = ─────────────────────────────────────           [ (TP + FP) * (FN + TN) * (FP + TN) * (TP + FN) ] ^ (0.5) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='280707a5-4913-46f6-b42e-78fc8d978948', embedding=None, metadata={'page_label': '72', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 72 We see that MCC takes into consideration TP, FP, TN and FN and thus can be used for problems where classes are skewed. You can quickly implement it in python by using what we have already implemented.  ═════════════════════════════════════════════════════════════════════════ def mcc(y_true, y_pred):     \"\"\"     This function calculates Matthew\\'s Correlation Coefficient     for binary classification.     :param y_true: list of true values     :param y_pred: list of predicted values     :return: mcc score     \"\"\"     tp = true_positive(y_true, y_pred)     tn = true_negative(y_true, y_pred)     fp = false_positive(y_true, y_pred)     fn = false_negative(y_true, y_pred)      numerator = (tp * tn) - (fp * fn)      denominator = (         (tp + fp) *         (fn + tn) *         (fp + tn) *         (tp + fn)     )      denominator = denominator ** 0.5      return numerator/denominator ═════════════════════════════════════════════════════════════════════════  These are the metrics that can help you get started and will apply to almost every machine learning problem.  One thing to keep in mind is that to evaluate un-supervised methods, for example, some kind of clustering, it’s better to create or manually label the test set and keep it separate from everything that is going on in your modelling part. When you are done with clustering, you can evaluate the performance on the test set simply by using any of the supervised learning metrics.  Once we understand what metric to use for a given problem, we can start looking more deeply into our models for improvements.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='578708b3-fe0d-419a-96c3-ce1e6e70bff9', embedding=None, metadata={'page_label': '73', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 73 Arranging machine learning projects  Finally, we are at a stage where we can start building our very first machine learning models.  Or are we?  Before we start, we must take care of a few things. Please remember that we will work in an IDE/text editor rather than jupyter notebooks. You can also work in jupyter notebooks, and it’s totally up to you. However, I will be using jupyter only for things like data exploration and for plotting charts and graphs. We will build the classification framework in such a way that most problems will become plug n’ play. You will be able to train a model without making too many changes to the code, and when you improve your models, you will be able to track them using git.  Let’s look at the structure of the files first of all. For any project that you are doing, create a new folder. For this example, I am calling the project “project”.  The inside of the project folder should look something like the following.  . ├── input │   ├── train.csv │   └── test.csv ├── src │       ├── create_folds.py │       ├── train.py │       ├── inference.py │       ├── models.py │       ├── config.py │       └── model_dispatcher.py ├── models │       ├── model_rf.bin │       └── model_et.bin ├── notebooks │       ├── exploration.ipynb │       └── check_data.ipynb ├── README.md └── LICENSE ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='408bc66c-c56a-48f5-830a-a08597503a1a', embedding=None, metadata={'page_label': '74', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 74 Let’s see what these folders and file are about.  input/: This folder consists of all the input files and data for your machine learning project. If you are working on NLP projects, you can keep your embeddings here. If you are working on image projects, all images go to a subfolder inside this folder.  src/: We will keep all the python scripts associated with the project here. If I talk about a python script, i.e. any *.py file, it is stored in the src folder.  models/: This folder keeps all the trained models.  notebooks/: All jupyter notebooks (i.e. any *.ipynb file) are stored in the notebooks folder.  README.md: This is a markdown file where you can describe your project and write instructions on how to train the model or to serve this in a production environment.  LICENSE: This is a simple text file that consists of a license for the project, such as MIT, Apache, etc. Going into details of the licenses is beyond the scope of this book.  Let’s assume you are building a model to classify MNIST dataset (a dataset that has been used in almost every machine learning book). If you remember, we touched MNIST dataset in cross-validation chapter too. So, I am not going to explain how this dataset looks like. There are many different formats of MNIST dataset available online, but we will be using the CSV format of the dataset.  In this format of the dataset, each row of the CSV consists of the label of the image and 784 pixel values ranging from 0 to 255. The dataset consists of 60000 images in this format.  We can use pandas to read this data format easily.  Please note that even though Figure 1 shows all pixel values as zeros, it is not the case.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='150bdf55-9a63-4487-a7a6-df628f4a7938', embedding=None, metadata={'page_label': '75', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 75  Figure 1: MNIST dataset in CSV format  Let’s take a look at the counts of the label column in this dataset.  \\n Figure 2: Counts of label in MNIST dataset  We don’t need much more exploration for this dataset. We already know what we have, and there is no need to make plots on different pixel values. From figure 2, it is quite clear that the distribution of labels is quite good and even. We can thus use accuracy/F1 as metrics. This is the first step when approaching a machine learning problem: decide the metric!  Now, we can code a little bit. We need to create the src/ folder and some python scripts.  Please note that the training CSV file is located in the input/ folder and is called mnist_train.csv.  How should these files look like for such a project?  The first script that one should create is create_folds.py.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='498e3815-7504-4679-8b79-6a79ff669061', embedding=None, metadata={'page_label': '76', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 76 This will create a new file in the input/ folder called mnist_train_folds.csv, and it’s the same as mnist_train.csv. The only differences are that this CSV is shuffled and has a new column called kfold.   Once we have decided what kind of evaluation metric we want to use and have created the folds, we are good to go with creating a basic model. This is done in train.py.  ═════════════════════════════════════════════════════════════════════════ # src/train.py import joblib import pandas as pd from sklearn import metrics from sklearn import tree   def run(fold):     # read the training data with folds     df = pd.read_csv(\"../input/mnist_train_folds.csv\")      # training data is where kfold is not equal to provided fold     # also, note that we reset the index     df_train = df[df.kfold != fold].reset_index(drop=True)      # validation data is where kfold is equal to provided fold     df_valid = df[df.kfold == fold].reset_index(drop=True)      # drop the label column from dataframe and convert it to     # a numpy array by using .values.     # target is label column in the dataframe     x_train = df_train.drop(\"label\", axis=1).values     y_train = df_train.label.values      # similarly, for validation, we have     x_valid = df_valid.drop(\"label\", axis=1).values     y_valid = df_valid.label.values      # initialize simple decision tree classifier from sklearn     clf = tree.DecisionTreeClassifier()      # fit the model on training data     clf.fit(x_train, y_train)      # create predictions for validation samples     preds = clf.predict(x_valid)  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7377e3e-9340-4c8b-8565-8e94b717b799', embedding=None, metadata={'page_label': '77', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 77     # calculate & print accuracy     accuracy = metrics.accuracy_score(y_valid, preds)     print(f\"Fold={fold}, Accuracy={accuracy}\")      # save the model     joblib.dump(clf, f\"../models/dt_{fold}.bin\")   if __name__ == \"__main__\":     run(fold=0)     run(fold=1)     run(fold=2)     run(fold=3)     run(fold=4) ═════════════════════════════════════════════════════════════════════════  You can run this script by calling python train.py in the console.  ═════════════════════════════════════════════════════════════════════════ ❯ python train.py Fold=0, Accuracy=0.8680833333333333 Fold=1, Accuracy=0.8685 Fold=2, Accuracy=0.8674166666666666 Fold=3, Accuracy=0.8703333333333333 Fold=4, Accuracy=0.8699166666666667 ═════════════════════════════════════════════════════════════════════════  When you look at the training script, you will see that there are still a few more things that are hardcoded, for example, the fold numbers, the training file and the output folder.  We can thus create a config file with all this information: config.py.  ═════════════════════════════════════════════════════════════════════════ # config.py  TRAINING_FILE = \"../input/mnist_train_folds.csv\"  MODEL_OUTPUT = \"../models/\" ═════════════════════════════════════════════════════════════════════════  And we make some changes to our training script too. The training file utilizes the config file now. Thus making it easier to change data or the model output.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2712826-ea04-4d96-b5ca-f3e55e06f379', embedding=None, metadata={'page_label': '78', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 78 ═════════════════════════════════════════════════════════════════════════ # train.py import os  import config  import joblib import pandas as pd from sklearn import metrics from sklearn import tree   def run(fold):     # read the training data with folds     df = pd.read_csv(config.TRAINING_FILE)      # training data is where kfold is not equal to provided fold     # also, note that we reset the index     df_train = df[df.kfold != fold].reset_index(drop=True)      # validation data is where kfold is equal to provided fold     df_valid = df[df.kfold == fold].reset_index(drop=True)      # drop the label column from dataframe and convert it to     # a numpy array by using .values.     # target is label column in the dataframe     x_train = df_train.drop(\"label\", axis=1).values     y_train = df_train.label.values      # similarly, for validation, we have     x_valid = df_valid.drop(\"label\", axis=1).values     y_valid = df_valid.label.values      # initialize simple decision tree classifier from sklearn     clf = tree.DecisionTreeClassifier()      # fir the model on training data     clf.fit(x_train, y_train)      # create predictions for validation samples     preds = clf.predict(x_valid)      # calculate & print accuracy     accuracy = metrics.accuracy_score(y_valid, preds)     print(f\"Fold={fold}, Accuracy={accuracy}\")      # save the model ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f13abb9-42ac-4e41-8091-e16473b9d05f', embedding=None, metadata={'page_label': '79', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 79     joblib.dump(         clf,          os.path.join(config.MODEL_OUTPUT, f\"dt_{fold}.bin\")     )   if __name__ == \"__main__\":     run(fold=0)     run(fold=1)     run(fold=2)     run(fold=3)     run(fold=4) ═════════════════════════════════════════════════════════════════════════  Please note that I am not showing the difference between this training script and the one before. Please take a careful look at both of them and find the differences yourself. There aren’t many of them.  There is still one more thing related to the training script that can be improved. As you can see, we call the run function multiple times for every fold. Sometimes it’s not advisable to run multiple folds in the same script as the memory consumption may keep increasing, and your program may crash. To take care of this problem, we can pass arguments to the training script. I like doing it using argparse.  ═════════════════════════════════════════════════════════════════════════ # train.py import argparse . . .  if __name__ == \"__main__\":     # initialize ArgumentParser class of argparse     parser = argparse.ArgumentParser()      # add the different arguments you need and their type     # currently, we only need fold     parser.add_argument(         \"--fold\",         type=int     )     # read the arguments from the command line     args = parser.parse_args()      # run the fold specified by command line arguments ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b02e8fcd-c412-4012-8bee-d29f77448d63', embedding=None, metadata={'page_label': '80', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 80     run(fold=args.fold) ═════════════════════════════════════════════════════════════════════════  Now, we can run the python script again, but only for a given fold.  ═════════════════════════════════════════════════════════════════════════ ❯ python train.py --fold 0 Fold=0, Accuracy=0.8656666666666667 ═════════════════════════════════════════════════════════════════════════  If you see carefully, our fold 0 score was a bit different before. This is because of the randomness in the model. We will come to handling randomness in later chapters.  Now, if you want, you can create a shell script with different commands for different folds and run them all together, as shown below.  ═════════════════════════════════════════════════════════════════════════ #!/bin/sh  python train.py --fold 0 python train.py --fold 1 python train.py --fold 2 python train.py --fold 3 python train.py --fold 4 ═════════════════════════════════════════════════════════════════════════  And you can run this by the following command.  ═════════════════════════════════════════════════════════════════════════ ❯ sh run.sh Fold=0, Accuracy=0.8675 Fold=1, Accuracy=0.8693333333333333 Fold=2, Accuracy=0.8683333333333333 Fold=3, Accuracy=0.8704166666666666 Fold=4, Accuracy=0.8685 ═════════════════════════════════════════════════════════════════════════  We have made quite some progress now, but if we look at our training script, we still are limited by a few things, for example, the model. The model is hardcoded in the training script, and the only way to change it is to modify the script. So, we will create a new python script called model_dispatcher.py. model_dispatcher.py, as the name suggests, will dispatch our models to our training script. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9f31a6a-06eb-46a6-8034-266f177ec6d1', embedding=None, metadata={'page_label': '81', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 81 ═════════════════════════════════════════════════════════════════════════ # model_dispatcher.py from sklearn import tree   models = {     \"decision_tree_gini\": tree.DecisionTreeClassifier(         criterion=\"gini\"     ),     \"decision_tree_entropy\": tree.DecisionTreeClassifier(         criterion=\"entropy\"     ), } ═════════════════════════════════════════════════════════════════════════  model_dispatcher.py imports tree from scikit-learn and defines a dictionary with keys that are names of the models and values are the models themselves. Here, we define two different decision trees, one with gini criterion and one with entropy. To use model_dispatcher.py, we need to make a few changes to our training script.  ═════════════════════════════════════════════════════════════════════════ # train.py import argparse import os  import joblib import pandas as pd from sklearn import metrics  import config import model_dispatcher   def run(fold, model):     # read the training data with folds     df = pd.read_csv(config.TRAINING_FILE)      # training data is where kfold is not equal to provided fold     # also, note that we reset the index     df_train = df[df.kfold != fold].reset_index(drop=True)      # validation data is where kfold is equal to provided fold     df_valid = df[df.kfold == fold].reset_index(drop=True)      # drop the label column from dataframe and convert it to     # a numpy array by using .values. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ee7843a-f773-46b8-bd02-31f7879cf050', embedding=None, metadata={'page_label': '82', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 82     # target is label column in the dataframe     x_train = df_train.drop(\"label\", axis=1).values     y_train = df_train.label.values      # similarly, for validation, we have     x_valid = df_valid.drop(\"label\", axis=1).values     y_valid = df_valid.label.values      # fetch the model from model_dispatcher     clf = model_dispatcher.models[model]      # fir the model on training data     clf.fit(x_train, y_train)      # create predictions for validation samples     preds = clf.predict(x_valid)      # calculate & print accuracy     accuracy = metrics.accuracy_score(y_valid, preds)     print(f\"Fold={fold}, Accuracy={accuracy}\")      # save the model     joblib.dump(         clf,          os.path.join(config.MODEL_OUTPUT, f\"dt_{fold}.bin\")     )   if __name__ == \"__main__\":     parser = argparse.ArgumentParser()      parser.add_argument(         \"--fold\",         type=int     )     parser.add_argument(         \"--model\",         type=str     )      args = parser.parse_args()      run(         fold=args.fold,         model=args.model     ) ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d80d9bf9-f5b6-4c97-8182-b919f6bc164f', embedding=None, metadata={'page_label': '83', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 83 There are a few major changes to train.py: • import model_dispatcher • add --model argument to ArgumentParser • add model argument to run() function • use the dispatcher to fetch the model given the name  Now, we can run the script using the following command:  ═════════════════════════════════════════════════════════════════════════ ❯ python train.py --fold 0 --model decision_tree_gini Fold=0, Accuracy=0.8665833333333334 ═════════════════════════════════════════════════════════════════════════  Or the following command  ═════════════════════════════════════════════════════════════════════════ ❯ python train.py --fold 0 --model decision_tree_entropy Fold=0, Accuracy=0.8705833333333334 ═════════════════════════════════════════════════════════════════════════  Now, if you add a new model, all you have to do is make changes to model_dispatcher.py. Let’s try adding random forest and see what happens to our accuracy.  ═════════════════════════════════════════════════════════════════════════ # model_dispatcher.py from sklearn import ensemble from sklearn import tree   models = {     \"decision_tree_gini\": tree.DecisionTreeClassifier(         criterion=\"gini\"     ),     \"decision_tree_entropy\": tree.DecisionTreeClassifier(         criterion=\"entropy\"     ),     \"rf\": ensemble.RandomForestClassifier(), } ═════════════════════════════════════════════════════════════════════════  Let’s run this code. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac7d65f8-8c6e-476d-bb84-b66befe0e6a7', embedding=None, metadata={'page_label': '84', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 84 ═════════════════════════════════════════════════════════════════════════ ❯ python train.py --fold 0 --model rf Fold=0, Accuracy=0.9670833333333333 ═════════════════════════════════════════════════════════════════════════  Wow, a simple change gave such a massive improvement in the score! Let’s run all 5 folds using our run.sh script now!  ═════════════════════════════════════════════════════════════════════════ #!/bin/sh  python train.py --fold 0 --model rf python train.py --fold 1 --model rf python train.py --fold 2 --model rf python train.py --fold 3 --model rf python train.py --fold 4 --model rf ═════════════════════════════════════════════════════════════════════════  And the scores look like the following.  ═════════════════════════════════════════════════════════════════════════ ❯ sh run.sh Fold=0, Accuracy=0.9674166666666667 Fold=1, Accuracy=0.9698333333333333 Fold=2, Accuracy=0.96575 Fold=3, Accuracy=0.9684166666666667 Fold=4, Accuracy=0.9666666666666667 ═════════════════════════════════════════════════════════════════════════  MNIST is a problem that is discussed in almost every book and every blog. But I tried to convert this problem to more fun and show you how to write a basic framework for almost any machine learning project you are doing, or you plan to do in the near future. There are many different ways to improve on this MNIST model and also this framework, and we will see that in future chapters.  I used some scripts like model_dispatcher.py and config.py and imported them in my training script. Please note that I did not import * and neither should you. If I had imported *, you would have never known where the models dictionary came from. Writing good, understandable code is an essential quality one can have, and many data scientists ignore it. If you work on a project that others can understand and use without consulting you, you save their time and your own time and can invest that time to improve your project or work on a new one.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65df5440-77aa-46a3-9d8d-4fd6d6bc39fb', embedding=None, metadata={'page_label': '85', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 85 Approaching categorical variables  Many people struggle a lot with the handling of categorical variables, and thus this deserves a full chapter. In this chapter, I will talk about different types of categorical data and how to approach a problem with categorical variables.  What are categorical variables?  Categorical variables/features are any feature type  can be classified into two major types: • Nominal • Ordinal  Nominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.  Ordinal variables, on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.  As far as definitions are concerned, we can also categorize categorical variables as binary, i.e., a categorical variable with only two categories. Some even talk about a type called “cyclic” for categorical variables. Cyclic variables are present in “cycles” for example, days in a week: Sunday, Monday, Tuesday, Wednesday, Thursday, Friday and Saturday. After Saturday, we have Sunday again. This is a cycle. Another example would be hours in a day if we consider them to be categories.  There are many different definitions of categorical variables, and many people talk about handling categorical variables differently depending on the type of categorical variable. However, I do not see any need for it. All problems with categorical variables can be approached in the same way.  Before we start, we need a dataset to work with (as always). One of the best free datasets to understand categorical variables is cat-in-the-dat from Categorical Features Encoding Challenge from Kaggle. There were two challenges, and we will be using the data from the second challenge as it had more variables and was more difficult than its previous version. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f662b86-0707-4e97-b00b-01d7a2d152a6', embedding=None, metadata={'page_label': '86', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 86 Let’s take a look at the data.  \\n Figure 1: Viewing a subset of the data. Cat-in-the-dat-ii challenge5  The dataset consists of all kinds of categorical variables:  • Nominal • Ordinal • Cyclical • Binary  In Figure 1, we see only a subset of all the variables that are present and the target variable.   It is a binary classification problem.   The target is not very important for us to learn categorical variables, but in the end, we will be building an end-to-end model so let’s take a look at the target distribution in figure 2. We see that the target is skewed and thus the best metric for this binary classification problem would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.  5 https://www.kaggle.com/c/cat-in-the-dat-ii \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8206ec39-5789-45e5-a7aa-f2140fa88d2c', embedding=None, metadata={'page_label': '87', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 87  Figure 2: Count of targets. the x-axis shows the label, and the y-axis shows the count of the label  Overall, there are: • Five binary variables • Ten nominal variables • Six ordinal variables • Two cyclic variables • And a target variable  Let’s look at ord_2 feature in the dataset. It consists of six different categories: • Freezing • Warm • Cold • Boiling Hot • Hot • Lava Hot  We have to know that computers do not understand text data and thus, we need to convert these categories to numbers. A simple way of doing this would be to create a dictionary that maps these values to numbers starting from 0 to N-1, where N is the total number of categories in a given feature.   \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa69bd30-ea7d-43d2-9f03-3b990b3acb4d', embedding=None, metadata={'page_label': '88', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 88 ═════════════════════════════════════════════════════════════════════════ mapping = {     \"Freezing\": 0,     \"Warm\": 1,     \"Cold\": 2,     \"Boiling Hot\": 3,     \"Hot\": 4,     \"Lava Hot\": 5   } ═════════════════════════════════════════════════════════════════════════  Now, we can read the dataset and convert these categories to numbers easily.   ═════════════════════════════════════════════════════════════════════════ import pandas as pd  df = pd.read_csv(\"../input/cat_train.csv\")  df.loc[:, \"ord_2\"] = df.ord_2.map(mapping) ═════════════════════════════════════════════════════════════════════════  Value counts before mapping:  ═════════════════════════════════════════════════════════════════════════ df.ord_2.value_counts()  Freezing       142726 Warm           124239 Cold            97822 Boiling Hot     84790 Hot             67508 Lava Hot        64840 Name: ord_2, dtype: int64 ═════════════════════════════════════════════════════════════════════════  Value counts after mapping:  ═════════════════════════════════════════════════════════════════════════ 0.0    142726 1.0    124239 2.0     97822 3.0     84790 4.0     67508 5.0     64840 Name: ord_2, dtype: int64 ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f684911-e096-467e-9c78-2fa2d1418257', embedding=None, metadata={'page_label': '89', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 89 This type of encoding of categorical variables is known as Label Encoding, i.e., we are encoding every category as a numerical label. We can do the same by using LabelEncoder from scikit-learn. ═════════════════════════════════════════════════════════════════════════ import pandas as pd from sklearn import preprocessing  # read the data df = pd.read_csv(\"../input/cat_train.csv\")  # fill NaN values in ord_2 column df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\")  # initialize LabelEncoder lbl_enc = preprocessing.LabelEncoder()  # fit label encoder and transform values on ord_2 column # P.S: do not use this directly. fit first, then transform df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values) ═════════════════════════════════════════════════════════════════════════  You will see that I use fillna from pandas. The reason is LabelEncoder from scikit-learn does not handle NaN values, and ord_2 column has NaN values in it.  We can use this directly in many tree-based models: • Decision trees • Random forest • Extra Trees • Or any kind of boosted trees model o XGBoost o GBM o LightGBM This type of encoding cannot be used in linear models, support vector machines or neural networks as they expect data to be normalized (or standardized).  For these types of models, we can binarize the data.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ba89d357-ba05-4fd8-8327-878e92ce2c88', embedding=None, metadata={'page_label': '90', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 90 ═════════════════════════════════════════════════════════════════════════ Freezing    --> 0 --> 0 0 0 Warm        --> 1 --> 0 0 1 Cold        --> 2 --> 0 1 0 Boiling Hot --> 3 --> 0 1 1 Hot         --> 4 --> 1 0 0 Lava Hot    --> 5 --> 1 0 1 ═════════════════════════════════════════════════════════════════════════  This is just converting the categories to numbers and then converting them to their binary representation. We are thus splitting one feature into three (in this case) features (or columns). If we have more categories, we might end up splitting into a lot more columns.  It becomes easy to store lots of binarized variables like this if we store them in a sparse format. A sparse format is nothing but a representation or way of storing data in memory in which you do not store all the values but only the values that matter. In the case of binary variables described above, all that matters is where we have ones (1s).   It’s difficult to imagine a format like this but should become clear with an example.  Let’s assume that we are provided with only one feature in the dataframe above: ord_2.   Index Feature 0 Warm 1 Hot 2 Lava hot  Currently, we are looking at only three samples in the dataset. Let’s convert this to binary representation where we have three items for each sample.   These three items are the three features.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2668c9f-ae9d-418e-844f-3456f337e781', embedding=None, metadata={'page_label': '91', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 91 Index Feature_0 Feature_1 Feature_2 0 0 0 1 1 1 0 0 2 1 0 1  So, our features are stored in a matrix which has 3 rows and 3 columns - 3x3. Each element of this matrix occupies 8 bytes. So, our total memory requirement for this array is 8x3x3 = 72 bytes.  We can also check this using a simple python snippet.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  # create our example feature matrix example = np.array(     [         [0, 0, 1],         [1, 0, 0],         [1, 0, 1]     ] )  # print size in bytes print(example.nbytes) ═════════════════════════════════════════════════════════════════════════  This code will print 72 as we calculated before. But do we need to store all the elements of this matrix? No. As mentioned before we are only interested in 1s. 0s are not that important because anything multiplied with 0 will be zero and 0 added/subtracted to/from anything doesn’t make any difference. One way to represent this matrix only with ones would be some kind of dictionary method in which keys are indices of rows and columns and value is 1:    ═════════════════════════════════════════════════════════════════════════   (0, 2) 1   (1, 0) 1   (2, 0) 1   (2, 2) 1 ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='50b7405f-e42d-4892-854c-8f3bdf33f558', embedding=None, metadata={'page_label': '92', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 92 A notation like this will occupy much less memory because it has to store only four values (in this case). The total memory used will be 8x4 = 32 bytes. Any numpy array can be converted to a sparse matrix by simple python code.  ═════════════════════════════════════════════════════════════════════════ import numpy as np from scipy import sparse  # create our example feature matrix example = np.array(     [         [0, 0, 1],         [1, 0, 0],         [1, 0, 1]     ] )  # convert numpy array to sparse CSR matrix sparse_example = sparse.csr_matrix(example)  # print size of this sparse matrix print(sparse_example.data.nbytes) ═════════════════════════════════════════════════════════════════════════  This will print 32, which is so less than our dense array! The total size of the sparse csr matrix is the sum of three values.  ═════════════════════════════════════════════════════════════════════════ print(     sparse_example.data.nbytes +      sparse_example.indptr.nbytes +      sparse_example.indices.nbytes ) ═════════════════════════════════════════════════════════════════════════  This will print 64, which is still less than our dense array. Unfortunately, I will not go into the details of these elements. You can read more about them in scipy docs. The difference in size becomes vast when we have much larger arrays, let’s say with thousands of samples and tens of thousands of features. For example, a text dataset where we are using count-based features.  ═════════════════════════════════════════════════════════════════════════ import numpy as np from scipy import sparse ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58f03042-5c44-4a32-8c7d-2d14b0d204ce', embedding=None, metadata={'page_label': '93', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 93 # number of rows n_rows = 10000  # number of columns n_cols = 100000  # create random binary matrix with only 5% values as 1s example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))  # print size in bytes print(f\"Size of dense array: {example.nbytes}\")  # convert numpy array to sparse CSR matrix sparse_example = sparse.csr_matrix(example)  # print size of this sparse matrix print(f\"Size of sparse array: {sparse_example.data.nbytes}\")  full_size = (     sparse_example.data.nbytes +      sparse_example.indptr.nbytes +      sparse_example.indices.nbytes )  # print full size of this sparse matrix print(f\"Full size of sparse array: {full_size}\") ═════════════════════════════════════════════════════════════════════════  This prints:  Size of dense array: 8000000000 Size of sparse array: 399932496 Full size of sparse array: 599938748  So, dense array takes ~8000MB or approximately 8GB of memory. The sparse array, on the other hand, takes only 399MB of memory.  And, that’s why we prefer sparse arrays over dense whenever we have a lot of zeros in our features.   Please note that there are many different ways of representing a sparse matrix. Here I have shown only one such (and probably the most popular) way. Going deep into these is beyond the scope of this book and is left as an exercise to the reader.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='866f3779-c99b-49a4-aae4-da70b0f5aaee', embedding=None, metadata={'page_label': '94', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 94 Even though the sparse representation of binarized features takes much less memory than its dense representation, there is another transformation for categorical variables that takes even less memory. This is known as One Hot Encoding.  One hot encoding is a binary encoding too in the sense that there are only two values, 0s and 1s. However, it must be noted that it’s not a binary representation. Its representation can be understood by looking at the following example.  Suppose we represent each category of the ord_2 variable by a vector. This vector is of the same size as the number of categories in the ord_2 variable. In this specific case, each vector is of size six and has all zeros except at one position. Let’s look at this particular table of vectors.  Freezing 0 0 0 0 0 1 Warm 0 0 0 0 1 0 Cold 0 0 0 1 0 0 Boiling Hot 0 0 1 0 0 0 Hot 0 1 0 0 0 0 Lava Hot 1 0 0 0 0 0  We see that the size of vectors is 1x6, i.e. there are six elements in the vector. Where does this number come from? If you look carefully, you will see that there are six categories, as mentioned before. When one-hot encoding, the vector size has to be same as the number of categories we are looking at. Each vector has a 1 and rest all other values are 0s. Now, let’s use these features instead of the binarized feature as before and see how much memory can we save.  If you remember the old data, it looked as follows:  Index Feature 0 Warm 1 Hot 2 Lava hot ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='067eaaca-a950-4754-bfe0-bf2eb8e501b7', embedding=None, metadata={'page_label': '95', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 95 And we had three features for each sample. But one-hot vectors, in this case, are of size 6. Thus, we have six features instead of 3.  Index F_0 F_1 F_2 F_3 F_4 F_5 0 0 0 0 0 1 0 1 0 1 0 0 0 0 2 1 0 0 0 0 0  So, we have six features, and in this 3x6 array, there are only 3 ones. Finding size using numpy is very similar to the binarization size calculation script. All you need to change is the array. Let’s take a look at this code.  ═════════════════════════════════════════════════════════════════════════ import numpy as np from scipy import sparse # create binary matrix example = np.array(     [         [0, 0, 0, 0, 1, 0],         [0, 1, 0, 0, 0, 0],         [1, 0, 0, 0, 0, 0]     ] )  # print size in bytes print(f\"Size of dense array: {example.nbytes}\")  # convert numpy array to sparse CSR matrix sparse_example = sparse.csr_matrix(example)  # print size of this sparse matrix print(f\"Size of sparse array: {sparse_example.data.nbytes}\")  full_size = (     sparse_example.data.nbytes +      sparse_example.indptr.nbytes +      sparse_example.indices.nbytes )  # print full size of this sparse matrix print(f\"Full size of sparse array: {full_size}\") ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b25554f-e7b1-47a1-9054-af214310ea8d', embedding=None, metadata={'page_label': '96', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 96 This will print the sizes as:  Size of dense array: 144 Size of sparse array: 24 Full size of sparse array: 52  We see that the dense array size is much larger than the one with binarization. However, the size of the sparse array is much less. Let’s try this with a much larger array. In this example, we will use OneHotEncoder from scikit-learn to transform our feature array with 1001 categories into dense and sparse matrices.  ═════════════════════════════════════════════════════════════════════════ import numpy as np from sklearn import preprocessing  # create random 1-d array with 1001 different categories (int) example = np.random.randint(1000, size=1000000)  # initialize OneHotEncoder from scikit-learn # keep sparse = False to get dense array ohe = preprocessing.OneHotEncoder(sparse=False)  # fit and transform data with dense one hot encoder ohe_example = ohe.fit_transform(example.reshape(-1, 1))  # print size in bytes for dense array print(f\"Size of dense array: {ohe_example.nbytes}\")  # initialize OneHotEncoder from scikit-learn # keep sparse = True to get sparse array ohe = preprocessing.OneHotEncoder(sparse=True)  # fit and transform data with sparse one-hot encoder ohe_example = ohe.fit_transform(example.reshape(-1, 1))  # print size of this sparse matrix print(f\"Size of sparse array: {ohe_example.data.nbytes}\")  full_size = (     ohe_example.data.nbytes +      ohe_example.indptr.nbytes + ohe_example.indices.nbytes )  # print full size of this sparse matrix print(f\"Full size of sparse array: {full_size}\") ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9340b199-7ed4-4b0d-ac2d-347d753f064b', embedding=None, metadata={'page_label': '97', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 97 And this code prints:  Size of dense array: 8000000000 Size of sparse array: 8000000 Full size of sparse array: 16000004  Dense array size here is approximately 8GB and sparse array is 8MB. If you had a choice, which one would you choose? Seems like a quite simple choice to me, isn’t it?  These three methods are the most important ways to handle categorical variables. There are, however, many other different methods you can use to handle categorical variables. An example of one such method is about converting categorical variables to numerical variables.  Suppose we go back to the categorical features dataframe (original cat-in-the-dat-ii) that we had. How many ids do we have in the dataframe where the value of ord_2 is Boiling Hot ?   We can easily calculate this value by calculating the shape of the dataframe where ord_2 column has the value Boiling Hot.  ═════════════════════════════════════════════════════════════════════════ In [X]: df[df.ord_2 == \"Boiling Hot\"].shape Out[X]: (84790, 25) ═════════════════════════════════════════════════════════════════════════  We see that there are 84790 rows with this value. We can also calculate this value for all the categories using groupby in pandas.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.groupby([\"ord_2\"])[\"id\"].count() Out[X]: ord_2 Boiling Hot     84790 Cold            97822 Freezing       142726 Hot             67508 Lava Hot        64840 Warm           124239 Name: id, dtype: int64 ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f622faec-dd43-4c15-8254-6870a0785fe3', embedding=None, metadata={'page_label': '98', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 98 If we just replace ord_2 column with its count values, we have converted it to a feature which is kind of numerical now. We can create a new column or replace this column by using the transform function of pandas along with groupby.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.groupby([\"ord_2\"])[\"id\"].transform(\"count\") Out[X]: 0          67508.0 1         124239.0 2         142726.0 3          64840.0 4          97822.0             ... 599995    142726.0 599996     84790.0 599997    142726.0 599998    124239.0 599999     84790.0 Name: id, Length: 600000, dtype: float64 ═════════════════════════════════════════════════════════════════════════  You can add counts of all the features or can also replace them or maybe group by multiple columns and their counts. For example, the following code counts by grouping on ord_1 and ord_2 columns.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.groupby(    ...:     [    ...:         \"ord_1\",    ...:         \"ord_2\"    ...:     ]    ...: )[\"id\"].count().reset_index(name=\"count\") Out[X]:           ord_1        ord_2  count 0   Contributor  Boiling Hot  15634 1   Contributor         Cold  17734 2   Contributor     Freezing  26082 3   Contributor          Hot  12428 4   Contributor     Lava Hot  11919 5   Contributor         Warm  22774 6        Expert  Boiling Hot  19477 7        Expert         Cold  22956 8        Expert     Freezing  33249 9        Expert          Hot  15792 10       Expert     Lava Hot  15078 11       Expert         Warm  28900 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1be1b7e1-8338-4914-bcb6-bea627b99175', embedding=None, metadata={'page_label': '99', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 99 12  Grandmaster  Boiling Hot  13623 13  Grandmaster         Cold  15464 14  Grandmaster     Freezing  22818 15  Grandmaster          Hot  10805 16  Grandmaster     Lava Hot  10363 17  Grandmaster         Warm  19899 18       Master  Boiling Hot  10800 . . . . ═════════════════════════════════════════════════════════════════════════  Please note that I have eliminated some rows from the output to fit them in one page. This is another kind of count that you can add as a feature. You must have noted by now that I am using the id column for counts. You can, however, also count other columns by grouping by on combinations of the columns.  One more trick is to create new features from these categorical variables. You can create new categorical features from existing features, and this can be done in an effortless manner.  ═════════════════════════════════════════════════════════════════════════ In [X]: df[\"new_feature\"] = (    ...:     df.ord_1.astype(str)    ...:     + \"_\"    ...:     + df.ord_2.astype(str)    ...: )  In [X]: df.new_feature  Out[X]: 0                 Contributor_Hot 1                Grandmaster_Warm 2                    nan_Freezing 3                 Novice_Lava Hot 4                Grandmaster_Cold                    ... 599995            Novice_Freezing 599996         Novice_Boiling Hot 599997       Contributor_Freezing 599998                Master_Warm 599999    Contributor_Boiling Hot Name: new_feature, Length: 600000, dtype: object ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0df7aee3-e843-466c-891a-77165f4b7e24', embedding=None, metadata={'page_label': '100', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 100 Here, we have combined ord_1 and ord_2 by an underscore, and before that, we convert these columns to string types. Note that NaN will also convert to string. But it’s okay. We can also treat NaN as a new category. Thus, we have a new feature which is a combination of these two features. You can also combine more than three columns or four or even more.  ═════════════════════════════════════════════════════════════════════════ In [X]: df[\"new_feature\"] = (     ...:     df.ord_1.astype(str)     ...:     + \"_\"     ...:     + df.ord_2.astype(str)     ...:     + \"_\"     ...:     + df.ord_3.astype(str)     ...: )  In [X]: df.new_feature Out[X]: 0                 Contributor_Hot_c 1                Grandmaster_Warm_e 2                    nan_Freezing_n 3                 Novice_Lava Hot_a 4                Grandmaster_Cold_h                     ... 599995            Novice_Freezing_a 599996         Novice_Boiling Hot_n 599997       Contributor_Freezing_n 599998                Master_Warm_m 599999    Contributor_Boiling Hot_b Name: new_feature, Length: 600000, dtype: object ═════════════════════════════════════════════════════════════════════════  So which categories should we combine? Well, there isn\\'t an easy answer to that. It depends on your data and the types of features. Some domain knowledge might be useful for creating features like this. But if you don’t have concerns about memory and CPU usage, you can go for a greedy approach where you can create many such combinations and then use a model to decide which features are useful and keep them. We will read about it later in this book.  Whenever you get categorical variables, follow these simple steps: • fill the NaN values (this is very important!) • convert them to integers by applying label encoding using LabelEncoder of scikit-learn or by using a mapping dictionary. If you didn’t fill up NaN values with something, you might have to take care of them in this step ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc6bd20e-0db5-42d5-9d71-aa85f2a9c275', embedding=None, metadata={'page_label': '101', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 101 • create one-hot encoding. Yes, you can skip binarization! • go for modelling! I mean the machine learning one. Not on the ramp.  Handling NaN data in categorical features is quite essential else you can get the infamous error from scikit-learn’s LabelEncoder:  ValueError: y contains previously unseen labels: [nan, nan, nan, nan, nan, nan, nan, nan]  This simply means that when you are transforming the test data, you have NaN values in it. It’s because you forgot to handle them during training. One simple way to handle NaN values would be to drop them. Well, it’s simple but not ideal. NaN values may have a lot of information in them, and you will lose it if you just drop these values. There might also be many situations where most of your data has NaN values, and thus, you cannot drop rows/samples with NaN values. Another way of handling NaN values is to treat them as a completely new category. This is the most preferred way of handling NaN values. And can be achieved in a very simple manner if you are using pandas.  Check this out on ord_2 column of the data we have been looking at till now.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.ord_2.value_counts() Out[X]: Freezing       142726 Warm           124239 Cold            97822 Boiling Hot     84790 Hot             67508 Lava Hot        64840 Name: ord_2, dtype: int64 ═════════════════════════════════════════════════════════════════════════  And after filling the NaN values, it becomes:  ═════════════════════════════════════════════════════════════════════════ In [X]: df.ord_2.fillna(\"NONE\").value_counts() Out[X]: Freezing       142726 Warm           124239 Cold            97822 Boiling Hot     84790 Hot             67508 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='062cd112-0021-45d2-950a-8fca8fa33b59', embedding=None, metadata={'page_label': '102', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 102 Lava Hot        64840 NONE            18075 Name: ord_2, dtype: int64 ═════════════════════════════════════════════════════════════════════════  Wow! There were 18075 NaN values in this column that we didn’t even consider using previously. With the addition of this new category, the total number of categories have now increased from 6 to 7. This is okay because now when we build our models, we will also consider NaN. The more relevant information we have, the better the model is.  Let’s assume that ord_2 did not have any NaN values. We see that all categories in this column have a significant count. There are no “rare” categories; i.e. the categories which appear only a small percentage of the total number of samples. Now, let’s assume that you have deployed this model which uses this column in production and when the model or the project is live, you get a category in ord_2 column that is not present in train. You model pipeline, in this case, will throw an error and there is nothing that you can do about it. If this happens, then probably something is wrong with your pipeline in production. If this is expected, then you must modify your model pipeline and include a new category to these six categories.   This new category is known as the “rare” category. A rare category is a category which is not seen very often and can include many different categories. You can also try to “predict” the unknown category by using a nearest neighbour model. Remember, if you predict this category, it will become one of the categories from the training data. \\n Figure 3: An illustration of a data set with different features and no targets where one feature might assume a new value when it’s seen in the test set or live data  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93c123d2-769b-41c1-8f5f-cfa0cac1217a', embedding=None, metadata={'page_label': '103', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 103 When we have a dataset like as shown in figure 3, we can build a simple model that’s trained on all features except “f3”. Thus, you will be creating a model that predicts “f3” when it’s not known or not available in training. I can’t say if this kind of model is going to give you an excellent performance but might be able to handle those missing values in test set or live data and one can’t say without trying just like everything else when it comes to machine learning.  If you have a fixed test set, you can add your test data to training to know about the categories in a given feature. This is very similar to semi-supervised learning in which you use data which is not available for training to improve your model. This will also take care of rare values that appear very less number of times in training data but are in abundance in test data. Your model will be more robust.   Many people think that this idea overfits. It may or may not overfit. There is a simple fix for that. If you design your cross-validation in such a way that it replicates the prediction process when you run your model on test data, then it’s never going to overfit. It means that the first step should be the separation of folds, and in each fold, you should apply the same pre-processing that you want to apply to test data. Suppose you want to concatenate training and test data, then in each fold you must concatenate training and validation data and also make sure that your validation dataset replicates the test set. In this specific case, you must design your validation sets in such a way that it has categories which are “unseen” in the training set.  \\n Figure 4: A simple concatenation of training and test sets to learn about the categories present in the test set but not in the training set or rare categories in the training set. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58402967-04c9-445a-9e98-6234d722020b', embedding=None, metadata={'page_label': '104', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 104 How this works is can be understood easily by looking at figure 4 and the following code.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd from sklearn import preprocessing  # read training data train = pd.read_csv(\"../input/cat_train.csv\")  #read test data test = pd.read_csv(\"../input/cat_test.csv\")  # create a fake target column for test data # since this column doesn\\'t exist test.loc[:, \"target\"] = -1  # concatenate both training and test data data = pd.concat([train, test]).reset_index(drop=True)  # make a list of features we are interested in # id and target is something we should not encode features = [x for x in train.columns if x not in [\"id\", \"target\"]]  # loop over the features list for feat in features:     # create a new instance of LabelEncoder for each feature     lbl_enc = preprocessing.LabelEncoder()          # note the trick here     # since its categorical data, we fillna with a string     # and we convert all the data to string type     # so, no matter its int or float, its converted to string     # int/float but categorical!!!     temp_col = data[feat].fillna(\"NONE\").astype(str).values      # we can use fit_transform here as we do not     # have any extra test data that we need to     # transform on separately     data.loc[:, feat] = lbl_enc.fit_transform(temp_col)       # split the training and test data again     train = data[data.target != -1].reset_index(drop=True) test = data[data.target == -1].reset_index(drop=True) ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e073497b-f403-4064-9d21-48e9a1d6bd56', embedding=None, metadata={'page_label': '105', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 105 This trick works when you have a problem where you already have the test dataset. It must be noted that this trick will not work in a live setting. For example, let’s say you are in a company that builds a real-time bidding solution (RTB). RTB systems bid on every user they see online to buy ad space. The features that can be used for such a model may include pages viewed in a website. Let’s assume that features are the last five categories/pages visited by the user. In this case, if the website introduces new categories, we will no longer be able to predict accurately. Our model, in this case, will fail. A situation like this can be avoided by using an “unknown” category.   In our cat-in-the-dat dataset, we already have unknowns in ord_2 column.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.ord_2.fillna(\"NONE\").value_counts() Out[X]: Freezing       142726 Warm           124239 Cold            97822 Boiling Hot     84790 Hot             67508 Lava Hot        64840 NONE            18075 Name: ord_2, dtype: int64 ═════════════════════════════════════════════════════════════════════════ We can treat “NONE” as unknown. So, if during live testing, we get new categories that we have not seen before, we will mark them as “NONE”.  This is very similar to natural language processing problems. We always build a model based on a fixed vocabulary. Increasing the size of the vocabulary increases the size of the model. Transformer models like BERT are trained on ~30000 words (for English). So, when we have a new word coming in, we mark it as UNK (unknown).  So, you can either assume that your test data will have the same categories as training or you can introduce a rare or unknown category to training to take care of new categories in test data.  Let’s see the value counts in ord_4 column after filling NaN values:  ═════════════════════════════════════════════════════════════════════════ In [X]: df.ord_4.fillna(\"NONE\").value_counts() ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e35a53a-f1d6-4c77-be5f-eac170a550bb', embedding=None, metadata={'page_label': '106', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 106 Out[X]: N       39978 P       37890 Y       36657 A       36633 R       33045 U       32897 . . . K       21676 I       19805 NONE    17930 D       17284 F       16721 W        8268 Z        5790 S        4595 G        3404 V        3107 J        1950 L        1657 Name: ord_4, dtype: int64 ═════════════════════════════════════════════════════════════════════════  We see that some values appear only a couple thousand times, and some appear almost 40000 times. NaNs are also seen a lot. Please note that I have removed some values from the output.   We can now define our criteria for calling a value “rare”. Let’s say the requirement for a value being rare in this column is a count of less than 2000. So, it seems, J and L can be marked as rare values. With pandas, it is quite easy to replace categories based on count threshold. Let’s take a look at how it’s done.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.ord_4 = df.ord_4.fillna(\"NONE\")  In [X]: df.loc[    ...:     df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000,    ...:     \"ord_4\"    ...: ] = \"RARE\"  In [X]: df.ord_4.value_counts() Out[X]: N       39978 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00fc3855-e22c-4abe-901d-19c49db8c369', embedding=None, metadata={'page_label': '107', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 107 P       37890 Y       36657 A       36633 R       33045 U       32897 M       32504 . . . B       25212 E       21871 K       21676 I       19805 NONE    17930 D       17284 F       16721 W        8268 Z        5790 S        4595 RARE     3607 G        3404 V        3107 Name: ord_4, dtype: int64  ═════════════════════════════════════════════════════════════════════════  We say that wherever the value count for a certain category is less than 2000, replace it with rare. So, now, when it comes to test data, all the new, unseen categories will be mapped to “RARE”, and all missing values will be mapped to “NONE”.  This approach will also ensure that the model works in a live setting, even if you have new categories.  Now we have everything we need to approach any kind of problem with categorical variables in it. Let’s try building our first model and try to improve its performance in a step-wise manner.  Before going to any kind of model building, it’s essential to take care of cross-validation. We have already seen the label/target distribution, and we know that it is a binary classification problem with skewed targets. Thus, we will be using StratifiedKFold to split the data here.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55db4119-7eb4-429f-85df-53c2247ad740', embedding=None, metadata={'page_label': '108', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 108 ═════════════════════════════════════════════════════════════════════════ # create_folds.py # import pandas and model_selection module of scikit-learn import pandas as pd from sklearn import model_selection  if __name__ == \"__main__\":      # Read training data     df = pd.read_csv(\"../input/cat_train.csv\")      # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1          # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)          # fetch labels     y = df.target.values          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):         df.loc[v_, \\'kfold\\'] = f          # save the new csv with kfold column     df.to_csv(\"../input/cat_train_folds.csv\", index=False) ═════════════════════════════════════════════════════════════════════════  We can now check our new folds csv to see the number of samples per fold:  ═════════════════════════════════════════════════════════════════════════ In [X]: import pandas as pd  In [X]: df = pd.read_csv(\"../input/cat_train_folds.csv\")  In [X]: df.kfold.value_counts() Out[X]: 4    120000 3    120000 2    120000 1    120000 0    120000 Name: kfold, dtype: int64 ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2a01974-3c61-454f-ba8a-dbf73675b197', embedding=None, metadata={'page_label': '109', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 109 All folds have 120000 samples. This is expected as training data has 600000 samples, and we made five folds. So far, so good.  Now, we can also check the target distribution per fold.  ═════════════════════════════════════════════════════════════════════════ In [X]: df[df.kfold==0].target.value_counts() Out[X]: 0    97536 1    22464 Name: target, dtype: int64  In [X]: df[df.kfold==1].target.value_counts() Out[X]: 0    97536 1    22464 Name: target, dtype: int64  In [X]: df[df.kfold==2].target.value_counts() Out[X]: 0    97535 1    22465 Name: target, dtype: int64   In [X]: df[df.kfold==3].target.value_counts() Out[X]: 0    97535 1    22465 Name: target, dtype: int64  In [X]: df[df.kfold==4].target.value_counts() Out[X]: 0    97535 1    22465 Name: target, dtype: int64 ═════════════════════════════════════════════════════════════════════════  We see that in each fold, the distribution of targets is the same. This is what we need. It can also be similar and doesn’t have to be the same all the time. Now, when we build our models, we will have the same distribution of targets across every fold.  One of the simplest models we can build is by one-hot encoding all the data and using logistic regression.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0251ee1a-e9cf-4175-9711-bd5a8ff6652e', embedding=None, metadata={'page_label': '110', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 110 ═════════════════════════════════════════════════════════════════════════ # ohe_logres.py import pandas as pd  from sklearn import linear_model from sklearn import metrics from sklearn import preprocessing  def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/cat_train_folds.csv\")      # all columns are features except id, target and kfold columns     features = [         f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesn’t matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # initialize OneHotEncoder from scikit-learn     ohe = preprocessing.OneHotEncoder()      # fit ohe on training + validation features     full_data = pd.concat(         [df_train[features], df_valid[features]],         axis=0     )     ohe.fit(full_data[features])      # transform training data     x_train = ohe.transform(df_train[features])      # transform validation data     x_valid = ohe.transform(df_valid[features])      # initialize Logistic Regression model     model = linear_model.LogisticRegression() ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ae5a7398-5023-4388-817a-9d08bfa381ca', embedding=None, metadata={'page_label': '111', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 111     # fit model on training data (ohe)     model.fit(x_train, df_train.target.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)      # print auc     print(auc)   if __name__ == \"__main__\":     # run function for fold = 0     # we can just replace this number and      # run this for any fold     run(0) ═════════════════════════════════════════════════════════════════════════  So, what’s happening?  We have created a function that splits data into training and validation, given a fold number, handles NaN values, applies one-hot encoding on all the data and trains a simple Logistic Regression model.  When we run this chunk of code, it produces an output like this:  ═════════════════════════════════════════════════════════════════════════ ❯ python ohe_logres.py /home/abhishek/miniconda3/envs/ml/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in:     https://scikit-learn.org/stable/modules/preprocessing.html. Please also refer to the documentation for alternative solver options:     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression   extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) 0.7847865042255127 ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='001a6d16-26a0-4ae2-ad7b-9626845b432a', embedding=None, metadata={'page_label': '112', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 112 There are a few warnings. It seems logistic regression did not converge for the max number of iterations. We didn’t play with the parameters, so that is fine. We see that AUC is ~ 0.785.  Let’s run it for all folds now with a simple change in code.  ═════════════════════════════════════════════════════════════════════════ # ohe_logres.py . . .      # initialize Logistic Regression model     model = linear_model.LogisticRegression()      # fit model on training data (ohe)     model.fit(x_train, df_train.target.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  Please note that we are not making a lot of changes and that’s why I have shown only some lines of the code; some of which have changes.  This gives:  ═════════════════════════════════════════════════════════════════════════ ❯ python -W ignore ohe_logres.py Fold = 0, AUC = 0.7847865042255127 Fold = 1, AUC = 0.7853553605899214 Fold = 2, AUC = 0.7879321942914885 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7aa514f-15c0-4f1d-b5c8-dfea9bc0440e', embedding=None, metadata={'page_label': '113', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 113 Fold = 3, AUC = 0.7870315929550808 Fold = 4, AUC = 0.7864668243125608 ═════════════════════════════════════════════════════════════════════════  Note that I use “-W ignore” to ignore all the warnings.   We see that AUC scores are quite stable across all folds. The average AUC is 0.78631449527. Quite good for our first model!  Many people will start this kind of problem with a tree-based model, such as random forest. For applying random forest in this dataset, instead of one-hot encoding, we can use label encoding and convert every feature in every column to an integer as discussed previously.  The code is not very different from one hot encoding code. Let’s take a look.  ═════════════════════════════════════════════════════════════════════════ # lbl_rf.py import pandas as pd  from sklearn import ensemble from sklearn import metrics from sklearn import preprocessing   def run(fold):      # load the full training data with folds     df = pd.read_csv(\"../input/cat_train_folds.csv\")      # all columns are features except id, target and kfold columns     features = [         f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now its time to label encode the features     for col in features:                  # initialize LabelEncoder for each feature column ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a54043e-a503-4d0e-8a95-9af5b3113c96', embedding=None, metadata={'page_label': '114', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 114         lbl = preprocessing.LabelEncoder()                  # fit label encoder on all data         lbl.fit(df[col])          # transform all the data         df.loc[:, col] = lbl.transform(df[col])      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # get training data     x_train = df_train[features].values      # get validation data     x_valid = df_valid[features].values      # initialize random forest model     model = ensemble.RandomForestClassifier(n_jobs=-1)      # fit model on training data (ohe)     model.fit(x_train, df_train.target.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  We use random forest from scikit-learn and have removed one-hot encoding. Instead of one-hot encoding, we use label encoding. Scores are as follows:   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='424498af-2fff-46a9-98c2-f1e11774748b', embedding=None, metadata={'page_label': '115', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 115 ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_rf.py Fold = 0, AUC = 0.7167390828113697 Fold = 1, AUC = 0.7165459672958506 Fold = 2, AUC = 0.7159709909587376 Fold = 3, AUC = 0.7161589664189556 Fold = 4, AUC = 0.7156020216155978 ═════════════════════════════════════════════════════════════════════════  Wow! Huge difference! The random forest model, without any tuning of hyperparameters, performs a lot worse than simple logistic regression.   And this is a reason why we should always start with simple models first. A fan of random forest would begin with it here and will ignore logistic regression model thinking it’s a very simple model that cannot bring any value better than random forest. That kind of person will make a huge mistake. In our implementation of random forest, the folds take a much longer time to complete compared to logistic regression. So, we are not only losing on AUC but also taking much longer to complete the training. Please note that inference is also time-consuming with random forest and it also takes much larger space.  If we want, we can also try to run random forest on sparse one-hot encoded data, but that is going to take a lot of time. We can also try reducing the sparse one-hot encoded matrices using singular value decomposition. This is a very common method of extracting topics in natural language processing.   ═════════════════════════════════════════════════════════════════════════ # ohe_svd_rf.py import pandas as pd  from scipy import sparse from sklearn import decomposition from sklearn import ensemble from sklearn import metrics from sklearn import preprocessing   def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/cat_train_folds.csv\")      # all columns are features except id, target and kfold columns     features = [         f for f in df.columns if f not in (\"id\", \"target\", \"kfold\") ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13bd1e8e-e054-466b-b761-d7b36c41631d', embedding=None, metadata={'page_label': '116', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 116     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # initialize OneHotEncoder from scikit-learn     ohe = preprocessing.OneHotEncoder()      # fit ohe on training + validation features     full_data = pd.concat(         [df_train[features], df_valid[features]],         axis=0     )     ohe.fit(full_data[features])      # transform training data     x_train = ohe.transform(df_train[features])      # transform validation data     x_valid = ohe.transform(df_valid[features])      # initialize Truncated SVD     # we are reducing the data to 120 components     svd = decomposition.TruncatedSVD(n_components=120)      # fit svd on full sparse training data     full_sparse = sparse.vstack((x_train, x_valid))     svd.fit(full_sparse)      # transform sparse training data     x_train = svd.transform(x_train)      # transform sparse validation data     x_valid = svd.transform(x_valid)      # initialize random forest model     model = ensemble.RandomForestClassifier(n_jobs=-1)  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6850af5-82ca-47b1-9feb-1f594881b868', embedding=None, metadata={'page_label': '117', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 117     # fit model on training data (ohe)     model.fit(x_train, df_train.target.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  We one-hot encode the full data and then fit TruncatedSVD from scikit-learn on sparse matrix with training + validation data. In this way, we reduce the high dimensional sparse matrix to 120 features and then fit random forest classifier.   Below is the output of this model:  ═════════════════════════════════════════════════════════════════════════ ❯ python ohe_svd_rf.py Fold = 0, AUC = 0.7064863038754249 Fold = 1, AUC = 0.706050102937374 Fold = 2, AUC = 0.7086069243167242 Fold = 3, AUC = 0.7066819080085971 Fold = 4, AUC = 0.7058154015055585 ═════════════════════════════════════════════════════════════════════════  We see that it is even worse. It seems like the best method for this problem is one-hot encoding with logistic regression. Random forest appears to be taking way too much time. Maybe we can give XGBoost a try. In case you don’t know about XGBoost, it is one of the most popular gradient boosting algorithms. Since it’s a tree-based algorithm, we will use label encoded data.  ═════════════════════════════════════════════════════════════════════════ # lbl_xgb.py import pandas as pd ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1dd69aa-1c96-48fb-b0c9-74d1422c59f8', embedding=None, metadata={'page_label': '118', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 118 import xgboost as xgb  from sklearn import metrics from sklearn import preprocessing   def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/cat_train_folds.csv\")      # all columns are features except id, target and kfold columns     features = [         f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now it’s time to label encode the features     for col in features:                  # initialize LabelEncoder for each feature column         lbl = preprocessing.LabelEncoder()                  # fit label encoder on all data         lbl.fit(df[col])          # transform all the data         df.loc[:, col] = lbl.transform(df[col])      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # get training data     x_train = df_train[features].values      # get validation data     x_valid = df_valid[features].values      # initialize xgboost model     model = xgb.XGBClassifier( ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='264b87e8-883c-4f87-870f-bdb1970c7572', embedding=None, metadata={'page_label': '119', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 119         n_jobs=-1,          max_depth=7,         n_estimators=200     )      # fit model on training data (ohe)     model.fit(x_train, df_train.target.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  It must be noted that in this code, I modified xgboost parameters a bit. Default max_depth for xgboost is 3, and I changed it to 7, and I also changed the number of estimators (n_estimators) from 100 to 200.  The 5 fold scores from this model are as follows:  ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb.py Fold = 0, AUC = 0.7656768851999011 Fold = 1, AUC = 0.7633006564148015 Fold = 2, AUC = 0.7654277821434345 Fold = 3, AUC = 0.7663609758878182 Fold = 4, AUC = 0.764914671468069 ═════════════════════════════════════════════════════════════════════════  We see that we have much better scores than plain random forest without any tuning and we can probably improve this further with more tuning of hyperparameters.  You can also try some feature engineering, dropping certain columns which don’t add any value to the model, etc. But it seems like there is not much we can do here ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ea754c0-624f-43e1-9ed3-3a02ac8efc4b', embedding=None, metadata={'page_label': '120', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 120 to demonstrate improvements in the model. Let’s change the dataset to another dataset with a lot of categorical variables. One more famous dataset is US adult census data. The dataset contains some features, and your job is to predict the salary bracket. Let’s take a look at this dataset. Figure 5 shows some of the columns from this dataset.  \\n Figure 5: Snapshot with few columns from the adult dataset6  This dataset has the following columns: • age • workclass • fnlwgt • education • education.num • marital.status • occupation • relationship • race • sex • capital.gain • capital.loss • hours.per.week  6 https://archive.ics.uci.edu/ml/datasets/adult \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f585667-7c57-4497-8724-7de6ff623527', embedding=None, metadata={'page_label': '121', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 121 • native.country • income  Most of these columns are self-explanatory. Those which are not, we can forget about it. Let’s try to build a model first.  We see that the income column is a string. Let’s do a value counts on that column.  ═════════════════════════════════════════════════════════════════════════ In [X]: import pandas as pd  In [X]: df = pd.read_csv(\"../input/adult.csv\")  In [X]: df.income.value_counts() Out[X]: <=50K    24720 >50K      7841 ═════════════════════════════════════════════════════════════════════════  We see that there are 7841 instances with income greater than 50K USD. This is ~24% of the total number of samples. Thus, we will keep the evaluation same as the cat-in-the-dat dataset, i.e. AUC.  Before we start modelling, for simplicity, we will be dropping a few columns, which are numerical, namely:  • fnlwgt • age • capital.gain • capital.loss • hours.per.week  Let’s try to quickly throw in one hot encoder with logistic regression and see what happens. The first step is always making cross-validation. I’m not going to show that part of the code here. It is left as an exercise for the reader.  ═════════════════════════════════════════════════════════════════════════ # ohe_logres.py import pandas as pd  from sklearn import linear_model from sklearn import metrics from sklearn import preprocessing  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c78e2f7-6ff7-4d26-883e-bbf19c65a6f2', embedding=None, metadata={'page_label': '122', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 122 def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/adult_folds.csv\")      # list of numerical columns     num_cols = [         \"fnlwgt\",         \"age\",         \"capital.gain\",         \"capital.loss\",         \"hours.per.week\"     ]      # drop numerical columns     df = df.drop(num_cols, axis=1)      # map targets to 0s and 1s     target_mapping = {         \"<=50K\": 0,         \">50K\": 1     }     df.loc[:, \"income\"] = df.income.map(target_mapping)      # all columns are features except income and kfold columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # initialize OneHotEncoder from scikit-learn     ohe = preprocessing.OneHotEncoder()      # fit ohe on training + validation features     full_data = pd.concat(         [df_train[features], df_valid[features]],         axis=0 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36bc2ea3-ff40-4869-ab5d-d3a16f426e55', embedding=None, metadata={'page_label': '123', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 123     )     ohe.fit(full_data[features])      # transform training data     x_train = ohe.transform(df_train[features])      # transform validation data     x_valid = ohe.transform(df_valid[features])      # initialize Logistic Regression model     model = linear_model.LogisticRegression()      # fit model on training data (ohe)     model.fit(x_train, df_train.income.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  And when we run this code, we get:  ═════════════════════════════════════════════════════════════════════════ ❯ python -W ignore ohe_logres.py Fold = 0, AUC = 0.8794809708119079 Fold = 1, AUC = 0.8875785068274882 Fold = 2, AUC = 0.8852609687685753 Fold = 3, AUC = 0.8681236223251438 Fold = 4, AUC = 0.8728581541840037 ═════════════════════════════════════════════════════════════════════════  This is a very good AUC for a model which is that simple!  Let’s try the label encoded xgboost without tuning any of hyperparameters now. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d64ff80a-e914-48e8-bebd-b638018ad88d', embedding=None, metadata={'page_label': '124', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 124 ═════════════════════════════════════════════════════════════════════════ # lbl_xgb.py import pandas as pd import xgboost as xgb  from sklearn import metrics from sklearn import preprocessing   def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/adult_folds.csv\")      # list of numerical columns     num_cols = [         \"fnlwgt\",         \"age\",         \"capital.gain\",         \"capital.loss\",         \"hours.per.week\"     ]      # drop numerical columns     df = df.drop(num_cols, axis=1)      # map targets to 0s and 1s     target_mapping = {         \"<=50K\": 0,         \">50K\": 1     }     df.loc[:, \"income\"] = df.income.map(target_mapping)      # all columns are features except kfold & income columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now its time to label encode the features     for col in features:                  # initialize LabelEncoder for each feature column ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9bcbdbe-65d3-4c76-8a0f-ad5dcabae5fa', embedding=None, metadata={'page_label': '125', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 125         lbl = preprocessing.LabelEncoder()                  # fit label encoder on all data         lbl.fit(df[col])          # transform all the data         df.loc[:, col] = lbl.transform(df[col])      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # get training data     x_train = df_train[features].values      # get validation data     x_valid = df_valid[features].values      # initialize xgboost model     model = xgb.XGBClassifier(         n_jobs=-1     )      # fit model on training data (ohe)     model.fit(x_train, df_train.income.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  Let’s run this!  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='382a1a40-3189-4ec7-97bc-bdcf72da8fc7', embedding=None, metadata={'page_label': '126', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 126 ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb.py Fold = 0, AUC = 0.8800810634234078 Fold = 1, AUC = 0.886811884948154 Fold = 2, AUC = 0.8854421433318472 Fold = 3, AUC = 0.8676319549361007 Fold = 4, AUC = 0.8714450054900602 ═════════════════════════════════════════════════════════════════════════  This seems quite good already. Let’s see the scores when we increase max_depth to 7 and n_estimators to 200.  ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb.py Fold = 0, AUC = 0.8764108944332032 Fold = 1, AUC = 0.8840708537662638 Fold = 2, AUC = 0.8816601162613102 Fold = 3, AUC = 0.8662335762581732 Fold = 4, AUC = 0.8698983461709926 ═════════════════════════════════════════════════════════════════════════  It looks like it doesn’t improve.   This shows that parameters from one dataset are not transferrable to another dataset. We must try tuning the parameters again, but we will do it in more details in next chapters.  Now, let’s try to include numerical features in the xgboost model without parameter tuning.  ═════════════════════════════════════════════════════════════════════════ # lbl_xgb_num.py import pandas as pd import xgboost as xgb  from sklearn import metrics from sklearn import preprocessing   def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/adult_folds.csv\")      # list of numerical columns ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='136cd330-f12c-4b66-8538-b530bcfd2b9c', embedding=None, metadata={'page_label': '127', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 127     num_cols = [         \"fnlwgt\",         \"age\",         \"capital.gain\",         \"capital.loss\",         \"hours.per.week\"     ]      # map targets to 0s and 1s     target_mapping = {         \"<=50K\": 0,         \">50K\": 1     }     df.loc[:, \"income\"] = df.income.map(target_mapping)      # all columns are features except kfold & income columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         # do not encode the numerical columns         if col not in num_cols:             df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now its time to label encode the features     for col in features:         if col not in num_cols:                     # initialize LabelEncoder for each feature column             lbl = preprocessing.LabelEncoder()                          # fit label encoder on all data             lbl.fit(df[col])              # transform all the data             df.loc[:, col] = lbl.transform(df[col])      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # get training data ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ded55fce-717f-4aa8-829a-8345e55ccaae', embedding=None, metadata={'page_label': '128', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 128     x_train = df_train[features].values      # get validation data     x_valid = df_valid[features].values      # initialize xgboost model     model = xgb.XGBClassifier(         n_jobs=-1     )      # fit model on training data (ohe)     model.fit(x_train, df_train.income.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_)  ═════════════════════════════════════════════════════════════════════════  So, we keep the numerical columns; we just do not label encode it. So, our final feature matrix consists of numerical columns (as it is) and encoded categorical columns. Any tree-based algorithm can handle this mix easily.  Please note that we do not need to normalize data when we use tree-based models. This is, however, a vital thing to do and cannot be missed when we are using linear models such as logistic regression.  Let’s run this script now!  ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb_num.py Fold = 0, AUC = 0.9209790185449889 Fold = 1, AUC = 0.9247157449144706 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f4743ff-ad40-46cc-a5ea-95e27bc33d18', embedding=None, metadata={'page_label': '129', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 129 Fold = 2, AUC = 0.9269329887598243 Fold = 3, AUC = 0.9119349082169275 Fold = 4, AUC = 0.9166408030141667 ═════════════════════════════════════════════════════════════════════════  Whoa!   That’s an excellent score!  Now, we can try to add some features. We will take all the categorical columns and create all combinations of degree two. Take a look at feature_engineering function in the snippet below to know how this is done.  ═════════════════════════════════════════════════════════════════════════ # lbl_xgb_num_feat.py import itertools import pandas as pd import xgboost as xgb  from sklearn import metrics from sklearn import preprocessing   def feature_engineering(df, cat_cols):     \"\"\"     This function is used for feature engineering     :param df: the pandas dataframe with train/test data     :param cat_cols: list of categorical columns     :return: dataframe with new features     \"\"\"     # this will create all 2-combinations of values     # in this list     # for example:     # list(itertools.combinations([1,2,3], 2)) will return     # [(1, 2), (1, 3), (2, 3)]     combi = list(itertools.combinations(cat_cols, 2))     for c1, c2 in combi:         df.loc[           :,            c1 + \"_\" + c2         ] = df[c1].astype(str) + \"_\" + df[c2].astype(str)     return df   def run(fold):     # load the full training data with folds ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8b81d12-4546-438a-933a-393e64c566d3', embedding=None, metadata={'page_label': '130', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 130     df = pd.read_csv(\"../input/adult_folds.csv\")      # list of numerical columns     num_cols = [         \"fnlwgt\",         \"age\",         \"capital.gain\",         \"capital.loss\",         \"hours.per.week\"     ]      # map targets to 0s and 1s     target_mapping = {         \"<=50K\": 0,         \">50K\": 1     }     df.loc[:, \"income\"] = df.income.map(target_mapping)      # list of categorical columns for feature engineering     cat_cols = [         c for c in df.columns if c not in num_cols         and c not in (\"kfold\", \"income\")     ]      # add new features     df = feature_engineering(df, cat_cols)      # all columns are features except kfold & income columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         # do not encode the numerical columns         if col not in num_cols:             df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now its time to label encode the features     for col in features:         if col not in num_cols:                     # initialize LabelEncoder for each feature column             lbl = preprocessing.LabelEncoder()                          # fit label encoder on all data ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a641a6de-4884-4931-b2b4-c87f1c7867ca', embedding=None, metadata={'page_label': '131', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 131             lbl.fit(df[col])              # transform all the data             df.loc[:, col] = lbl.transform(df[col])      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # get training data     x_train = df_train[features].values      # get validation data     x_valid = df_valid[features].values      # initialize xgboost model     model = xgb.XGBClassifier(         n_jobs=-1     )      # fit model on training data (ohe)     model.fit(x_train, df_train.income.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     for fold_ in range(5):         run(fold_) ═════════════════════════════════════════════════════════════════════════  This is a very naïve way of creating features from categorical columns. One should take a look at the data and see which combinations make the most sense. If you use this method, you might end up creating a lot of features, and in that case, you will ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0395311a-4948-41fd-9a18-bdb24da83d43', embedding=None, metadata={'page_label': '132', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 132 need to use some kind of feature selection to select the best features. We will read more about feature selection later. Let’s see the scores now.  ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb_num_feat.py Fold = 0, AUC = 0.9211483465031423 Fold = 1, AUC = 0.9251499446866125 Fold = 2, AUC = 0.9262344766486692 Fold = 3, AUC = 0.9114264068794995 Fold = 4, AUC = 0.9177914453099201 ═════════════════════════════════════════════════════════════════════════  It seems that even without changing any hyperparameters and just by adding a bunch of features, we can improve our fold scores a bit. Let’s see if increasing max_depth to 7 helps.  ═════════════════════════════════════════════════════════════════════════ ❯ python lbl_xgb_num_feat.py Fold = 0, AUC = 0.9286668430204137 Fold = 1, AUC = 0.9329340656165378 Fold = 2, AUC = 0.9319817543218744 Fold = 3, AUC = 0.919046187194538 Fold = 4, AUC = 0.9245692057162671 ═════════════════════════════════════════════════════════════════════════  And yet again, we have been able to improve our model.  Note that we have not yet used rare values, binary features, a combination of one-hot and label encoded features and several other methods.  One more way of feature engineering from categorical features is to use target encoding. However, you have to be very careful here as this might overfit your model. Target encoding is a technique in which you map each category in a given feature to its mean target value, but this must always be done in a cross-validated manner. It means that the first thing you do is create the folds, and then use those folds to create target encoding features for different columns of the data in the same way you fit and predict the model on folds. So, if you have created 5 folds, you have to create target encoding 5 times such that in the end, you have encoding for variables in each fold which are not derived from the same fold. And then when you fit your model, you must use the same folds again. Target encoding for unseen test data can be derived from the full training data or can be an average of all the 5 folds.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d8ed3f79-d67a-47b5-8739-3f1aaef0d402', embedding=None, metadata={'page_label': '133', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 133 Let’s see how we can use target encoding on the same adult dataset so that we can compare.  ═════════════════════════════════════════════════════════════════════════ # target_encoding.py import copy import pandas as pd  from sklearn import metrics from sklearn import preprocessing import xgboost as xgb   def mean_target_encoding(data):      # make a copy of dataframe     df = copy.deepcopy(data)      # list of numerical columns     num_cols = [         \"fnlwgt\",         \"age\",         \"capital.gain\",         \"capital.loss\",         \"hours.per.week\"     ]      # map targets to 0s and 1s     target_mapping = {         \"<=50K\": 0,         \">50K\": 1     }      df.loc[:, \"income\"] = df.income.map(target_mapping)          # all columns are features except income and kfold columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")         and f not in num_cols     ]      # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         # do not encode the numerical columns         if col not in num_cols: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ebec4ea3-539e-425a-a136-e2e9517abbf7', embedding=None, metadata={'page_label': '134', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 134             df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")          # now its time to label encode the features     for col in features:         if col not in num_cols:                     # initialize LabelEncoder for each feature column             lbl = preprocessing.LabelEncoder()                          # fit label encoder on all data             lbl.fit(df[col])              # transform all the data             df.loc[:, col] = lbl.transform(df[col])      # a list to store 5 validation dataframes     encoded_dfs = []      # go over all folds     for fold in range(5):         # fetch training and validation data         df_train = df[df.kfold != fold].reset_index(drop=True)         df_valid = df[df.kfold == fold].reset_index(drop=True)         # for all feature columns, i.e. categorical columns         for column in features:             # create dict of category:mean target             mapping_dict = dict(                 df_train.groupby(column)[\"income\"].mean()             )             # column_enc is the new column we have with mean encoding             df_valid.loc[                 :, column + \"_enc\"             ] = df_valid[column].map(mapping_dict)         # append to our list of encoded validation dataframes         encoded_dfs.append(df_valid)     # create full data frame again and return     encoded_df = pd.concat(encoded_dfs, axis=0)     return encoded_df   def run(df, fold):     # note that folds are same as before     # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e8b74a3-1ee4-4571-829d-fc1387944c47', embedding=None, metadata={'page_label': '135', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 135     # all columns are features except income and kfold columns     features = [         f for f in df.columns if f not in (\"kfold\", \"income\")     ]      # scale training data     x_train = df_train[features].values      # scale validation data     x_valid = df_valid[features].values      # initialize xgboost model     model = xgb.XGBClassifier(         n_jobs=-1,         max_depth=7     )      # fit model on training data (ohe)     model.fit(x_train, df_train.income.values)      # predict on validation data     # we need the probability values as we are calculating AUC     # we will use the probability of 1s     valid_preds = model.predict_proba(x_valid)[:, 1]      # get roc auc score     auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)      # print auc     print(f\"Fold = {fold}, AUC = {auc}\")   if __name__ == \"__main__\":     # read data     df = pd.read_csv(\"../input/adult_folds.csv\")          # create mean target encoded categories and     # munge data     df = mean_target_encoding(df)      # run training and validation for 5 folds     for fold_ in range(5):         run(df, fold_) ═════════════════════════════════════════════════════════════════════════  It must be noted that in the above snippet, I had not dropped categorical columns when I did the target encoding. I kept all the features and added target encoded ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f369c0ee-9297-42b9-86a2-cedf08807eab', embedding=None, metadata={'page_label': '136', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 136 features on top of it. Also, I used mean. You can use mean, median, standard deviation or any other function of targets.   Let’s see the results.  ═════════════════════════════════════════════════════════════════════════ Fold = 0, AUC = 0.9332240662017529 Fold = 1, AUC = 0.9363551625140347 Fold = 2, AUC = 0.9375013544556173 Fold = 3, AUC = 0.92237621307625 Fold = 4, AUC = 0.9292131180445478 ═════════════════════════════════════════════════════════════════════════  Nice! It seems like we have improved again. However, you must be very careful when using target encoding as it is too prone to overfitting. When we use target encoding, it’s better to use some kind of smoothing or adding noise in the encoded values. Scikit-learn has contrib repository which has target encoding with smoothing, or you can create your own smoothing. Smoothing introduces some kind of regularization that helps with not overfitting the model. It’s not very difficult.  Handling categorical features is a complicated task. There is a lot of information floating around in several resources. This chapter should help you get started with any problem with categorical variables. For most of the problems, however, you won’t need anything more than one-hot encoding and label encoding. For improving the models further, you might need a lot more!  We cannot end this chapter without using a neural network on this data. So, let’s take a look at a technique known as entity embedding. In entity embeddings, the categories are represented as vectors. We represent categories by vectors in both binarization and one hot encoding approaches. But what if we have tens of thousands of categories. This will create huge matrices and will take a long time for us to train complicated models. We can thus represent them by vectors with float values instead.   The idea is super simple. You have an embedding layer for each categorical feature. So, every category in a column can now be mapped to an embedding (like mapping words to embeddings in natural language processing). You then reshape these embeddings to their dimension to make them flat and then concatenate all the flattened inputs embeddings. Then add a bunch of dense layers, an output layer and you are done. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa1eacc3-9748-4240-8012-699997c9a3d7', embedding=None, metadata={'page_label': '137', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 137  \\n Figure 6: Categories are converted to vectors of float or embeddings  For some reason, I find it super easy to do using TF/Keras. So, let’s see how it’s implemented using TF/Keras. Also, this is the only example using TF/Keras in this book and its super easy to convert it to PyTorch (using cat-in-the-dat-ii dataset).  ═════════════════════════════════════════════════════════════════════════ # entity_emebddings.py import os import gc import joblib import pandas as pd import numpy as np from sklearn import metrics, preprocessing from tensorflow.keras import layers from tensorflow.keras import optimizers from tensorflow.keras.models import Model, load_model from tensorflow.keras import callbacks from tensorflow.keras import backend as K from tensorflow.keras import utils   \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a09590f1-c91c-4796-9ecf-526114ff4cac', embedding=None, metadata={'page_label': '138', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 138 def create_model(data, catcols):     \"\"\"     This function returns a compiled tf.keras model     for entity embeddings     :param data: this is a pandas dataframe     :param catcols: list of categorical column names     :return: compiled tf.keras model     \"\"\"     # init list of inputs for embeddings     inputs = []      # init list of outputs for embeddings     outputs = []      # loop over all categorical columns     for c in catcols:         # find the number of unique values in the column         num_unique_values = int(data[c].nunique())         # simple dimension of embedding calculator         # min size is half of the number of unique values         # max size is 50. max size depends on the number of unique         # categories too. 50 is quite sufficient most of the times         # but if you have millions of unique values, you might need         # a larger dimension         embed_dim = int(min(np.ceil((num_unique_values)/2), 50))          # simple keras input layer with size 1         inp = layers.Input(shape=(1,))          # add embedding layer to raw input         # embedding size is always 1 more than unique values in input         out = layers.Embedding(             num_unique_values + 1, embed_dim, name=c         )(inp)                  # 1-d spatial dropout is the standard for emebedding layers          # you can use it in NLP tasks too         out = layers.SpatialDropout1D(0.3)(out)          # reshape the input to the dimension of embedding         # this becomes our output layer for current feature         out = layers.Reshape(target_shape=(embed_dim, ))(out)          # add input to input list         inputs.append(inp)          # add output to output list ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9329757e-e8da-41cc-be06-ff0b0596148b', embedding=None, metadata={'page_label': '139', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 139         outputs.append(out)          # concatenate all output layers     x = layers.Concatenate()(outputs)      # add a batchnorm layer.     # from here, everything is up to you     # you can try different architectures     # this is the architecture I like to use     # if you have numerical features, you should add     # them here or in concatenate layer     x = layers.BatchNormalization()(x)          # a bunch of dense layers with dropout.     # start with 1 or two layers only     x = layers.Dense(300, activation=\"relu\")(x)     x = layers.Dropout(0.3)(x)     x = layers.BatchNormalization()(x)          x = layers.Dense(300, activation=\"relu\")(x)     x = layers.Dropout(0.3)(x)     x = layers.BatchNormalization()(x)          # using softmax and treating it as a two class problem     # you can also use sigmoid, then you need to use only one     # output class     y = layers.Dense(2, activation=\"softmax\")(x)      # create final model     model = Model(inputs=inputs, outputs=y)      # compile the model     # we use adam and binary cross entropy.     # feel free to use something else and see how model behaves     model.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\')     return model   def run(fold):     # load the full training data with folds     df = pd.read_csv(\"../input/cat_train_folds.csv\")      # all columns are features except id, target and kfold columns     features = [         f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")     ]  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5427a4a-ffc1-4dca-a8e3-b4387b17fe70', embedding=None, metadata={'page_label': '140', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 140     # fill all NaN values with NONE     # note that I am converting all columns to \"strings\"     # it doesnt matter because all are categories     for col in features:         df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")      # encode all features with label encoder individually     # in a live setting you need to save all label encoders     for feat in features:         lbl_enc = preprocessing.LabelEncoder()         df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)      # get training data using folds     df_train = df[df.kfold != fold].reset_index(drop=True)      # get validation data using folds     df_valid = df[df.kfold == fold].reset_index(drop=True)      # create tf.keras model     model = create_model(df, features)      # our features are lists of lists     xtrain = [         df_train[features].values[:, k] for k in range(len(features))     ]     xvalid = [         df_valid[features].values[:, k] for k in range(len(features))     ]     # fetch target columns     ytrain = df_train.target.values     yvalid = df_valid.target.values      # convert target columns to categories     # this is just binarization     ytrain_cat = utils.to_categorical(ytrain)     yvalid_cat = utils.to_categorical(yvalid)          # fit the model     model.fit(xtrain,               ytrain_cat,               validation_data=(xvalid, yvalid_cat),               verbose=1,               batch_size=1024,               epochs=3              )      # generate validation predictions ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5001de52-8c71-4598-a034-eb4dca823877', embedding=None, metadata={'page_label': '141', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 141     valid_preds = model.predict(xvalid)[:, 1]      # print roc auc score     print(metrics.roc_auc_score(yvalid, valid_preds))      # clear session to free up some GPU memory     K.clear_session()  if __name__ == \"__main__\":     run(0)     run(1)     run(2)     run(3)     run(4) ═════════════════════════════════════════════════════════════════════════  You will notice that this approach gives the best results and is also super-fast if you have a GPU! This can also be improved further, and you don’t need to worry about feature engineering as neural network handles it on its own. This is definitely worth a try when dealing with a large dataset of categorical features. When embedding size is the same as the number of unique categories, we have one-hot-encoding.  This chapter is basically all about feature engineering. Let’s see how you can do some more feature engineering when it comes to numerical features and combination of different types of features in the next chapter.       ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8309363-f1ae-46a8-9b94-0e470f5ca287', embedding=None, metadata={'page_label': '142', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 142 Feature engineering  Feature engineering is one of the most crucial parts of building a good machine learning model. If we have useful features, the model will perform better. There are many situations where you can avoid large, complicated models and use simple models with crucially engineered features. We must keep in mind that feature engineering is something that is done in the best possible manner only when you have some knowledge about the domain of the problem and depends a lot on the data in concern. However, there are some general techniques that you can try to create features from almost all kinds of numerical and categorical variables. Feature engineering is not just about creating new features from data but also includes different types of normalization and transformations.  In the chapter about categorical features, we have seen a way to combine different categorical variables, how we can convert categorical variables to counts, target encoding and using embeddings. These are almost all kinds of ways to engineer features from categorical variables. Thus, in this chapter, our focus will be limited to numerical variables and a combination of numerical and categorical variables.  Let’s start with the most simple but most widely used feature engineering techniques. Let’s say that you are dealing with date and time data. So, we have a pandas dataframe with a datetime type column. Using this column, we can create features like:  - Year - Week of year - Month - Day of week - Weekend - Hour - And many more.  And this can be done using pandas very easily.  ═════════════════════════════════════════════════════════════════════════ df.loc[:, 'year'] = df['datetime_column'].dt.year df.loc[:, 'weekofyear'] = df['datetime_column'].dt.weekofyear df.loc[:, 'month'] = df['datetime_column'].dt.month df.loc[:, 'dayofweek'] = df['datetime_column'].dt.dayofweek \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be398d62-b859-4e2f-9a0f-92ab5934ecc8', embedding=None, metadata={'page_label': '143', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 143 df.loc[:, \\'weekend\\'] = (df.datetime_column.dt.weekday >=5).astype(int) df.loc[:, \\'hour\\'] = df[\\'datetime_column\\'].dt.hour ═════════════════════════════════════════════════════════════════════════  So, we are creating a bunch of new columns using the datetime column. Let’s see some of the sample features that can be created.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd  # create a series of datetime with a frequency of 10 hours s = pd.date_range(\\'2020-01-06\\', \\'2020-01-10\\', freq=\\'10H\\').to_series()  # create some features based on datetime features = {     \"dayofweek\": s.dt.dayofweek.values,     \"dayofyear\": s.dt.dayofyear.values,     \"hour\": s.dt.hour.values,     \"is_leap_year\": s.dt.is_leap_year.values,     \"quarter\": s.dt.quarter.values,     \"weekofyear\": s.dt.weekofyear.values } ═════════════════════════════════════════════════════════════════════════  This will generate a dictionary of features from a given series. You can apply this to any datetime column in a pandas dataframe. These are some of the many date time features that pandas offer. Date time features are critical when you are dealing with time-series data, for example, predicting sales of a store but would like to use a model like xgboost on aggregated features.  Suppose we have a dataframe that looks like the following:  \\n Figure 1: A sample dataframe with categorical and date features \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dac82b17-68b7-4a34-b12f-921be20e3bc8', embedding=None, metadata={'page_label': '144', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 144 In figure 1, we see that we have a date column, and we can easily extract features like the year, month, quarter, etc. from that. Then we have a customer_id column which has multiple entries, so a customer is seen many times (not visible in the screenshot). And each date and customer id has three categorical and one numerical feature attached to it. There are a bunch of features we can create from it:  - What’s the month a customer is most active in - What is the count of cat1, cat2, cat3 for a customer - What is the count of cat1, cat2, cat3 for a customer for a given week of the year - What is the mean of num1 for a given customer - And so on.  Using aggregates in pandas, it is quite easy to create features like these. Let’s see how.  ═════════════════════════════════════════════════════════════════════════ def generate_features(df):     # create a bunch of features using the date column     df.loc[:, 'year'] = df['date'].dt.year     df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear     df.loc[:, 'month'] = df['date'].dt.month     df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek     df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)          # create an aggregate dictionary     aggs = {}     # for aggregation by month, we calculate the     # number of unique month values and also the mean     aggs['month'] = ['nunique', 'mean']     aggs['weekofyear'] = ['nunique', 'mean']     # we aggregate by num1 and calculate sum, max, min      # and mean values of this column     aggs['num1'] = ['sum','max','min','mean']     # for customer_id, we calculate the total count     aggs['customer_id'] = ['size']     # again for customer_id, we calculate the total unique     aggs['customer_id'] = ['nunique']          # we group by customer_id and calculate the aggregates     agg_df = df.groupby('customer_id').agg(aggs)     agg_df = agg_df.reset_index()     return agg_df ═════════════════════════════════════════════════════════════════════════ \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9cce751-ee2a-45b4-92e9-0fb133189bca', embedding=None, metadata={'page_label': '145', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 145 Please note that in the above function, we have skipped the categorical variables, but you can use them in the same way as other aggregates.  \\n Figure 2: Aggregate and other features  Now, we can join this dataframe in figure 2 with the original dataframe with customer_id column to start training a model. Here, we are not trying to predict anything; we are just creating generic features. However, it would have been easier to create features if we were trying to predict something here.  Sometimes, for example, when dealing with time-series problems, you might have features which are not individual values but a list of values. For example, transactions by a customer in a given period of time. In these cases, we create different types of features such as: with numerical features, when you are grouping on a categorical column, you will get features like a list of values which are time distributed. In these cases, you can create a bunch of statistical features such as:  - Mean - Max - Min - Unique - Skew - Kurtosis - Kstat - Percentile - Quantile \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15482832-66f0-4576-91ec-db8f98455129', embedding=None, metadata={'page_label': '146', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 146 - Peak to peak - And many more  These can be created using simple numpy functions, as shown in the following python snippet.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  feature_dict = {}  # calculate mean feature_dict['mean'] = np.mean(x)  # calculate max feature_dict['max'] = np.max(x)  # calculate min feature_dict['min'] = np.min(x)  # calculate standard deviation feature_dict['std'] = np.std(x)  # calculate variance feature_dict['var'] = np.var(x)  # peak-to-peak feature_dict['ptp'] = np.ptp(x)  # percentile features feature_dict['percentile_10'] = np.percentile(x, 10) feature_dict['percentile_60'] = np.percentile(x, 60) feature_dict['percentile_90'] = np.percentile(x, 90)  # quantile features feature_dict['quantile_5'] = np.quantile(x, 0.05) feature_dict['quantile_95'] = np.quantile(x, 0.95) feature_dict['quantile_99'] = np.quantile(x, 0.99) ═════════════════════════════════════════════════════════════════════════  The time series data (list of values) can be converted to a lot of features.   A python library called tsfresh is instrumental in this case.  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2cba4d06-975c-43fb-adc7-7587978b9345', embedding=None, metadata={'page_label': '147', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 147 ═════════════════════════════════════════════════════════════════════════ from tsfresh.feature_extraction import feature_calculators as fc  # tsfresh based features feature_dict[\\'abs_energy\\'] = fc.abs_energy(x) feature_dict[\\'count_above_mean\\'] = fc.count_above_mean(x) feature_dict[\\'count_below_mean\\'] = fc.count_below_mean(x) feature_dict[\\'mean_abs_change\\'] = fc.mean_abs_change(x) feature_dict[\\'mean_change\\'] = fc.mean_change(x) ═════════════════════════════════════════════════════════════════════════  This is not all; tsfresh offers hundreds of features and tens of variations of different features that you can use for time series (list of values) based features. In the examples above, x is a list of values. But that’s not all. There are many other features that you can create for numerical data with or without categorical data. A simple way to generate many features is just to create a bunch of polynomial features. For example, a second-degree polynomial feature from two features “a” and “b” would include: “a”, “b”, “ab”, “a2” and “b2”.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  # generate a random dataframe with  # 2 columns and 100 rows df = pd.DataFrame(     np.random.rand(100, 2),     columns=[f\"f_{i}\" for i in range(1, 3)] ) ═════════════════════════════════════════════════════════════════════════  Which gives a dataframe, as shown in figure 3.  \\n Figure 3: A random dataframe with two numerical features \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a15df94f-56d8-4cfb-bc30-6ad2dec21830', embedding=None, metadata={'page_label': '148', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 148 And we can create two-degree polynomial features using PolynomialFeatures from scikit-learn.  ═════════════════════════════════════════════════════════════════════════ from sklearn import preprocessing  # initialize polynomial features class object # for two-degree polynomial features pf = preprocessing.PolynomialFeatures(     degree=2,     interaction_only=False,     include_bias=False )  # fit to the features pf.fit(df)  # create polynomial features poly_feats = pf.transform(df)  # create a dataframe with all the features num_feats = poly_feats.shape[1] df_transformed = pd.DataFrame(     poly_feats,     columns=[f\"f_{i}\" for i in range(1, num_feats + 1)] ) ═════════════════════════════════════════════════════════════════════════  And that would give a dataframe, as shown in figure 4.  \\n Figure 4: A sample dataframe with polynomial features  So, now we have created some polynomial features. If you create third-degree polynomial features, you will end up with nine features in total. The more the \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d23064d-68ad-4f64-9e11-494af464a2c4', embedding=None, metadata={'page_label': '149', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 149 number of features, the more the number of polynomial features and you must also remember that if you have a lot of samples in the dataset, it is going to take a while creating these kinds of features. \\n Figure 5: Histogram of a numerical feature column  Another interesting feature converts the numbers to categories. It’s known as binning. Let’s look at figure 5, which shows a sample histogram of a random numerical feature. We use ten bins for this figure, and we see that we can divide the data into ten parts. This is accomplished using the pandas’ cut function.  ═════════════════════════════════════════════════════════════════════════ # create bins of the numerical columns # 10 bins df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False) # 100 bins df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False) ═════════════════════════════════════════════════════════════════════════  Which generates two new features in the dataframe, as shown in figure 6.   Figure 6: Binning numerical features \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb49da74-a1a1-4842-905d-c28fa53682a9', embedding=None, metadata={'page_label': '150', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 150 When you bin, you can use both the bin and the original feature. We will learn a bit more about selecting features later in this chapter. Binning also enables you to treat numerical features as categorical.  Yet another interesting type of feature that you can create from numerical features is log transformation. Take a look at feature f_3 in figure 7.   Figure 7: Example of a feature that has a high variance  f_3 is a special feature with a very high variance. Compared to other features that have a low variance (let’s assume that). Thus, we would want to reduce the variance of this column, and that can be done by taking a log transformation.   The values in column f_3 range from 0 to 10000 and a histogram is shown in figure 8. \\n Figure 8: Histogram of feature f_3.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33f71c06-0808-453b-971d-692d1493577e', embedding=None, metadata={'page_label': '151', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 151 And we can apply log(1 + x) to this column to reduce its variance. Figure 9 shows what happens to the histogram when the log transformation is applied. \\n Figure 9: Histogram of f_3 after applying log transformation.  Let’s take a look at the variance without and with the log transformation.  ═════════════════════════════════════════════════════════════════════════ In [X]: df.f_3.var() Out[X]: 8077265.875858586  In [X]: df.f_3.apply(lambda x: np.log(1 + x)).var() Out[X]: 0.6058771732119975 ═════════════════════════════════════════════════════════════════════════  Sometimes, instead of log, you can also take exponential. A very interesting case is when you use a log-based evaluation metric, for example, RMSLE. In that case, you can train on log-transformed targets and convert back to original using exponential on the prediction. That would help optimize the model for the metric.  Most of the time, these kinds of numerical features are created based on intuition. There is no formula. If you are working in an industry, you will create your industry-specific features.   When dealing with both categorical and numerical variables, you might encounter missing values. We saw some ways to handle missing values in categorical features in the previous chapter, but there are many more ways to handle missing/NaN values. This is also considered feature engineering.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90874405-eb06-4f8e-9e97-dfa2e1f4d3f8', embedding=None, metadata={'page_label': '152', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 152 For categorical features, let’s keep it super simple. If you ever encounter missing values in categorical features, treat is as a new category! As simple as this is, it (almost) always works!  One way to fill missing values in numerical data would be to choose a value that does not appear in the specific feature and fill using that. For example, let’s say 0 is not seen in the feature. So, we fill all the missing values using 0. This is one of the ways but might not be the most effective. One of the methods that works better than filling 0s for numerical data is to fill with mean instead. You can also try to fill with the median of all the values for that feature, or you can use the most common value to fill the missing values. There are just so many ways to do this.  A fancy way of filling in the missing values would be to use a k-nearest neighbour method. You can select a sample with missing values and find the nearest neighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values like this.   \\n Figure 10: A 2d array with missing values  Let’s see how a matrix with missing values, as shown in figure 10 is handled by KNNImputer.  ═════════════════════════════════════════════════════════════════════════ import numpy as np from sklearn import impute  # create a random numpy array with 10 samples # and 6 features and values ranging from 1 to 15 X = np.random.randint(1, 15, (10, 6))  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb8c6b42-e738-42dd-aebb-4d4a1c5eb0a9', embedding=None, metadata={'page_label': '153', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 153 # convert the array to float X = X.astype(float)  # randomly assign 10 elements to NaN (missing) X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan  # use 2 nearest neighbours to fill na values knn_imputer = impute.KNNImputer(n_neighbors=2) knn_imputer.fit_transform(X) ═════════════════════════════════════════════════════════════════════════  Which fills the above matrix, as shown in figure 11.  \\n Figure 11: Values imputed by KNN Imputer  Another way of imputing missing values in a column would be to train a regression model that tries to predict missing values in a column based on other columns. So, you start with one column that has a missing value and treat this column as the target column for regression model without the missing values. Using all the other columns, you now train a model on samples for which there is no missing value in the concerned column and then try to predict target (the same column) for the samples that were removed earlier. This way, you have a more robust model based imputation.  Always remember that imputing values for tree-based models is unnecessary as they can handle it themselves.  What I have shown until now are some of the ways of creating features in general. Now, let’s say you are working on a problem of predicting store sales of different items (per week or month). You have items, and you have store ids. So, you can create features like items per store. Now, this is one of the features that is not discussed above. These kinds of features cannot be generalized and come purely from domain, data and business knowledge. Look at the data and see what fits and \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3cd76fee-8d99-42d8-b39b-6b818b0d3d32', embedding=None, metadata={'page_label': '154', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 154 create features accordingly. And always remember to scale or normalize your features if you are using linear models like logistic regression or a model like SVM. Tree-based models will always work fine without any normalization of features.    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8662116-ceba-426d-afce-bd7fe4e19758', embedding=None, metadata={'page_label': '155', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 155 Feature selection  When you are done creating hundreds of thousands of features, it’s time for selecting a few of them. Well, we should never create hundreds of thousands of useless features. Having too many features pose a problem well known as the curse of dimensionality. If you have a lot of features, you must also have a lot of training samples to capture all the features. What’s considered a “lot” is not defined correctly and is up to you to figure out by validating your models properly and checking how much time it takes to train your models.  The simplest form of selecting features would be to remove features with very low variance. If the features have a very low variance (i.e. very close to 0), they are close to being constant and thus, do not add any value to any model at all. It would just be nice to get rid of them and hence lower the complexity. Please note that the variance also depends on scaling of the data. Scikit-learn has an implementation for VarianceThreshold that does precisely this.  ═════════════════════════════════════════════════════════════════════════ from sklearn.feature_selection import VarianceThreshold data = ... var_thresh = VarianceThreshold(threshold=0.1) transformed_data = var_thresh.fit_transform(data) # transformed data will have all columns with variance less  # than 0.1 removed ═════════════════════════════════════════════════════════════════════════  We can also remove features which have a high correlation. For calculating the correlation between different numerical features, you can use the Pearson correlation.   ═════════════════════════════════════════════════════════════════════════ import pandas as pd from sklearn.datasets import fetch_california_housing  # fetch a regression dataset data = fetch_california_housing() X = data[\"data\"] col_names = data[\"feature_names\"] y = data[\"target\"]  # convert to pandas dataframe ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3cc61918-d951-4446-ac1c-db7dddf9cd20', embedding=None, metadata={'page_label': '156', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 156 df = pd.DataFrame(X, columns=col_names) # introduce a highly correlated column df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)  # get correlation matrix (pearson) df.corr() ═════════════════════════════════════════════════════════════════════════  Which gives a correlation matrix, as shown in figure 1.  \\n Figure 1: A sample Pearson correlation matrix  We see that the feature MedInc_Sqrt has a very high correlation with MedInc. We can thus remove one of them.   And now we can move to some univariate ways of feature selection. Univariate feature selection is nothing but a scoring of each feature against a given target. Mutual information, ANOVA F-test and chi2 are some of the most popular methods for univariate feature selection. There are two ways of using these in scikit-learn.  - SelectKBest: It keeps the top-k scoring features - SelectPercentile: It keeps the top features which are in a percentage specified by the user  It must be noted that you can use chi2 only for data which is non-negative in nature. This is a particularly useful feature selection technique in natural language processing when we have a bag of words or tf-idf based features. It’s best to create a wrapper for univariate feature selection that you can use for almost any new problem.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3c7cec7-c124-475f-8731-ca08d83537a5', embedding=None, metadata={'page_label': '157', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 157 ═════════════════════════════════════════════════════════════════════════ from sklearn.feature_selection import chi2 from sklearn.feature_selection import f_classif from sklearn.feature_selection import f_regression from sklearn.feature_selection import mutual_info_classif from sklearn.feature_selection import mutual_info_regression from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import SelectPercentile   class UnivariateFeatureSelction:     def __init__(self, n_features, problem_type, scoring):         \"\"\"         Custom univariate feature selection wrapper on         different univariate feature selection models from         scikit-learn.         :param n_features: SelectPercentile if float else SelectKBest         :param problem_type: classification or regression         :param scoring: scoring function, string         \"\"\"         # for a given problem type, there are only         # a few valid scoring methods         # you can extend this with your own custom         # methods if you wish         if problem_type == \"classification\":             valid_scoring = {                 \"f_classif\": f_classif,                 \"chi2\": chi2,                 \"mutual_info_classif\": mutual_info_classif             }         else:             valid_scoring = {                 \"f_regression\": f_regression,                 \"mutual_info_regression\": mutual_info_regression             }                  # raise exception if we do not have a valid scoring method         if scoring not in valid_scoring:             raise Exception(\"Invalid scoring function\")                  # if n_features is int, we use selectkbest         # if n_features is float, we use selectpercentile         # please note that it is int in both cases in sklearn         if isinstance(n_features, int):             self.selection = SelectKBest(                 valid_scoring[scoring],                 k=n_features ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6def8ee6-933a-4ca4-8002-b19524c1d3de', embedding=None, metadata={'page_label': '158', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 158             )         elif isinstance(n_features, float):             self.selection = SelectPercentile(                 valid_scoring[scoring],                 percentile=int(n_features * 100)             )         else:             raise Exception(\"Invalid type of feature\")          # same fit function     def fit(self, X, y):         return self.selection.fit(X, y)          # same transform function     def transform(self, X):         return self.selection.transform(X)          # same fit_transform function     def fit_transform(self, X, y):         return self.selection.fit_transform(X, y) ═════════════════════════════════════════════════════════════════════════  Using this class is pretty simple.  ═════════════════════════════════════════════════════════════════════════ ufs = UnivariateFeatureSelction(     n_features=0.1,      problem_type=\"regression\",      scoring=\"f_regression\" ) ufs.fit(X, y) X_transformed = ufs.transform(X) ═════════════════════════════════════════════════════════════════════════  That should take care of most of your univariate feature selection needs. Please note that it’s usually better to create less and important features than to create hundreds of features in the first place. Univariate feature selection may not always perform well. Most of the time, people prefer doing feature selection using a machine learning model. Let’s see how that is done.  The simplest form of feature selection that uses a model for selection is known as greedy feature selection. In greedy feature selection, the first step is to choose a model. The second step is to select a loss/scoring function. And the third and final step is to iteratively evaluate each feature and add it to the list of “good” features if ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='830c4bec-9bb7-45a6-8421-ccc3c8e554ca', embedding=None, metadata={'page_label': '159', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 159 it improves loss/score. It can’t get simpler than this. But you must keep in mind that this is known as greedy feature selection for a reason. This feature selection process will fit a given model each time it evaluates a feature. The computational cost associated with this kind of method is very high. It will also take a lot of time for this kind of feature selection to finish. And if you do not use this feature selection properly, then you might even end up overfitting the model.   Let’s see how it works by looking at how its implemented.  ═════════════════════════════════════════════════════════════════════════ # greedy.py import pandas as pd  from sklearn import linear_model from sklearn import metrics from sklearn.datasets import make_classification   class GreedyFeatureSelection:         \"\"\"     A simple and custom class for greedy feature selection.     You will need to modify it quite a bit to make it suitable     for your dataset.     \"\"\"     def evaluate_score(self, X, y):         \"\"\"         This function evaluates model on data and returns         Area Under ROC Curve (AUC)         NOTE: We fit the data and calculate AUC on same data.         WE ARE OVERFITTING HERE.          But this is also a way to achieve greedy selection.         k-fold will take k times longer.          If you want to implement it in really correct way,         calculate OOF AUC and return mean AUC over k folds.         This requires only a few lines of change and has been          shown a few times in this book.          :param X: training data         :param y: targets         :return: overfitted area under the roc curve         \"\"\"         # fit the logistic regression model,         # and calculate AUC on same data         # again: BEWARE         # you can choose any model that suits your data ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='168b0847-5ea9-4dcf-8315-b9cae1fc6235', embedding=None, metadata={'page_label': '160', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 160         model = linear_model.LogisticRegression()         model.fit(X, y)         predictions = model.predict_proba(X)[:, 1]         auc = metrics.roc_auc_score(y, predictions)         return auc          def _feature_selection(self, X, y):         \"\"\"         This function does the actual greedy selection         :param X: data, numpy array         :param y: targets, numpy array         :return: (best scores, best features)         \"\"\"         # initialize good features list          # and best scores to keep track of both         good_features = []         best_scores = []                  # calculate the number of features         num_features = X.shape[1]                  # infinite loop         while True:             # initialize best feature and score of this loop             this_feature = None             best_score = 0              # loop over all features             for feature in range(num_features):                 # if feature is already in good features,                 # skip this for loop                 if feature in good_features:                     continue                 # selected features are all good features till now                 # and current feature                 selected_features = good_features + [feature]                 # remove all other features from data                 xtrain = X[:, selected_features]                 # calculate the score, in our case, AUC                 score = self.evaluate_score(xtrain, y)                 # if score is greater than the best score                 # of this loop, change best score and best feature                 if score > best_score:                     this_feature = feature                     best_score = score              # if we have selected a feature, add it ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4dd50c9e-ba30-464f-bbfd-c4e6bbab63ba', embedding=None, metadata={'page_label': '161', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 161             # to the good feature list and update best scores list             if this_feature != None:                 good_features.append(this_feature)                 best_scores.append(best_score)              # if we didnt improve during the previous round,             # exit the while loop             if len(best_scores) > 2:                 if best_scores[-1] < best_scores[-2]:                     break         # return best scores and good features         # why do we remove the last data point?         return best_scores[:-1], good_features[:-1]      def __call__(self, X, y):         \"\"\"         Call function will call the class on a set of arguments         \"\"\"         # select features, return scores and selected indices         scores, features = self._feature_selection(X, y)         # transform data with selected features         return X[:, features], scores  if __name__ == \"__main__\":     # generate binary classification data     X, y = make_classification(n_samples=1000, n_features=100)      # transform data by greedy feature selection     X_transformed, scores = GreedyFeatureSelection()(X, y) ═════════════════════════════════════════════════════════════════════════  The greedy feature selection implemented the way returns scores and a list of feature indices. Figure 2 shows how this score improves with the addition of a new feature in every iteration. We see that we are not able to improve our score after a certain point, and that’s where we stop.  Another greedy approach is known as recursive feature elimination (RFE). In the previous method, we started with one feature and kept adding new features, but in RFE, we start with all features and keep removing one feature in every iteration that provides the least value to a given model. But how to do we know which feature offers the least value? Well, if we use models like linear support vector machine (SVM) or logistic regression, we get a coefficient for each feature which decides the importance of the features. In case of any tree-based models, we get feature importance in place of coefficients. In each iteration, we can eliminate the least ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0229402-9762-4e86-a248-7dd57c6613c6', embedding=None, metadata={'page_label': '162', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 162 important feature and keep eliminating it until we reach the number of features needed. So, yes, we have the ability to decide how many features we want to keep. \\n Figure 2: How AUC score varies in greedy feature selection with the addition of new features  When we are doing recursive feature elimination, in each iteration, we remove the feature which has the feature importance or the feature which has a coefficient close to 0. Please remember that when you use a model like logistic regression for binary classification, the coefficients for features are more positive if they are important for the positive class and more negative if they are important for the negative class. It’s very easy to modify our greedy feature selection class to create a new class for recursive feature elimination, but scikit-learn also provides RFE out of the box. A simple usage is shown in the following example.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd  from sklearn.feature_selection import RFE from sklearn.linear_model import LinearRegression from sklearn.datasets import fetch_california_housing  # fetch a regression dataset data = fetch_california_housing() X = data[\"data\"] col_names = data[\"feature_names\"] y = data[\"target\"] \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='badbc30d-dd03-44c2-8c10-9abd506f9770', embedding=None, metadata={'page_label': '163', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 163  # initialize the model model = LinearRegression() # initialize RFE rfe = RFE(     estimator=model,     n_features_to_select=3 )  # fit RFE rfe.fit(X, y)  # get the transformed data with # selected columns X_transformed = rfe.transform(X) ═════════════════════════════════════════════════════════════════════════  We saw two different greedy ways to select features from a model. But you can also fit the model to the data and select features from the model by the feature coefficients or the importance of features. If you use coefficients, you can select a threshold, and if the coefficient is above that threshold, you can keep the feature else eliminate it.  Let’s see how we can get feature importance from a model like random forest.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor  # fetch a regression dataset # in diabetes data we predict diabetes progression # after one year based on some features data = load_diabetes() X = data[\"data\"] col_names = data[\"feature_names\"] y = data[\"target\"]  # initialize the model model = RandomForestRegressor()  # fit the model model.fit(X, y) ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ffb01c1-76b9-4311-a0d8-3c0c58290d5d', embedding=None, metadata={'page_label': '164', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 164 Feature importance from random forest (or any model) can be plotted as follows.  ═════════════════════════════════════════════════════════════════════════ importances = model.feature_importances_ idxs = np.argsort(importances) plt.title('Feature Importances') plt.barh(range(len(idxs)), importances[idxs], align='center') plt.yticks(range(len(idxs)), [col_names[i] for i in idxs]) plt.xlabel('Random Forest Feature Importance') plt.show() ═════════════════════════════════════════════════════════════════════════  The resulting plot is shown in figure 3. \\n Figure 3: Plot of feature importance  Well, selecting the best features from the model is nothing new. You can choose features from one model and use another model to train. For example, you can use Logistic Regression coefficients to select the features and then use Random Forest to train the model on chosen features. Scikit-learn also offers SelectFromModel class that helps you choose features directly from a given model. You can also specify the threshold for coefficients or feature importance if you want and the maximum number of features you want to select.   \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1177ba0c-5532-4159-b5e8-fcbdf468b640', embedding=None, metadata={'page_label': '165', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 165 Take a look at the following snippet where we select the features using default parameters in SelectFromModel. ═════════════════════════════════════════════════════════════════════════ import pandas as pd from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor from sklearn.feature_selection import SelectFromModel  # fetch a regression dataset # in diabetes data we predict diabetes progression # after one year based on some features data = load_diabetes() X = data[\"data\"] col_names = data[\"feature_names\"] y = data[\"target\"]  # initialize the model model = RandomForestRegressor()  # select from the model sfm = SelectFromModel(estimator=model) X_transformed = sfm.fit_transform(X, y)  # see which features were selected support = sfm.get_support()  # get feature names print([     x for x, y in zip(col_names, support) if y == True ]) ═════════════════════════════════════════════════════════════════════════  Which prints: [\\'bmi\\', \\'s5\\']. When we look at figure 3, we see that these are the top-2 features. Thus, we could have also selected directly from feature importance provided by random forest. One more thing that we are missing here is feature selection using models that have L1 (Lasso) penalization. When we have L1 penalization for regularization, most coefficients will be 0 (or close to 0), and we select the features with non-zero coefficients. You can do it by just replacing random forest in the snippet of selection from a model with a model that supports L1 penalty, e.g. lasso regression. All tree-based models provide feature importance so all the model-based snippets shown in this chapter can be used for XGBoost, LightGBM or CatBoost. The feature importance function names might be different and may produce results in a different format, but the usage will remain the same. In the end, you must be careful when doing feature selection. Select features on ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a176051-08ee-4ee9-90fb-ec63ce11db89', embedding=None, metadata={'page_label': '166', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 166 training data and validate the model on validation data for proper selection of features without overfitting the model.   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ee378a8-7a5e-48f5-acb9-50b94fa4b562', embedding=None, metadata={'page_label': '167', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 167 Hyperparameter optimization  With great models, comes the great problem of optimizing hyper-parameters to get the best scoring model. So, what is this hyper-parameter optimization? Suppose there is a simple pipeline for your machine learning project. There is a dataset, you directly apply a model, and then you have results. The parameters that the model has here are known as hyper-parameters, i.e. the parameters that control the training/fitting process of the model. If we train a linear regression with SGD, parameters of a model are the slope and the bias and hyperparameter is learning rate. You will notice that I use these terms interchangeably in this chapter and throughout this book. Let’s say there are three parameters a, b, c in the model, and all these parameters can be integers between 1 and 10. A “correct” combination of these parameters will provide you with the best result. So, it’s kind of like a suitcase with a 3-dial combination lock. However, in 3 dial combination lock has only one correct answer. The model has many right answers. So, how would you find the best parameters? A method would be to evaluate all the combinations and see which one improves the metric. Let’s see how this is done.  ═════════════════════════════════════════════════════════════════════════ # define the best accuracy to be 0 # if you choose loss as a metric, # you can make best loss to be inf (np.inf) best_accuracy = 0 best_parameters = {\"a\": 0, \"b\": 0, \"c\": 0}  # loop over all values for a, b & c for a in range(1, 11):     for b in range(1, 11):         for c in range(1, 11):             # inititalize model with current parameters             model = MODEL(a, b, c)             # fit the model             model.fit(training_data)             # make predictions             preds = model.predict(validation_data)             # calculate accuracy             accuracy = metrics.accuracy_score(targets, preds)             # save params if current accuracy             # is greater than best accuracy             if accuracy > best_accuracy:                 best_accuracy = accuracy                 best_parameters[\"a\"] = a ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d96cabf3-c91a-42af-a1dc-ae10f298f291', embedding=None, metadata={'page_label': '168', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 168                 best_parameters[\"b\"] = b                 best_parameters[\"c\"] = c ═════════════════════════════════════════════════════════════════════════  In the above code, we go through all the parameters from 1 to 10. So, we have a total of 1000 (10 x 10 x 10) fits for the model. Well, that might be expensive because the model can take a long time to train. In this situation, it should, however, be okay, but in a real-world scenario, there are not only three parameters and not only ten values for each parameter. Most models parameters are real-valued, and the combinations of different parameters can be infinite.  Let’s look at the random forest model from scikit-learn.  RandomForestClassifier(     n_estimators=100,     criterion=\\'gini\\',     max_depth=None,     min_samples_split=2,     min_samples_leaf=1,     min_weight_fraction_leaf=0.0,     max_features=\\'auto\\',     max_leaf_nodes=None,     min_impurity_decrease=0.0,     min_impurity_split=None,     bootstrap=True,     oob_score=False,     n_jobs=None,     random_state=None,     verbose=0,     warm_start=False,     class_weight=None,     ccp_alpha=0.0,     max_samples=None, )  There are nineteen parameters, and all the combinations of all these parameters for all the values they can assume are going to be infinite. Normally, we don’t have the resource and time to do this. Thus, we specify a grid of parameters. A search over this grid to find the best combination of parameters is known as grid search. We can say that n_estimators can be 100, 200, 250, 300, 400, 500; max_depth can be 1, 2, 5, 7, 11, 15 and criterion can be gini or entropy. These may not look like a lot of parameters, but it would take a lot of time for computation if the dataset is too large. We can make this grid search work by creating three for loops like before and ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b16db6a-1d54-447f-8173-141e569bff21', embedding=None, metadata={'page_label': '169', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 169 calculating the score on the validation set. It must also be noted that if you have k-fold cross-validation, you need even more loops which implies even more time to find the perfect parameters. Grid search is therefore not very popular. Let’s look at how it’s done with an example of predicting mobile phone price range given the specifications.  \\n Figure 1: A snapshot of the mobile price dataset7  We have 20 features like dual sim, battery power, etc. and a range of price which has 4 categories from 0 to 3. There are only 2000 samples in the training set. We can easily use stratified kfold and accuracy as a metric to evaluate. We will use a random forest model with the aforementioned parameter ranges and see how we can do a grid search in the following example.  ═════════════════════════════════════════════════════════════════════════ # rf_grid_search.py import numpy as np import pandas as pd  from sklearn import ensemble from sklearn import metrics from sklearn import model_selection  if __name__ == \"__main__\":     # read the training data     df = pd.read_csv(\"../input/mobile_train.csv\")      # features are all columns without price_range     # note that there is no id column in this dataset  7 https://www.kaggle.com/iabhishekofficial/mobile-price-classification \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='993a7c1e-331e-4fbe-ad4c-5f408cdfd64a', embedding=None, metadata={'page_label': '170', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 170     # here we have training features     X = df.drop(\"price_range\", axis=1).values     # and the targets     y = df.price_range.values      # define the model here     # i am using random forest with n_jobs=-1     # n_jobs=-1 => use all cores     classifier = ensemble.RandomForestClassifier(n_jobs=-1)      # define a grid of parameters     # this can be a dictionary or a list of     # dictionaries     param_grid = {         \"n_estimators\": [100, 200, 250, 300, 400, 500],         \"max_depth\": [1, 2, 5, 7, 11, 15],         \"criterion\": [\"gini\", \"entropy\"]     }          # initialize grid search     # estimator is the model that we have defined     # param_grid is the grid of parameters     # we use accuracy as our metric. you can define your own     # higher value of verbose implies a lot of details are printed     # cv=5 means that we are using 5 fold cv (not stratified)     model = model_selection.GridSearchCV(         estimator=classifier,          param_grid=param_grid,          scoring=\"accuracy\",         verbose=10,          n_jobs=1,         cv=5     )      # fit the model and extract best score     model.fit(X, y)     print(f\"Best score: {model.best_score_}\")      print(\"Best parameters set:\")     best_parameters = model.best_estimator_.get_params()     for param_name in sorted(param_grid.keys()):      print(f\"\\\\t{param_name}: {best_parameters[param_name]}\") ═════════════════════════════════════════════════════════════════════════  This prints a lot of stuff, let’s look at the last few lines.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73b513fd-ab23-4b83-9f0c-f6da2dea2338', embedding=None, metadata={'page_label': '171', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 171 ═════════════════════════════════════════════════════════════════════════ [CV]  criterion=entropy, max_depth=15, n_estimators=500, score=0.895, total=   1.0s [CV] criterion=entropy, max_depth=15, n_estimators=500 ............... [CV]  criterion=entropy, max_depth=15, n_estimators=500, score=0.890, total=   1.1s [CV] criterion=entropy, max_depth=15, n_estimators=500 ............... [CV]  criterion=entropy, max_depth=15, n_estimators=500, score=0.910, total=   1.1s [CV] criterion=entropy, max_depth=15, n_estimators=500 ............... [CV]  criterion=entropy, max_depth=15, n_estimators=500, score=0.880, total=   1.1s [CV] criterion=entropy, max_depth=15, n_estimators=500 ............... [CV]  criterion=entropy, max_depth=15, n_estimators=500, score=0.870, total=   1.1s [Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:  3.7min finished Best score: 0.889 Best parameters set:  criterion: \\'entropy\\'  max_depth: 15  n_estimators: 500 ═════════════════════════════════════════════════════════════════════════  In the end, we see that our best five fold accuracy score was 0.889, and we have the best parameters from our grid search. Next best thing that we can use is random search. In random search, we randomly select a combination of parameters and calculate the cross-validation score. The time consumed here is less than grid search because we do not evaluate over all different combinations of parameters. We choose how many times we want to evaluate our models, and that’s what decides how much time the search takes. The code is not very different from above. Except for GridSearchCV, we use RandomizedSearchCV.  ═════════════════════════════════════════════════════════════════════════ # rf_random_search.py . . .  if __name__ == \"__main__\":     .     .     .     # define the model here     # i am using random forest with n_jobs=-1     # n_jobs=-1 => use all cores ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1872b345-fa18-4e94-b2e0-190f7c6912ec', embedding=None, metadata={'page_label': '172', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 172     classifier = ensemble.RandomForestClassifier(n_jobs=-1)      # define a grid of parameters     # this can be a dictionary or a list of     # dictionaries     param_grid = {         \"n_estimators\": np.arange(100, 1500, 100),         \"max_depth\": np.arange(1, 31),         \"criterion\": [\"gini\", \"entropy\"]     }     # initialize random search     # estimator is the model that we have defined     # param_distributions is the grid/distribution of parameters     # we use accuracy as our metric. you can define your own     # higher value of verbose implies a lot of details are printed     # cv=5 means that we are using 5 fold cv (not stratified)     # n_iter is the number of iterations we want     # if param_distributions has all the values as list,     # random search will be done by sampling without replacement     # if any of the parameters come from a distribution,     # random search uses sampling with replacement     model = model_selection.RandomizedSearchCV(         estimator=classifier,          param_distributions=param_grid,         n_iter=20,         scoring=\"accuracy\",         verbose=10,          n_jobs=1,         cv=5     )      # fit the model and extract best score     model.fit(X, y)     print(f\"Best score: {model.best_score_}\")      print(\"Best parameters set:\")     best_parameters = model.best_estimator_.get_params()     for param_name in sorted(param_grid.keys()):      print(f\"\\\\t{param_name}: {best_parameters[param_name]}\") ═════════════════════════════════════════════════════════════════════════  We have changed the grid of parameters for random search, and it seems like we even improved the results a little bit.  ═════════════════════════════════════════════════════════════════════════ Best score: 0.8905 Best parameters set: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b41f8e4-059e-4062-bf17-4ccf66c5d5a2', embedding=None, metadata={'page_label': '173', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 173  criterion: entropy  max_depth: 25  n_estimators: 300 ═════════════════════════════════════════════════════════════════════════ Random search is faster than grid search if the number of iterations is less. Using these two, you can find the optimal (?) parameters for all kinds of models as long as they have a fit and predict function, which is the standard of scikit-learn. Sometimes, you might want to use a pipeline. For example, let’s say that we are dealing with a multiclass classification problem. In this problem, the training data consists of two text columns, and you are required to build a model to predict the class. Let’s assume that the pipeline you choose is to first apply tf-idf in a semi-supervised manner and then use SVD with SVM classifier. Now, the problem is we have to select the components of SVD and also need to tune the parameters of SVM. How to do this is shown in the following snippet.  ═════════════════════════════════════════════════════════════════════════ # pipeline_search.py import numpy as np import pandas as pd  from sklearn import metrics from sklearn import model_selection from sklearn import pipeline  from sklearn.decomposition import TruncatedSVD from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC   def quadratic_weighted_kappa(y_true, y_pred):     \"\"\"     Create a wrapper for cohen\\'s kappa     with quadratic weights     \"\"\"     return metrics.cohen_kappa_score(         y_true,          y_pred,          weights=\"quadratic\"     )   if __name__ == \\'__main__\\':      # Load the training file ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48481f25-16b8-4ceb-8476-822ad1796bef', embedding=None, metadata={'page_label': '174', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 174     train = pd.read_csv('../input/train.csv')          # we dont need ID columns     idx = test.id.values.astype(int)     train = train.drop('id', axis=1)     test = test.drop('id', axis=1)          # create labels. drop useless columns     y = train.relevance.values          # do some lambda magic on text columns     traindata = list(         train.apply(lambda x:'%s %s' % (x['text1'], x['text2']),axis=1)     )     testdata = list(         test.apply(lambda x:'%s %s' % (x['text1'], x['text2']),axis=1)     )          # tfidf vectorizer     tfv = TfidfVectorizer(         min_df=3,           max_features=None,          strip_accents='unicode',          analyzer='word',         token_pattern=r'\\\\w{1,}',         ngram_range=(1, 3),          use_idf=1,         smooth_idf=1,         sublinear_tf=1,         stop_words='english'     )          # Fit TFIDF     tfv.fit(traindata)     X =  tfv.transform(traindata)      X_test = tfv.transform(testdata)          # Initialize SVD     svd = TruncatedSVD()          # Initialize the standard scaler      scl = StandardScaler()          # We will use SVM here..     svm_model = SVC()          # Create the pipeline  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8597b6d7-7e58-42b1-9597-f9020f6c0ba3', embedding=None, metadata={'page_label': '175', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 175     clf = pipeline.Pipeline(         [             (\\'svd\\', svd),             (\\'scl\\', scl),             (\\'svm\\', svm_model)         ]     )          # Create a parameter grid to search for      # best parameters for everything in the pipeline     param_grid = {         \\'svd__n_components\\' : [200, 300],         \\'svm__C\\': [10, 12]     }          # Kappa Scorer      kappa_scorer = metrics.make_scorer(         quadratic_weighted_kappa,          greater_is_better=True     )          # Initialize Grid Search Model     model = model_selection.GridSearchCV(         estimator=clf,         param_grid=param_grid,         scoring=kappa_scorer,         verbose=10,         n_jobs=-1,         refit=True,         cv=5     )                                           # Fit Grid Search Model     model.fit(X, y)     print(\"Best score: %0.3f\" % model.best_score_)     print(\"Best parameters set:\")     best_parameters = model.best_estimator_.get_params()     for param_name in sorted(param_grid.keys()):      print(\"\\\\t%s: %r\" % (param_name, best_parameters[param_name]))          # Get best model     best_model = model.best_estimator_          # Fit model with best parameters optimized for QWK     best_model.fit(X, y)     preds = best_model.predict(...) ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7eb8e0cb-7dec-48a9-9a72-69a82892a346', embedding=None, metadata={'page_label': '176', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 176  The pipeline shown here has SVD (Singular Value Decomposition), standard scaling and an SVM (Support Vector Machines) model. Please note that you won’t be able to run the above code as it is as training data is not available. When we go into advanced hyperparameter optimization techniques, we can take a look at minimization of functions using different kinds of minimization algorithms. This can be achieved by using many minimization functions such as downhill simplex algorithm, Nelder-Mead optimization, using a Bayesian technique with Gaussian process for finding optimal parameters or by using a genetic algorithm. I will talk more about the application of downhill simplex and Nelder-Mead in ensembling and stacking chapter. First, let’s see how the gaussian process can be used for hyper-parameter optimization. These kinds of algorithms need a function they can optimize. Most of the time, it’s about the minimization of this function, like we minimize loss.   So, let’s say, you want to find the best parameters for best accuracy and obviously, the more the accuracy is better. Now we cannot minimize the accuracy, but we can minimize it when we multiply it by -1. This way, we are minimizing the negative of accuracy, but in fact, we are maximizing accuracy. Using Bayesian optimization with gaussian process can be accomplished by using gp_minimize function from scikit-optimize (skopt) library. Let’s take a look at how we can tune the parameters of our random forest model using this function.  ═════════════════════════════════════════════════════════════════════════ # rf_gp_minimize.py import numpy as np import pandas as pd  from functools import partial  from sklearn import ensemble from sklearn import metrics from sklearn import model_selection  from skopt import gp_minimize from skopt import space   def optimize(params, param_names, x, y):     \"\"\"     The main optimization function.      This function takes all the arguments from the search space     and training features and targets. It then initializes ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60507af3-81ce-46d4-b35b-7d640f35753b', embedding=None, metadata={'page_label': '177', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 177     the models by setting the chosen parameters and runs      cross-validation and returns a negative accuracy score     :param params: list of params from gp_minimize     :param param_names: list of param names. order is important!     :param x: training data     :param y: labels/targets     :return: negative accuracy after 5 folds     \"\"\"     # convert params to dictionary     params = dict(zip(param_names, params))      # initialize model with current parameters     model = ensemble.RandomForestClassifier(**params)      # initialize stratified k-fold     kf = model_selection.StratifiedKFold(n_splits=5)      # initialize accuracy list     accuracies = []      # loop over all folds     for idx in kf.split(X=x, y=y):         train_idx, test_idx = idx[0], idx[1]         xtrain = x[train_idx]         ytrain = y[train_idx]          xtest = x[test_idx]         ytest = y[test_idx]          # fit model for current fold         model.fit(xtrain, ytrain)          #create predictions         preds = model.predict(xtest)          # calculate and append accuracy         fold_accuracy = metrics.accuracy_score(             ytest,             preds         )         accuracies.append(fold_accuracy)          # return negative accuracy     return -1 * np.mean(accuracies)   if __name__ == \"__main__\": ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a95841d-bb88-4f81-9d88-7610bdc2f3e8', embedding=None, metadata={'page_label': '178', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 178     # read the training data     df = pd.read_csv(\"../input/mobile_train.csv\")      # features are all columns without price_range     # note that there is no id column in this dataset     # here we have training features     X = df.drop(\"price_range\", axis=1).values     # and the targets     y = df.price_range.values      # define a parameter space     param_space = [         # max_depth is an integer between 3 and 10         space.Integer(3, 15, name=\"max_depth\"),         # n_estimators is an integer between 50 and 1500         space.Integer(100, 1500, name=\"n_estimators\"),         # criterion is a category. here we define list of categories         space.Categorical([\"gini\", \"entropy\"], name=\"criterion\"),         # you can also have Real numbered space and define a          # distribution you want to pick it from         space.Real(0.01, 1, prior=\"uniform\", name=\"max_features\")     ]      # make a list of param names     # this has to be same order as the search space     # inside the main function     param_names = [         \"max_depth\",         \"n_estimators\",         \"criterion\",         \"max_features\"     ]      # by using functools partial, i am creating a      # new function which has same parameters as the      # optimize function except for the fact that     # only one param, i.e. the \"params\" parameter is     # required. this is how gp_minimize expects the      # optimization function to be. you can get rid of this     # by reading data inside the optimize function or by     # defining the optimize function here.     optimization_function = partial(         optimize,         param_names=param_names,         x=X,         y=y     ) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0fa9576-9881-4332-a812-9ffb361e5c1f', embedding=None, metadata={'page_label': '179', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 179      # now we call gp_minimize from scikit-optimize     # gp_minimize uses bayesian optimization for      # minimization of the optimization function.     # we need a space of parameters, the function itself,     # the number of calls/iterations we want to have     result = gp_minimize(         optimization_function,         dimensions=param_space,         n_calls=15,         n_random_starts=10,         verbose=10     )      # create best params dict and print it     best_params = dict(         zip(             param_names,             result.x         )     )     print(best_params) ═════════════════════════════════════════════════════════════════════════  Yet again, this produces a lot of output, and the last part of it is shown below.  ═════════════════════════════════════════════════════════════════════════ Iteration No: 14 started. Searching for the next optimal point. Iteration No: 14 ended. Search finished for the next optimal point. Time taken: 4.7793 Function value obtained: -0.9075 Current minimum: -0.9075 Iteration No: 15 started. Searching for the next optimal point. Iteration No: 15 ended. Search finished for the next optimal point. Time taken: 49.4186 Function value obtained: -0.9075 Current minimum: -0.9075 {'max_depth': 12, 'n_estimators': 100, 'criterion': 'entropy', 'max_features': 1.0} ═════════════════════════════════════════════════════════════════════════  It seems like we have managed to crack 0.90 accuracy. That’s just amazing!  We can also see (plot) how we achieved convergence by using the following snippet.  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ace66d13-7afc-48cd-a230-5e020bd23f7e', embedding=None, metadata={'page_label': '180', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 180 ═════════════════════════════════════════════════════════════════════════ from skopt.plots import plot_convergence  plot_convergence(result) ═════════════════════════════════════════════════════════════════════════ The convergence plot is shown in figure 2.  \\n Figure 2: Convergence plot of our random forest parameter optimization   There are many libraries available that offer hyperparameter optimization. scikit-optimize is one such library that you can use. Another useful library for hyperparameter optimization is hyperopt. hyperopt uses Tree-structured Parzen Estimator (TPE) to find the most optimal parameters. Take a look at the following snippet where I use hyperopt with minimal changes to the previous code.  ═════════════════════════════════════════════════════════════════════════ # rf_hyperopt.py import numpy as np import pandas as pd  from functools import partial  from sklearn import ensemble from sklearn import metrics from sklearn import model_selection  from hyperopt import hp, fmin, tpe, Trials \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41c1f4aa-735d-45e5-915e-ff49e5a81925', embedding=None, metadata={'page_label': '181', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 181 from hyperopt.pyll.base import scope   def optimize(params, x, y):     \"\"\"     The main optimization function.      This function takes all the arguments from the search space     and training features and targets. It then initializes     the models by setting the chosen parameters and runs      cross-validation and returns a negative accuracy score     :param params: dict of params from hyperopt     :param x: training data     :param y: labels/targets     :return: negative accuracy after 5 folds     \"\"\"      # initialize model with current parameters     model = ensemble.RandomForestClassifier(**params)      # initialize stratified k-fold     kf = model_selection.StratifiedKFold(n_splits=5)      .     .     .          # return negative accuracy     return -1 * np.mean(accuracies)   if __name__ == \"__main__\":     # read the training data     df = pd.read_csv(\"../input/mobile_train.csv\")      # features are all columns without price_range     # note that there is no id column in this dataset     # here we have training features     X = df.drop(\"price_range\", axis=1).values     # and the targets     y = df.price_range.values      # define a parameter space     # now we use hyperopt      param_space = {         # quniform gives round(uniform(low, high) / q) * q         # we want int values for depth and estimators ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f1e2b976-edbb-476b-a60b-001e7bc8338b', embedding=None, metadata={'page_label': '182', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 182         \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 15, 1)),         \"n_estimators\": scope.int(             hp.quniform(\"n_estimators\", 100, 1500, 1)         ),         # choice chooses from a list of values         \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),         # uniform chooses a value between two values         \"max_features\": hp.uniform(\"max_features\", 0, 1)     }      # partial function     optimization_function = partial(         optimize,         x=X,         y=y     )      # initialize trials to keep logging information     trials = Trials()          # run hyperopt     hopt = fmin(         fn=optimization_function,         space=param_space,         algo=tpe.suggest,         max_evals=15,         trials=trials      )     print(hopt) ═════════════════════════════════════════════════════════════════════════  As you can see, this is not very different from the previous code. You have to define the parameter space in a different format, and you also need to change the actual optimization part by using hyperopt instead of gp_minimize. The results are quite good!  ═════════════════════════════════════════════════════════════════════════ ❯ python rf_hyperopt.py 100%|██████████████████| 15/15 [04:38<00:00, 18.57s/trial, best loss: -0.9095000000000001] {\\'criterion\\': 1, \\'max_depth\\': 11.0, \\'max_features\\': 0.821163568049807, \\'n_estimators\\': 806.0} ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61ca468f-c594-44c2-8c24-e3230e48f3ae', embedding=None, metadata={'page_label': '183', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 183 We get an accuracy which is a little better than before and a set of parameters that we can use. Please note that criterion is 1 in the final result. This implies that choice 1 was selected, i.e., entropy.  The ways of tuning hyperparameters described above are the most common, and these will work with almost all models: linear regression, logistic regression, tree-based methods, gradient boosting models such as xgboost, lightgbm, and even neural networks!  Although, these methods exist, to learn, one must start with tuning the hyper-parameters manually, i.e., by hand. Hand tuning will help you learn the basics, for example, in gradient boosting, when you increase the depth, you should reduce the learning rate. It won’t be possible to learn this if you use automated tools. Refer to the following table to know what to tune. RS* implies random search should be better.  Once you get better with hand-tuning the parameters, you might not even need any automated hyper-parameter tuning. When you create large models or introduce a lot of features, you also make it susceptible to overfitting the training data. To avoid overfitting, you need to introduce noise in training data features or penalize the cost function. This penalization is called regularization and helps with generalizing the model. In linear models, the most common types of regularizations are L1 and L2. L1 is also known as Lasso regression and L2 as Ridge regression. When it comes to neural networks, we use dropouts, the addition of augmentations, noise, etc. to regularize our models. Using hyper-parameter optimization, you can also find the correct penalty to use.     ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6de84d9d-6e26-4e0c-89da-b446403c3155', embedding=None, metadata={'page_label': '184', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 184   Model Optimize Range of values Linear Regression  - fit_intercept - normalize   - True/False - True/False Ridge  - alpha - fit_intercept - normalize   - 0.01, 0.1, 1.0, 10, 100 - True/False - True/False k-neighbors  - n_neighbors - p  - 2, 4, 8, 16 …. - 2, 3  SVM  - C - gamma - class_weight   - 0.001,0.01..10..100..1000 - ‘auto’, RS* - ‘balanced’ , None Logistic Regression  - Penalty - C   - l1 or l2 - 0.001, 0.01…..10...100  Lasso  - Alpha - Normalize  - 0.1, 1.0, 10 - True/False  Random Forest  - n_estimators - max_depth - min_samples_split - min_samples_leaf - max features  - 120, 300, 500, 800, 1200 - 5, 8, 15, 25, 30, None - 1, 2, 5, 10, 15, 100 - 1, 2, 5, 10 - log2, sqrt, None  XGBoost  - eta - gamma - max_depth - min_child_weight - subsample - colsample_bytree - lambda - alpha   - 0.01,0.015, 0.025, 0.05, 0.1 - 0.05-0.1,0.3,0.5,0.7,0.9,1.0 - 3, 5, 7, 9, 12, 15, 17, 25 - 1, 3, 5, 7 - 0.6, 0.7, 0.8, 0.9, 1.0 - 0.6, 0.7, 0.8, 0.9, 1.0 - 0.01-0.1, 1.0 , RS* - 0, 0.1, 0.5, 1.0 RS*  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce27c63e-e6ac-488a-a7c2-a602deabd0ea', embedding=None, metadata={'page_label': '185', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 185 Approaching image classification & segmentation  When it comes to images, a lot has been achieved in the last few years. Computer vision is progressing quite fast, and it feels like many problems of computer vision are now much easier to solve. With the advent of pretrained models and cheaper compute, it’s now as easy as pie to train a near state-of-the-art model at home for most of the problems related to images. But there are many different types of image problems. You can have the standard classification of images in two or more categories to a challenging problem like self-driving cars. We won’t look at self-driving cars in this book, but we will obviously deal with some of the most common image problems.  What are the different approaches that we can apply to images? Image is nothing but a matrix of numbers. The computer cannot see the images as humans do. It only looks at numbers, and that’s what the images are. A grayscale image is a two-dimensional matrix with values ranging from 0 to 255. 0 is black, 255 is white and in between you have all shades of grey. Previously, when there was no deep learning (or when deep learning was not popular), people used to look at pixels. Each pixel was a feature. You can do this easily in Python. Just read the grayscale image using OpenCV or Python-PIL, convert to a numpy array and ravel (flatten) the matrix. If you are dealing with RGB images, then you have three matrices instead of one. But the idea remains the same.  ═════════════════════════════════════════════════════════════════════════ import numpy as np import matplotlib.pyplot as plt  # generate random numpy array with values from 0 to 255 # and a size of 256x256 random_image = np.random.randint(0, 256, (256, 256)) # initialize plot plt.figure(figsize=(7, 7)) # show grayscale image, nb: cmap, vmin and vmax plt.imshow(random_image, cmap='gray', vmin=0, vmax=255) plt.show() ═════════════════════════════════════════════════════════════════════════  The code above generates a random matrix using numpy. This matrix consists of values ranging from 0 to 255 (included) and is of size 256x256 (also known as pixels). \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7edb9570-2dc3-4069-bcf5-4a066539974d', embedding=None, metadata={'page_label': '186', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 186 Image Ravelled version of the image     Figure 1: A 2-D image array (single channel) and its ravelled version  As you can see that the ravelled version is nothing but a vector of size M, where M = N * N. In this case, this vector is of the size 256 * 256 = 65536.  Now, if we go ahead and do it for all the images in our dataset, we have 65536 features for each sample. We can now quickly build a decision tree model or random forest or SVM-based model on this data. The models will look at pixel values and would try to separate positive samples from negative samples (in case of a binary classification problem).  All of you must have heard about the cats vs dogs problem. It's a classic one. But let's try something different. If you remember, at the beginning of the chapter on evaluation metrics, I introduced you to a dataset of pneumothorax images. So, let’s try building a model to detect if an X-ray image of a lung has pneumothorax or not. That is, a (not so) simple binary classification.   No pneumothorax Pneumothorax \\n  Figure 2: Comparison of non-pneumothorax and pneumothorax x-ray images8.  8 https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation \\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a34a19a9-61d8-4746-b543-18d19fe0dccc', embedding=None, metadata={'page_label': '187', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 187 In figure 2, you can see a comparison between non-pneumothorax and pneumothorax images. As you must have already noticed, it is quite difficult for a non-expert (like me) to even identify which of these images have pneumothorax.   The original dataset is about detecting where exactly pneumothorax is present, but we have modified the problem to find if the given x-ray image has pneumothorax or not. Don’t worry; we will cover the where part in this chapter. The dataset consists of 10675 unique images and 2379 have pneumothorax (note that these numbers are after some cleaning of data and thus do not match original dataset). As a data doctor would say: this is a classic case of skewed binary classification. Therefore, we choose the evaluation metric to be AUC and go for a stratified k-fold cross-validation scheme.  You can flatten out the features and try some classical methods like SVM, RF for doing classification, which is perfectly fine, but it won\\'t get you anywhere near state of the art. Also, the images are of size 1024x1024. It’s going to take a long time to train a model on this dataset. For what it’s worth, let’s try building a simple random forest model on this data. Since the images are grayscale, we do not need to do any kind of conversion. We will resize the images to 256x256 to make them smaller and use AUC as a metric as discussed before.  Let’s see how this performs.  ═════════════════════════════════════════════════════════════════════════ import os  import numpy as np import pandas as pd  from PIL import Image from sklearn import ensemble from sklearn import metrics from sklearn import model_selection from tqdm import tqdm   def create_dataset(training_df, image_dir):     \"\"\"     This function takes the training dataframe     and outputs training array and labels     :param training_df: dataframe with ImageId, Target columns     :param image_dir: location of images (folder), string     :return: X, y (training array with features and labels) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1a0c259-8718-4dba-9c46-a89c23d836dd', embedding=None, metadata={'page_label': '188', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 188     \"\"\"     # create empty list to store image vectors     images = []     # create empty list to store targets     targets = []     # loop over the dataframe     for index, row in tqdm(         training_df.iterrows(),          total=len(training_df),          desc=\"processing images\"     ):         # get image id         image_id = row[\"ImageId\"]         # create image path         image_path = os.path.join(image_dir, image_id)         # open image using PIL         image = Image.open(image_path + \".png\")         # resize image to 256x256. we use bilinear resampling         image = image.resize((256, 256), resample=Image.BILINEAR)         # convert image to array         image = np.array(image)         # ravel         image = image.ravel()         # append images and targets lists         images.append(image)         targets.append(int(row[\"target\"]))     # convert list of list of images to numpy array     images = np.array(images)     # print size of this array     print(images.shape)     return images, targets   if __name__ == \"__main__\":     csv_path = \"/home/abhishek/workspace/siim_png/train.csv\"     image_path = \"/home/abhishek/workspace/siim_png/train_png/\"      # read CSV with imageid and target columns     df = pd.read_csv(csv_path)      # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1          # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)          # fetch labels ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61cd9aed-6d1a-4302-ae6b-b119383c7d1d', embedding=None, metadata={'page_label': '189', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 189     y = df.target.values          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):         df.loc[v_, \\'kfold\\'] = f      # we go over the folds created     for fold_ in range(5):         # temporary dataframes for train and test         train_df = df[df.kfold != fold_].reset_index(drop=True)         test_df = df[df.kfold == fold_].reset_index(drop=True)                  # create train dataset         # you can move this outside to save some computation time         xtrain, ytrain = create_dataset(train_df, image_path)          # create test dataset         # you can move this outside to save some computation time         xtest, ytest = create_dataset(test_df, image_path)          # fit random forest without any modification of params         clf = ensemble.RandomForestClassifier(n_jobs=-1)         clf.fit(xtrain, ytrain)          # predict probability of class 1         preds = clf.predict_proba(xtest)[:, 1]          # print results         print(f\"FOLD: {fold_}\")         print(f\"AUC = {metrics.roc_auc_score(ytest, preds)}\")         print(\"\") ═════════════════════════════════════════════════════════════════════════  This gives a mean AUC of around 0.72.  Which is not bad but hopefully, we can do a lot better. You can use this approach for images, and this is how it was used in good old times. SVM was quite famous for image datasets. Deep Learning has been proved to be the state of the art when solving such problems, hence we could try that next.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b892fc68-c49d-4753-b76d-eb2dd0269f5b', embedding=None, metadata={'page_label': '190', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 190 I won’t go into the history of deep learning and who invented what. Instead, let’s take a look at one of the most famous deep learning models AlexNet and see what’s happening there.   \\n Figure 3: AlexNet architecture9. Please note that the input size in this figure is not 224x224 but 227x227  Nowadays, you might say that it is a basic deep convolutional neural network, but it is the foundation of many new deep nets (deep neural networks). We see that the network in figure 3 is a convolutional neural network with five convolution layers, two dense layers and an output layer. We see that there is also max pooling. What is it? Let’s look at some terms which you will come across when doing deep learning. \\n Figure 4: An image of size 8x8 with a filter of size 3x3 and stride of 2.  Figure 4 introduces two new terms: filter and strides. Filters are nothing but two-dimensional matrices which are initialized by a given function. “He initialization”  9 A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012 \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1db0d5b6-71ea-44ff-8b2a-3523216cd53e', embedding=None, metadata={'page_label': '191', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 191 which is also known “Kaiming normal initialization” is a good choice for convolutional neural networks. It is because most modern networks use ReLU (Rectified Linear Units) activation function and proper initialization is required to avoid the problem of vanishing gradients (when gradients approach zero and weights of network do not change). This filter is convolved with the image. Convolution is nothing but a summation of elementwise multiplication (cross-correlation) between the filter and the pixels it is currently overlapping in a given image. You can read more about convolution in any high school mathematics textbook. We start convolution of this filter from the top left corner of the image, and we move it horizontally. If we move it by 1 pixel, the stride is 1. If we move it by 2 pixels, the stride is 2. And that’s what stride is.   Stride is a useful concept even in natural language processing, e.g. in question and answering systems when you have to filter answer from a large text corpus. When we are exhausted horizontally, we move the filter by the same stride downwards vertically, starting from left again. Figure 4 also shows a filter going outside the image. In these cases, it’s not possible to calculate the convolution. So, we skip it. If you don’t want to skip it, you will need to pad the image. It must also be noted that convolution will decrease the size of the image. Padding is also a way to keep the size of the image the same. In figure 4, A 3x3 filter is moving horizontally and vertically, and every time it moves, it skips two columns and two rows (i.e. pixels) respectively. Since it skips two pixels, stride = 2. And resulting image size is [(8-3) / 2] + 1 = 3.5. We take the floor of 3.5, so its 3x3. You can do it by hand by moving the filters on a pen and paper.  \\n Figure 5: Padding enables us to provide an image with the same size as the input \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='505e350d-4ab2-40b9-ae25-3cc6c8cd6e51', embedding=None, metadata={'page_label': '192', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 192 We see the effect of padding in figure 5. Now, we have a 3x3 filter which is moving with a stride of 1. Size of the original image is 6x6, and we have added padding of 1. The padding of 1 means increasing the size of the image by adding zero pixels on each side once. In this case, the resulting image will be of the same size as the input image, i.e. 6x6. Another relevant term that you might come across when dealing with deep neural networks is dilation, as shown in figure 6.  \\n Figure 6: Example of dilation  In dilation, we expand the filter by N-1, where N is the value of dilation rate or simply known as dilation. In this kind of kernel with dilation, you skip some pixels in each convolution. This is particularly effective in segmentation tasks. Please note that we have only talked about 2-dimensional convolutions. There are 1-d convolutions too and also in higher dimensions. All work on the same underlying concept.  Next comes max-pooling. Max pooling is nothing but a filter which returns max. So, instead of convolution, we are extracting the max value of pixels. Similarly, average pooling or mean-pooling returns mean of pixels. They are used in the same way as the convolutional kernel. Pooling is faster than convolution and is a way to down-sample the image. Max pooling detects edges and average pooling smoothens the image.  \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d02bd37f-6b18-41b5-a7a8-fdc9f9e1860d', embedding=None, metadata={'page_label': '193', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 193 There are way too many concepts in convolutional neural networks and deep learning. The ones that I discussed are some of the basics that will help you get started. Now, we are well prepared to start building our first convolutional neural network in PyTorch. PyTorch provides an intuitive and easy way to implement deep neural networks, and you don’t need to care about back-propagation. We define the network in a python class and a forward function that tells PyTorch how the layers are connected to each other. In PyTorch, the image notation is BS, C, H, W, where, BS is the batch size, C channels, H is height and W is the width. Let’s see how AlexNet is implemented in PyTorch.  ═════════════════════════════════════════════════════════════════════════ import torch import torch.nn as nn import torch.nn.functional as F   class AlexNet(nn.Module):     def __init__(self):         super(AlexNet, self).__init__()         # convolution part         self.conv1 = nn.Conv2d(             in_channels=3,              out_channels=96,              kernel_size=11,              stride=4,              padding=0         )         self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)         self.conv2 = nn.Conv2d(             in_channels=96,             out_channels=256,             kernel_size=5,             stride=1,             padding=2         )         self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)         self.conv3 = nn.Conv2d(             in_channels=256,             out_channels=384,             kernel_size=3,             stride=1,             padding=1         )         self.conv4 = nn.Conv2d(in_channels=384,out_channels=384,             kernel_size=3, stride=1,padding=1) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ae2b582-a25e-458c-a633-906ce3c3b2a3', embedding=None, metadata={'page_label': '194', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 194         self.conv5 = nn.Conv2d(in_channels=384, out_channels=256,             kernel_size=3,             stride=1,             padding=1         )         self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)         # dense part         self.fc1 = nn.Linear(             in_features=9216,              out_features=4096         )         self.dropout1 = nn.Dropout(0.5)         self.fc2 = nn.Linear(             in_features=4096,              out_features=4096         )         self.dropout2 = nn.Dropout(0.5)         self.fc3 = nn.Linear(             in_features=4096,              out_features=1000         )     def forward(self, image):         # get the batch size, channels, height and width         # of the input batch of images         # original size: (bs, 3, 227, 227)         bs, c, h, w = image.size()         x = F.relu(self.conv1(image))  # size: (bs, 96, 55, 55)         x = self.pool1(x)  # size: (bs, 96, 27, 27)         x = F.relu(self.conv2(x))  # size: (bs, 256, 27, 27)         x = self.pool2(x)  # size: (bs, 256, 13, 13)         x = F.relu(self.conv3(x))  # size: (bs, 384, 13, 13)         x = F.relu(self.conv4(x))  # size: (bs, 384, 13, 13)         x = F.relu(self.conv5(x))  # size: (bs, 256, 13, 13)         x = self.pool3(x)  # size: (bs, 256, 6, 6)         x = x.view(bs, -1)  # size: (bs, 9216)         x = F.relu(self.fc1(x))  # size: (bs, 4096)         x = self.dropout1(x)  # size: (bs, 4096)         # dropout does not change size         # dropout is used for regularization         # 0.3 dropout means that only 70% of the nodes          # of the current layer are used for the next layer         x = F.relu(self.fc2(x))  # size: (bs, 4096)         x = self.dropout2(x)  # size: (bs, 4096)         x = F.relu(self.fc3(x))  # size: (bs, 1000)         # 1000 is number of classes in ImageNet Dataset         # softmax is an activation function that converts         # linear output to probabilities that add up to 1 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e2426961-b263-48c2-aa20-f2a3efc92b57', embedding=None, metadata={'page_label': '195', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 195         # for each sample in the batch         x = torch.softmax(x, axis=1)  # size: (bs, 1000)         return x ═════════════════════════════════════════════════════════════════════════  When you have a 3x227x227 image, and you apply a convolutional filter of size 11x11, it means that you are applying a filter of size 11x11x3 and convolving it with an image of size 227x227x3. So, now, you need to think in 3 dimensions instead of 2. The number of output channels is the number of different convolutional filters of the same size applied to the image individually. So, in the first convolutional layer, the input channels are 3, which is the original input, i.e. the R, G, B channels. PyTorch’s torchvision offers many different models like AlexNet, and it must be noted that this implementation of AlexNet is not the same as torchvision’s. Torchvision’s implementation of AlexNet is a modified AlexNet from another paper: Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.  You can design your own convolutional neural networks for your task, and many times it is a good idea to start from something on your own. Let’s build a network to classify images from our initial dataset of this chapter into categories of having pneumothorax or not. But first, let’s prepare some files. The first step would be to create a folds file, i.e. train.csv but with a new column kfold. We will create five folds. Since I have shown how to do this for different datasets in this book, I will skip this part and leave it an exercise for you. For PyTorch based neural networks, we need to create a dataset class. The objective of the dataset class is to return an item or sample of data. This sample of data should consist of everything you need in order to train or evaluate your model.  ═════════════════════════════════════════════════════════════════════════ # dataset.py import torch  import numpy as np  from PIL import Image from PIL import ImageFile  # sometimes, you will have images without an ending bit # this takes care of those kind of (corrupt) images ImageFile.LOAD_TRUNCATED_IMAGES = True   class ClassificationDataset: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='409edee3-9c62-4606-9bc2-f42e2613cd60', embedding=None, metadata={'page_label': '196', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 196     \"\"\"     A general classification dataset class that you can use for all     kinds of image classification problems. For example,     binary classification, multi-class, multi-label classification     \"\"\"     def __init__(         self,          image_paths,          targets,          resize=None,          augmentations=None     ):         \"\"\"         :param image_paths: list of path to images         :param targets: numpy array         :param resize: tuple, e.g. (256, 256), resizes image if not None         :param augmentations: albumentation augmentations         \"\"\"         self.image_paths = image_paths         self.targets = targets         self.resize = resize         self.augmentations = augmentations      def __len__(self):         \"\"\"         Return the total number of samples in the dataset         \"\"\"         return len(self.image_paths)      def __getitem__(self, item):         \"\"\"         For a given \"item\" index, return everything we need         to train a given model         \"\"\"         # use PIL to open the image         image = Image.open(self.image_paths[item])         # convert image to RGB, we have single channel images         image = image.convert(\"RGB\")         # grab correct targets         targets = self.targets[item]          # resize if needed         if self.resize is not None:             image = image.resize(                 (self.resize[1], self.resize[0]),                  resample=Image.BILINEAR             ) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68ddd1c8-cdc0-4ea1-95ac-a8d677fe4139', embedding=None, metadata={'page_label': '197', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 197         # convert image to numpy array         image = np.array(image)          # if we have albumentation augmentations         # add them to the image         if self.augmentations is not None:             augmented = self.augmentations(image=image)             image = augmented[\"image\"]          # pytorch expects CHW instead of HWC         image = np.transpose(image, (2, 0, 1)).astype(np.float32)          # return tensors of image and targets         # take a look at the types!         # for regression tasks,          # dtype of targets will change to torch.float         return {             \"image\": torch.tensor(image, dtype=torch.float),             \"targets\": torch.tensor(targets, dtype=torch.long),         } ═════════════════════════════════════════════════════════════════════════  Now we need engine.py. engine.py has training and evaluation functions. Let’s see how engine.py looks like.  ═════════════════════════════════════════════════════════════════════════ # engine.py import torch import torch.nn as nn  from tqdm import tqdm   def train(data_loader, model, optimizer, device):     \"\"\"     This function does training for one epoch     :param data_loader: this is the pytorch dataloader     :param model: pytorch model     :param optimizer: optimizer, for e.g. adam, sgd, etc     :param device: cuda/cpu     \"\"\"      # put the model in train mode     model.train()      # go over every batch of data in data loader     for data in data_loader: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6d374dd-b08d-499f-abc7-7adaab16c445', embedding=None, metadata={'page_label': '198', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 198         # remember, we have image and targets         # in our dataset class         inputs = data[\"image\"]         targets = data[\"targets\"]          # move inputs/targets to cuda/cpu device         inputs = inputs.to(device, dtype=torch.float)         targets = targets.to(device, dtype=torch.float)          # zero grad the optimizer         optimizer.zero_grad()         # do the forward step of model         outputs = model(inputs)         # calculate loss         loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))         # backward step the loss         loss.backward()         # step optimizer         optimizer.step()         # if you have a scheduler, you either need to         # step it here or you have to step it after         # the epoch. here, we are not using any learning         # rate scheduler   def evaluate(data_loader, model, device):     \"\"\"     This function does evaluation for one epoch     :param data_loader: this is the pytorch dataloader     :param model: pytorch model     :param device: cuda/cpu     \"\"\"     # put model in evaluation mode     model.eval()      # init lists to store targets and outputs     final_targets = []     final_outputs = []      # we use no_grad context     with torch.no_grad():          for data in data_loader:             inputs = data[\"image\"]             targets = data[\"targets\"]             inputs = inputs.to(device, dtype=torch.float)             targets = targets.to(device, dtype=torch.float) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a06f2ef8-4d09-4077-ab4e-a605ef8c44c8', embedding=None, metadata={'page_label': '199', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 199             # do the forward step to generate prediction             output = model(inputs)              # convert targets and outputs to lists             targets = targets.detach().cpu().numpy().tolist()             output = output.detach().cpu().numpy().tolist()                          # extend the original list             final_targets.extend(targets)             final_outputs.extend(output)          # return final output and final targets     return final_outputs, final_targets ═════════════════════════════════════════════════════════════════════════  Once we have engine.py, we are ready to create a new file: model.py. model.py will consist of our model. It’s a good idea to keep it separate because that allows us to easily experiment with different models and different architectures. A PyTorch library called pretrainedmodels has a lot of different model architectures, such as AlexNet, ResNet, DenseNet, etc. There are different model architectures which have been trained on large image dataset called ImageNet. We can use them with their weights after training on ImageNet, and we can also use them without these weights. If we train without the ImageNet weights, it means our network is learning everything from scratch. This is what model.py looks like.  ═════════════════════════════════════════════════════════════════════════ # model.py import torch.nn as nn import pretrainedmodels   def get_model(pretrained):     if pretrained:         model = pretrainedmodels.__dict__[\"alexnet\"](             pretrained=\\'imagenet\\'         )     else:         model = pretrainedmodels.__dict__[\"alexnet\"](             pretrained=None         )     # print the model here to know whats going on.     model.last_linear = nn.Sequential(         nn.BatchNorm1d(4096),         nn.Dropout(p=0.25),         nn.Linear(in_features=4096, out_features=2048), ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='196a7ae0-8356-4170-827b-731b0f7f8a84', embedding=None, metadata={'page_label': '200', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 200         nn.ReLU(),         nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),         nn.Dropout(p=0.5),         nn.Linear(in_features=2048, out_features=1),     )     return model ═════════════════════════════════════════════════════════════════════════  If you print the final model, you will be able to see what it looks like:  ═════════════════════════════════════════════════════════════════════════ AlexNet(   (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))   (_features): Sequential(     (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))     (1): ReLU(inplace=True)     (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)     (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))     (4): ReLU(inplace=True)     (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)     (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))     (7): ReLU(inplace=True)     (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))     (9): ReLU(inplace=True)     (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))     (11): ReLU(inplace=True)     (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)   )   (dropout0): Dropout(p=0.5, inplace=False)   (linear0): Linear(in_features=9216, out_features=4096, bias=True)   (relu0): ReLU(inplace=True)   (dropout1): Dropout(p=0.5, inplace=False)   (linear1): Linear(in_features=4096, out_features=4096, bias=True)   (relu1): ReLU(inplace=True)   (last_linear): Sequential(     (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)     (1): Dropout(p=0.25, inplace=False)     (2): Linear(in_features=4096, out_features=2048, bias=True) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b1906fe-2332-4d16-9e97-e9c9e5d90362', embedding=None, metadata={'page_label': '201', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 201     (3): ReLU()     (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)     (5): Dropout(p=0.5, inplace=False)     (6): Linear(in_features=2048, out_features=1, bias=True)   ) ) ═════════════════════════════════════════════════════════════════════════  Now, we have everything, and we can start training. We will do so using train.py.  ═════════════════════════════════════════════════════════════════════════ # train.py import os  import pandas as pd import numpy as np  import albumentations import torch  from sklearn import metrics from sklearn.model_selection import train_test_split  import dataset import engine from model import get_model   if __name__ == \"__main__\":     # location of train.csv and train_png folder     # with all the png images      data_path = \"/home/abhishek/workspace/siim_png/\"      # cuda/cpu device     device = \"cuda\"      # let\\'s train for 10 epochs     epochs = 10      # load the dataframe     df = pd.read_csv(os.path.join(data_path, \"train.csv\"))      # fetch all image ids     images = df.ImageId.values.tolist()  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c00334ff-fc39-465f-86ed-3574f70f9940', embedding=None, metadata={'page_label': '202', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 202     # a list with image locations     images = [       os.path.join(data_path, \"train_png\", i + \".png\") for i in images     ]      # binary targets numpy array     targets = df.target.values      # fetch out model, we will try both pretrained     # and non-pretrained weights     model = get_model(pretrained=True)      # move model to device     model.to(device)      # mean and std values of RGB channels for imagenet dataset     # we use these pre-calculated values when we use weights     # from imagenet.      # when we do not use imagenet weights, we use the mean and     # standard deviation values of the original dataset     # please note that this is a separate calculation     mean = (0.485, 0.456, 0.406)     std = (0.229, 0.224, 0.225)      # albumentations is an image augmentation library     # that allows you to do many different types of image     # augmentations. here, i am using only normalization     # notice always_apply=True. we always want to apply     # normalization     aug = albumentations.Compose(         [             albumentations.Normalize(                 mean, std, max_pixel_value=255.0, always_apply=True             )         ]     )      # instead of using kfold, i am using train_test_split     # with a fixed random state     train_images, valid_images, train_targets, valid_targets = train_test_split(         images, targets, stratify=targets, random_state=42     )      # fetch the ClassificationDataset class     train_dataset = dataset.ClassificationDataset(         image_paths=train_images, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd16a5b9-9200-4bde-86ef-bdcaf8459c1e', embedding=None, metadata={'page_label': '203', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 203         targets=train_targets,         resize=(227, 227),         augmentations=aug,     )      # torch dataloader creates batches of data     # from classification dataset class     train_loader = torch.utils.data.DataLoader(         train_dataset, batch_size=16, shuffle=True, num_workers=4     )      # same for validation data     valid_dataset = dataset.ClassificationDataset(         image_paths=valid_images,         targets=valid_targets,         resize=(227, 227),         augmentations=aug,     )      valid_loader = torch.utils.data.DataLoader(         valid_dataset, batch_size=16, shuffle=False, num_workers=4     )      # simple Adam optimizer     optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)      # train and print auc score for all epochs     for epoch in range(epochs):         engine.train(train_loader, model, optimizer, device=device)         predictions, valid_targets = engine.evaluate(             valid_loader, model, device=device         )         roc_auc = metrics.roc_auc_score(valid_targets, predictions)         print(             f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\"         ) ═════════════════════════════════════════════════════════════════════════  Let’s train it without pretrained weights:  ═════════════════════════════════════════════════════════════════════════ Epoch=0, Valid ROC AUC=0.5737161981475328 Epoch=1, Valid ROC AUC=0.5362868001588292 Epoch=2, Valid ROC AUC=0.6163448214387008 Epoch=3, Valid ROC AUC=0.6119219143780944 Epoch=4, Valid ROC AUC=0.6229718888519726 Epoch=5, Valid ROC AUC=0.5983014999635341 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a852b773-deee-4bee-9620-010c9accb8ca', embedding=None, metadata={'page_label': '204', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 204 Epoch=6, Valid ROC AUC=0.5523236874306134 Epoch=7, Valid ROC AUC=0.4717721611306046 Epoch=8, Valid ROC AUC=0.6473408263980617 Epoch=9, Valid ROC AUC=0.6639862888260415 ═════════════════════════════════════════════════════════════════════════  This AUC is around 0.66 which is even lower than our random forest model. What happens when we use pretrained weights?  ═════════════════════════════════════════════════════════════════════════ Epoch=0, Valid ROC AUC=0.5730387429803165 Epoch=1, Valid ROC AUC=0.5319813942934937 Epoch=2, Valid ROC AUC=0.627111577514323 Epoch=3, Valid ROC AUC=0.6819736959393209 Epoch=4, Valid ROC AUC=0.5747117168950512 Epoch=5, Valid ROC AUC=0.5994619255609669 Epoch=6, Valid ROC AUC=0.5080889443530546 Epoch=7, Valid ROC AUC=0.6323792776512727 Epoch=8, Valid ROC AUC=0.6685753182661686 Epoch=9, Valid ROC AUC=0.6861802387300147 ═════════════════════════════════════════════════════════════════════════  The AUC is much better now. However, it is still lower. The good thing about pretrained models is that we can try many different models easily. Let’s try resnet18 with pretrained weights.  ═════════════════════════════════════════════════════════════════════════ # model.py import torch.nn as nn import pretrainedmodels   def get_model(pretrained):     if pretrained:         model = pretrainedmodels.__dict__[\"resnet18\"](             pretrained=\\'imagenet\\'         )     else:         model = pretrainedmodels.__dict__[\"resnet18\"](             pretrained=None         )     # print the model here to know whats going on.     model.last_linear = nn.Sequential(         nn.BatchNorm1d(512),         nn.Dropout(p=0.25),         nn.Linear(in_features=512, out_features=2048), ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='560c29d3-9b78-4405-9172-b85f1fbc7873', embedding=None, metadata={'page_label': '205', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 205         nn.ReLU(),         nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),         nn.Dropout(p=0.5),         nn.Linear(in_features=2048, out_features=1),     )     return model ═════════════════════════════════════════════════════════════════════════  When trying this model, I also changed the image size to 512x512 and added a step learning rate scheduler which multiplies the learning rate by 0.5 after every 3 epochs.  ═════════════════════════════════════════════════════════════════════════ Epoch=0, Valid ROC AUC=0.5988225569880796 Epoch=1, Valid ROC AUC=0.730349343208836 Epoch=2, Valid ROC AUC=0.5870943169939142 Epoch=3, Valid ROC AUC=0.5775864444138311 Epoch=4, Valid ROC AUC=0.7330502499939224 Epoch=5, Valid ROC AUC=0.7500336296524395 Epoch=6, Valid ROC AUC=0.7563722113724951 Epoch=7, Valid ROC AUC=0.7987463837994215 Epoch=8, Valid ROC AUC=0.798505708937384 Epoch=9, Valid ROC AUC=0.8025477500546988 ═════════════════════════════════════════════════════════════════════════  This model seems to perform the best. However, you might be able to tune the different parameters and image size in AlexNet to get a better score. Using augmentations will improve the score further. Optimising deep neural networks is difficult but not impossible. Choose Adam optimizer, use a low learning rate, reduce learning rate on a plateau of validation loss, try some augmentations, try preprocessing the images (e.g. cropping if needed, this can also be considered pre-processing), change the batch size, etc. There’s a lot that you can do to optimize your deep neural network.   ResNet is an architecture much more complicated compared to AlexNet. ResNet stands for Residual Neural Network and was introduced by K. He, X. Zhang, S. Ren and J. Sun in the paper, deep residual learning for image recognition, in 2015. ResNet consists of residual blocks that transfer the knowledge from one layer to further layers by skipping some layers in between. These kinds of connections of layers are known as skip-connections since we are skipping one or more layers. Skip-connections help with the vanishing gradient issue by propagating the gradients to further layers. This allows us to train very large convolutional neural networks without loss of performance. Usually, the training loss increases at a given ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f578ec62-40c7-4999-9e25-b3d2c62f1c71', embedding=None, metadata={'page_label': '206', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 206 point if we are using a large neural network, but that can be prevented by using skip-connections. This can be better understood by figure 7.  \\n Figure 7: Comparison of simple convnet and residual convnet10. See the skip connections. Please note that the final layers are omitted in this figure.  A residual block is quite simple to understand. You take the output from a layer, skip some layers and add that output to a layer further in the network. The dotted lines mean that the input shape needs to be adjusted as max-pooling is being used and use of max-pooling changes the size of the output.   ResNet comes in many different variations: 18, 34, 50, 101 and 152 layers and all of them are available with weights pre-trained on ImageNet dataset. These days pretrained models work for (almost) everything but make sure that you start with smaller models, for example, begin with resnet-18 rather than resnet-50. Some other ImageNet pre-trained models include:  - Inception - DenseNet (different variations)  10 K. He, X. Zhang, S. Ren and J. Sun, Deep residual learning for image recognition, 2015 \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4251b6cd-ca1f-4b30-8eb3-00f50df1f59a', embedding=None, metadata={'page_label': '207', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 207 - NASNet - PNASNet - VGG - Xception - ResNeXt - EfficientNet, etc.  A majority of the pre-trained state-of-the-art models can be found in pytorch-pretrainedmodels repository on GitHub: https://github.com/Cadene/pretrained-models.pytorch. It is out of the scope of this chapter (and this book) to discuss these models in details. Since we are only looking at applications, let’s see how a pre-trained model like this can be used for a segmentation task.   \\n Figure 8: U-Net architecture11.  Segmentation is a task which is quite popular in computer vision. In a segmentation task, we try to remove/extract foreground from background. Foreground and  11 O. Ronneberger, P. Fischer and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015 \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4bd966a2-cf0c-4534-a3ba-ab906d03f6fa', embedding=None, metadata={'page_label': '208', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 208 background can have different definitions. We can also say that it is a pixel-wise classification task in which your job is to assign a class to each pixel in a given image. The pneumothorax dataset that we are working on is, in fact, a segmentation task. In this task, given the chest radiographic images, we are required to segment pneumothorax. The most popular model used for segmentation tasks is U-Net. The structure is represented in figure 8.  U-Nets have two parts: encoder and decoder. The encoder is the same as any convnet you have seen till now. The decoder is a bit different. Decoder consists of up-convolutional layers. In up-convolutions (transposed convolutions), we use filters that when applied to a small image, creates a larger image. In PyTorch, you can use ConvTranspose2d for this operation. It must be noted that up-convolution is not the same as up-sampling. Up-sampling is an easy process in which we apply a function to an image to resize it. In up-convolution, we learn the filters. We take some parts of the encoder as inputs to some of the decoders. This is important for the up-convolutional layers.  Let’s see how this U-Net is implemented.  ═════════════════════════════════════════════════════════════════════════ # simple_unet.py import torch import torch.nn as nn from torch.nn import functional as F   def double_conv(in_channels, out_channels):     \"\"\"     This function applies two convolutional layers     each followed by a ReLU activation function     :param in_channels: number of input channels     :param out_channels: number of output channels     :return: a down-conv layer     \"\"\"     conv = nn.Sequential(         nn.Conv2d(in_channels, out_channels, kernel_size=3),         nn.ReLU(inplace=True),         nn.Conv2d(out_channels, out_channels, kernel_size=3),         nn.ReLU(inplace=True)     )     return conv   def crop_tensor(tensor, target_tensor): ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5d8af55-6028-4511-ad02-9628afeca117', embedding=None, metadata={'page_label': '209', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 209     \"\"\"     Center crops a tensor to size of a given target tensor size     Please note that this function is applicable only to      this implementation of unet. There are a few assumptions     in this implementation that might not be applicable to all     networks and all other use-cases.     Both tensors are of shape (bs, c, h, w)     :param tensor: a tensor that needs to be cropped     :param target_tensor: target tensor of smaller size     :return: cropped tensor     \"\"\"     target_size = target_tensor.size()[2]     tensor_size = tensor.size()[2]     delta = tensor_size - target_size     delta = delta // 2     return tensor[         :,          :,          delta:tensor_size - delta,           delta:tensor_size - delta     ]   class UNet(nn.Module):     def __init__(self):         super(UNet, self).__init__()          # we need only one max_pool as it is not learned         self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)          self.down_conv_1 = double_conv(1, 64)         self.down_conv_2 = double_conv(64, 128)         self.down_conv_3 = double_conv(128, 256)         self.down_conv_4 = double_conv(256, 512)         self.down_conv_5 = double_conv(512, 1024)          self.up_trans_1 = nn.ConvTranspose2d(             in_channels=1024,             out_channels=512,             kernel_size=2,             stride=2         )         self.up_conv_1 = double_conv(1024, 512)          self.up_trans_2 = nn.ConvTranspose2d(             in_channels=512,             out_channels=256, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2bb795b3-fb4a-4830-bec4-d476fbe03aaf', embedding=None, metadata={'page_label': '210', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 210             kernel_size=2,             stride=2         )         self.up_conv_2 = double_conv(512, 256)          self.up_trans_3 = nn.ConvTranspose2d(             in_channels=256,             out_channels=128,             kernel_size=2,             stride=2         )         self.up_conv_3 = double_conv(256, 128)          self.up_trans_4 = nn.ConvTranspose2d(             in_channels=128,             out_channels=64,             kernel_size=2,             stride=2         )         self.up_conv_4 = double_conv(128, 64)          self.out = nn.Conv2d(             in_channels=64,             out_channels=2,             kernel_size=1         )      def forward(self, image):         # encoder         x1 = self.down_conv_1(image)         x2 = self.max_pool_2x2(x1)         x3 = self.down_conv_2(x2)         x4 = self.max_pool_2x2(x3)         x5 = self.down_conv_3(x4)         x6 = self.max_pool_2x2(x5)         x7 = self.down_conv_4(x6)         x8 = self.max_pool_2x2(x7)         x9 = self.down_conv_5(x8)                  # decoder         x = self.up_trans_1(x9)         y = crop_tensor(x7, x)         x = self.up_conv_1(torch.cat([x, y], axis=1))         x = self.up_trans_2(x)         y = crop_tensor(x5, x)         x = self.up_conv_2(torch.cat([x, y], axis=1))         x = self.up_trans_3(x) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70758dd7-33d7-4d40-b002-04ca01e696c7', embedding=None, metadata={'page_label': '211', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 211         y = crop_tensor(x3, x)         x = self.up_conv_3(torch.cat([x, y], axis=1))         x = self.up_trans_4(x)         y = crop_tensor(x1, x)         x = self.up_conv_4(torch.cat([x, y], axis=1))                  # output layer         out = self.out(x)          return out   if __name__ == \"__main__\":     image = torch.rand((1, 1, 572, 572))     model = UNet()     print(model(image)) ═════════════════════════════════════════════════════════════════════════  Please note that the implementation of U-Net that I have shown above is the original implementation of the U-Net paper. There are many variations that can be found on the internet. Some prefer to use bilinear sampling instead of transposed convolutions for up-sampling, but that’s not the real implementation of the paper. It might, however, perform better. In the original implementation shown above, there is a single channel image with two channels in the output: one for foreground and one for the background. As you can see, this can be customized for any number of classes and any number of input channels very easily. The input image size is different than output image size in this implementation as we are using convolutions without padding.   We see that the encoder part of the U-Net is a nothing but a simple convolutional network. We can, thus, replace this with any network such as ResNet. The replacement can also be done with pretrained weights. Thus, we can use a ResNet based encoder which is pretrained on ImageNet and a generic decoder. In place of ResNet, many different network architectures can be used. Segmentation Models Pytorch12 by Pavel Yakubovskiy is an implementation of many such variations where an encoder can be replaced by a pretrained model. Let’s apply a ResNet based U-Net for pneumothorax detection problem.  Most of the problems like this should have two inputs: the original image and a mask. In the case of multiple objects, there will be multiple masks. In our pneumothorax dataset, we are provided with RLE instead. RLE stands for run- 12 https://github.com/qubvel/segmentation_models.pytorch ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bce286ef-c743-403a-b871-75f9408affb5', embedding=None, metadata={'page_label': '212', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 212 length encoding and is a way to represent binary masks to save space. Going deep into RLE is beyond the scope of this chapter. So, let’s assume that we have an input image and corresponding mask. Let’s first design a dataset class which outputs image and mask images. Please note that we will create these scripts in such a way that they can be applied to almost any segmentation problem. The training dataset is a CSV file consisting only of image ids which are also filenames.  ═════════════════════════════════════════════════════════════════════════ # dataset.py import os import glob import torch  import numpy as np import pandas as pd  from PIL import Image, ImageFile  from tqdm import tqdm from collections import defaultdict from torchvision import transforms  from albumentations import (     Compose,     OneOf,     RandomBrightnessContrast,     RandomGamma,     ShiftScaleRotate, )  ImageFile.LOAD_TRUNCATED_IMAGES = True   class SIIMDataset(torch.utils.data.Dataset):     def __init__(         self,          image_ids,          transform=True,          preprocessing_fn=None     ):         \"\"\"         Dataset class for segmentation problem         :param image_ids: ids of the images, list         :param transform: True/False, no transform in validation         :param preprocessing_fn: a function for preprocessing image         \"\"\" ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='488b1195-5bdd-470b-bcf0-f922c00c0d42', embedding=None, metadata={'page_label': '213', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 213         # we create a empty dictionary to store iamge         # and mask paths         self.data = defaultdict(dict)          # for augmentations         self.transform = transform          # preprocessing function to normalize         # images         self.preprocessing_fn = preprocessing_fn          # albumentation augmentations         # we have shift, scale & rotate         # applied with 80% probability         # and then one of gamma and brightness/contrast         # is applied to the image         # albumentation takes care of which augmentation         # is applied to image and mask         self.aug = Compose(                 [                     ShiftScaleRotate(                         shift_limit=0.0625,                          scale_limit=0.1,                          rotate_limit=10, p=0.8                     ),                     OneOf(                         [                             RandomGamma(                                 gamma_limit=(90, 110)                             ),                             RandomBrightnessContrast(                                 brightness_limit=0.1,                                  contrast_limit=0.1                             ),                         ],                         p=0.5,                     ),                 ]             )                  # going over all image_ids to store         # image and mask paths         for imgid in image_ids:             files = glob.glob(os.path.join(TRAIN_PATH, imgid, \"*.png\"))             self.data[counter] = {                 \"img_path\": os.path.join(                     TRAIN_PATH, imgid + \".png\" ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='df9cf683-9926-4b02-8d1e-84c98b3984bf', embedding=None, metadata={'page_label': '214', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 214                 ),                 \"mask_path\": os.path.join(                     TRAIN_PATH, imgid + \"_mask.png\"                 ),             }      def __len__(self):         # return length of dataset         return len(self.data)      def __getitem__(self, item):         # for a given item index,          # return image and mask tensors         # read image and mask paths         img_path = self.data[item][\"img_path\"]         mask_path = self.data[item][\"mask_path\"]          # read image and convert to RGB         img = Image.open(img_path)         img = img.convert(\"RGB\")          # PIL image to numpy array         img = np.array(img)          # read mask image         mask = Image.open(mask_path)          # convert to binary float matrix         mask = (mask >= 1).astype(\"float32\")          # if this is training data, apply transforms         if self.transform is True:             augmented = self.aug(image=img, mask=mask)             img = augmented[\"image\"]             mask = augmented[\"mask\"]          # preprocess the image using provided          # preprocessing tensors. this is basically         # image normalization         img = self.preprocessing_fn(img)          # return image and mask tensors         return {             \"image\": transforms.ToTensor()(img),             \"mask\": transforms.ToTensor()(mask).float(),         } ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70927e7b-bf9f-4514-abd5-3648375e5a19', embedding=None, metadata={'page_label': '215', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 215 Once, we have the dataset class; we can create a training function.  ═════════════════════════════════════════════════════════════════════════ # train.py import os import sys import torch  import numpy as np import pandas as pd import segmentation_models_pytorch as smp import torch.nn as nn import torch.optim as optim  from apex import amp from collections import OrderedDict from sklearn import model_selection from tqdm import tqdm from torch.optim import lr_scheduler  from dataset import SIIMDataset  # training csv file path TRAINING_CSV = \"../input/train_pneumothorax.csv\"  # training and test batch sizes TRAINING_BATCH_SIZE = 16 TEST_BATCH_SIZE = 4  # number of epochs EPOCHS = 10  # define the encoder for U-Net # check: https://github.com/qubvel/segmentation_models.pytorch # for all supported encoders ENCODER = \"resnet18\"  # we use imagenet pretrained weights for the encoder ENCODER_WEIGHTS = \"imagenet\"  # train on gpu DEVICE = \"cuda\"  def train(dataset, data_loader, model, criterion, optimizer):     \"\"\"     training function that trains for one epoch     :param dataset: dataset class (SIIMDataset) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='32687ce8-07ba-4de3-b799-a515ba8e0f7b', embedding=None, metadata={'page_label': '216', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 216     :param data_loader: torch dataset loader     :param model: model     :param criterion: loss function     :param optimizer: adam, sgd, etc.     \"\"\"     # put the model in train mode     model.train()      # calculate number of batches     num_batches = int(len(dataset) / data_loader.batch_size)      # init tqdm to track progress     tk0 = tqdm(data_loader, total=num_batches)      # loop over all batches     for d in tk0:         # fetch input images and masks          # from dataset batch         inputs = d[\"image\"]         targets = d[\"mask\"]          # move images and masks to cpu/gpu device         inputs = inputs.to(DEVICE, dtype=torch.float)         targets = targets.to(DEVICE, dtype=torch.float)          # zero grad the optimizer         optimizer.zero_grad()          # forward step of model         outputs = model(inputs)          # calculate loss         loss = criterion(outputs, targets)          # backward loss is calculated on a scaled loss         # context since we are using mixed precision training         # if you are not using mixed precision training,         # you can use loss.backward() and delete the following         # two lines of code         with amp.scale_loss(loss, optimizer) as scaled_loss:             scaled_loss.backward()                  # step the optimizer         optimizer.step()      # close tqdm     tk0.close() ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f312ea6-1c5f-4606-a3d6-d8c6a9bc0e7b', embedding=None, metadata={'page_label': '217', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 217 def evaluate(dataset, data_loader, model):     \"\"\"     evaluation function to calculate loss on validation     set for one epoch     :param dataset: dataset class (SIIMDataset)     :param data_loader: torch dataset loader     :param model: model     \"\"\"     # put model in eval mode     model.eval()     # init final_loss to 0     final_loss = 0     # calculate number of batches and init tqdm     num_batches = int(len(dataset) / data_loader.batch_size)     tk0 = tqdm(data_loader, total=num_batches)      # we need no_grad context of torch. this save memory     with torch.no_grad():         for d in tk0:             inputs = d[\"image\"]             targets = d[\"mask\"]             inputs = inputs.to(DEVICE, dtype=torch.float)             targets = targets.to(DEVICE, dtype=torch.float)             output = model(inputs)             loss = criterion(output, targets)             # add loss to final loss             final_loss += loss     # close tqdm     tk0.close()     # return average loss over all batches     return final_loss / num_batches   if __name__ == \"__main__\":      # read the training csv file     df = pd.read_csv(TRAINING_CSV)      # split data into training and validation     df_train, df_valid = model_selection.train_test_split(         df, random_state=42, test_size=0.1     )      # training and validation images lists/arrays     training_images = df_train.image_id.values     validation_images = df_valid.image_id.values  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad9e7150-d9be-4131-b5b7-85f0ab862351', embedding=None, metadata={'page_label': '218', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 218     # fetch unet model from segmentation models     # with specified encoder architecture     model = smp.Unet(         encoder_name=ENCODER,         encoder_weights=ENCODER_WEIGHTS,         classes=1,         activation=None,     )     # segmentation model provides you with a preprocessing     # function that can be used for normalizing images     # normalization is only applied on images and not masks     prep_fn = smp.encoders.get_preprocessing_fn(         ENCODER,         ENCODER_WEIGHTS     )      # send model to device     model.to(DEVICE)     # init training dataset     # transform is True for training data     train_dataset = SIIMDataset(         training_images,         transform=True,         preprocessing_fn=prep_fn,     )      # wrap training dataset in torch's dataloader     train_loader = torch.utils.data.DataLoader(         train_dataset,         batch_size=TRAINING_BATCH_SIZE,          shuffle=True,         num_workers=12     )      # init validation dataset     # augmentations is disabled     valid_dataset = SIIMDataset(         validation_images,         transform=False,         preprocessing_fn=prep_fn,     )      # wrap validation dataset in torch's dataloader     valid_loader = torch.utils.data.DataLoader(         valid_dataset,          batch_size=TEST_BATCH_SIZE,          shuffle=True,  \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e04a4aa-62f1-4026-960b-3a38a86b0ca8', embedding=None, metadata={'page_label': '219', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 219         num_workers=4     )      # NOTE: define the criterion here     # this is left as an excercise     # code won\\'t work without defining this     # criterion = ……      # we will use Adam optimizer for faster convergence     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)     # reduce learning rate when we reach a plateau on loss     scheduler = lr_scheduler.ReduceLROnPlateau(         optimizer, mode=\"min\", patience=3, verbose=True     )     # wrap model and optimizer with NVIDIA\\'s apex     # this is used for mixed precision training     # if you have a GPU that supports mixed precision,     # this is very helpful as it will allow us to fit larger images     # and larger batches     model, optimizer = amp.initialize(         model, optimizer, opt_level=\"O1\", verbosity=0     )     # if we have more than one GPU, we can use both of them!     if torch.cuda.device_count() > 1:         print(f\"Let\\'s use {torch.cuda.device_count()} GPUs!\")         model = nn.DataParallel(model)      # some logging     print(f\"Training batch size: {TRAINING_BATCH_SIZE}\")     print(f\"Test batch size: {TEST_BATCH_SIZE}\")     print(f\"Epochs: {EPOCHS}\")     print(f\"Image size: {IMAGE_SIZE}\")     print(f\"Number of training images: {len(train_dataset)}\")     print(f\"Number of validation images: {len(valid_dataset)}\")     print(f\"Encoder: {ENCODER}\")      # loop over all epochs     for epoch in range(EPOCHS):         print(f\"Training Epoch: {epoch}\")         # train for one epoch         train(             train_dataset,              train_loader,              model,              criterion,              optimizer         ) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bfbbf8fc-f403-4edf-b7ab-2060ad26e63a', embedding=None, metadata={'page_label': '220', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 220         print(f\"Validation Epoch: {epoch}\")         # calculate validation loss         val_log = evaluate(             valid_dataset,             valid_loader,             model         )         # step the scheduler         scheduler.step(val_log[\"loss\"])         print(\"\\\\n\") ═════════════════════════════════════════════════════════════════════════  In segmentation problems you can use a variety of loss functions, for example, pixel-wise binary cross-entropy, focal loss, dice loss etc. I’m leaving it for the reader to decide the appropriate loss given the evaluation metric. When you train a model like this, you will create a model that tries to predict the location of pneumothorax, as shown in figure 9. In the above code, we used mixed precision training using NVIDIA apex. Please note that this is available natively in PyTorch from version 1.6.0+.   \\n Figure 9: Example of pneumothorax detection from the trained model (might not be correct prediction).  I have included some of the commonly used functions in a python package called Well That’s Fantastic Machine Learning (WTFML). Let’s see how this helps us to build a multi-class classification model for plant images from the plant pathology challenge of FGVC 202013.  13 Ranjita Thapa , Noah Snavely , Serge Belongie , Awais Khan. The Plant Pathology 2020 challenge dataset to classify foliar disease of apples. ArXiv e-prints \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='068e0bd6-a8b2-4a25-8c6b-8669e9118a08', embedding=None, metadata={'page_label': '221', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 221 ═════════════════════════════════════════════════════════════════════════import os  import pandas as pd import numpy as np  import albumentations import argparse import torch import torchvision import torch.nn as nn import torch.nn.functional as F  from sklearn import metrics from sklearn.model_selection import train_test_split  from wtfml.engine import Engine from wtfml.data_loaders.image import ClassificationDataLoader   class DenseCrossEntropy(nn.Module):     # Taken from:      # https://www.kaggle.com/pestipeti/plant-pathology-2020-pytorch     def __init__(self):         super(DenseCrossEntropy, self).__init__()      def forward(self, logits, labels):         logits = logits.float()         labels = labels.float()          logprobs = F.log_softmax(logits, dim=-1)          loss = -labels * logprobs         loss = loss.sum(-1)          return loss.mean()   class Model(nn.Module):     def __init__(self):         super().__init__()         self.base_model = torchvision.models.resnet18(pretrained=True)         in_features = self.base_model.fc.in_features         self.out = nn.Linear(in_features, 4)      def forward(self, image, targets=None):         batch_size, C, H, W = image.shape ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='acc394fd-b7f8-414f-8ebd-69c25a95f924', embedding=None, metadata={'page_label': '222', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 222          x = self.base_model.conv1(image)         x = self.base_model.bn1(x)         x = self.base_model.relu(x)         x = self.base_model.maxpool(x)          x = self.base_model.layer1(x)         x = self.base_model.layer2(x)         x = self.base_model.layer3(x)         x = self.base_model.layer4(x)          x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)         x = self.out(x)          loss = None         if targets is not None:             loss = DenseCrossEntropy()(x, targets.type_as(x))          return x, loss   if __name__ == \"__main__\":     parser = argparse.ArgumentParser()     parser.add_argument(         \"--data_path\", type=str,     )     parser.add_argument(         \"--device\", type=str,     )     parser.add_argument(         \"--epochs\", type=int,     )     args = parser.parse_args()      df = pd.read_csv(os.path.join(args.data_path, \"train.csv\"))     images = df.image_id.values.tolist()     images = [         os.path.join(args.data_path, \"images\", i + \".jpg\")          for i in images     ]     targets = df[[\"healthy\", \"multiple_diseases\", \"rust\", \"scab\"]].values      model = Model()     model.to(args.device)      mean = (0.485, 0.456, 0.406)     std = (0.229, 0.224, 0.225) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e95e2f4e-b8c8-4f93-ae9d-63235dc09e72', embedding=None, metadata={'page_label': '223', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 223     aug = albumentations.Compose(         [             albumentations.Normalize(                 mean,                  std,                  max_pixel_value=255.0,                  always_apply=True             )         ]     )      (         train_images, valid_images,          train_targets, valid_targets     ) = train_test_split(images, targets)      train_loader = ClassificationDataLoader(         image_paths=train_images,         targets=train_targets,         resize=(128, 128),         augmentations=aug,     ).fetch(         batch_size=16,          num_workers=4,          drop_last=False,          shuffle=True,          tpu=False     )      valid_loader = ClassificationDataLoader(         image_paths=valid_images,         targets=valid_targets,         resize=(128, 128),         augmentations=aug,     ).fetch(         batch_size=16,          num_workers=4,          drop_last=False,          shuffle=False,          tpu=False     )      optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)     scheduler = torch.optim.lr_scheduler.StepLR(         optimizer, step_size=15, gamma=0.6     )  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='821a9d72-92f6-4a2a-a545-cf917997c01f', embedding=None, metadata={'page_label': '224', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 224     for epoch in range(args.epochs):         train_loss = Engine.train(             train_loader, model, optimizer, device=args.device         )         valid_loss = Engine.evaluate(             valid_loader, model, device=args.device         )         print(             f\"{epoch}, Train Loss={train_loss} Valid Loss={valid_loss}\"         ) ═════════════════════════════════════════════════════════════════════════  Once you have the data14, you can run the script using:   ═════════════════════════════════════════════════════════════════════════❯ python plant.py --data_path ../../plant_pathology --device cuda -- epochs 2 100%|█████████████| 86/86 [00:12<00:00, 6.73it/s, loss=0.723] 100%|█████████████ 29/29 [00:04<00:00, 6.62it/s, loss=0.433] 0, Train Loss=0.7228777609592261 Valid Loss=0.4327834551704341 100%|█████████████| 86/86 [00:12<00:00, 6.74it/s, loss=0.271] 100%|█████████████ 29/29 [00:04<00:00, 6.63it/s, loss=0.568] 1, Train Loss=0.2708700496790021 Valid Loss=0.56841839541649 ═════════════════════════════════════════════════════════════════════════ As you can see, how this makes our life simple and code easy to read and understand. PyTorch without any wrappers work best. There is a lot more in images than just classification, and if I start writing about all of them, I’ll have to write another book. So, I have decided to do that instead: Approaching (Almost) Any Image Problem. \\n 14 https://www.kaggle.com/c/plant-pathology-2020-fgvc7 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c60b6c0c-44be-4aca-8cc3-4e7b931435e4', embedding=None, metadata={'page_label': '225', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 225 Approaching text classification/regression  Text problems are my favourite. In general, these problems are also known as Natural Language Processing (NLP) problems. NLP problems are also like images in the sense that, it’s quite different. You need to create pipelines you have never created before for tabular problems. You need to understand the business case to build a good model. By the way, that is true for anything in machine learning. Building models will take you to a certain level, but to improve and contribute to a business you are building the model for, you must understand how it impacts the business. Let’s not get too philosophical here.   There are many different types of NLP problems, and the most common type is the classification of strings. Many times, it is seen that people are doing well with tabular data or with images, but when it comes to text, they don’t even have a clue where to start from. Text data is no different than other types of datasets. For computers, everything is numbers.  Let’s say we start with a fundamental task of sentiment classification. We will try to classify sentiment from movie reviews. So, you have a text, and there is a sentiment associated with it. How will you approach this kind of problem? Apply a deep neural network right, or maybe muppets can come and save you? No, absolutely wrong. You start with the basics. Let’s see what this data looks like first.  We start with IMDB movie review dataset15 that consists of 25000 reviews for positive sentiment and 25000 reviews for negative sentiment.   The concepts that I will discuss here can be applied to almost any text classification dataset.  This dataset is quite easy to understand. One review maps to one target variable. Note that I wrote review instead of sentence. A review is a bunch of sentences. So, until now you must have seen classifying only a single sentence, but in this problem, we will be classifying multiple sentences. In simple words, it means that not only  15 Maas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 142–150. Association for Computational Linguistics, 2011.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ebe8ec9-9430-46ba-a497-a06d644eeb4b', embedding=None, metadata={'page_label': '226', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 226 one sentence contributes to the sentiment, but the sentiment score is a combination of score from multiple sentences. A snapshot of the data is presented in figure 1. \\n Figure 1. Snapshot of IMDB movie review dataset.  How would you start with such a problem?   A simple way would be just to create two handmade lists of words. One list will contain all the positive words you can imagine, for example, good, awesome, nice, etc. and another list will include all the negative words, such as bad, evil, etc. Let’s leave examples of bad words else I’ll have to make this book available only for 18+. Once you have these lists, you do not even need a model to make a prediction. These lists are also known as sentiment lexicons. A bunch of them for different languages are available on the internet.  You can have a simple counter that counts the number of positive and negative words in the sentence. If the number of positive words is higher, it is a positive sentiment, and if the number of negative words is higher, it is a sentence with a negative sentiment. If none of them are present in the sentence, you can say that the sentence has a neutral sentiment. This is one of the oldest ways, and some people still use it. It does not require much code either.  ═════════════════════════════════════════════════════════════════════════ def find_sentiment(sentence, pos, neg):     \"\"\"     This function returns sentiment of sentence     :param sentence: sentence, a string     :param pos: set of positive words     :param neg: set of negative words     :return: returns positive, negative or neutral sentiment     \"\"\"          # split sentence by a space \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fadce3b3-7fc1-464f-8162-0d47e0d0c8ce', embedding=None, metadata={'page_label': '227', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 227     # \"this is a sentence!\" becomes:     # [\"this\", \"is\" \"a\", \"sentence!\"]     # note that im splitting on all whitespaces     # if you want to split by space use .split(\" \")     sentence = sentence.split()          # make sentence into a set     sentence = set(sentence)          # check number of common words with positive     num_common_pos = len(sentence.intersection(pos))          # check number of common words with negative     num_common_neg = len(sentence.intersection(neg))          # make conditions and return     # see how return used eliminates if else     if num_common_pos > num_common_neg:         return \"positive\"     if num_common_pos < num_common_neg:         return \"negative\"     return \"neutral\" ═════════════════════════════════════════════════════════════════════════  However, this kind of approach does not take a lot into consideration. And as you can see that our split() is also not perfect. If you use split(), a sentence like:  “hi, how are you?”  gets split into  [“hi,”, “how”, “are”, “you?”]  This is not ideal, because you see the comma and question mark, they are not split. It is therefore not recommended to use this method if you don’t have a pre-processing that handles these special characters before the split. Splitting a string into a list of words is known as tokenization. One of the most popular tokenization comes from NLTK (Natural Language Tool Kit).  ═════════════════════════════════════════════════════════════════════════ In [X]: from nltk.tokenize import word_tokenize  In [X]: sentence = \"hi, how are you?\"  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fb13d08-c8fd-4e31-9799-c264f46a9370', embedding=None, metadata={'page_label': '228', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 228 In [X]: sentence.split() Out[X]: [\\'hi,\\', \\'how\\', \\'are\\', \\'you?\\']  In [X]: word_tokenize(sentence) Out[X]: [\\'hi\\', \\',\\', \\'how\\', \\'are\\', \\'you\\', \\'?\\'] ═════════════════════════════════════════════════════════════════════════  As you can see, using NLTK’s word tokenize, the same sentence is split in a much better manner. Comparing using a list of words will also work much better now! This is what we will apply to our first model to detect sentiment.  One of the basic models that you should always try with a classification problem in NLP is bag of words. In bag of words, we create a huge sparse matrix that stores counts of all the words in our corpus (corpus = all the documents = all the sentences). For this, we will use CountVectorizer from scikit-learn. Let’s see how it works.  ═════════════════════════════════════════════════════════════════════════ from sklearn.feature_extraction.text import CountVectorizer  # create a corpus of sentences corpus = [     \"hello, how are you?\",     \"im getting bored at home. And you? What do you think?\",     \"did you know about counts\",     \"let\\'s see if this works!\",     \"YES!!!!\" ]  # initialize CountVectorizer ctv = CountVectorizer()  # fit the vectorizer on corpus ctv.fit(corpus)  corpus_transformed = ctv.transform(corpus) ═════════════════════════════════════════════════════════════════════════  If we print corpus_transformed, we get something like the following:  ═════════════════════════════════════════════════════════════════════════      (0, 2) 1   (0, 9) 1   (0, 11) 1   (0, 22) 1 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aad2e2af-abd8-490b-9911-f0d6554885c8', embedding=None, metadata={'page_label': '229', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 229   (1, 1) 1   (1, 3) 1   (1, 4) 1   (1, 7) 1   (1, 8) 1   (1, 10) 1   (1, 13) 1   (1, 17) 1   (1, 19) 1   (1, 22) 2   (2, 0) 1   (2, 5) 1   (2, 6) 1   (2, 14) 1   (2, 22) 1   (3, 12) 1   (3, 15) 1   (3, 16) 1   (3, 18) 1   (3, 20) 1   (4, 21) 1 ═════════════════════════════════════════════════════════════════════════  We have already seen this representation in previous chapters. It is the sparse representation. So, our corpus is now a sparse matrix, where, for first sample, we have four elements, for sample 2 we have ten elements, and so on, for sample 3 we have five elements and so on. We also see that these elements have a count associated with them. Some are seen twice, some are seen only once. For example, in sample 2 (row 1), we see that column 22 has a value of two. Why is that? And what is column 22?  The way CountVectorizer works is it first tokenizes the sentence and then assigns a value to each token. So, each token is represented by a unique index. These unique indices are the columns that we see. The CountVectorizer stores this information.  ═════════════════════════════════════════════════════════════════════════ print(ctv.vocabulary_) {'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21} ═════════════════════════════════════════════════════════════════════════ \\t\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3bc7e27-c2e0-49ea-ba16-191a81c27424', embedding=None, metadata={'page_label': '230', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 230 We see that index 22 belongs to “you” and in the second sentence, we have used “you” twice. Thus, the count is 2. I hope it’s clear now what is bag of words. But we are missing some special characters. Sometimes those special characters can be useful too. For example, “?” denotes a question in most sentences. Let’s integrate word_tokenize from scikit-learn in CountVectorizer and see what happens.  ═════════════════════════════════════════════════════════════════════════ from sklearn.feature_extraction.text import CountVectorizer from nltk.tokenize import word_tokenize   # create a corpus of sentences corpus = [     \"hello, how are you?\",     \"im getting bored at home. And you? What do you think?\",     \"did you know about counts\",     \"let\\'s see if this works!\",     \"YES!!!!\" ]  # initialize CountVectorizer with word_tokenize from nltk # as the tokenizer ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)  # fit the vectorizer on corpus ctv.fit(corpus)  corpus_transformed = ctv.transform(corpus) print(ctv.vocabulary_) ═════════════════════════════════════════════════════════════════════════  This changes our vocabulary to:  ═════════════════════════════════════════════════════════════════════════ {\\'hello\\': 14, \\',\\': 2, \\'how\\': 16, \\'are\\': 7, \\'you\\': 27, \\'?\\': 4, \\'im\\': 18, \\'getting\\': 13, \\'bored\\': 9, \\'at\\': 8, \\'home\\': 15, \\'.\\': 3, \\'and\\': 6, \\'what\\': 24, \\'do\\': 12, \\'think\\': 22, \\'did\\': 11, \\'know\\': 19, \\'about\\': 5, \\'counts\\': 10, \\'let\\': 20, \"\\'s\": 1, \\'see\\': 21, \\'if\\': 17, \\'this\\': 23, \\'works\\': 25, \\'!\\': 0, \\'yes\\': 26} ═════════════════════════════════════════════════════════════════════════  Now, we have more words in the vocabulary. Thus, we can now create a sparse matrix by using all the sentences in IMDB dataset and can build a model. The ratio to positive and negative samples in this dataset is 1:1, and thus, we can use accuracy as the metric. We will use StratifiedKFold and create a single script to train five ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f2c0be1-4d13-4ab8-b4cf-fd901da8401d', embedding=None, metadata={'page_label': '231', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 231 folds. Which model to use you ask? Which is the fastest model for high dimensional sparse data? Logistic regression. We will use logistic regression for this dataset to start with and to create our first actual benchmark.  Let’s see how this is done.  ═════════════════════════════════════════════════════════════════════════ # import what we need import pandas as pd  from nltk.tokenize import word_tokenize from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn.feature_extraction.text import CountVectorizer   if __name__ == \"__main__\":     # read the training data     df = pd.read_csv(\"../input/imdb.csv\")      # map positive to 1 and negative to 0     df.sentiment = df.sentiment.apply(         lambda x: 1 if x == \"positive\" else 0     )      # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1          # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)          # fetch labels     y = df.sentiment.values          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):         df.loc[v_, \\'kfold\\'] = f      # we go over the folds created     for fold_ in range(5):         # temporary dataframes for train and test         train_df = df[df.kfold != fold_].reset_index(drop=True) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6db4297e-f7d5-4ef9-ba8a-9cb2feb949bb', embedding=None, metadata={'page_label': '232', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 232         test_df = df[df.kfold == fold_].reset_index(drop=True)          # initialize CountVectorizer with NLTK\\'s word_tokenize         # function as tokenizer         count_vec = CountVectorizer(             tokenizer=word_tokenize,             token_pattern=None         )          # fit count_vec on training data reviews         count_vec.fit(train_df.review)          # transform training and validation data reviews         xtrain = count_vec.transform(train_df.review)         xtest = count_vec.transform(test_df.review)          # initialize logistic regression model         model = linear_model.LogisticRegression()          # fit the model on training data reviews and sentiment         model.fit(xtrain, train_df.sentiment)          # make predictions on test data         # threshold for predictions is 0.5         preds = model.predict(xtest)          # calculate accuracy         accuracy = metrics.accuracy_score(test_df.sentiment, preds)          print(f\"Fold: {fold_}\")         print(f\"Accuracy = {accuracy}\")         print(\"\") ═════════════════════════════════════════════════════════════════════════  This piece of code takes time to run but should give you the following output:  ═════════════════════════════════════════════════════════════════════════ ❯ python ctv_logres.py Fold: 0 Accuracy = 0.8903  Fold: 1 Accuracy = 0.897  Fold: 2 Accuracy = 0.891  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2fa26772-22d6-4e3e-b44e-e2a6cac6bc17', embedding=None, metadata={'page_label': '233', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 233 Fold: 3 Accuracy = 0.8914  Fold: 4 Accuracy = 0.8931 ═════════════════════════════════════════════════════════════════════════  Wow, we are already at 89% accuracy, and all we did was use bag of words with logistic regression! This is super amazing! However, this model took a lot of time to train, let’s see if we can improve the time by using naïve bayes classifier. Naïve bayes classifier is quite popular in NLP tasks as the sparse matrices are huge and naïve bayes is a simple model. To use this model, we need to change one import and the line with the model. Let’s see how this model performs. We will use MultinomialNB from scikit-learn.  ═════════════════════════════════════════════════════════════════════════ # import what we need import pandas as pd  from nltk.tokenize import word_tokenize from sklearn import naive_bayes from sklearn import metrics from sklearn import model_selection from sklearn.feature_extraction.text import CountVectorizer . . . .         # initialize naive bayes model         model = naive_bayes.MultinomialNB()          # fit the model on training data reviews and sentiment         model.fit(xtrain, train_df.sentiment) . . . ═════════════════════════════════════════════════════════════════════════  Results are as follows:  ═════════════════════════════════════════════════════════════════════════ ❯ python ctv_nb.py Fold: 0 Accuracy = 0.8444  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a6125a83-338f-4672-83e1-38457211230b', embedding=None, metadata={'page_label': '234', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 234 Fold: 1 Accuracy = 0.8499  Fold: 2 Accuracy = 0.8422  Fold: 3 Accuracy = 0.8443  Fold: 4 Accuracy = 0.8455 ═════════════════════════════════════════════════════════════════════════  We see that this score is low. But the naïve bayes model is superfast.   Another method in NLP that most of the people these days tend to ignore or don’t care to know about is called TF-IDF. TF is term frequencies, and IDF is inverse document frequency. It might seem difficult from these terms, but things will become apparent with the formulae for TF and IDF.                                        Number of times a term t appears in a document  TF(t) =               ──────────────────────────────────                                            Total number of terms in the document                                                         Total number of documents IDF(t) =     LOG(  ──────────────────────────────────  )         Number of documents with term t in it   And TF-IDF for a term t is defined as:  TF-IDF(t) = TF(t) * IDF(t)  Similar to CountVectorizer in scikit-learn, we have TfidfVectorizer. Let’s try using it the same way we used CountVectorizer.  ═════════════════════════════════════════════════════════════════════════ from sklearn.feature_extraction.text import TfidfVectorizer from nltk.tokenize import word_tokenize  # create a corpus of sentences corpus = [ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbd571cb-61a0-4aba-97c7-d62796ac761e', embedding=None, metadata={'page_label': '235', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 235     \"hello, how are you?\",     \"im getting bored at home. And you? What do you think?\",     \"did you know about counts\",     \"let\\'s see if this works!\",     \"YES!!!!\" ]  # initialize TfidfVectorizer with word_tokenize from nltk # as the tokenizer tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)  # fit the vectorizer on corpus tfv.fit(corpus)  corpus_transformed = tfv.transform(corpus) print(corpus_transformed) ═════════════════════════════════════════════════════════════════════════  This gives the following output:  ═════════════════════════════════════════════════════════════════════════          (0, 27) 0.2965698850220162   (0, 16) 0.4428321995085722   (0, 14) 0.4428321995085722   (0, 7) 0.4428321995085722   (0, 4) 0.35727423026525224   (0, 2) 0.4428321995085722   (1, 27) 0.35299699146792735   (1, 24) 0.2635440111190765   (1, 22) 0.2635440111190765   (1, 18) 0.2635440111190765   (1, 15) 0.2635440111190765   (1, 13) 0.2635440111190765   (1, 12) 0.2635440111190765   (1, 9) 0.2635440111190765   (1, 8) 0.2635440111190765   (1, 6) 0.2635440111190765   (1, 4) 0.42525129752567803   (1, 3) 0.2635440111190765   (2, 27) 0.31752680284846835   (2, 19) 0.4741246485558491   (2, 11) 0.4741246485558491   (2, 10) 0.4741246485558491   (2, 5) 0.4741246485558491   (3, 25) 0.38775666010579296   (3, 23) 0.38775666010579296   (3, 21) 0.38775666010579296 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2fddb913-4323-41e0-94f0-b8842f714e95', embedding=None, metadata={'page_label': '236', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 236   (3, 20) 0.38775666010579296   (3, 17) 0.38775666010579296   (3, 1) 0.38775666010579296   (3, 0) 0.3128396318588854   (4, 26) 0.2959842226518677   (4, 0) 0.9551928286692534 ═════════════════════════════════════════════════════════════════════════  We see that instead of integer values, this time we get floats. Replacing CountVectorizer with TfidfVectorizer is also a piece of cake. Scikit-learn also offers TfidfTransformer. If you have count values, you can use TfidfTransformer and get the same behaviour as TfidfVectorizer.   ═════════════════════════════════════════════════════════════════════════ # import what we need import pandas as pd  from nltk.tokenize import word_tokenize from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn.feature_extraction.text import TfidfVectorizer . . .     # we go over the folds created     for fold_ in range(5):         # temporary dataframes for train and test         train_df = df[df.kfold != fold_].reset_index(drop=True)         test_df = df[df.kfold == fold_].reset_index(drop=True)          # initialize TfidfVectorizer with NLTK's word_tokenize         # function as tokenizer         tfidf_vec = TfidfVectorizer(             tokenizer=word_tokenize,             token_pattern=None         )          # fit tfidf_vec on training data reviews         tfidf_vec.fit(train_df.review)          # transform training and validation data reviews         xtrain = tfidf_vec.transform(train_df.review)         xtest = tfidf_vec.transform(test_df.review)          # initialize logistic regression model \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ed158fd-3409-4657-bc2e-064ca2bda133', embedding=None, metadata={'page_label': '237', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 237         model = linear_model.LogisticRegression()          # fit the model on training data reviews and sentiment         model.fit(xtrain, train_df.sentiment)          # make predictions on test data         # threshold for predictions is 0.5         preds = model.predict(xtest)          # calculate accuracy         accuracy = metrics.accuracy_score(test_df.sentiment, preds)          print(f\"Fold: {fold_}\")         print(f\"Accuracy = {accuracy}\")         print(\"\") ═════════════════════════════════════════════════════════════════════════  It would be interesting to see how TF-IDF performs with our old logistic regression model on the sentiment dataset.  ═════════════════════════════════════════════════════════════════════════ ❯ python tfv_logres.py Fold: 0 Accuracy = 0.8976  Fold: 1 Accuracy = 0.8998  Fold: 2 Accuracy = 0.8948  Fold: 3 Accuracy = 0.8912  Fold: 4 Accuracy = 0.8995 ═════════════════════════════════════════════════════════════════════════  We see that these scores are a bit higher than CountVectorizer, and thus, it becomes the new benchmark that we would want to beat.   Another interesting concept in NLP is n-grams. N-grams are combinations of words in order. N-grams are easy to create. You just need to take care of the order. To make things even more comfortable, we can use n-gram implementation from NLTK. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95afd3d1-824b-43d5-ac28-530391c9a2fe', embedding=None, metadata={'page_label': '238', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 238 ═════════════════════════════════════════════════════════════════════════ from nltk import ngrams from nltk.tokenize import word_tokenize  # let\\'s see 3 grams N = 3 # input sentence sentence = \"hi, how are you?\" # tokenized sentence tokenized_sentence = word_tokenize(sentence) # generate n_grams n_grams = list(ngrams(tokenized_sentence, N)) print(n_grams) ═════════════════════════════════════════════════════════════════════════  Which gives:  ═════════════════════════════════════════════════════════════════════════ [(\\'hi\\', \\',\\', \\'how\\'),  (\\',\\', \\'how\\', \\'are\\'),  (\\'how\\', \\'are\\', \\'you\\'),  (\\'are\\', \\'you\\', \\'?\\')] ═════════════════════════════════════════════════════════════════════════  Similarly, we can also create 2-grams, or 4-grams, etc. Now, these n-grams become a part of our vocab, and when we calculate counts or tf-idf, we consider one n-gram as one entirely new token. So, in a way, we are incorporating context to some extent. Both CountVectorizer and TfidfVectorizer implementations of scikit-learn offers n-grams by ngram_range parameter, which has a minimum and maximum limit. By default, this is (1, 1). When we change it to (1, 3), we are looking at unigrams, bigrams and trigrams. The code change is minimal. Since we had the best result till now with tf-idf, let’s see if including n-grams up to trigrams improves the model.  The only change required is in the initialization of TfidfVectorizer.  ═════════════════════════════════════════════════════════════════════════         tfidf_vec = TfidfVectorizer(             tokenizer=word_tokenize,             token_pattern=None,             ngram_range=(1, 3)         ) ═════════════════════════════════════════════════════════════════════════  Let’s see if we get any kind of improvements. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06dea602-6e77-43f0-b52a-a80f0f30c2e6', embedding=None, metadata={'page_label': '239', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 239 ═════════════════════════════════════════════════════════════════════════ ❯ python tfv_logres_trigram.py Fold: 0 Accuracy = 0.8931  Fold: 1 Accuracy = 0.8941  Fold: 2 Accuracy = 0.897  Fold: 3 Accuracy = 0.8922  Fold: 4 Accuracy = 0.8847 ═════════════════════════════════════════════════════════════════════════  This looks okay, but we do not see any improvements. Maybe we can get improvements by using only up to bigrams. I’m not showing that part here. Probably you can try to do it on your own.  There are a lot more things in the basics of NLP. One term that you must be aware of is stemming. Another is lemmatization. Stemming and lemmatization reduce a word to its smallest form. In the case of stemming, the processed word is called the stemmed word, and in the case of lemmatization, it is known as the lemma. It must be noted that lemmatization is more aggressive than stemming and stemming is more popular and widely used. Both stemming and lemmatization come from linguistics. And you need to have an in-depth knowledge of a given language if you plan to make a stemmer or lemmatizer for that language. Going into too much detail of these would mean adding one more chapter in this book. Both stemming and lemmatization can be done easily by using the NLTK package. Let’s take a look at some examples for both of them. There are many different types of stemmers and lemmatizers. I will show an example using the most common Snowball Stemmer and WordNet Lemmatizer.  ═════════════════════════════════════════════════════════════════════════ from nltk.stem import WordNetLemmatizer  from nltk.stem.snowball import SnowballStemmer  # initialize lemmatizer lemmatizer = WordNetLemmatizer()  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='579478dd-9b9f-41cc-81aa-d901ea24ee96', embedding=None, metadata={'page_label': '240', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 240 # initialize stemmer stemmer = SnowballStemmer(\"english\")  words = [\"fishing\", \"fishes\", \"fished\"]  for word in words:     print(f\"word={word}\")     print(f\"stemmed_word={stemmer.stem(word)}\")     print(f\"lemma={lemmatizer.lemmatize(word)}\")     print(\"\") ═════════════════════════════════════════════════════════════════════════  This will print:  ═════════════════════════════════════════════════════════════════════════ word=fishing stemmed_word=fish lemma=fishing  word=fishes stemmed_word=fish lemma=fish  word=fished stemmed_word=fish lemma=fished ═════════════════════════════════════════════════════════════════════════  As you can see, stemming and lemmatization are very different from each other. When we do stemming, we are given the smallest form of a word which may or may not be a word in the dictionary for the language the word belongs to. However, in the case of lemmatization, this will be a word. You can now try on your own to add stemming and lemmatizations and see if it improves your result.  One more topic that you should be aware of is topic extraction. Topic extraction can be done using non-negative matrix factorization (NMF) or latent semantic analysis (LSA), which is also popularly known as singular value decomposition or SVD. These are decomposition techniques that reduce the data to a given number of components. You can fit any of these on sparse matrix obtained from CountVectorizer or TfidfVectorizer.   Let’s apply it on TfidfVetorizer that we have used before.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='652a9241-eb47-4e00-a9cd-746ef8283f1e', embedding=None, metadata={'page_label': '241', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 241 ═════════════════════════════════════════════════════════════════════════ import pandas as pd from nltk.tokenize import word_tokenize from sklearn import decomposition from sklearn.feature_extraction.text import TfidfVectorizer  # create a corpus of sentences # we read only 10k samples from training data # for this example corpus = pd.read_csv(\"../input/imdb.csv\", nrows=10000) corpus = corpus.review.values  # initialize TfidfVectorizer with word_tokenize from nltk # as the tokenizer tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)  # fit the vectorizer on corpus tfv.fit(corpus)  # transform the corpus using tfidf corpus_transformed = tfv.transform(corpus)  # initialize SVD with 10 components svd = decomposition.TruncatedSVD(n_components=10)  # fit SVD corpus_svd = svd.fit(corpus_transformed)  # choose first sample and create a dictionary # of feature names and their scores from svd # you can change the sample_index variable to # get dictionary for any other sample sample_index = 0 feature_scores = dict(     zip(         tfv.get_feature_names(),         corpus_svd.components_[sample_index]     ) )  # once we have the dictionary, we can now # sort it in decreasing order and get the  # top N topics N = 5 print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N]) ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dc3a837-00ef-4765-80de-44efa3c6e45a', embedding=None, metadata={'page_label': '242', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 242 You can run it for multiple samples by using a loop.  ═════════════════════════════════════════════════════════════════════════ N = 5  for sample_index in range(5):     feature_scores = dict(         zip(             tfv.get_feature_names(),             corpus_svd.components_[sample_index]         )     )     print(         sorted(             feature_scores,              key=feature_scores.get,              reverse=True         )[:N]     ) ═════════════════════════════════════════════════════════════════════════  This gives the following output.  ═════════════════════════════════════════════════════════════════════════ [\\'the\\', \\',\\', \\'.\\', \\'a\\', \\'and\\'] [\\'br\\', \\'<\\', \\'>\\', \\'/\\', \\'-\\'] [\\'i\\', \\'movie\\', \\'!\\', \\'it\\', \\'was\\'] [\\',\\', \\'!\\', \"\\'\\'\", \\'``\\', \\'you\\'] [\\'!\\', \\'the\\', \\'...\\', \"\\'\\'\", \\'``\\'] ═════════════════════════════════════════════════════════════════════════  You can see that it doesn’t make any sense at all. It happens. What can one do? Let’s try cleaning and see if it makes any sense.  To clean any text data, especially when it’s in pandas dataframe, you can make a function.  ═════════════════════════════════════════════════════════════════════════ import re import string  def clean_text(s):     \"\"\"     This function cleans the text a bit     :param s: string ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95c190cb-526d-4f47-b842-f3f4896d9250', embedding=None, metadata={'page_label': '243', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 243     :return: cleaned string     \"\"\"     # split by all whitespaces     s = s.split()          # join tokens by single space     # why we do this?     # this will remove all kinds of weird space     # \"hi.   how are you\" becomes     # \"hi. how are you\"     s = \" \".join(s)          # remove all punctuations using regex and string module     s = re.sub(f\\'[{re.escape(string.punctuation)}]\\', \\'\\', s)          # you can add more cleaning here if you want     # and then return the cleaned string     return s ═════════════════════════════════════════════════════════════════════════  This function will convert a string like “hi, how are you????” to “hi how are you”. Let’s apply this function to the old SVD code and see if it brings any value to the extracted topics. With pandas, you can use the apply function to “apply” the clean-up code to any given column.  ═════════════════════════════════════════════════════════════════════════ import pandas as pd . corpus = pd.read_csv(\"../input/imdb.csv\", nrows=10000) corpus.loc[:, \"review\"] = corpus.review.apply(clean_text) . . ═════════════════════════════════════════════════════════════════════════  Note that we have added only one line of code to our main SVD script and that’s the beauty of using a function and apply from pandas. The topics generated this time look like the following.  ═════════════════════════════════════════════════════════════════════════ [\\'the\\', \\'a\\', \\'and\\', \\'of\\', \\'to\\'] [\\'i\\', \\'movie\\', \\'it\\', \\'was\\', \\'this\\'] [\\'the\\', \\'was\\', \\'i\\', \\'were\\', \\'of\\'] [\\'her\\', \\'was\\', \\'she\\', \\'i\\', \\'he\\'] [\\'br\\', \\'to\\', \\'they\\', \\'he\\', \\'show\\'] ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ec65d6c-4c89-4406-a108-f0fbfd1ef747', embedding=None, metadata={'page_label': '244', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 244 Phew! At least this is better than what we had earlier. But you know what? You can make it even better by removing stopwords in your cleaning function. What are stopwords? These are high-frequency words that exist in every language. For example, in the English language, these words are “a”, “an”, “the”, “for”, etc. Removing stopwords is not always a wise choice and depends a lot on the business problem. A sentence like “I need a new dog” after removing stopwords will become “need new dog”, so we don’t know who needs a new dog.   We lose a lot of context information if we remove stopwords all the time. You can find stopwords for many languages in NLTK, and if it’s not there, you can find it by a quick search on your favourite search engine.  Let’s move to an approach most of us like to use these days: deep learning. But first, we must know what word embeddings are. You have seen that till now we converted the tokens into numbers. So, if there are N unique tokens in a given corpus, they can be represented by integers ranging from 0 to N-1. Now we will represent these integer tokens with vectors. This representation of words into vectors is known as word embeddings or word vectors. Google’s Word2Vec is one of the oldest approaches to convert words into vectors. We also have FastText from Facebook and GloVe (Global Vectors for Word Representation) from Stanford. These approaches are quite different from each other.   The basic idea is to build a shallow network that learns the embeddings for words by reconstruction of an input sentence. So, you can train a network to predict a missing word by using all the words around and during this process, the network will learn and update embeddings for all the words involved. This approach is also known as Continuous Bag of Words or CBoW model. You can also try to take one word and predict the context words instead. This is called skip-gram model. Word2Vec can learn embedding using these two methods.   FastText learns embeddings for character n-grams instead. Just like word n-grams, if we use characters, it is known as character n-grams, and finally, GloVe learns these embeddings by using co-occurrence matrices. So, we can say that all these different types of embeddings are in the end returning a dictionary where the key is a word in the corpus (for example English Wikipedia) and value is a vector of size N (usually 300).    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a665b50-3e07-4226-a2c7-cc597c5a2a06', embedding=None, metadata={'page_label': '245', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 245  Figure 1: Visualizing word embeddings in two-dimensions.  Figure 1 shows a visualization of word embeddings in two-dimensions. Suppose, we have done it somehow and represented the words in two dimensions. Figure 1 shows that if you subtract the vector for Germany from the vector of Berlin (capital of Germany) and add the vector of France to it, you will get a vector close to the vector for Paris (capital of France). This shows that embeddings also work with analogies. This is not always true, but examples like these are useful for understanding the usefulness of word embeddings. A sentence like “hi, how are you ” can be represented by a bunch of vectors as follows.  hi ─>  [vector (v1) of size 300] ,  ─>   [vector (v2) of size 300] how ─>  [vector (v3) of size 300] are ─>  [vector (v4) of size 300] you ─>  [vector (v5) of size 300] ? ─>  [vector (v6) of size 300]  There are multiple ways to use this information. One of the simplest ways would be to use the embeddings as they are. As you can see in the example above, we have a 1x300 embedding vector for each word. Using this information, we can calculate the embedding for the whole sentence. There are multiple ways to do it. One such \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f508ab58-fc2b-4129-96fb-e5451f032ac2', embedding=None, metadata={'page_label': '246', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 246 method is shown as follows. In this function, we take all the individual word vectors in a given sentence and create a normalized word vector from all word vectors of the tokens. This provides us with a sentence vector.  ═════════════════════════════════════════════════════════════════════════ import numpy as np  def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):     \"\"\"     Given a sentence and other information,     this function returns embedding for the whole sentence     :param s: sentence, string     :param embedding_dict: dictionary word:vector     :param stop_words: list of stop words, if any     :param tokenizer: a tokenization function     \"\"\"     # convert sentence to string and lowercase it     words = str(s).lower()          # tokenize the sentence     words = tokenizer(words)          # remove stop word tokens     words = [w for w in words if not w in stop_words]          # keep only alpha-numeric tokens     words = [w for w in words if w.isalpha()]          # initialize empty list to store embeddings     M = []     for w in words:         # for every word, fetch the embedding from         # the dictionary and append to list of          # embeddings         if w in embedding_dict:             M.append(embedding_dict[w])          # if we dont have any vectors, return zeros     if len(M) == 0:         return np.zeros(300)      # convert list of embeddings to array     M = np.array(M)          # calculate sum over axis=0     v = M.sum(axis=0)      ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dcfd30de-7c57-44d3-bc61-6c359d46b010', embedding=None, metadata={'page_label': '247', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 247     # return normalized vector     return v / np.sqrt((v ** 2).sum()) ═════════════════════════════════════════════════════════════════════════  We can use this method to convert all our examples to one vector. Can we use fastText vectors to improve the previous results? We have 300 features for every review.  ═════════════════════════════════════════════════════════════════════════ # fasttext.py  import io import numpy as np import pandas as pd  from nltk.tokenize import word_tokenize from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn.feature_extraction.text import TfidfVectorizer  def load_vectors(fname):     # taken from: https://fasttext.cc/docs/en/english-vectors.html     fin = io.open(               fname,                \\'r\\',                encoding=\\'utf-8\\',                newline=\\'\\\\n\\',                errors=\\'ignore\\'     )     n, d = map(int, fin.readline().split())     data = {}     for line in fin:         tokens = line.rstrip().split(\\' \\')         data[tokens[0]] = list(map(float, tokens[1:]))     return data   def sentence_to_vec(s, embedding_dict, stop_words, tokenizer): . . . if __name__ == \"__main__\":     # read the training data     df = pd.read_csv(\"../input/imdb.csv\")  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97d8d9d7-2f56-4f0a-b546-d63a78264561', embedding=None, metadata={'page_label': '248', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 248     # map positive to 1 and negative to 0     df.sentiment = df.sentiment.apply(         lambda x: 1 if x == \"positive\" else 0     )      # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)      # load embeddings into memory     print(\"Loading embeddings\")     embeddings = load_vectors(\"../input/crawl-300d-2M.vec\")      # create sentence embeddings     print(\"Creating sentence vectors\")     vectors = []     for review in df.review.values:         vectors.append(             sentence_to_vec(                 s = review,                 embedding_dict = embeddings,                 stop_words = [],                  tokenizer = word_tokenize             )         )          vectors = np.array(vectors)      # fetch labels     y = df.sentiment.values          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     for fold_, (t_, v_) in enumerate(kf.split(X=vectors, y=y)):         print(f\"Training fold: {fold_}\")         # temporary dataframes for train and test         xtrain = vectors[t_, :]         ytrain = y[t_]          xtest = vectors[v_, :]         ytest = y[v_]          # initialize logistic regression model         model = linear_model.LogisticRegression()          # fit the model on training data reviews and sentiment ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5128b77b-2046-4e41-83d7-75ea8fc04d5c', embedding=None, metadata={'page_label': '249', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 249         model.fit(xtrain, ytrain)          # make predictions on test data         # threshold for predictions is 0.5         preds = model.predict(xtest)          # calculate accuracy         accuracy = metrics.accuracy_score(ytest, preds)         print(f\"Accuracy = {accuracy}\")         print(\"\") ═════════════════════════════════════════════════════════════════════════  This gives the following results.  ═════════════════════════════════════════════════════════════════════════ ❯ python fasttext.py Loading embeddings Creating sentence vectors Training fold: 0 Accuracy = 0.8619  Training fold: 1 Accuracy = 0.8661  Training fold: 2 Accuracy = 0.8544  Training fold: 3 Accuracy = 0.8624  Training fold: 4 Accuracy = 0.8595 ═════════════════════════════════════════════════════════════════════════  Wow! That’s quite unexpected. We get excellent results, and all we did was to use the FastText embeddings. Try changing the embeddings to GloVe and see what happens. I’m leaving it as an exercise for you.  When we talk about text data, we must keep one thing in our mind. Text data is very similar to the time series data. Any sample in our reviews is a sequence of tokens at different timestamps which are in increasing order, and each token can be represented as a vector/embedding, as shown in figure 2. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af4c7ba6-6ebf-4d7d-9f83-f1640430c1d8', embedding=None, metadata={'page_label': '250', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 250  Figure 2: Representing tokens as embeddings and treating it as a time series  This means that we can use models that are widely used for time series data such as Long Short Term Memory (LSTM) or Gated Recurrent Units (GRU) or even Convolutional Neural Networks (CNNs). Let’s see how to train a simple bi-directional LSTM model on this dataset.   First of all, we will create a project. Feel free to name it whatever you want. And then our first step will be splitting the data for cross-validation.  ═════════════════════════════════════════════════════════════════════════ # create_folds.py # import pandas and model_selection module of scikit-learn import pandas as pd from sklearn import model_selection  if __name__ == \"__main__\":     # Read training data     df = pd.read_csv(\"../input/imdb.csv\")      # map positive to 1 and negative to 0     df.sentiment = df.sentiment.apply(         lambda x: 1 if x == \"positive\" else 0     )      # we create a new column called kfold and fill it with -1     df[\"kfold\"] = -1 \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1db25ef8-62de-4030-aac7-4e8f5c940d69', embedding=None, metadata={'page_label': '251', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 251     # the next step is to randomize the rows of the data     df = df.sample(frac=1).reset_index(drop=True)          # fetch labels     y = df.sentiment.values          # initiate the kfold class from model_selection module     kf = model_selection.StratifiedKFold(n_splits=5)          # fill the new kfold column     for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):         df.loc[v_, \\'kfold\\'] = f          # save the new csv with kfold column     df.to_csv(\"../input/imdb_folds.csv\", index=False) ═════════════════════════════════════════════════════════════════════════  Once we have the dataset divided into folds, we create a simple dataset class in dataset.py. Dataset class returns one sample of the training or validation data.  ═════════════════════════════════════════════════════════════════════════ # dataset.py import torch   class IMDBDataset:     def __init__(self, reviews, targets):         \"\"\"         :param reviews: this is a numpy array         :param targets: a vector, numpy array         \"\"\"         self.reviews = reviews         self.target = targets      def __len__(self):         # returns length of the dataset         return len(self.reviews)          def __getitem__(self, item):         # for any given item, which is an int,         # return review and targets as torch tensor         # item is the index of the item in concern         review = self.reviews[item, :]         target = self.target[item]           return { ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97aefe09-b179-455e-93cb-85e1de9ee3f1', embedding=None, metadata={'page_label': '252', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 252             \"review\": torch.tensor(review, dtype=torch.long),             \"target\": torch.tensor(target, dtype=torch.float)         } ═════════════════════════════════════════════════════════════════════════  Once the dataset class is done, we can create lstm.py which consists of our LSTM model.  ═════════════════════════════════════════════════════════════════════════ # lstm.py  import torch import torch.nn as nn   class LSTM(nn.Module):     def __init__(self, embedding_matrix):         \"\"\"         :param embedding_matrix: numpy array with vectors for all words         \"\"\"         super(LSTM, self).__init__()         # number of words = number of rows in embedding matrix         num_words = embedding_matrix.shape[0]          # dimension of embedding is num of columns in the matrix         embed_dim = embedding_matrix.shape[1]          # we define an input embedding layer         self.embedding = nn.Embedding(             num_embeddings=num_words,              embedding_dim=embed_dim         )          # embedding matrix is used as weights of          # the embedding layer         self.embedding.weight = nn.Parameter(             torch.tensor(                 embedding_matrix,                  dtype=torch.float32             )         )          # we dont want to train the pretrained embeddings         self.embedding.weight.requires_grad = False          # a simple bidirectional LSTM with         # hidden size of 128 ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='285f96c0-f7cd-4dc8-9f33-c34f88424f40', embedding=None, metadata={'page_label': '253', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 253         self.lstm = nn.LSTM(             embed_dim,             128,             bidirectional=True,             batch_first=True,         )          # output layer which is a linear layer         # we have only one output         # input (512) = 128 + 128 for mean and same for max pooling         self.out = nn.Linear(512, 1)      def forward(self, x):         # pass data through embedding layer         # the input is just the tokens         x = self.embedding(x)          # move embedding output to lstm         x, _ = self.lstm(x)          # apply mean and max pooling on lstm output         avg_pool = torch.mean(x, 1)         max_pool, _ = torch.max(x, 1)                  # concatenate mean and max pooling         # this is why size is 512         # 128 for each direction = 256         # avg_pool = 256 and max_pool = 256         out = torch.cat((avg_pool, max_pool), 1)          # pass through the output layer and return the output         out = self.out(out)          # return linear output         return out ═════════════════════════════════════════════════════════════════════════  Now, we create engine.py which consists of our training and evaluation functions.  ═════════════════════════════════════════════════════════════════════════ # engine.py import torch import torch.nn as nn   def train(data_loader, model, optimizer, device):     \"\"\" ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='52892936-ac70-44ef-a5c3-d8cc3825ad45', embedding=None, metadata={'page_label': '254', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 254     This is the main training function that trains model     for one epoch     :param data_loader: this is the torch dataloader     :param model: model (lstm model)     :param optimizer: torch optimizer, e.g. adam, sgd, etc.     :param device: this can be \"cuda\" or \"cpu\"     \"\"\"     # set model to training mode     model.train()      # go through batches of data in data loader     for data in data_loader:         # fetch review and target from the dict         reviews = data[\"review\"]         targets = data[\"target\"]          # move the data to device that we want to use         reviews = reviews.to(device, dtype=torch.long)         targets = targets.to(device, dtype=torch.float)          # clear the gradients         optimizer.zero_grad()          # make predictions from the model         predictions = model(reviews)          # calculate the loss         loss = nn.BCEWithLogitsLoss()(             predictions,             targets.view(-1, 1)         )          # compute gradient of loss w.r.t.         # all parameters of the model that are trainable         loss.backward()          # single optimization step         optimizer.step()   def evaluate(data_loader, model, device):     # initialize empty lists to store predictions     # and targets     final_predictions = []     final_targets = []      # put the model in eval mode ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58627719-54ac-4dfe-9170-76959b64a967', embedding=None, metadata={'page_label': '255', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 255     model.eval()      # disable gradient calculation     with torch.no_grad():         for data in data_loader:             reviews = data[\"review\"]             targets = data[\"target\"]             reviews = reviews.to(device, dtype=torch.long)             targets = targets.to(device, dtype=torch.float)              # make predictions             predictions = model(reviews)              # move predictions and targets to list             # we need to move predictions and targets to cpu too             predictions = predictions.cpu().numpy().tolist()             targets = data[\"target\"].cpu().numpy().tolist()             final_predictions.extend(predictions)             final_targets.extend(targets)      # return final predictions and targets     return final_predictions, final_targets ═════════════════════════════════════════════════════════════════════════  These functions will help us in train.py which is used for training multiple folds.  ═════════════════════════════════════════════════════════════════════════ # train.py import io import torch  import numpy as np import pandas as pd  # yes, we use tensorflow # but not for training the model! import tensorflow as tf  from sklearn import metrics  import config import dataset import engine import lstm  def load_vectors(fname):     # taken from: https://fasttext.cc/docs/en/english-vectors.html ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='070aefb4-8332-4da9-a53c-0525ce09e170', embedding=None, metadata={'page_label': '256', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 256     fin = io.open(         fname,          \\'r\\',          encoding=\\'utf-8\\',          newline=\\'\\\\n\\',          errors=\\'ignore\\'     )     n, d = map(int, fin.readline().split())     data = {}     for line in fin:         tokens = line.rstrip().split(\\' \\')         data[tokens[0]] = list(map(float, tokens[1:]))     return data   def create_embedding_matrix(word_index, embedding_dict):     \"\"\"     This function creates the embedding matrix.     :param word_index: a dictionary with word:index_value     :param embedding_dict: a dictionary with word:embedding_vector     :return: a numpy array with embedding vectors for all known words     \"\"\"     # initialize matrix with zeros     embedding_matrix = np.zeros((len(word_index) + 1, 300))     # loop over all the words     for word, i in word_index.items():         # if word is found in pre-trained embeddings,          # update the matrix. if the word is not found,         # the vector is zeros!         if word in embedding_dict:             embedding_matrix[i] = embedding_dict[word]     # return embedding matrix     return embedding_matrix   def run(df, fold):     \"\"\"     Run training and validation for a given fold     and dataset     :param df: pandas dataframe with kfold column     :param fold: current fold, int     \"\"\"      # fetch training dataframe     train_df = df[df.kfold != fold].reset_index(drop=True)      # fetch validation dataframe ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4568087-ea85-4746-b90f-7262833342a4', embedding=None, metadata={'page_label': '257', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 257     valid_df = df[df.kfold == fold].reset_index(drop=True)      print(\"Fitting tokenizer\")     # we use tf.keras for tokenization     # you can use your own tokenizer and then you can      # get rid of tensorflow     tokenizer = tf.keras.preprocessing.text.Tokenizer()     tokenizer.fit_on_texts(df.review.values.tolist())      # convert training data to sequences     # for example : \"bad movie\" gets converted to     # [24, 27] where 24 is the index for bad and 27 is the     # index for movie     xtrain = tokenizer.texts_to_sequences(train_df.review.values)      # similarly convert validation data to     # sequences     xtest = tokenizer.texts_to_sequences(valid_df.review.values)      # zero pad the training sequences given the maximum length     # this padding is done on left hand side     # if sequence is > MAX_LEN, it is truncated on left hand side too     xtrain = tf.keras.preprocessing.sequence.pad_sequences(         xtrain, maxlen=config.MAX_LEN     )      # zero pad the validation sequences     xtest = tf.keras.preprocessing.sequence.pad_sequences(         xtest, maxlen=config.MAX_LEN     )      # initialize dataset class for training     train_dataset = dataset.IMDBDataset(         reviews=xtrain,         targets=train_df.sentiment.values     )      # create torch dataloader for training     # torch dataloader loads the data using dataset     # class in batches specified by batch size     train_data_loader = torch.utils.data.DataLoader(         train_dataset,         batch_size=config.TRAIN_BATCH_SIZE,         num_workers=2     )      # initialize dataset class for validation ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1dad69d5-6fb5-466e-93fb-eb9d6986b4a1', embedding=None, metadata={'page_label': '258', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 258     valid_dataset = dataset.IMDBDataset(         reviews=xtest,         targets=valid_df.sentiment.values     )          # create torch dataloader for validation     valid_data_loader = torch.utils.data.DataLoader(         valid_dataset,         batch_size=config.VALID_BATCH_SIZE,         num_workers=1     )      print(\"Loading embeddings\")     # load embeddings as shown previously     embedding_dict = load_vectors(\"../input/crawl-300d-2M.vec\")     embedding_matrix = create_embedding_matrix(         tokenizer.word_index, embedding_dict     )      # create torch device, since we use gpu, we are using cuda     device = torch.device(\"cuda\")      # fetch our LSTM model     model = lstm.LSTM(embedding_matrix)      # send model to device     model.to(device)          # initialize Adam optimizer     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)      print(\"Training Model\")     # set best accuracy to zero     best_accuracy = 0     # set early stopping counter to zero     early_stopping_counter = 0     # train and validate for all epochs     for epoch in range(config.EPOCHS):         # train one epoch         engine.train(train_data_loader, model, optimizer, device)         # validate         outputs, targets = engine.evaluate(                                valid_data_loader, model, device         )          # use threshold of 0.5          # please note we are using linear layer and no sigmoid ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dcc17e15-2c23-4c8e-a660-2410d52aa0c6', embedding=None, metadata={'page_label': '259', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 259         # you should do this 0.5 threshold after sigmoid         outputs = np.array(outputs) >= 0.5          # calculate accuracy         accuracy = metrics.accuracy_score(targets, outputs)         print(           f\"FOLD:{fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\"         )          # simple early stopping         if accuracy > best_accuracy:             best_accuracy = accuracy         else:             early_stopping_counter += 1          if early_stopping_counter > 2:             break   if __name__ == \"__main__\":      # load data     df = pd.read_csv(\"../input/imdb_folds.csv\")      # train for all folds     run(df, fold=0)     run(df, fold=1)     run(df, fold=2)     run(df, fold=3)     run(df, fold=4) ═════════════════════════════════════════════════════════════════════════  And finally, we have config.py.  ═════════════════════════════════════════════════════════════════════════ # config.py # we define all the configuration here MAX_LEN = 128 TRAIN_BATCH_SIZE = 16 VALID_BATCH_SIZE = 8 EPOCHS = 10 ═════════════════════════════════════════════════════════════════════════  Let’s see what this gives us.  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5fa6efa-04be-41cd-94b4-d5a4aed2e7a9', embedding=None, metadata={'page_label': '260', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 260 ═════════════════════════════════════════════════════════════════════════ ❯ python train.py  FOLD:0, Epoch: 3, Accuracy Score = 0.9015 FOLD:1, Epoch: 4, Accuracy Score = 0.9007 FOLD:2, Epoch: 3, Accuracy Score = 0.8924 FOLD:3, Epoch: 2, Accuracy Score = 0.9 FOLD:4, Epoch: 1, Accuracy Score = 0.878 ═════════════════════════════════════════════════════════════════════════  This is by far the best score we have obtained. Please note that I have only shown the epochs with best accuracies in each fold.  You must have noticed that we used pre-trained embeddings and a simple bi-directional LSTM. If you want to change the model, you can just change model in lstm.py and keep everything as it is. This kind of code requires minimal changes for experiments and is easily understandable. For example, you can learn the embeddings on your own instead of using pretrained embeddings, you can use some other pretrained embeddings, you can combine multiple pretrained embeddings, you can use GRU, you can use spatial dropout after embedded, you can add a GRU layer after LSTM, you can add two LSTM layers, you can have LSTM-GRU-LSTM config, you can replace LSTM with a convolutional layer, etc. without making many changes to the code. Most of what I mention requires changes only to model class.  When you use pretrained embeddings, try to see for how many words you are not able to find embeddings and why. The more words for which you have pre-trained embeddings, the better are the results. I present to you the following un-commented (!) function that you can use to create embedding matrix for any kind of pre-trained embedding which is in the same format as glove or fastText (some changes might be needed).  ═════════════════════════════════════════════════════════════════════════ def load_embeddings(word_index, embedding_file, vector_length=300):     \"\"\"     A general function to create embedding matrix     :param word_index: word:index dictionary     :param embedding_file: path to embeddings file     :param vector_length: length of vector     \"\"\"     max_features = len(word_index) + 1     words_to_find = list(word_index.keys())     more_words_to_find = [] ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f091b63-9159-48a8-a8ee-d4c7499e4c8a', embedding=None, metadata={'page_label': '261', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 261     for wtf in words_to_find:         more_words_to_find.append(wtf)         more_words_to_find.append(str(wtf).capitalize())     more_words_to_find = set(more_words_to_find)      def get_coefs(word, *arr):         return word, np.asarray(arr, dtype=\\'float32\\')      embeddings_index = dict(         get_coefs(*o.strip().split(\" \"))          for o in open(embedding_file)          if o.split(\" \")[0]          in more_words_to_find          and len(o) > 100     )      embedding_matrix = np.zeros((max_features, vector_length))     for word, i in word_index.items():         if i >= max_features:             continue         embedding_vector = embeddings_index.get(word)         if embedding_vector is None:             embedding_vector = embeddings_index.get(                 str(word).capitalize()             )         if embedding_vector is None:             embedding_vector = embeddings_index.get(                 str(word).upper()             )         if (embedding_vector is not None              and len(embedding_vector) == vector_length):             embedding_matrix[i] = embedding_vector     return embedding_matrix ═════════════════════════════════════════════════════════════════════════  Read and run the function above and see what’s happening. The function can also be modified to use stemmed words or lemmatized words. In the end, you want to have the least number of unknown words in your training corpus. One more trick is to learn the embedding layer, i.e., make it trainable and then train the network.  So far, we have built a lot of models for a classification problem. However, it is the era of muppets, and more and more people are moving towards transformer-based models. Transformer based networks are able to handle dependencies which are long term in nature. LSTM looks at the next word only when it has seen the previous word. This is not the case with transformers. It can look at all the words in the whole ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ec42ec3-3dbe-4ffb-a65c-8a8a202e159c', embedding=None, metadata={'page_label': '262', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 262 sentence simultaneously. Due to this, one more advantage is that it can easily be parallelized and uses GPUs more efficiently.  Transformers is a very broad topic, and there are too many models: BERT, RoBERTa, XLNet, XLM-RoBERTa, T5, etc. I will show you a general approach that you can use for all these models (except T5) for the classification problem that we have been discussing. Please note that these transformers are hungry in terms of computational power needed to train them. Thus, if you do not have a high-end system, it might take much longer to train a model compared to LSTM or TF-IDF based models.  The first thing we do is to create a config file.  ═════════════════════════════════════════════════════════════════════════ # config.py import transformers  # this is the maximum number of tokens in the sentence MAX_LEN = 512  # batch sizes is small because model is huge! TRAIN_BATCH_SIZE = 8 VALID_BATCH_SIZE = 4  # let\\'s train for a maximum of 10 epochs EPOCHS = 10  # define path to BERT model files BERT_PATH = \"../input/bert_base_uncased/\"  # this is where you want to save the model MODEL_PATH = \"model.bin\"  # training file TRAINING_FILE = \"../input/imdb.csv\"  # define the tokenizer # we use tokenizer and model  # from huggingface\\'s transformers TOKENIZER = transformers.BertTokenizer.from_pretrained(     BERT_PATH,      do_lower_case=True ) ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d04e10c6-9436-4852-ac2a-f1d2daea4b9c', embedding=None, metadata={'page_label': '263', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 263 The config file here is the only place where we define tokenizer and other parameters we would like to change frequently—this way we can do many experiments without requiring a lot of changes.  Next step is to build a dataset class.  ═════════════════════════════════════════════════════════════════════════ # dataset.py import config import torch   class BERTDataset:     def __init__(self, review, target):         \"\"\"         :param review: list or numpy array of strings         :param targets: list or numpy array which is binary         \"\"\"         self.review = review         self.target = target         # we fetch max len and tokenizer from config.py         self.tokenizer = config.TOKENIZER         self.max_len = config.MAX_LEN      def __len__(self):         # this returns the length of dataset         return len(self.review)      def __getitem__(self, item):         # for a given item index, return a dictionary         # of inputs         review = str(self.review[item])         review = \" \".join(review.split())          # encode_plus comes from hugginface\\'s transformers         # and exists for all tokenizers they offer         # it can be used to convert a given string         # to ids, mask and token type ids which are         # needed for models like BERT         # here, review is a string         inputs = self.tokenizer.encode_plus(             review,             None,             add_special_tokens=True,             max_length=self.max_len,             pad_to_max_length=True, ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f299a671-9e94-4ac3-9d8a-c9bdb216f61c', embedding=None, metadata={'page_label': '264', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 264         )         # ids are ids of tokens generated         # after tokenizing reviews         ids = inputs[\"input_ids\"]         # mask is 1 where we have input         # and 0 where we have padding         mask = inputs[\"attention_mask\"]         # token type ids behave the same way as          # mask in this specific case         # in case of two sentences, this is 0         # for first sentence and 1 for second sentence         token_type_ids = inputs[\"token_type_ids\"]          # now we return everything         # note that ids, mask and token_type_ids         # are all long datatypes and targets is float         return {             \"ids\": torch.tensor(                 ids, dtype=torch.long             ),             \"mask\": torch.tensor(                 mask, dtype=torch.long             ),             \"token_type_ids\": torch.tensor(                 token_type_ids, dtype=torch.long             ),             \"targets\": torch.tensor(                 self.target[item], dtype=torch.float             )         } ═════════════════════════════════════════════════════════════════════════  And now we come to the heart of the project, i.e. the model.  ═════════════════════════════════════════════════════════════════════════ # model.py import config import transformers import torch.nn as nn   class BERTBaseUncased(nn.Module):     def __init__(self):         super(BERTBaseUncased, self).__init__()         # we fetch the model from the BERT_PATH defined in          # config.py         self.bert = transformers.BertModel.from_pretrained( ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7972f58b-4e52-420a-b437-f1527260dfcd', embedding=None, metadata={'page_label': '265', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 265             config.BERT_PATH         )         # add a dropout for regularization         self.bert_drop = nn.Dropout(0.3)         # a simple linear layer for output         # yes, there is only one output         self.out = nn.Linear(768, 1)      def forward(self, ids, mask, token_type_ids):         # BERT in its default settings returns two outputs         # last hidden state and output of bert pooler layer         # we use the output of the pooler which is of the size         # (batch_size, hidden_size)         # hidden size can be 768 or 1024 depending on         # if we are using bert base or large respectively         # in our case, it is 768         # note that this model is pretty simple         # you might want to use last hidden state         # or several hidden states         _, o2 = self.bert(             ids,              attention_mask=mask,              token_type_ids=token_type_ids         )         # pass through dropout layer         bo = self.bert_drop(o2)         # pass through linear layer         output = self.out(bo)         # return output         return output ═════════════════════════════════════════════════════════════════════════  This model returns a single output. We can use binary cross-entropy loss with logits which first applies sigmoid and then calculates the loss. This is done in engine.py.  ═════════════════════════════════════════════════════════════════════════ # engine.py  import torch import torch.nn as nn   def loss_fn(outputs, targets):     \"\"\"     This function returns the loss. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9190c173-c2d2-4761-b621-37535d728bea', embedding=None, metadata={'page_label': '266', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 266     :param outputs: output from the model (real numbers)     :param targets: input targets (binary)     \"\"\"     return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))   def train_fn(data_loader, model, optimizer, device, scheduler):     \"\"\"     This is the training function which trains for one epoch     :param data_loader: it is the torch dataloader object     :param model: torch model, bert in our case     :param optimizer: adam, sgd, etc     :param device: can be cpu or cuda     :param scheduler: learning rate scheduler     \"\"\"     # put the model in training mode     model.train()      # loop over all batches     for d in data_loader:         # extract ids, token type ids and mask         # from current batch         # also extract targets         ids = d[\"ids\"]         token_type_ids = d[\"token_type_ids\"]         mask = d[\"mask\"]         targets = d[\"targets\"]          # move everything to specified device         ids = ids.to(device, dtype=torch.long)         token_type_ids = token_type_ids.to(device, dtype=torch.long)         mask = mask.to(device, dtype=torch.long)         targets = targets.to(device, dtype=torch.float)          # zero-grad the optimizer         optimizer.zero_grad()         # pass through the model         outputs = model(             ids=ids,             mask=mask,             token_type_ids=token_type_ids         )         # calculate loss         loss = loss_fn(outputs, targets)         # backward step the loss         loss.backward()         # step optimizer ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ecf0da67-b96f-4540-85c4-1e8e665fc696', embedding=None, metadata={'page_label': '267', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 267         optimizer.step()         # step scheduler         scheduler.step()   def eval_fn(data_loader, model, device):     \"\"\"     this is the validation function that generates     predictions on validation data     :param data_loader: it is the torch dataloader object     :param model: torch model, bert in our case     :param device: can be cpu or cuda     :return: output and targets     \"\"\"     # put model in eval mode     model.eval()     # initialize empty lists for     # targets and outputs     fin_targets = []     fin_outputs = []     # use the no_grad scope     # its very important else you might     # run out of gpu memory     with torch.no_grad():         # this part is same as training function         # except for the fact that there is no         # zero_grad of optimizer and there is no loss         # calculation or scheduler steps.         for d in data_loader:             ids = d[\"ids\"]             token_type_ids = d[\"token_type_ids\"]             mask = d[\"mask\"]             targets = d[\"targets\"]              ids = ids.to(device, dtype=torch.long)             token_type_ids = token_type_ids.to(device, dtype=torch.long)             mask = mask.to(device, dtype=torch.long)             targets = targets.to(device, dtype=torch.float)              outputs = model(                 ids=ids,                  mask=mask,                  token_type_ids=token_type_ids             )             # convert targets to cpu and extend the final list             targets = targets.cpu().detach()             fin_targets.extend(targets.numpy().tolist()) ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='deaf87b3-e255-4baa-9fff-db4fb2381640', embedding=None, metadata={'page_label': '268', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 268              # convert outputs to cpu and extend the final list             outputs = torch.sigmoid(outputs).cpu().detach()             fin_outputs.extend(outputs.numpy().tolist())     return fin_outputs, fin_targets ═════════════════════════════════════════════════════════════════════════  And finally, we are ready to train. Let’s look at the training script!  ═════════════════════════════════════════════════════════════════════════ # train.py import config import dataset import engine import torch import pandas as pd import torch.nn as nn import numpy as np  from model import BERTBaseUncased from sklearn import model_selection from sklearn import metrics from transformers import AdamW from transformers import get_linear_schedule_with_warmup   def train():     # this function trains the model          # read the training file and fill NaN values with \"none\"     # you can also choose to drop NaN values in this      # specific dataset     dfx = pd.read_csv(config.TRAINING_FILE).fillna(\"none\")      # sentiment = 1 if its positive     # else sentiment = 0     dfx.sentiment = dfx.sentiment.apply(         lambda x: 1 if x == \"positive\" else 0     )      # we split the data into single training     # and validation fold     df_train, df_valid = model_selection.train_test_split(         dfx,          test_size=0.1,          random_state=42,          stratify=dfx.sentiment.values ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3eb5d9a8-513d-4dee-9c0c-9ac7a3c90f35', embedding=None, metadata={'page_label': '269', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 269     )      # reset index     df_train = df_train.reset_index(drop=True)     df_valid = df_valid.reset_index(drop=True)      # initialize BERTDataset from dataset.py     # for training dataset     train_dataset = dataset.BERTDataset(         review=df_train.review.values,          target=df_train.sentiment.values     )      # create training dataloader     train_data_loader = torch.utils.data.DataLoader(         train_dataset,          batch_size=config.TRAIN_BATCH_SIZE,          num_workers=4     )      # initialize BERTDataset from dataset.py     # for validation dataset     valid_dataset = dataset.BERTDataset(         review=df_valid.review.values,          target=df_valid.sentiment.values     )      # create validation data loader     valid_data_loader = torch.utils.data.DataLoader(         valid_dataset,          batch_size=config.VALID_BATCH_SIZE,          num_workers=1     )      # initialize the cuda device     # use cpu if you dont have GPU     device = torch.device(\"cuda\")     # load model and send it to the device     model = BERTBaseUncased()     model.to(device)      # create parameters we want to optimize     # we generally dont use any decay for bias     # and weight layers      param_optimizer = list(model.named_parameters())     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]     optimizer_parameters = [ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee944568-f4ed-40a2-8856-60ca976ee27d', embedding=None, metadata={'page_label': '270', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 270         {             \"params\": [                 p for n, p in param_optimizer if                  not any(nd in n for nd in no_decay)             ],             \"weight_decay\": 0.001,         },         {             \"params\": [                 p for n, p in param_optimizer if                  any(nd in n for nd in no_decay)             ],             \"weight_decay\": 0.0,         },     ]      # calculate the number of training steps     # this is used by scheduler     num_train_steps = int(         len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS     )      # AdamW optimizer     # AdamW is the most widely used optimizer     # for transformer based networks     optimizer = AdamW(optimizer_parameters, lr=3e-5)      # fetch a scheduler     # you can also try using reduce lr on plateau     scheduler = get_linear_schedule_with_warmup(         optimizer,          num_warmup_steps=0,          num_training_steps=num_train_steps     )      # if you have multiple GPUs     # model model to DataParallel     # to use multiple GPUs     model = nn.DataParallel(model)      # start training the epochs     best_accuracy = 0     for epoch in range(config.EPOCHS):         engine.train_fn(             train_data_loader, model, optimizer, device, scheduler         )         outputs, targets = engine.eval_fn( ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29100299-6a0d-4c75-ad88-52b11fee0a01', embedding=None, metadata={'page_label': '271', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 271             valid_data_loader, model, device         )         outputs = np.array(outputs) >= 0.5         accuracy = metrics.accuracy_score(targets, outputs)         print(f\"Accuracy Score = {accuracy}\")         if accuracy > best_accuracy:             torch.save(model.state_dict(), config.MODEL_PATH)             best_accuracy = accuracy   if __name__ == \"__main__\":     train() ═════════════════════════════════════════════════════════════════════════  It might look like a lot at first, but it isn’t once you understand the individual components. You can easily change it to any other transformer model you want to use just by changing a few lines of code.   This model gives an accuracy of 93%! Whoa! That’s much better than any other model. But is it worth it?  We were able to achieve 90% using LSTMs, and they are much simpler, easier to train and faster when it comes to inference. We could improve that model probably by a percent by using different data processing or by tuning the parameters such as layers, nodes, dropout, learning rate, changing the optimizer, etc. Then we will have ~2% benefit from BERT. BERT, on the other hand, took much longer to train, has a lot of parameters and is also slow when it comes to inference. In the end, you should look at your business and choose wisely. Don’t choose BERT only because it’s “cool”.  It must be noted that the only task we discussed here is classification but changing it to regression, multi-label or multi-class will require only a couple of lines of code changes. For example, the same problem in multi-class classification setting will have multiple outputs and Cross-Entropy loss. Everything else should remain the same. Natural language processing is huge, and we discussed only a small fraction of it. Apparently, this is a huge fraction as most of the industrial models are classification or regression models. If I start writing in detail about everything I might end up writing a few hundred pages, and that’s why I have decided to include everything in a separate book: Approaching (Almost) Any NLP Problem! ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='facf6289-ec51-4b87-86c2-41b4ea899536', embedding=None, metadata={'page_label': '272', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 272 Approaching ensembling and stacking  When we hear these two words, the first thing that comes to our mind is that it’s all about online/offline machine learning competitions. This used to be the case a few years ago, but now with the advancements in computing power and cheaper virtual instances, people have started using ensemble models even in industries. For example, it’s very easy to deploy multiple neural networks and serve them in real-time with a response time of less than 500ms. Sometimes, a huge neural network or a large model can also be replaced by a few other models which are small in size, perform similar to the large model and are twice as fast. If this is the case, which model(s) will you choose? I, personally, would prefer multiple small models, which are faster and give the same performance as a much larger and slower model. Please remember that smaller models are also easier and faster to tune.  Ensembling is nothing but a combination of different models. The models can be combined by their predictions/probabilities. The simplest way to combine models would be just to do an average.  Ensemble Probabilities = (M1_proba + M2_proba + … + Mn_Proba) / n  This is simple and yet the most effective way of combining models. In simple averaging, the weights are equal to all models. One thing that you should keep in mind for any method of combining is that you should always combine predictions/probabilities of models which are different from each other. In simple words, the combination of models which are not highly correlated works better than the combination of models which are very correlated with each other.  If you do not have probabilities, you can combine predictions too. The most simple way of doing this is to take a vote. Suppose we are doing a multi-class classification with three classes: 0, 1 and 2.  [0, 0, 1] : Highest voted class: 0  [0, 1, 2] : Highest voted class: None (Choose one randomly)  [2, 2, 2] : Highest voted class: 2  The following simple functions can accomplish these simple operations. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8233ac33-86dd-4922-aa37-c2bfc6f7a084', embedding=None, metadata={'page_label': '273', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 273 ═════════════════════════════════════════════════════════════════════════ import numpy as np   def mean_predictions(probas):     \"\"\"     Create mean predictions     :param probas: 2-d array of probability values     :return: mean probability     \"\"\"     return np.mean(probas, axis=1)   def max_voting(preds):     \"\"\"     Create mean predictions     :param probas: 2-d array of prediction values     :return: max voted predictions     \"\"\"     idxs = np.argmax(preds, axis=1)     return np.take_along_axis(preds, idxs[:, None], axis=1) ═════════════════════════════════════════════════════════════════════════  Please note that probas have a single probability (i.e. binary classification, usually class 1) in each column. Each column is thus a new model. Similarly, for preds, each column is a prediction from different models. Both these functions assume a 2-dimensional numpy array. You can modify it according to your requirements. For example, you might have a 2-d array of probabilities for each model. In that case, the function will change a bit.  Another way of combining multiple models is by ranks of their probabilities. This type of combination works quite good when the concerned metric is the area under curve as AUC is all about ranking samples.  ═════════════════════════════════════════════════════════════════════════ def rank_mean(probas):     \"\"\"     Create mean predictions using ranks     :param probas: 2-d array of probability values     :return: mean ranks     \"\"\"      ranked = []     for i in range(probas.shape[1]):         rank_data = stats.rankdata(probas[:, i])         ranked.append(rank_data)  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e2dbcf18-6fdf-4458-8210-f5fd81970d7b', embedding=None, metadata={'page_label': '274', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 274     ranked = np.column_stack(ranked)     return np.mean(ranked, axis=1) ═════════════════════════════════════════════════════════════════════════  Please note that in scipy’s rankdata, ranks start at 1.  Why do these kinds of ensembles work? Let’s look at figure 1.  \\n Figure 1: Three people guessing the height of an elephant  Figure 1 shows that if three people are guessing the height of an elephant, the original height will be very close to the average of the three guesses. Let’s assume these people can guess very close to the original height of the elephant. Close estimate means an error, but this error can be minimized when we average the three predictions. This is the main idea behind the averaging of multiple models.  Probabilities can also be combined by weights.  Final Probabilities = w1*M1_proba + w2*M2_proba + … + wn*Mn_proba  Where (w1 + w2 + w3 + … + wn) = 1.0 For example, if you have a random forest model that gives very high AUC and a logistic regression model with a little lower AUC, you can combine them with 70% \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b396a6c8-e565-4258-a0ce-b2416a57f3f1', embedding=None, metadata={'page_label': '275', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 275 for random forest and 30% for logistic regression. So, how did I come up with these numbers? Let’s add another model, let’s say now we also have an xgboost model that gives an AUC higher than random forest. Now I will combine them with a ratio of 3:2:1 for xgboost : random forest : logistic regression. Easy right? Arriving at these numbers is a piece of cake. Let’s see how.  Assume that we have three monkeys with three knobs with values that range between 0 and 1. These monkeys turn the knobs, and we calculate the AUC score at each value they turn the knob to. Eventually, the monkeys will find a combination that gives the best AUC. Yes, it is a random search! Before doing these kinds of searches, you must remember the two most important rules of ensembling.  The first rule of ensembling is that you always create folds before starting with ensembling.  The second rule of ensembling is that you always create folds before starting with ensembling.  YES. These are the two most important rules, and no, there is no mistake in what I wrote. The first step is to create folds. For simplicity, let’s say we divide the data into two parts: fold 1 and fold 2. Please note that this is only done for simplicity in explaining. In a real-world scenario, you should create more folds.   Now, we train our random forest model, logistic regression model and our xgboost model on fold 1 and make predictions on fold 2. After this, we train the models from scratch on fold 2 and make predictions on fold 1. Thus, we have created predictions for all of the training data. Now to combine these models, we take fold 1 and all the predictions for fold 1 and create an optimization function that tries to find the best weights so as to minimize error or maximize AUC against the targets for fold 2. So, we are kind of training an optimization model on fold 1 with the predicted probabilities for the three models and evaluating it on fold 2. Let’s first look at a class we can use to find the best weights of multiple models to optimize for AUC (or any kind of prediction-metric combination in general).  ═════════════════════════════════════════════════════════════════════════ import numpy as np  from functools import partial from scipy.optimize import fmin from sklearn import metrics  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b752193-de52-4564-a64d-e940bb9b636d', embedding=None, metadata={'page_label': '276', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 276 class OptimizeAUC:     \"\"\"     Class for optimizing AUC.     This class is all you need to find best weights for      any model and for any metric and for any types of predictions.     With very small changes, this class can be used for optimization of      weights in ensemble models of _any_ type of predictions     \"\"\"     def __init__(self):         self.coef_ = 0      def _auc(self, coef, X, y):         \"\"\"         This functions calulates and returns AUC.         :param coef: coef list, of the same length as number of models         :param X: predictions, in this case a 2d array         :param y: targets, in our case binary 1d array         \"\"\"         # multiply coefficients with every column of the array         # with predictions.         # this means: element 1 of coef is multiplied by column 1         # of the prediction array, element 2 of coef is multiplied          # by column 2 of the prediction array and so on!         x_coef = X * coef          # create predictions by taking row wise sum         predictions = np.sum(x_coef, axis=1)                  # calculate auc score         auc_score = metrics.roc_auc_score(y, predictions)          # return negative auc         return -1.0 * auc_score      def fit(self, X, y):         # remember partial from hyperparameter optimization chapter?         loss_partial = partial(self._auc, X=X, y=y)                  # dirichlet distribution. you can use any distribution you want         # to initialize the coefficients         # we want the coefficients to sum to 1         initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)          # use scipy fmin to minimize the loss function, in our case auc         self.coef_ = fmin(loss_partial, initial_coef, disp=True)      def predict(self, X): ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6be669f5-c9d3-48a5-a66a-2863f87b39b7', embedding=None, metadata={'page_label': '277', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 277         # this is similar to _auc function         x_coef = X * self.coef_         predictions = np.sum(x_coef, axis=1)         return predictions ═════════════════════════════════════════════════════════════════════════  Let’s see how to use this and compare it with simple averaging.  ═════════════════════════════════════════════════════════════════════════ import xgboost as xgb from sklearn.datasets import make_classification from sklearn import ensemble from sklearn import linear_model from sklearn import metrics from sklearn import model_selection   # make a binary classification dataset with 10k samples # and 25 features X, y = make_classification(n_samples=10000, n_features=25)  # split into two folds (for this example) xfold1, xfold2, yfold1, yfold2 = model_selection.train_test_split(     X,     y,     test_size=0.5,     stratify=y )  # fit models on fold 1 and make predictions on fold 2 # we have 3 models: # logistic regression, random forest and xgboost logreg = linear_model.LogisticRegression() rf = ensemble.RandomForestClassifier() xgbc = xgb.XGBClassifier()  # fit all models on fold 1 data logreg.fit(xfold1, yfold1) rf.fit(xfold1, yfold1) xgbc.fit(xfold1, yfold1)  # predict all models on fold 2 # take probability for class 1 pred_logreg = logreg.predict_proba(xfold2)[:, 1] pred_rf = rf.predict_proba(xfold2)[:, 1] pred_xgbc = xgbc.predict_proba(xfold2)[:, 1]  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1398220-536d-4939-a0e6-eb242088a047', embedding=None, metadata={'page_label': '278', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 278 # create an average of all predictions # that is the simplest ensemble avg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3  # a 2d array of all predictions fold2_preds = np.column_stack((     pred_logreg,     pred_rf,     pred_xgbc,     avg_pred ))  # calculate and store individual AUC values aucs_fold2 = [] for i in range(fold2_preds.shape[1]):     auc = metrics.roc_auc_score(yfold2, fold2_preds[:, i])     aucs_fold2.append(auc)  print(f\"Fold-2: LR AUC = {aucs_fold2[0]}\") print(f\"Fold-2: RF AUC = {aucs_fold2[1]}\") print(f\"Fold-2: XGB AUC = {aucs_fold2[2]}\") print(f\"Fold-2: Average Pred AUC = {aucs_fold2[3]}\")  # now we repeat the same for the other fold # this is not the ideal way, if you ever have to repeat code,  # create a function! # fit models on fold 2 and make predictions on fold 1 logreg = linear_model.LogisticRegression() rf = ensemble.RandomForestClassifier() xgbc = xgb.XGBClassifier()  logreg.fit(xfold2, yfold2) rf.fit(xfold2, yfold2) xgbc.fit(xfold2, yfold2)  pred_logreg = logreg.predict_proba(xfold1)[:, 1] pred_rf = rf.predict_proba(xfold1)[:, 1] pred_xgbc = xgbc.predict_proba(xfold1)[:, 1] avg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3  fold1_preds = np.column_stack((     pred_logreg,     pred_rf,     pred_xgbc,     avg_pred ))  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3add8ab-f442-45d1-bcd8-e950480e4b7e', embedding=None, metadata={'page_label': '279', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 279 aucs_fold1 = [] for i in range(fold1_preds.shape[1]):     auc = metrics.roc_auc_score(yfold1, fold1_preds[:, i])     aucs_fold1.append(auc)  print(f\"Fold-1: LR AUC = {aucs_fold1[0]}\") print(f\"Fold-1: RF AUC = {aucs_fold1[1]}\") print(f\"Fold-1: XGB AUC = {aucs_fold1[2]}\") print(f\"Fold-1: Average prediction AUC = {aucs_fold1[3]}\")  # find optimal weights using the optimizer opt = OptimizeAUC() # dont forget to remove the average column opt.fit(fold1_preds[:, :-1], yfold1) opt_preds_fold2 = opt.predict(fold2_preds[:, :-1]) auc = metrics.roc_auc_score(yfold2, opt_preds_fold2) print(f\"Optimized AUC, Fold 2 = {auc}\") print(f\"Coefficients = {opt.coef_}\")  opt = OptimizeAUC() opt.fit(fold2_preds[:, :-1], yfold2) opt_preds_fold1 = opt.predict(fold1_preds[:, :-1]) auc = metrics.roc_auc_score(yfold1, opt_preds_fold1) print(f\"Optimized AUC, Fold 1 = {auc}\") print(f\"Coefficients = {opt.coef_}\") ═════════════════════════════════════════════════════════════════════════  Let’s look at the output.  ═════════════════════════════════════════════════════════════════════════ ❯ python auc_opt.py Fold-2: LR AUC = 0.9145446769443348 Fold-2: RF AUC = 0.9269918948683287 Fold-2: XGB AUC = 0.9302436595508696 Fold-2: Average Pred AUC = 0.927701495890154  Fold-1: LR AUC = 0.9050872233256017 Fold-1: RF AUC = 0.9179382818311258 Fold-1: XGB AUC = 0.9195837242005629 Fold-1: Average prediction AUC = 0.9189669233123695  Optimization terminated successfully.          Current function value: -0.920643          Iterations: 50          Function evaluations: 109 Optimized AUC, Fold 2 = 0.9305386199756128 Coefficients = [-0.00188194  0.19328336  0.35891836] ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68f2dc9b-3ca3-4ab6-9bf5-75d53a52ceef', embedding=None, metadata={'page_label': '280', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 280 Optimization terminated successfully.          Current function value: -0.931232          Iterations: 56          Function evaluations: 113 Optimized AUC, Fold 1 = 0.9192523637234037 Coefficients = [-0.15655124  0.22393151  0.58711366] ═════════════════════════════════════════════════════════════════════════  We see that average is better but using the optimizer to find the threshold is even better! Sometimes, the average is the best choice. As you can see, the coefficients do not add up to 1.0, but that’s okay as we are dealing with AUC and AUC cares only about ranks.  Even random forest is an ensemble model. Random forest is just a combination of many simple decision trees. Random forest comes in a category of ensemble models which is popularly known as bagging. In bagging, we create small subsets of data and train multiple simple models. The final result is obtained by a combination of predictions, such as average, of all such small models.  And the xgboost model that we used is also an ensemble model. All gradient boosting models are ensemble models and come under the umbrella name: boosting. Boosting models work similar to bagging models, except for the fact that consecutive models in boosting are trained on error residuals and tend to minimize the errors of preceding models. This way, boosting models can learn the data perfectly and are thus susceptible to overfitting.  What we saw in the code snippets till now considers only one column. This is not always the case, and there will be many times when you have to deal with multiple columns for predictions. For example, you might have a problem where you are predicting one class out of multiple classes, i.e., multi-class classification problem. For a multi-class classification problem, you can easily choose the voting approach. But voting might not always be the best approach. If you want to combine the probabilities, you will have a two-dimensional array instead of a vector as we had previously when we were optimizing for AUC. With multiple classes, you can try optimizing for log-loss instead (or some other business-relevant metric). To combine, you can use a list of numpy arrays instead of a numpy array in the fit function (X) and subsequently, you would also need to change the optimizer and the predict function. I’m going to leave it as an exercise for you.  And now we can move to the next interesting topic which is quite popular and is known as stacking. Figure 2 shows how you can stack models. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0021d1db-5022-40ed-be83-a1d5a620da00', embedding=None, metadata={'page_label': '281', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 281  \\n Figure 2: Stacking  Stacking is not rocket science. It’s straightforward. If you have correct cross-validation and keep the folds same throughout the journey of your modelling task, nothing should overfit.  Let me describe the idea to you in simple points.  - Divide the training data into folds. - Train a bunch of models: M1, M2…..Mn. - Create full training predictions (using out of fold training) and test predictions using all these models. - Till here it is Level – 1 (L1). - Use the fold predictions from these models as features to another model. This is now a Level – 2 (L2) model. - Use the same folds as before to train this L2 model. - Now create OOF (out of fold) predictions on the training set and the test set. - Now you have L2 predictions for training data and also the final test set predictions. \\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0fb3e12e-5da0-4c6b-b2a4-499d7dc67fd3', embedding=None, metadata={'page_label': '282', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 282  You can keep repeating the L1 part and can create as many levels as you want.  Sometimes, you will also come across a term called blending. If you do, don’t worry about it much. It is nothing but stacking with a holdout set instead of multiple folds.  It must be noted that what I have described in this chapter can be applied to any kind of problem: classification, regression, multi-label classification, etc.     ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5dae7649-a8c0-4718-a6e4-38597aabcd21', embedding=None, metadata={'page_label': '283', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 283 Approaching reproducible code & model serving  We have now reached a stage where we should be able to distribute our models/training code to others so that they can use it. You can distribute or share the code with others in a floppy disk, but that’s not ideal. Is it? May be many years ago, it was ideal but not anymore. The preferred way of sharing code and collaborating with others is by using a source code management system. Git is one of the most popular source code management systems. So, let’s say you have learned git and formatted the code properly, have written proper documentation and have open-sourced your project. Is that enough? No. It’s not. It’s because you wrote code on your computer and that might not work on someone else’s computer because of many different reasons. So, it would be nice if when you distribute the code, you could replicate your computer and others can too when they install your software or run your code. To do this, the most popular way these days is to use Docker Containers. To use docker containers, you need to install docker.  Let’s install docker using the following commands.   ═════════════════════════════════════════════════════════════════════════ $ sudo apt install docker.io $ sudo systemctl start docker $ sudo systemctl enable docker  $ sudo groupadd docker $ sudo usermod -aG docker $USER ═════════════════════════════════════════════════════════════════════════  These commands work in Ubuntu 18.04. The best thing about docker is that it can be installed on any machine: Linux, Windows, OSX. So, it doesn’t matter which machine you have if you work inside the docker container all the time!  Docker containers can be considered as small virtual machines. You can create a container for your code, and then everyone will be able to use it and access it. Let’s see how we can create containers that can be used for training a model. We will use the BERT model that we trained in the natural language processing chapter and try to containerize the training code.  First and foremost, you need a file with requirements for your python project. Requirements are contained in a file called requirements.txt. The filename is the ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6123504e-cd23-4a3c-be42-f87018a27ad4', embedding=None, metadata={'page_label': '284', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 284 standard. The file consists of all the python libraries that you are using in your project. That is the python libraries that can be downloaded via PyPI (pip). For training our BERT model to detect positive/negative sentiment, we use torch, transformers, tqdm, scikit-learn, pandas and numpy. Let’s write them in requirements.txt. You can just write the names, or you can also include the version. It’s always the best to include version, and that’s what you should do. When you include version, it makes sure that others have the same version as yours and not the latest version as the latest version might change something and if that’s the case, model won’t be trained the same way as was done by you.   The following snippet shows requirements.txt.  ═════════════════════════════════════════════════════════════════════════ # requirements.txt pandas==1.0.4 scikit-learn==0.22.1 torch==1.5.0 transformers==2.11.0 ═════════════════════════════════════════════════════════════════════════  Now, we will create a docker file called Dockerfile. No extension. There are several elements to Dockerfile. Let’s take a look.  ═════════════════════════════════════════════════════════════════════════ # Dockerfile # First of all, we include where we are getting the image # from. Image can be thought of as an operating system. # You can do \"FROM ubuntu:18.04\" # this will start from a clean ubuntu 18.04 image. # All images are downloaded from dockerhub # Here are we grabbing image from nvidia\\'s repo # they created a docker image using ubuntu 18.04 # and installed cuda 10.1 and cudnn7 in it. Thus, we don\\'t have to  # install it. Makes our life easy. FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04  # this is the same apt-get command that you are used to # except the fact that, we have -y argument. Its because # when we build this container, we cannot press Y when asked for RUN apt-get update && apt-get install -y \\\\     git \\\\     curl \\\\     ca-certificates \\\\     python3 \\\\ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa49e9a9-5cbd-4568-acd1-86c9189d7773', embedding=None, metadata={'page_label': '285', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 285     python3-pip \\\\     sudo \\\\     && rm -rf /var/lib/apt/lists/*  # We add a new user called \"abhishek\" # this can be anything. Anything you want it # to be. Usually, we don\\'t use our own name, # you can use \"user\" or \"ubuntu\" RUN useradd -m abhishek  # make our user own its own home directory RUN chown -R abhishek:abhishek /home/abhishek/  # copy all files from this direrctory to a  # directory called app inside the home of abhishek # and abhishek owns it. COPY --chown=abhishek *.* /home/abhishek/app/  # change to user abhishek USER abhishek RUN mkdir /home/abhishek/data/  # Now we install all the requirements # after moving to the app directory # PLEASE NOTE that ubuntu 18.04 image # has python 3.6.9 and not python 3.7.6 # you can also install conda python here and use that # however, to simplify it, I will be using python 3.6.9 # inside the docker container!!!! RUN cd /home/abhishek/app/ && pip3 install -r requirements.txt # install mkl. its needed for transformers RUN pip3 install mkl  # when we log into the docker container, # we will go inside this directory automatically WORKDIR /home/abhishek/app ═════════════════════════════════════════════════════════════════════════  Once we have created the docker file, we need to build it. Building the docker container is a very simple command.  ═════════════════════════════════════════════════════════════════════════  docker build -f Dockerfile -t bert:train .  ═════════════════════════════════════════════════════════════════════════ ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c259229-2716-4373-925c-f1cd1a404bce', embedding=None, metadata={'page_label': '286', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 286  This command builds a container from the provided Dockerfile. The name of the docker container is bert:train. This produces the following output:  ═════════════════════════════════════════════════════════════════════════ ❯ docker build -f Dockerfile -t bert:train . Sending build context to Docker daemon  19.97kB Step 1/7 : FROM nvidia/cuda:10.1-cudnn7-ubuntu18.04  ---> 3b55548ae91f Step 2/7 : RUN apt-get update && apt-get install -y   git  curl     ca-certificates     python3 python3-pip     sudo     && rm -rf /var/lib/apt/lists/* . . . . Removing intermediate container 8f6975dd08ba  ---> d1802ac9f1b4 Step 7/7 : WORKDIR /home/abhishek/app  ---> Running in 257ff09502ed Removing intermediate container 257ff09502ed  ---> e5f6eb4cddd7 Successfully built e5f6eb4cddd7 Successfully tagged bert:train ═════════════════════════════════════════════════════════════════════════  Please note that I have removed many lines from the output. Now, you can log into the container using the following command.  ═════════════════════════════════════════════════════════════════════════  $ docker run -ti bert:train /bin/bash  ═════════════════════════════════════════════════════════════════════════  You need to remember that whatever you do in this shell will be lost once you exit the shell. And you can run the training inside the docker container using:  ═════════════════════════════════════════════════════════════════════════  $ docker run -ti bert:train python3 train.py  ═════════════════════════════════════════════════════════════════════════  Which gives the following output: ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de45da4b-dbba-49d0-a1d3-93815c2a0b97', embedding=None, metadata={'page_label': '287', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 287  ═════════════════════════════════════════════════════════════════════════ Traceback (most recent call last):   File \"train.py\", line 2, in <module>     import config   File \"/home/abhishek/app/config.py\", line 28, in <module>     do_lower_case=True   File \"/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\", line 393, in from_pretrained     return cls._from_pretrained(*inputs, **kwargs)   File \"/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\", line 496, in _from_pretrained     list(cls.vocab_files_names.values()), OSError: Model name \\'../input/bert_base_uncased/\\' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed \\'../input/bert_base_uncased/\\' was a path, a model identifier, or url to a directory containing vocabulary files named [\\'vocab.txt\\'] but couldn\\'t find such vocabulary files at this path or url. ═════════════════════════════════════════════════════════════════════════  Oops, it’s an error!   And why would I print an error in a book?   Because it’s very important to understand this error. This error says that the code was unable to find the directory “../input/bert_base_cased”. Why does this happen?  We were able to train without docker, and we can see that the directory and all the files exist. It happens because docker is like a virtual machine! It has its own filesystem and the files from your local machine are not shared to the docker container. If you want to use a path from your local machine and want to modify it too, you would need to mount it to the docker container when running it. When we look at this folder path, we know that it is one level up in a folder called input. Let’s change the config.py file a bit!  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='321e41d8-0110-414b-9c84-5a67a74f3f7e', embedding=None, metadata={'page_label': '288', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 288 ═════════════════════════════════════════════════════════════════════════ # config.py import os import transformers  # fetch home directory # in our docker container, it is # /home/abhishek HOME_DIR = os.path.expanduser(\"~\")  # this is the maximum number of tokens in the sentence MAX_LEN = 512  # batch sizes is low because model is huge! TRAIN_BATCH_SIZE = 8 VALID_BATCH_SIZE = 4  # let\\'s train for a maximum of 10 epochs EPOCHS = 10  # define path to BERT model files # Now we assume that all the data is stored inside # /home/abhishek/data BERT_PATH = os.path.join(HOME_DIR, \"data\", \"bert_base_uncased\")  # this is where you want to save the model MODEL_PATH = os.path.join(HOME_DIR, \"data\", \"model.bin\")  # training file TRAINING_FILE = os.path.join(HOME_DIR, \"data\", \"imdb.csv\")  TOKENIZER = transformers.BertTokenizer.from_pretrained(     BERT_PATH,      do_lower_case=True ) ═════════════════════════════════════════════════════════════════════════  Now, the code assumes everything to be inside a folder called data inside the home directory.   Note that any change in the python scripts, means that the docker container needs to be rebuilt! So, we rebuild the container and rerun the docker command but this time with a twist. However, this won’t work either if we do not have the NVIDIA docker runtimes. Don’t worry. It’s just a docker container again, and you need to ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51047830-9f17-4c86-92d1-5fb3ff69774e', embedding=None, metadata={'page_label': '289', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 289 do it only once. To install the NVIDIA docker runtime, you can run the following commands in Ubuntu 18.04.  ═════════════════════════════════════════════════════════════════════════ # taken from: https://github.com/NVIDIA/nvidia-docker/ # Add the package repositories distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list  sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker ═════════════════════════════════════════════════════════════════════════  Now we can build our container again and start the training process:  ═════════════════════════════════════════════════════════════════════════ $ docker run --gpus 1 -v /home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/ -ti bert:train python3 train.py ═════════════════════════════════════════════════════════════════════════  Where –gpus 1 says that we use 1 GPU inside the docker container and -v is mounting a volume. So, we are mounting our local directory, /home/abhishek/workspace/approaching_almost/input/ to /home/abhishek/data/ in the docker container. This step is going to take a while, but when it’s done, you will have model.bin inside the local folder.  So, with some very simple changes, you have now “dockerized” your training code. You can now take this code and train on (almost) any system you want.  The next part is “serving” this model that we have trained to the end-user. Suppose, you want to extract sentiment from a stream of incoming tweets. To do this kind of task, you must create an API that can be used to input the sentence and in turns returns an output with sentiment probabilities. The most common way of building an API using Python is with Flask, which is a micro web service framework.  ═════════════════════════════════════════════════════════════════════════ # api.py import config import flask ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='391eb50d-6cc3-4233-91dc-8348fb3a350d', embedding=None, metadata={'page_label': '290', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 290 import time import torch import torch.nn as nn from flask import Flask from flask import request from model import BERTBaseUncased  app = Flask(__name__)  # init model to None MODEL = None  # choose device # please note that we are using cuda device # you can also use cpu! DEVICE = \"cuda\"   def sentence_prediction(sentence):     \"\"\"     A prediction function that takes an input sentence     and returns the probability for it being associated     to a positive sentiment     \"\"\"     # fetch the tokenizer and max len of tokens from config.py     tokenizer = config.TOKENIZER     max_len = config.MAX_LEN      # the processing is same as it was done for training     review = str(sentence)     review = \" \".join(review.split())      # encode the sentence into ids,     # truncate to max length &     # add CLS and SEP tokens     inputs = tokenizer.encode_plus(         review,          None,          add_special_tokens=True,          max_length=max_len     )      # fetch input ids, mask & token type ids     ids = inputs[\"input_ids\"]     mask = inputs[\"attention_mask\"]     token_type_ids = inputs[\"token_type_ids\"]  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39859f16-86dc-4147-a45a-7361f32b1f30', embedding=None, metadata={'page_label': '291', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 291     # add padding if needed     padding_length = max_len - len(ids)     ids = ids + ([0] * padding_length)     mask = mask + ([0] * padding_length)     token_type_ids = token_type_ids + ([0] * padding_length)      # convert all the inputs to torch tensors     # we use unsqueeze(0) since we have only one sample     # this makes the batch size 1     ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)     mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)     token_type_ids = torch.tensor(token_type_ids,                                       dtype=torch.long).unsqueeze(0)      # send everything to device     ids = ids.to(DEVICE, dtype=torch.long)     token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)     mask = mask.to(DEVICE, dtype=torch.long)      # use the model to make predictions     outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)     # take sigmoid of prediction and return the output     outputs = torch.sigmoid(outputs).cpu().detach().numpy()     return outputs[0][0]   @app.route(\"/predict\", methods=[\"GET\"]) def predict():     # this is our endpoint!     # this endpoint can be accessed by http://HOST:PORT/predict     # the endpoint needs sa sentence and can only use GET     # POST request is not allowed     sentence = request.args.get(\"sentence\")      # keep track of time     start_time = time.time()      # make prediction     positive_prediction = sentence_prediction(sentence)      # negative = 1 - positive     negative_prediction = 1 - positive_prediction      # create return dictionary     response = {}     response[\"response\"] = {         \"positive\": str(positive_prediction), ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1bf7d05a-b009-4660-a262-716c1ae27588', embedding=None, metadata={'page_label': '292', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 292         \"negative\": str(negative_prediction),         \"sentence\": str(sentence),         \"time_taken\": str(time.time() - start_time),     }     # we use jsonify from flask for dictionaries     return flask.jsonify(response)   if __name__ == \"__main__\":     # init the model     MODEL = BERTBaseUncased()      # load the dictionary     MODEL.load_state_dict(torch.load(         config.MODEL_PATH, map_location=torch.device(DEVICE)         ))          # send model to device     MODEL.to(DEVICE)          # put model in eval mode     MODEL.eval()      # start the application     # 0.0.0.0 means that this endpoint can be      # accessed from all computers in a network     app.run(host=\"0.0.0.0\") ═════════════════════════════════════════════════════════════════════════  And you start the API by running the command “python api.py”. The API will start on localhost on port 5000.  A sample cURL request and its response is shown as follows.  ═════════════════════════════════════════════════════════════════════════ ❯ curl $\\'http://192.168.86.48:5000/predict?sentence=this%20is%20the%20best%20book%20ever\\'  {\"response\":{\"negative\":\"0.0032927393913269043\",\"positive\":\"0.99670726\",\"sentence\":\"this is the best book ever\",\"time_taken\":\"0.029126882553100586\"}} ═════════════════════════════════════════════════════════════════════════  ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3479628d-4d13-4aa8-ae7c-ff35c454eccf', embedding=None, metadata={'page_label': '293', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 293 As you can see that we got a high probability for positive sentiment for the provided input sentence. You can also access the results by visiting http://127.0.0.1:5000/predict?sentence=this%20book%20is%20too%20complicated%20for%20me in your favourite browser. This will return a JSON again.  ═════════════════════════════════════════════════════════════════════════ {     response: {         negative: \"0.8646619468927383\",         positive: \"0.13533805\",         sentence: \"this book is too complicated for me\",         time_taken: \"0.03852701187133789\"         } } ═════════════════════════════════════════════════════════════════════════  Now, we have created a simple API that we can use to serve a small number of users. Why small? Because this API will serve only one request at a time. Let’s use CPU and make it work for many parallel requests using gunicorn which is a python WSGI HTTP server for UNIX. Gunicorn can create multiple processes for the API, and thus, we can serve many customers at once. You can install gunicorn by using “pip install gunicorn”.  To convert the code compatible with gunicorn, we need to remove init main and move everything out of it to the global scope. Also, we are now using CPU instead of the GPU. See the modified code as follows.  ═════════════════════════════════════════════════════════════════════════ # api.py import config import flask import time import torch import torch.nn as nn from flask import Flask from flask import request from model import BERTBaseUncased  app = Flask(__name__)  # now we use cpu! DEVICE = \"cpu\"  # init the model ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbf55330-e8f8-4dda-aa62-68703931698d', embedding=None, metadata={'page_label': '294', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 294 MODEL = BERTBaseUncased()  # load the dictionary MODEL.load_state_dict(torch.load(     config.MODEL_PATH, map_location=torch.device(DEVICE)     ))  # send model to device MODEL.to(DEVICE)  # put model in eval mode MODEL.eval()   def sentence_prediction(sentence):     \"\"\"     A prediction function that takes an input sentence     and returns the probability for it being associated     to a positive sentiment     \"\"\"     .     .     .     return outputs[0][0]   @app.route(\"/predict\", methods=[\"GET\"]) def predict():     # this is our endpoint!     .     .     .     return flask.jsonify(response) ═════════════════════════════════════════════════════════════════════════  And we run this API using the following command.  $ gunicorn api:app --bind 0.0.0.0:5000 --workers 4  This means we are running our flask app with 4 workers on a provided IP address and port. Since there are 4 workers, we are now serving 4 simultaneous requests. Please note that now our endpoint uses CPU and thus, it does not need a GPU machine and can run on any standard server/VM. Still, we have one problem, we have done everything in our local machine, so we must dockerize it. Take a look at the following uncommented Dockerfile which can be used to deploy this API. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44b088d0-1e45-4625-8499-b48e1fc1cbdc', embedding=None, metadata={'page_label': '295', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 295 Notice the difference between the old Dockerfile for training and this one. There are not many differences.  ═════════════════════════════════════════════════════════════════════════ # CPU Dockerfile FROM ubuntu:18.04  RUN apt-get update && apt-get install -y \\\\     git \\\\     curl \\\\     ca-certificates \\\\     python3 \\\\     python3-pip \\\\     sudo \\\\     && rm -rf /var/lib/apt/lists/*  RUN useradd -m abhishek  RUN chown -R abhishek:abhishek /home/abhishek/  COPY --chown=abhishek *.* /home/abhishek/app/  USER abhishek RUN mkdir /home/abhishek/data/  RUN cd /home/abhishek/app/ && pip3 install -r requirements.txt RUN pip3 install mkl  WORKDIR /home/abhishek/app ═════════════════════════════════════════════════════════════════════════  Let’s build a new docker container.  $ docker build -f Dockerfile -t bert:api .  When the docker container is built, we can now run the API directly by using the following command.  $ docker run -p 5000:5000 -v /home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/ -ti bert:api /home/abhishek/.local/bin/gunicorn api:app --bind 0.0.0.0:5000 --workers 4  Please note that we expose port 5000 from the container to 5000 outside the container. This can also be done in a nice way if you use docker-compose. Docker ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a68421a0-8dd7-45ad-85bf-f897f3f39801', embedding=None, metadata={'page_label': '296', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 296 compose is a tool that can allow you to run different services from different or the same containers at the same time. You can install docker-compose using “pip install docker-compose” and then run “docker-compose up” after building the container. To use docker-compose, you need a docker-compose.yml file.  ═════════════════════════════════════════════════════════════════════════ # docker-compose.yml # specify a version of the compose version: '3.7'  # you can add multiple services services:   # specify service name. we call our service: api    api:     # specify image name     image: bert:api     # the command that you would like to run inside the container     command: /home/abhishek/.local/bin/gunicorn api:app --bind 0.0.0.0:5000 --workers 4     # mount the volume     volumes:       - /home/abhishek/workspace/approaching_almost/input/:/home/abhishek/data/     # this ensures that our ports from container will be      # exposed as it is     network_mode: host ═════════════════════════════════════════════════════════════════════════  Now you can rerun the API just by using the command mentioned above, and it will work the same way as before. Congratulations! Now you have managed to dockerized the prediction API too, and it is ready for deployment anywhere you want. In this chapter, we learned docker, building APIs using flask, serving API using gunicorn and docker and docker-compose. There is a lot more to docker than we have seen here, but this should give you a start. Rest can be learned as you progress. We have also skipped on many tools like kubernetes, bean-stalk, sagemaker, heroku and many others that people use these days for deploying models in production. “What am I going to write? Click on modify docker container in figure X”? It’s not feasible and advisable to describe these in a book, so I will be using a different medium complimenting this part of the book. Remember that once you have dockerized your application, deploying using any of these technologies/platforms is a piece of cake. Always remember to make your code and model usable and well-documented for others so that anyone can use what you have developed without asking you several times. This will save you time, and it will \", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26e7a3c7-d6eb-4852-a36b-4993de0ab661', embedding=None, metadata={'page_label': '297', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 297 also save their time. Good, open-source, re-usable code also looks good in your portfolio. J     ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ae1d8990-2beb-4133-9c6d-fca063ba85ae', embedding=None, metadata={'page_label': '298', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 298 Notes   ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4569bccd-6681-401f-a9d4-f8cbd3912e36', embedding=None, metadata={'page_label': '299', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 299 Notes    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf6cecf8-86c8-4cbf-a83b-44bbdd49e432', embedding=None, metadata={'page_label': '300', 'file_name': 'AbhishekThakur.pdf', 'file_path': '/content/pdf/AbhishekThakur.pdf', 'file_type': 'application/pdf', 'file_size': 7471358, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Approaching (Almost) Any Machine Learning Problem \\nGiveaway Version 300 Notes    ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='50e049b8-f5e2-4a5d-99dd-7f5e66dd395f', embedding=None, metadata={'page_label': '1', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MATHEMATICS  FOR \\nMACHINE LEAR NING\\nMarc Peter Deisenroth\\nA. Aldo Faisal\\nCheng Soon On g\\nMATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\\nThe fundamental mathematical tools needed to understand machine \\nlearning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ  ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ  rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site.\\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine \\nLearning at the Department of Computing, Împerial College London.\\nA. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College \\nLondon, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing.\\nCHENG SOON ONG  is Principal Research Scientist at the Machine Learning \\nResearch Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University.\\nCover image courtesy of Daniel Bosma / Moment / Getty Images\\nCover design by Holly Johnson\\nDeisenrith et al. 9781108455145 Cover. C M Y K', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e3b12ab-4e17-4fad-bd52-68dc00d9bd4a', embedding=None, metadata={'page_label': '2', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1dd1b1ef-420b-49cf-ad17-2d87f510a5bb', embedding=None, metadata={'page_label': 'i', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents\\nForeword 1\\nPart I Mathematical Foundations 9\\n1 Introduction and Motivation 11\\n1.1 Finding Words for Intuitions 12\\n1.2 Two Ways to Read This Book 13\\n1.3 Exercises and Feedback 16\\n2 Linear Algebra 17\\n2.1 Systems of Linear Equations 19\\n2.2 Matrices 22\\n2.3 Solving Systems of Linear Equations 27\\n2.4 Vector Spaces 35\\n2.5 Linear Independence 40\\n2.6 Basis and Rank 44\\n2.7 Linear Mappings 48\\n2.8 Afﬁne Spaces 61\\n2.9 Further Reading 63\\nExercises 64\\n3 Analytic Geometry 70\\n3.1 Norms 71\\n3.2 Inner Products 72\\n3.3 Lengths and Distances 75\\n3.4 Angles and Orthogonality 76\\n3.5 Orthonormal Basis 78\\n3.6 Orthogonal Complement 79\\n3.7 Inner Product of Functions 80\\n3.8 Orthogonal Projections 81\\n3.9 Rotations 91\\n3.10 Further Reading 94\\nExercises 96\\n4 Matrix Decompositions 98\\n4.1 Determinant and Trace 99\\ni\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d61141d5-ae93-4660-b92c-4e37e78d2c36', embedding=None, metadata={'page_label': 'ii', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ii Contents\\n4.2 Eigenvalues and Eigenvectors 105\\n4.3 Cholesky Decomposition 114\\n4.4 Eigendecomposition and Diagonalization 115\\n4.5 Singular Value Decomposition 119\\n4.6 Matrix Approximation 129\\n4.7 Matrix Phylogeny 134\\n4.8 Further Reading 135\\nExercises 137\\n5 Vector Calculus 139\\n5.1 Differentiation of Univariate Functions 141\\n5.2 Partial Differentiation and Gradients 146\\n5.3 Gradients of Vector-Valued Functions 149\\n5.4 Gradients of Matrices 155\\n5.5 Useful Identities for Computing Gradients 158\\n5.6 Backpropagation and Automatic Differentiation 159\\n5.7 Higher-Order Derivatives 164\\n5.8 Linearization and Multivariate Taylor Series 165\\n5.9 Further Reading 170\\nExercises 170\\n6 Probability and Distributions 172\\n6.1 Construction of a Probability Space 172\\n6.2 Discrete and Continuous Probabilities 178\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\n6.4 Summary Statistics and Independence 186\\n6.5 Gaussian Distribution 197\\n6.6 Conjugacy and the Exponential Family 205\\n6.7 Change of Variables/Inverse Transform 214\\n6.8 Further Reading 221\\nExercises 222\\n7 Continuous Optimization 225\\n7.1 Optimization Using Gradient Descent 227\\n7.2 Constrained Optimization and Lagrange Multipliers 233\\n7.3 Convex Optimization 236\\n7.4 Further Reading 246\\nExercises 247\\nPart II Central Machine Learning Problems 249\\n8 When Models Meet Data 251\\n8.1 Data, Models, and Learning 251\\n8.2 Empirical Risk Minimization 258\\n8.3 Parameter Estimation 265\\n8.4 Probabilistic Modeling and Inference 272\\n8.5 Directed Graphical Models 278\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4fcf1e73-a8a6-4dc4-b2f9-db63ef5b9af6', embedding=None, metadata={'page_label': 'iii', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents iii\\n8.6 Model Selection 283\\n9 Linear Regression 289\\n9.1 Problem Formulation 291\\n9.2 Parameter Estimation 292\\n9.3 Bayesian Linear Regression 303\\n9.4 Maximum Likelihood as Orthogonal Projection 313\\n9.5 Further Reading 315\\n10 Dimensionality Reduction with Principal Component Analysis 317\\n10.1 Problem Setting 318\\n10.2 Maximum Variance Perspective 320\\n10.3 Projection Perspective 325\\n10.4 Eigenvector Computation and Low-Rank Approximations 333\\n10.5 PCA in High Dimensions 335\\n10.6 Key Steps of PCA in Practice 336\\n10.7 Latent Variable Perspective 339\\n10.8 Further Reading 343\\n11 Density Estimation with Gaussian Mixture Models 348\\n11.1 Gaussian Mixture Model 349\\n11.2 Parameter Learning via Maximum Likelihood 350\\n11.3 EM Algorithm 360\\n11.4 Latent-Variable Perspective 363\\n11.5 Further Reading 368\\n12 Classiﬁcation with Support Vector Machines 370\\n12.1 Separating Hyperplanes 372\\n12.2 Primal Support Vector Machine 374\\n12.3 Dual Support Vector Machine 383\\n12.4 Kernels 388\\n12.5 Numerical Solution 390\\n12.6 Further Reading 392\\nReferences 395\\nIndex 407\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55bb0d91-3da4-487d-a60f-1ee6cce920da', embedding=None, metadata={'page_label': 'iv', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a92a8484-1458-4011-acc4-684add44e079', embedding=None, metadata={'page_label': '1', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword\\nMachine learning is the latest in a long line of attempts to distill human\\nknowledge and reasoning into a form that is suitable for constructing ma-\\nchines and engineering automated systems. As machine learning becomes\\nmore ubiquitous and its software packages become easier to use, it is nat-\\nural and desirable that the low-level technical details are abstracted away\\nand hidden from the practitioner. However, this brings with it the danger\\nthat a practitioner becomes unaware of the design decisions and, hence,\\nthe limits of machine learning algorithms.\\nThe enthusiastic practitioner who is interested to learn more about the\\nmagic behind successful machine learning algorithms currently faces a\\ndaunting set of pre-requisite knowledge:\\nProgramming languages and data analysis tools\\nLarge-scale computation and the associated frameworks\\nMathematics and statistics and how machine learning builds on it\\nAt universities, introductory courses on machine learning tend to spend\\nearly parts of the course covering some of these pre-requisites. For histori-\\ncal reasons, courses in machine learning tend to be taught in the computer\\nscience department, where students are often trained in the ﬁrst two areas\\nof knowledge, but not so much in mathematics and statistics.\\nCurrent machine learning textbooks primarily focus on machine learn-\\ning algorithms and methodologies and assume that the reader is com-\\npetent in mathematics and statistics. Therefore, these books only spend\\none or two chapters of background mathematics, either at the beginning\\nof the book or as appendices. We have found many people who want to\\ndelve into the foundations of basic machine learning methods who strug-\\ngle with the mathematical knowledge required to read a machine learning\\ntextbook. Having taught undergraduate and graduate courses at universi-\\nties, we ﬁnd that the gap between high school mathematics and the math-\\nematics level required to read a standard machine learning textbook is too\\nbig for many people.\\nThis book brings the mathematical foundations of basic machine learn-\\ning concepts to the fore and collects the information in a single place so\\nthat this skills gap is narrowed or even closed.\\n1\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b500b1b-5c6e-45cb-9a52-eee09b2ba60a', embedding=None, metadata={'page_label': '2', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 Foreword\\nWhy Another Book on Machine Learning?\\nMachine learning builds upon the language of mathematics to express\\nconcepts that seem intuitively obvious but that are surprisingly difﬁcult\\nto formalize. Once formalized properly, we can gain insights into the task\\nwe want to solve. One common complaint of students of mathematics\\naround the globe is that the topics covered seem to have little relevance\\nto practical problems. We believe that machine learning is an obvious and\\ndirect motivation for people to learn mathematics.\\nThis book is intended to be a guidebook to the vast mathematical lit-\\nerature that forms the foundations of modern machine learning. We mo- “Math is linked in\\nthe popular mind\\nwith phobia and\\nanxiety. You’d think\\nwe’re discussing\\nspiders.” (Strogatz,\\n2014, page 281)tivate the need for mathematical concepts by directly pointing out their\\nusefulness in the context of fundamental machine learning problems. In\\nthe interest of keeping the book short, many details and more advanced\\nconcepts have been left out. Equipped with the basic concepts presented\\nhere, and how they ﬁt into the larger context of machine learning, the\\nreader can ﬁnd numerous resources for further study, which we provide at\\nthe end of the respective chapters. For readers with a mathematical back-\\nground, this book provides a brief but precisely stated glimpse of machine\\nlearning. In contrast to other books that focus on methods and models\\nof machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-\\nber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers\\nand Girolami, 2016) or programmatic aspects of machine learning (M ¨uller\\nand Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),\\nwe provide only four representative examples of machine learning algo-\\nrithms. Instead, we focus on the mathematical concepts behind the models\\nthemselves. We hope that readers will be able to gain a deeper understand-\\ning of the basic questions in machine learning and connect practical ques-\\ntions arising from the use of machine learning with fundamental choices\\nin the mathematical model.\\nWe do not aim to write a classical machine learning book. Instead, our\\nintention is to provide the mathematical background, applied to four cen-\\ntral machine learning problems, to make it easier to read other machine\\nlearning textbooks.\\nWho Is the Target Audience?\\nAs applications of machine learning become widespread in society, we\\nbelieve that everybody should have some understanding of its underlying\\nprinciples. This book is written in an academic mathematical style, which\\nenables us to be precise about the concepts behind machine learning. We\\nencourage readers unfamiliar with this seemingly terse style to persevere\\nand to keep the goals of each topic in mind. We sprinkle comments and\\nremarks throughout the text, in the hope that it provides useful guidance\\nwith respect to the big picture.\\nThe book assumes the reader to have mathematical knowledge commonly\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a5c9533-b0aa-431a-a147-756fe8f3e9eb', embedding=None, metadata={'page_label': '3', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword 3\\ncovered in high school mathematics and physics. For example, the reader\\nshould have seen derivatives and integrals before, and geometric vectors\\nin two or three dimensions. Starting from there, we generalize these con-\\ncepts. Therefore, the target audience of the book includes undergraduate\\nuniversity students, evening learners and learners participating in online\\nmachine learning courses.\\nIn analogy to music, there are three types of interaction that people\\nhave with machine learning:\\nAstute Listener The democratization of machine learning by the pro-\\nvision of open-source software, online tutorials and cloud-based tools al-\\nlows users to not worry about the speciﬁcs of pipelines. Users can focus on\\nextracting insights from data using off-the-shelf tools. This enables non-\\ntech-savvy domain experts to beneﬁt from machine learning. This is sim-\\nilar to listening to music; the user is able to choose and discern between\\ndifferent types of machine learning, and beneﬁts from it. More experi-\\nenced users are like music critics, asking important questions about the\\napplication of machine learning in society such as ethics, fairness, and pri-\\nvacy of the individual. We hope that this book provides a foundation for\\nthinking about the certiﬁcation and risk management of machine learning\\nsystems, and allows them to use their domain expertise to build better\\nmachine learning systems.\\nExperienced Artist Skilled practitioners of machine learning can plug\\nand play different tools and libraries into an analysis pipeline. The stereo-\\ntypical practitioner would be a data scientist or engineer who understands\\nmachine learning interfaces and their use cases, and is able to perform\\nwonderful feats of prediction from data. This is similar to a virtuoso play-\\ning music, where highly skilled practitioners can bring existing instru-\\nments to life and bring enjoyment to their audience. Using the mathe-\\nmatics presented here as a primer, practitioners would be able to under-\\nstand the beneﬁts and limits of their favorite method, and to extend and\\ngeneralize existing machine learning algorithms. We hope that this book\\nprovides the impetus for more rigorous and principled development of\\nmachine learning methods.\\nFledgling Composer As machine learning is applied to new domains,\\ndevelopers of machine learning need to develop new methods and extend\\nexisting algorithms. They are often researchers who need to understand\\nthe mathematical basis of machine learning and uncover relationships be-\\ntween different tasks. This is similar to composers of music who, within\\nthe rules and structure of musical theory, create new and amazing pieces.\\nWe hope this book provides a high-level overview of other technical books\\nfor people who want to become composers of machine learning. There is\\na great need in society for new researchers who are able to propose and\\nexplore novel approaches for attacking the many challenges of learning\\nfrom data.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='712f4494-dcf3-4fb7-bb98-888e83fdcf78', embedding=None, metadata={'page_label': '4', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 Foreword\\nAcknowledgments\\nWe are grateful to many people who looked at early drafts of the book and\\nsuffered through painful expositions of concepts. We tried to implement\\ntheir ideas that we did not vehemently disagree with. We would like to\\nespecially acknowledge Christfried Webers for his careful reading of many\\nparts of the book, and his detailed suggestions on structure and presen-\\ntation. Many friends and colleagues have also been kind enough to pro-\\nvide their time and energy on different versions of each chapter. We have\\nbeen lucky to beneﬁt from the generosity of the online community, who\\nhave suggested improvements via github.com , which greatly improved\\nthe book.\\nThe following people have found bugs, proposed clariﬁcations and sug-\\ngested relevant literature, either via github.com or personal communica-\\ntion. Their names are sorted alphabetically.\\nAbdul-Ganiy Usman\\nAdam Gaier\\nAdele Jackson\\nAditya Menon\\nAlasdair Tran\\nAleksandar Krnjaic\\nAlexander Makrigiorgos\\nAlfredo Canziani\\nAli Shafti\\nAmr Khalifa\\nAndrew Tanggara\\nAngus Gruen\\nAntal A. Buss\\nAntoine Toisoul Le Cann\\nAreg Sarvazyan\\nArtem Artemev\\nArtyom Stepanov\\nBill Kromydas\\nBob Williamson\\nBoon Ping Lim\\nChao Qu\\nCheng Li\\nChris Sherlock\\nChristopher Gray\\nDaniel McNamara\\nDaniel Wood\\nDarren Siegel\\nDavid Johnston\\nDawei ChenEllen Broad\\nFengkuangtian Zhu\\nFiona Condon\\nGeorgios Theodorou\\nHe Xin\\nIrene Raissa Kameni\\nJakub Nabaglo\\nJames Hensman\\nJamie Liu\\nJean Kaddour\\nJean-Paul Ebejer\\nJerry Qiang\\nJitesh Sindhare\\nJohn Lloyd\\nJonas Ngnawe\\nJon Martin\\nJustin Hsi\\nKai Arulkumaran\\nKamil Dreczkowski\\nLily Wang\\nLionel Tondji Ngoupeyou\\nLydia Kn ¨uﬁng\\nMahmoud Aslan\\nMark Hartenstein\\nMark van der Wilk\\nMarkus Hegland\\nMartin Hewing\\nMatthew Alger\\nMatthew Lee\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='401bb09a-47a5-489d-949b-1a513b6e1e28', embedding=None, metadata={'page_label': '5', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword 5\\nMaximus McCann\\nMengyan Zhang\\nMichael Bennett\\nMichael Pedersen\\nMinjeong Shin\\nMohammad Malekzadeh\\nNaveen Kumar\\nNico Montali\\nOscar Armas\\nPatrick Henriksen\\nPatrick Wieschollek\\nPattarawat Chormai\\nPaul Kelly\\nPetros Christodoulou\\nPiotr Januszewski\\nPranav Subramani\\nQuyu Kong\\nRagib Zaman\\nRui Zhang\\nRyan-Rhys Grifﬁths\\nSalomon Kabongo\\nSamuel Ogunmola\\nSandeep Mavadia\\nSarvesh Nikumbh\\nSebastian Raschka\\nSenanayak Sesh Kumar Karri\\nSeung-Heon Baek\\nShahbaz ChaudharyShakir Mohamed\\nShawn Berry\\nSheikh Abdul Raheem Ali\\nSheng Xue\\nSridhar Thiagarajan\\nSyed Nouman Hasany\\nSzymon Brych\\nThomas B ¨uhler\\nTimur Sharapov\\nTom Melamed\\nVincent Adam\\nVincent Dutordoir\\nVu Minh\\nWasim Aftab\\nWen Zhi\\nWojciech Stokowiec\\nXiaonan Chong\\nXiaowei Zhang\\nYazhou Hao\\nYicheng Luo\\nYoung Lee\\nYu Lu\\nYun Cheng\\nYuxiao Huang\\nZac Cranko\\nZijian Cao\\nZoe Nolan\\nContributors through github, whose real names were not listed on their\\ngithub proﬁle, are:\\nSamDataMad\\nbumptiousmonkey\\nidoamihai\\ndeepakiiminsad\\nHorizonP\\ncs-maillist\\nkudo23empet\\nvictorBigand\\n17SKYE\\njessjing1995\\nWe are also very grateful to Parameswaran Raman and the many anony-\\nmous reviewers, organized by Cambridge University Press, who read one\\nor more chapters of earlier versions of the manuscript, and provided con-\\nstructive criticism that led to considerable improvements. A special men-\\ntion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\\nadvice about LATEX-related issues. Last but not least, we are very grateful\\nto our editor Lauren Cowles, who has been patiently guiding us through\\nthe gestation process of this book.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ce0daa2-a45a-442e-ad88-8799197b0ebb', embedding=None, metadata={'page_label': '6', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6 Foreword\\nTable of Symbols\\nSymbol Typical meaning\\na,b,c,α,β,γ Scalars are lowercase\\nx,y,z Vectors are bold lowercase\\nA,B,C Matrices are bold uppercase\\nx⊤,A⊤Transpose of a vector or matrix\\nA−1Inverse of a matrix\\n⟨x,y⟩ Inner product of xandy\\nx⊤y Dot product of xandy\\nB= (b1,b2,b3)(Ordered) tuple\\nB= [b1,b2,b3]Matrix of column vectors stacked horizontally\\nB={b1,b2,b3}Set of vectors (unordered)\\nZ,N Integers and natural numbers, respectively\\nR,C Real and complex numbers, respectively\\nRnn-dimensional vector space of real numbers\\n∀x Universal quantiﬁer: for all x\\n∃x Existential quantiﬁer: there exists x\\na:=b a is deﬁned as b\\na=:b b is deﬁned as a\\na∝b a is proportional to b, i.e.,a=constant·b\\ng◦f Function composition: “ gafterf”\\n⇐⇒ If and only if\\n=⇒ Implies\\nA,C Sets\\na∈A ais an element of set A\\n∅ Empty set\\nA\\\\B A withoutB: the set of elements in Abut not inB\\nD Number of dimensions; indexed by d= 1,...,D\\nN Number of data points; indexed by n= 1,...,N\\nIm Identity matrix of size m×m\\n0m,n Matrix of zeros of size m×n\\n1m,n Matrix of ones of size m×n\\nei Standard/canonical vector (where iis the component that is 1)\\ndim Dimensionality of vector space\\nrk(A) Rank of matrix A\\nIm(Φ) Image of linear mapping Φ\\nker(Φ) Kernel (null space) of a linear mapping Φ\\nspan[b1] Span (generating set) of b1\\ntr(A) Trace ofA\\ndet(A) Determinant of A\\n|·| Absolute value or determinant (depending on context)\\n∥·∥ Norm; Euclidean, unless speciﬁed\\nλ Eigenvalue or Lagrange multiplier\\nEλ Eigenspace corresponding to eigenvalue λ\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='383d14c8-7c96-4c7c-8b2c-1955811eecb9', embedding=None, metadata={'page_label': '7', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Foreword 7\\nSymbol Typical meaning\\nx⊥y Vectorsxandyare orthogonal\\nV Vector space\\nV⊥Orthogonal complement of vector space V∑N\\nn=1xn Sum of thexn:x1+...+xN∏N\\nn=1xn Product of the xn:x1·...·xN\\nθ Parameter vector\\n∂f\\n∂xPartial derivative of fwith respect to x\\ndf\\ndxTotal derivative of fwith respect to x\\n∇ Gradient\\nf∗= minxf(x) The smallest function value of f\\nx∗∈arg minxf(x)The valuex∗that minimizes f(note: arg min returns a set of values)\\nL Lagrangian\\nL Negative log-likelihood(n\\nk)\\nBinomial coefﬁcient, nchoosek\\nVX[x] Variance ofxwith respect to the random variable X\\nEX[x] Expectation of xwith respect to the random variable X\\nCovX,Y[x,y] Covariance between xandy.\\nX⊥ ⊥Y|Z X is conditionally independent of YgivenZ\\nX∼p Random variable Xis distributed according to p\\nN(µ,Σ)\\nGaussian distribution with mean µand covariance Σ\\nBer(µ) Bernoulli distribution with parameter µ\\nBin(N,µ) Binomial distribution with parameters N,µ\\nBeta(α,β) Beta distribution with parameters α,β\\nTable of Abbreviations and Acronyms\\nAcronym Meaning\\ne.g. Exempli gratia (Latin: for example)\\nGMM Gaussian mixture model\\ni.e. Id est (Latin: this means)\\ni.i.d. Independent, identically distributed\\nMAP Maximum a posteriori\\nMLE Maximum likelihood estimation/estimator\\nONB Orthonormal basis\\nPCA Principal component analysis\\nPPCA Probabilistic principal component analysis\\nREF Row-echelon form\\nSPD Symmetric, positive deﬁnite\\nSVM Support vector machine\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bcf95e44-6776-49a3-ab92-ff2d316cacc7', embedding=None, metadata={'page_label': '8', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99771b81-2594-49ed-9ccd-fe9063fbf347', embedding=None, metadata={'page_label': '9', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Part I\\nMathematical Foundations\\n9\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ed06c14-fd8a-4caf-b834-b888feec23ee', embedding=None, metadata={'page_label': '10', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='964eaccd-464b-4166-8694-14a52b3c2285', embedding=None, metadata={'page_label': '11', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1\\nIntroduction and Motivation\\nMachine learning is about designing algorithms that automatically extract\\nvaluable information from data. The emphasis here is on “automatic”, i.e.,\\nmachine learning is concerned about general-purpose methodologies that\\ncan be applied to many datasets, while producing something that is mean-\\ningful. There are three concepts that are at the core of machine learning:\\ndata, a model, and learning.\\nSince machine learning is inherently data driven, data is at the core data\\nof machine learning. The goal of machine learning is to design general-\\npurpose methodologies to extract valuable patterns from data, ideally\\nwithout much domain-speciﬁc expertise. For example, given a large corpus\\nof documents (e.g., books in many libraries), machine learning methods\\ncan be used to automatically ﬁnd relevant topics that are shared across\\ndocuments (Hoffman et al., 2010). To achieve this goal, we design mod-\\nelsthat are typically related to the process that generates data, similar to model\\nthe dataset we are given. For example, in a regression setting, the model\\nwould describe a function that maps inputs to real-valued outputs. To\\nparaphrase Mitchell (1997): A model is said to learn from data if its per-\\nformance on a given task improves after the data is taken into account.\\nThe goal is to ﬁnd good models that generalize well to yet unseen data,\\nwhich we may care about in the future. Learning can be understood as a learning\\nway to automatically ﬁnd patterns and structure in data by optimizing the\\nparameters of the model.\\nWhile machine learning has seen many success stories, and software is\\nreadily available to design and train rich and ﬂexible machine learning\\nsystems, we believe that the mathematical foundations of machine learn-\\ning are important in order to understand fundamental principles upon\\nwhich more complicated machine learning systems are built. Understand-\\ning these principles can facilitate creating new machine learning solutions,\\nunderstanding and debugging existing approaches, and learning about the\\ninherent assumptions and limitations of the methodologies we are work-\\ning with.\\n11\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98f0ec43-64ff-4bb8-93b6-fc9cc09586fa', embedding=None, metadata={'page_label': '12', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Introduction and Motivation\\n1.1 Finding Words for Intuitions\\nA challenge we face regularly in machine learning is that concepts and\\nwords are slippery, and a particular component of the machine learning\\nsystem can be abstracted to different mathematical concepts. For example,\\nthe word “algorithm” is used in at least two different senses in the con-\\ntext of machine learning. In the ﬁrst sense, we use the phrase “machine\\nlearning algorithm” to mean a system that makes predictions based on in-\\nput data. We refer to these algorithms as predictors . In the second sense, predictor\\nwe use the exact same phrase “machine learning algorithm” to mean a\\nsystem that adapts some internal parameters of the predictor so that it\\nperforms well on future unseen input data. Here we refer to this adapta-\\ntion as training a system. training\\nThis book will not resolve the issue of ambiguity, but we want to high-\\nlight upfront that, depending on the context, the same expressions can\\nmean different things. However, we attempt to make the context sufﬁ-\\nciently clear to reduce the level of ambiguity.\\nThe ﬁrst part of this book introduces the mathematical concepts and\\nfoundations needed to talk about the three main components of a machine\\nlearning system: data, models, and learning. We will brieﬂy outline these\\ncomponents here, and we will revisit them again in Chapter 8 once we\\nhave discussed the necessary mathematical concepts.\\nWhile not all data is numerical, it is often useful to consider data in\\na number format. In this book, we assume that data has already been\\nappropriately converted into a numerical representation suitable for read-\\ning into a computer program. Therefore, we think of data as vectors. As data as vectors\\nanother illustration of how subtle words are, there are (at least) three\\ndifferent ways to think about vectors: a vector as an array of numbers (a\\ncomputer science view), a vector as an arrow with a direction and magni-\\ntude (a physics view), and a vector as an object that obeys addition and\\nscaling (a mathematical view).\\nAmodel is typically used to describe a process for generating data, sim- model\\nilar to the dataset at hand. Therefore, good models can also be thought\\nof as simpliﬁed versions of the real (unknown) data-generating process,\\ncapturing aspects that are relevant for modeling the data and extracting\\nhidden patterns from it. A good model can then be used to predict what\\nwould happen in the real world without performing real-world experi-\\nments.\\nWe now come to the crux of the matter, the learning component of learning\\nmachine learning. Assume we are given a dataset and a suitable model.\\nTraining the model means to use the data available to optimize some pa-\\nrameters of the model with respect to a utility function that evaluates how\\nwell the model predicts the training data. Most training methods can be\\nthought of as an approach analogous to climbing a hill to reach its peak.\\nIn this analogy, the peak of the hill corresponds to a maximum of some\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d7231b0-fa5b-42f8-8c41-d593fb5371bd', embedding=None, metadata={'page_label': '13', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.2 Two Ways to Read This Book 13\\ndesired performance measure. However, in practice, we are interested in\\nthe model to perform well on unseen data. Performing well on data that\\nwe have already seen (training data) may only mean that we found a\\ngood way to memorize the data. However, this may not generalize well to\\nunseen data, and, in practical applications, we often need to expose our\\nmachine learning system to situations that it has not encountered before.\\nLet us summarize the main concepts of machine learning that we cover\\nin this book:\\nWe represent data as vectors.\\nWe choose an appropriate model, either using the probabilistic or opti-\\nmization view.\\nWe learn from available data by using numerical optimization methods\\nwith the aim that the model performs well on data not used for training.\\n1.2 Two Ways to Read This Book\\nWe can consider two strategies for understanding the mathematics for\\nmachine learning:\\nBottom-up: Building up the concepts from foundational to more ad-\\nvanced. This is often the preferred approach in more technical ﬁelds,\\nsuch as mathematics. This strategy has the advantage that the reader\\nat all times is able to rely on their previously learned concepts. Unfor-\\ntunately, for a practitioner many of the foundational concepts are not\\nparticularly interesting by themselves, and the lack of motivation means\\nthat most foundational deﬁnitions are quickly forgotten.\\nTop-down: Drilling down from practical needs to more basic require-\\nments. This goal-driven approach has the advantage that the readers\\nknow at all times why they need to work on a particular concept, and\\nthere is a clear path of required knowledge. The downside of this strat-\\negy is that the knowledge is built on potentially shaky foundations, and\\nthe readers have to remember a set of words that they do not have any\\nway of understanding.\\nWe decided to write this book in a modular way to separate foundational\\n(mathematical) concepts from applications so that this book can be read\\nin both ways. The book is split into two parts, where Part I lays the math-\\nematical foundations and Part II applies the concepts from Part I to a set\\nof fundamental machine learning problems, which form four pillars of\\nmachine learning as illustrated in Figure 1.1: regression, dimensionality\\nreduction, density estimation, and classiﬁcation. Chapters in Part I mostly\\nbuild upon the previous ones, but it is possible to skip a chapter and work\\nbackward if necessary. Chapters in Part II are only loosely coupled and\\ncan be read in any order. There are many pointers forward and backward\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81e9752a-d022-417a-bf95-b45dff6a4d98', embedding=None, metadata={'page_label': '14', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 Introduction and Motivation\\nFigure 1.1 The\\nfoundations and\\nfour pillars of\\nmachine learning.\\nClassiﬁcation  \\nDensity  \\nEstimation  \\nRegression  \\nDimensionality  \\nReduction  \\nMachine Learning\\nVector Calculus Probability & Distributions Optimization\\nAnalytic Geometry Matrix Decomposition Linear Algebra\\nbetween the two parts of the book to link mathematical concepts with\\nmachine learning algorithms.\\nOf course there are more than two ways to read this book. Most readers\\nlearn using a combination of top-down and bottom-up approaches, some-\\ntimes building up basic mathematical skills before attempting more com-\\nplex concepts, but also choosing topics based on applications of machine\\nlearning.\\nPart I Is about Mathematics\\nThe four pillars of machine learning we cover in this book (see Figure 1.1)\\nrequire a solid mathematical foundation, which is laid out in Part I.\\nWe represent numerical data as vectors and represent a table of such\\ndata as a matrix. The study of vectors and matrices is called linear algebra ,\\nwhich we introduce in Chapter 2. The collection of vectors as a matrix is linear algebra\\nalso described there.\\nGiven two vectors representing two objects in the real world, we want\\nto make statements about their similarity. The idea is that vectors that\\nare similar should be predicted to have similar outputs by our machine\\nlearning algorithm (our predictor). To formalize the idea of similarity be-\\ntween vectors, we need to introduce operations that take two vectors as\\ninput and return a numerical value representing their similarity. The con-\\nstruction of similarity and distances is central to analytic geometry and is analytic geometry\\ndiscussed in Chapter 3.\\nIn Chapter 4, we introduce some fundamental concepts about matri-\\nces and matrix decomposition . Some operations on matrices are extremely matrix\\ndecomposition useful in machine learning, and they allow for an intuitive interpretation\\nof the data and more efﬁcient learning.\\nWe often consider data to be noisy observations of some true underly-\\ning signal. We hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning what “noise” means. We often would also like to have predictors that\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97d75ddd-47ff-4332-a435-1073cdc5a0f6', embedding=None, metadata={'page_label': '15', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.2 Two Ways to Read This Book 15\\nallow us to express some sort of uncertainty, e.g., to quantify the conﬁ-\\ndence we have about the value of the prediction at a particular test data\\npoint. Quantiﬁcation of uncertainty is the realm of probability theory and probability theory\\nis covered in Chapter 6.\\nTo train machine learning models, we typically ﬁnd parameters that\\nmaximize some performance measure. Many optimization techniques re-\\nquire the concept of a gradient, which tells us the direction in which to\\nsearch for a solution. Chapter 5 is about vector calculus and details the vector calculus\\nconcept of gradients, which we subsequently use in Chapter 7, where we\\ntalk about optimization to ﬁnd maxima/minima of functions. optimization\\nPart II Is about Machine Learning\\nThe second part of the book introduces four pillars of machine learning\\nas shown in Figure 1.1. We illustrate how the mathematical concepts in-\\ntroduced in the ﬁrst part of the book are the foundation for each pillar.\\nBroadly speaking, chapters are ordered by difﬁculty (in ascending order).\\nIn Chapter 8, we restate the three components of machine learning\\n(data, models, and parameter estimation) in a mathematical fashion. In\\naddition, we provide some guidelines for building experimental set-ups\\nthat guard against overly optimistic evaluations of machine learning sys-\\ntems. Recall that the goal is to build a predictor that performs well on\\nunseen data.\\nIn Chapter 9, we will have a close look at linear regression , where our linear regression\\nobjective is to ﬁnd functions that map inputs x∈RDto corresponding ob-\\nserved function values y∈R, which we can interpret as the labels of their\\nrespective inputs. We will discuss classical model ﬁtting (parameter esti-\\nmation) via maximum likelihood and maximum a posteriori estimation,\\nas well as Bayesian linear regression, where we integrate the parameters\\nout instead of optimizing them.\\nChapter 10 focuses on dimensionality reduction , the second pillar in Fig- dimensionality\\nreduction ure 1.1, using principal component analysis. The key objective of dimen-\\nsionality reduction is to ﬁnd a compact, lower-dimensional representation\\nof high-dimensional data x∈RD, which is often easier to analyze than\\nthe original data. Unlike regression, dimensionality reduction is only con-\\ncerned about modeling the data – there are no labels associated with a\\ndata pointx.\\nIn Chapter 11, we will move to our third pillar: density estimation . The density estimation\\nobjective of density estimation is to ﬁnd a probability distribution that de-\\nscribes a given dataset. We will focus on Gaussian mixture models for this\\npurpose, and we will discuss an iterative scheme to ﬁnd the parameters of\\nthis model. As in dimensionality reduction, there are no labels associated\\nwith the data points x∈RD. However, we do not seek a low-dimensional\\nrepresentation of the data. Instead, we are interested in a density model\\nthat describes the data.\\nChapter 12 concludes the book with an in-depth discussion of the fourth\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='007a9f3f-0566-4ee9-bd36-38d91a5621f3', embedding=None, metadata={'page_label': '16', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 Introduction and Motivation\\npillar: classiﬁcation . We will discuss classiﬁcation in the context of support classiﬁcation\\nvector machines. Similar to regression (Chapter 9), we have inputs xand\\ncorresponding labels y. However, unlike regression, where the labels were\\nreal-valued, the labels in classiﬁcation are integers, which requires special\\ncare.\\n1.3 Exercises and Feedback\\nWe provide some exercises in Part I, which can be done mostly by pen and\\npaper. For Part II, we provide programming tutorials (jupyter notebooks)\\nto explore some properties of the machine learning algorithms we discuss\\nin this book.\\nWe appreciate that Cambridge University Press strongly supports our\\naim to democratize education and learning by making this book freely\\navailable for download at\\nhttps://mml-book.com\\nwhere tutorials, errata, and additional materials can be found. Mistakes\\ncan be reported and feedback provided using the preceding URL.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e266a31-e5bf-496e-91cc-5f5ea7043ab3', embedding=None, metadata={'page_label': '17', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2\\nLinear Algebra\\nWhen formalizing intuitive concepts, a common approach is to construct a\\nset of objects (symbols) and a set of rules to manipulate these objects. This\\nis known as an algebra . Linear algebra is the study of vectors and certain algebra\\nrules to manipulate vectors. The vectors many of us know from school are\\ncalled “geometric vectors”, which are usually denoted by a small arrow\\nabove the letter, e.g.,− →xand− →y. In this book, we discuss more general\\nconcepts of vectors and use a bold letter to represent them, e.g., xandy.\\nIn general, vectors are special objects that can be added together and\\nmultiplied by scalars to produce another object of the same kind. From\\nan abstract mathematical viewpoint, any object that satisﬁes these two\\nproperties can be considered a vector. Here are some examples of such\\nvector objects:\\n1. Geometric vectors. This example of a vector may be familiar from high\\nschool mathematics and physics. Geometric vectors – see Figure 2.1(a)\\n– are directed segments, which can be drawn (at least in two dimen-\\nsions). Two geometric vectors→x,→ycan be added, such that→x+→y=→z\\nis another geometric vector. Furthermore, multiplication by a scalar\\nλ→x,λ∈R, is also a geometric vector. In fact, it is the original vector\\nscaled byλ. Therefore, geometric vectors are instances of the vector\\nconcepts introduced previously. Interpreting vectors as geometric vec-\\ntors enables us to use our intuitions about direction and magnitude to\\nreason about mathematical operations.\\n2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can\\nFigure 2.1\\nDifferent types of\\nvectors. Vectors can\\nbe surprising\\nobjects, including\\n(a) geometric\\nvectors\\nand (b) polynomials.→x→y→x+→y\\n(a) Geometric vectors.\\n−2 0 2\\nx−6−4−2024y (b) Polynomials.\\n17\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='19eb0fab-df61-45f9-83b9-9ff3c2f5ca57', embedding=None, metadata={'page_label': '18', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 Linear Algebra\\nbe added together, which results in another polynomial; and they can\\nbe multiplied by a scalar λ∈R, and the result is a polynomial as\\nwell. Therefore, polynomials are (rather unusual) instances of vectors.\\nNote that polynomials are very different from geometric vectors. While\\ngeometric vectors are concrete “drawings”, polynomials are abstract\\nconcepts. However, they are both vectors in the sense previously de-\\nscribed.\\n3. Audio signals are vectors. Audio signals are represented as a series of\\nnumbers. We can add audio signals together, and their sum is a new\\naudio signal. If we scale an audio signal, we also obtain an audio signal.\\nTherefore, audio signals are a type of vector, too.\\n4. Elements of Rn(tuples ofnreal numbers) are vectors. Rnis more\\nabstract than polynomials, and it is the concept we focus on in this\\nbook. For instance,\\na=\\uf8ee\\n\\uf8f01\\n2\\n3\\uf8f9\\n\\uf8fb∈R3(2.1)\\nis an example of a triplet of numbers. Adding two vectors a,b∈Rn\\ncomponent-wise results in another vector: a+b=c∈Rn. Moreover,\\nmultiplyinga∈Rnbyλ∈Rresults in a scaled vector λa∈Rn.\\nConsidering vectors as elements of Rnhas an additional beneﬁt that Be careful to check\\nwhether array\\noperations actually\\nperform vector\\noperations when\\nimplementing on a\\ncomputer.it loosely corresponds to arrays of real numbers on a computer. Many\\nprogramming languages support array operations, which allow for con-\\nvenient implementation of algorithms that involve vector operations.\\nLinear algebra focuses on the similarities between these vector concepts.\\nWe can add them together and multiply them by scalars. We will largelyPavel Grinfeld’s\\nseries on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/nahclwm\\nGilbert Strang’s\\ncourse on linear\\nalgebra:\\nhttp://tinyurl.\\ncom/29p5q8j\\n3Blue1Brown series\\non linear algebra:\\nhttps://tinyurl.\\ncom/h5g4kpsfocus on vectors in Rnsince most algorithms in linear algebra are for-\\nmulated in Rn. We will see in Chapter 8 that we often consider data to\\nbe represented as vectors in Rn. In this book, we will focus on ﬁnite-\\ndimensional vector spaces, in which case there is a 1:1correspondence\\nbetween any kind of vector and Rn. When it is convenient, we will use\\nintuitions about geometric vectors and consider array-based algorithms.\\nOne major idea in mathematics is the idea of “closure”. This is the ques-\\ntion: What is the set of all things that can result from my proposed oper-\\nations? In the case of vectors: What is the set of vectors that can result by\\nstarting with a small set of vectors, and adding them to each other and\\nscaling them? This results in a vector space (Section 2.4). The concept of\\na vector space and its properties underlie much of machine learning. The\\nconcepts introduced in this chapter are summarized in Figure 2.2.\\nThis chapter is mostly based on the lecture notes and books by Drumm\\nand Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann\\n(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='35931ad4-04d7-41f4-a0fe-ff7c1590540d', embedding=None, metadata={'page_label': '19', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.1 Systems of Linear Equations 19\\nFigure 2.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.Vector\\nVector spaceMatrixChapter 5\\nVector calculus\\nGroup\\nSystem of\\nlinear equations\\nMatrix\\ninverse\\nGaussian\\neliminationLinear/afﬁne\\nmappingLinear\\nindependence\\nBasis\\nChapter 10\\nDimensionality\\nreductionChapter 12\\nClassiﬁcationChapter 3\\nAnalytic geometrycomposesclosure\\nAbelian\\nwith +represents\\nrepresents\\nsolved by solvesproperty ofmaximal set\\nresources are Gilbert Strang’s Linear Algebra course at MIT and the Linear\\nAlgebra Series by 3Blue1Brown.\\nLinear algebra plays an important role in machine learning and gen-\\neral mathematics. The concepts introduced in this chapter are further ex-\\npanded to include the idea of geometry in Chapter 3. In Chapter 5, we\\nwill discuss vector calculus, where a principled knowledge of matrix op-\\nerations is essential. In Chapter 10, we will use projections (to be intro-\\nduced in Section 3.8) for dimensionality reduction with principal compo-\\nnent analysis (PCA). In Chapter 9, we will discuss linear regression, where\\nlinear algebra plays a central role for solving least-squares problems.\\n2.1 Systems of Linear Equations\\nSystems of linear equations play a central part of linear algebra. Many\\nproblems can be formulated as systems of linear equations, and linear\\nalgebra gives us the tools for solving them.\\nExample 2.1\\nA company produces products N1,...,Nnfor which resources\\nR1,...,Rmare required. To produce a unit of product Nj,aijunits of\\nresourceRiare needed, where i= 1,...,m andj= 1,...,n .\\nThe objective is to ﬁnd an optimal production plan, i.e., a plan of how\\nmany units xjof productNjshould be produced if a total of biunits of\\nresourceRiare available and (ideally) no resources are left over.\\nIf we produce x1,...,xnunits of the corresponding products, we need\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fe31241-5199-4139-8acc-0f1e94b4fa1e', embedding=None, metadata={'page_label': '20', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 Linear Algebra\\na total of\\nai1x1+···+ainxn (2.2)\\nmany units of resource Ri. An optimal production plan (x1,...,xn)∈Rn,\\ntherefore, has to satisfy the following system of equations:\\na11x1+···+a1nxn=b1\\n...\\nam1x1+···+amnxn=bm, (2.3)\\nwhereaij∈Randbi∈R.\\nEquation (2.3) is the general form of a system of linear equations , and system of linear\\nequations x1,...,xnare the unknowns of this system. Every n-tuple (x1,...,xn)∈\\nRnthat satisﬁes (2.3) is a solution of the linear equation system. solution\\nExample 2.2\\nThe system of linear equations\\nx1+x2+x3= 3 (1)\\nx1−x2+ 2x3= 2 (2)\\n2x1 + 3x3= 1 (3)(2.4)\\nhasno solution: Adding the ﬁrst two equations yields 2x1+3x3= 5, which\\ncontradicts the third equation (3).\\nLet us have a look at the system of linear equations\\nx1+x2+x3= 3 (1)\\nx1−x2+ 2x3= 2 (2)\\nx2+x3= 2 (3). (2.5)\\nFrom the ﬁrst and third equation, it follows that x1= 1. From (1) +(2),\\nwe get 2x1+ 3x3= 5, i.e.,x3= 1. From (3), we then get that x2= 1.\\nTherefore, (1,1,1)is the only possible and unique solution (verify that\\n(1,1,1)is a solution by plugging in).\\nAs a third example, we consider\\nx1+x2+x3= 3 (1)\\nx1−x2+ 2x3= 2 (2)\\n2x1 + 3x3= 5 (3). (2.6)\\nSince (1) +(2)=(3), we can omit the third equation (redundancy). From\\n(1) and (2), we get 2x1= 5−3x3and2x2= 1+x3. We deﬁnex3=a∈R\\nas a free variable, such that any triplet\\n(5\\n2−3\\n2a,1\\n2+1\\n2a,a)\\n, a∈R (2.7)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98d0f0ad-38de-455c-ae21-ae956138c16e', embedding=None, metadata={'page_label': '21', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.1 Systems of Linear Equations 21\\nFigure 2.3 The\\nsolution space of a\\nsystem of two linear\\nequations with two\\nvariables can be\\ngeometrically\\ninterpreted as the\\nintersection of two\\nlines. Every linear\\nequation represents\\na line.\\n2x1−4x2= 1\\n4x1+ 4x2= 5\\nx1\\nx2\\nis a solution of the system of linear equations, i.e., we obtain a solution\\nset that contains inﬁnitely many solutions.\\nIn general, for a real-valued system of linear equations we obtain either\\nno, exactly one, or inﬁnitely many solutions. Linear regression (Chapter 9)\\nsolves a version of Example 2.1 when we cannot solve the system of linear\\nequations.\\nRemark (Geometric Interpretation of Systems of Linear Equations) .In a\\nsystem of linear equations with two variables x1,x2, each linear equation\\ndeﬁnes a line on the x1x2-plane. Since a solution to a system of linear\\nequations must satisfy all equations simultaneously, the solution set is the\\nintersection of these lines. This intersection set can be a line (if the linear\\nequations describe the same line), a point, or empty (when the lines are\\nparallel). An illustration is given in Figure 2.3 for the system\\n4x1+ 4x2= 5\\n2x1−4x2= 1(2.8)\\nwhere the solution space is the point (x1,x2) = (1,1\\n4). Similarly, for three\\nvariables, each linear equation determines a plane in three-dimensional\\nspace. When we intersect these planes, i.e., satisfy all linear equations at\\nthe same time, we can obtain a solution set that is a plane, a line, a point\\nor empty (when the planes have no common intersection). ♦\\nFor a systematic approach to solving systems of linear equations, we\\nwill introduce a useful compact notation. We collect the coefﬁcients aij\\ninto vectors and collect the vectors into matrices. In other words, we write\\nthe system from (2.3) in the following form:\\n\\uf8ee\\n\\uf8ef\\uf8f0a11\\n...\\nam1\\uf8f9\\n\\uf8fa\\uf8fbx1+\\uf8ee\\n\\uf8ef\\uf8f0a12\\n...\\nam2\\uf8f9\\n\\uf8fa\\uf8fbx2+···+\\uf8ee\\n\\uf8ef\\uf8f0a1n\\n...\\namn\\uf8f9\\n\\uf8fa\\uf8fbxn=\\uf8ee\\n\\uf8ef\\uf8f0b1\\n...\\nbm\\uf8f9\\n\\uf8fa\\uf8fb (2.9)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8aec8238-2de9-4309-bc18-043c0c1569d6', embedding=None, metadata={'page_label': '22', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 Linear Algebra\\n⇐⇒\\uf8ee\\n\\uf8ef\\uf8f0a11···a1n\\n......\\nam1···amn\\uf8f9\\n\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8f0x1\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8f0b1\\n...\\nbm\\uf8f9\\n\\uf8fa\\uf8fb. (2.10)\\nIn the following, we will have a close look at these matrices and de-\\nﬁne computation rules. We will return to solving linear equations in Sec-\\ntion 2.3.\\n2.2 Matrices\\nMatrices play a central role in linear algebra. They can be used to com-\\npactly represent systems of linear equations, but they also represent linear\\nfunctions (linear mappings) as we will see later in Section 2.7. Before we\\ndiscuss some of these interesting topics, let us ﬁrst deﬁne what a matrix\\nis and what kind of operations we can do with matrices. We will see more\\nproperties of matrices in Chapter 4.\\nDeﬁnition 2.1 (Matrix) .Withm,n∈Na real-valued (m,n)matrixAis matrix\\nanm·n-tuple of elements aij,i= 1,...,m ,j= 1,...,n , which is ordered\\naccording to a rectangular scheme consisting of mrows andncolumns:\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0a11a12···a1n\\na21a22···a2n\\n.........\\nam1am2···amn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb, aij∈R. (2.11)\\nBy convention (1,n)-matrices are called rows and(m,1)-matrices are called row\\ncolumns . These special matrices are also called row/column vectors . column\\nrow vector\\ncolumn vector\\nFigure 2.4 By\\nstacking its\\ncolumns, a matrix A\\ncan be represented\\nas a long vector a.\\nre-shapeA∈R4×2a∈R8Rm×nis the set of all real-valued (m,n)-matrices.A∈Rm×ncan be\\nequivalently represented as a∈Rmnby stacking all ncolumns of the\\nmatrix into a long vector; see Figure 2.4.\\n2.2.1 Matrix Addition and Multiplication\\nThe sum of two matrices A∈Rm×n,B∈Rm×nis deﬁned as the element-\\nwise sum, i.e.,\\nA+B:=\\uf8ee\\n\\uf8ef\\uf8f0a11+b11···a1n+b1n\\n......\\nam1+bm1···amn+bmn\\uf8f9\\n\\uf8fa\\uf8fb∈Rm×n. (2.12)\\nFor matricesA∈Rm×n,B∈Rn×k, the elements cijof the product Note the size of the\\nmatrices. C=AB∈Rm×kare computed as\\nC =\\nnp.einsum(’il,\\nlj’, A, B) cij=n∑\\nl=1ailblj, i = 1,...,m, j = 1,...,k. (2.13)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='30340bc7-67d8-4f50-b75a-6061bd650f73', embedding=None, metadata={'page_label': '23', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.2 Matrices 23\\nThis means, to compute element cijwe multiply the elements of the ith There arencolumns\\ninAandnrows in\\nBso that we can\\ncomputeailbljfor\\nl= 1,...,n .\\nCommonly, the dot\\nproduct between\\ntwo vectorsa,bis\\ndenoted bya⊤bor\\n⟨a,b⟩.row ofAwith thejth column ofBand sum them up. Later in Section 3.2,\\nwe will call this the dot product of the corresponding row and column. In\\ncases, where we need to be explicit that we are performing multiplication,\\nwe use the notation A·Bto denote multiplication (explicitly showing\\n“·”).\\nRemark. Matrices can only be multiplied if their “neighboring” dimensions\\nmatch. For instance, an n×k-matrixAcan be multiplied with a k×m-\\nmatrixB, but only from the left side:\\nA\\ued19\\ued18\\ued17\\ued1a\\nn×kB\\ued19\\ued18\\ued17\\ued1a\\nk×m=C\\ued19\\ued18\\ued17\\ued1a\\nn×m(2.14)\\nThe productBAis not deﬁned if m̸=nsince the neighboring dimensions\\ndo not match. ♦\\nRemark. Matrix multiplication is notdeﬁned as an element-wise operation\\non matrix elements, i.e., cij̸=aijbij(even if the size of A,Bwas cho-\\nsen appropriately). This kind of element-wise multiplication often appears\\nin programming languages when we multiply (multi-dimensional) arrays\\nwith each other, and is called a Hadamard product . ♦ Hadamard product\\nExample 2.3\\nForA=[1 2 3\\n3 2 1]\\n∈R2×3,B=\\uf8ee\\n\\uf8f00 2\\n1−1\\n0 1\\uf8f9\\n\\uf8fb∈R3×2, we obtain\\nAB=[1 2 3\\n3 2 1]\\uf8ee\\n\\uf8f00 2\\n1−1\\n0 1\\uf8f9\\n\\uf8fb=[2 3\\n2 5]\\n∈R2×2, (2.15)\\nBA=\\uf8ee\\n\\uf8f00 2\\n1−1\\n0 1\\uf8f9\\n\\uf8fb[1 2 3\\n3 2 1]\\n=\\uf8ee\\n\\uf8f06 4 2\\n−2 0 2\\n3 2 1\\uf8f9\\n\\uf8fb∈R3×3. (2.16)\\nFigure 2.5 Even if\\nboth matrix\\nmultiplications AB\\nandBA are\\ndeﬁned, the\\ndimensions of the\\nresults can be\\ndifferent.\\nFrom this example, we can already see that matrix multiplication is not\\ncommutative, i.e., AB̸=BA; see also Figure 2.5 for an illustration.\\nDeﬁnition 2.2 (Identity Matrix) .InRn×n, we deﬁne the identity matrix\\nidentity matrixIn:=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 0···0···0\\n0 1···0···0\\n..................\\n0 0···1···0\\n..................\\n0 0···0···1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rn×n(2.17)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb0d79b5-8adb-4f2f-9f6a-bf7dbb3c85e1', embedding=None, metadata={'page_label': '24', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 Linear Algebra\\nas then×n-matrix containing 1on the diagonal and 0everywhere else.\\nNow that we deﬁned matrix multiplication, matrix addition and the\\nidentity matrix, let us have a look at some properties of matrices:\\nassociativity\\nAssociativity:\\n∀A∈Rm×n,B∈Rn×p,C∈Rp×q: (AB)C=A(BC) (2.18)\\ndistributivity\\nDistributivity:\\n∀A,B∈Rm×n,C,D∈Rn×p: (A+B)C=AC+BC (2.19a)\\nA(C+D) =AC+AD (2.19b)\\nMultiplication with the identity matrix:\\n∀A∈Rm×n:ImA=AIn=A (2.20)\\nNote thatIm̸=Inform̸=n.\\n2.2.2 Inverse and Transpose\\nDeﬁnition 2.3 (Inverse) .Consider a square matrix A∈Rn×n. Let matrix A square matrix\\npossesses the same\\nnumber of columns\\nand rows.B∈Rn×nhave the property that AB =In=BA.Bis called the\\ninverse ofAand denoted by A−1.\\ninverseUnfortunately, not every matrix Apossesses an inverse A−1. If this\\ninverse does exist, Ais called regular /invertible /nonsingular , otherwise regular\\ninvertible\\nnonsingularsingular /noninvertible . When the matrix inverse exists, it is unique. In Sec-\\nsingular\\nnoninvertibletion 2.3, we will discuss a general way to compute the inverse of a matrix\\nby solving a system of linear equations.\\nRemark (Existence of the Inverse of a 2×2-matrix) .Consider a matrix\\nA:=[a11a12\\na21a22]\\n∈R2×2. (2.21)\\nIf we multiply Awith\\nA′:=[a22−a12\\n−a21a11]\\n(2.22)\\nwe obtain\\nAA′=[a11a22−a12a21 0\\n0a11a22−a12a21]\\n= (a11a22−a12a21)I.\\n(2.23)\\nTherefore,\\nA−1=1\\na11a22−a12a21[a22−a12\\n−a21a11]\\n(2.24)\\nif and only if a11a22−a12a21̸= 0. In Section 4.1, we will see that a11a22−\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b19f582-199d-430d-b975-f8e7f2b4b604', embedding=None, metadata={'page_label': '25', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.2 Matrices 25\\na12a21is the determinant of a 2×2-matrix. Furthermore, we can generally\\nuse the determinant to check whether a matrix is invertible. ♦\\nExample 2.4 (Inverse Matrix)\\nThe matrices\\nA=\\uf8ee\\n\\uf8f01 2 1\\n4 4 5\\n6 7 7\\uf8f9\\n\\uf8fb,B=\\uf8ee\\n\\uf8f0−7−7 6\\n2 1−1\\n4 5−4\\uf8f9\\n\\uf8fb (2.25)\\nare inverse to each other since AB=I=BA.\\nDeﬁnition 2.4 (Transpose) .ForA∈Rm×nthe matrixB∈Rn×mwith\\nbij=ajiis called the transpose ofA. We writeB=A⊤. transpose\\nThe main diagonal\\n(sometimes called\\n“principal diagonal”,\\n“primary diagonal”,\\n“leading diagonal”,\\nor “major diagonal”)\\nof a matrixAis the\\ncollection of entries\\nAijwherei=j.In general,A⊤can be obtained by writing the columns of Aas the rows\\nofA⊤. The following are important properties of inverses and transposes:\\nThe scalar case of\\n(2.28) is\\n1\\n2+4=1\\n6̸=1\\n2+1\\n4.AA−1=I=A−1A (2.26)\\n(AB)−1=B−1A−1(2.27)\\n(A+B)−1̸=A−1+B−1(2.28)\\n(A⊤)⊤=A (2.29)\\n(A+B)⊤=A⊤+B⊤(2.30)\\n(AB)⊤=B⊤A⊤(2.31)\\nDeﬁnition 2.5 (Symmetric Matrix) .A matrixA∈Rn×nissymmetric if symmetric matrix\\nA=A⊤.\\nNote that only (n,n)-matrices can be symmetric. Generally, we call\\n(n,n)-matrices also square matrices because they possess the same num- square matrix\\nber of rows and columns. Moreover, if Ais invertible, then so is A⊤, and\\n(A−1)⊤= (A⊤)−1=:A−⊤.\\nRemark (Sum and Product of Symmetric Matrices) .The sum of symmet-\\nric matricesA,B∈Rn×nis always symmetric. However, although their\\nproduct is always deﬁned, it is generally not symmetric:\\n[1 0\\n0 0][1 1\\n1 1]\\n=[1 1\\n0 0]\\n. (2.32)\\n♦\\n2.2.3 Multiplication by a Scalar\\nLet us look at what happens to matrices when they are multiplied by a\\nscalarλ∈R. LetA∈Rm×nandλ∈R. ThenλA=K,Kij=λaij.\\nPractically,λscales each element of A. Forλ,ψ∈R, the following holds:\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b245a2e1-1290-4a82-ac74-ee6752aa99e1', embedding=None, metadata={'page_label': '26', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26 Linear Algebra\\nassociativityAssociativity:\\n(λψ)C=λ(ψC),C∈Rm×n\\nλ(BC) = (λB)C=B(λC) = (BC)λ,B∈Rm×n,C∈Rn×k.\\nNote that this allows us to move scalar values around.\\n(λC)⊤=C⊤λ⊤=C⊤λ=λC⊤sinceλ=λ⊤for allλ∈R.distributivity\\nDistributivity:\\n(λ+ψ)C=λC+ψC,C∈Rm×n\\nλ(B+C) =λB+λC,B,C∈Rm×n\\nExample 2.5 (Distributivity)\\nIf we deﬁne\\nC:=[1 2\\n3 4]\\n, (2.33)\\nthen for any λ,ψ∈Rwe obtain\\n(λ+ψ)C=[(λ+ψ)1 (λ+ψ)2\\n(λ+ψ)3 (λ+ψ)4]\\n=[λ+ψ 2λ+ 2ψ\\n3λ+ 3ψ4λ+ 4ψ]\\n(2.34a)\\n=[λ2λ\\n3λ4λ]\\n+[ψ2ψ\\n3ψ4ψ]\\n=λC+ψC. (2.34b)\\n2.2.4 Compact Representations of Systems of Linear Equations\\nIf we consider the system of linear equations\\n2x1+ 3x2+ 5x3= 1\\n4x1−2x2−7x3= 8\\n9x1+ 5x2−3x3= 2(2.35)\\nand use the rules for matrix multiplication, we can write this equation\\nsystem in a more compact form as\\n\\uf8ee\\n\\uf8f02 3 5\\n4−2−7\\n9 5−3\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f0x1\\nx2\\nx3\\uf8f9\\n\\uf8fb=\\uf8ee\\n\\uf8f01\\n8\\n2\\uf8f9\\n\\uf8fb. (2.36)\\nNote thatx1scales the ﬁrst column, x2the second one, and x3the third\\none.\\nGenerally, a system of linear equations can be compactly represented in\\ntheir matrix form as Ax=b; see (2.3), and the product Axis a (linear)\\ncombination of the columns of A. We will discuss linear combinations in\\nmore detail in Section 2.5.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91dfbe18-8239-4abb-acb1-b5c18f945b1c', embedding=None, metadata={'page_label': '27', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Solving Systems of Linear Equations 27\\n2.3 Solving Systems of Linear Equations\\nIn (2.3), we introduced the general form of an equation system, i.e.,\\na11x1+···+a1nxn=b1\\n...\\nam1x1+···+amnxn=bm,(2.37)\\nwhereaij∈Randbi∈Rare known constants and xjare unknowns,\\ni= 1,...,m ,j= 1,...,n . Thus far, we saw that matrices can be used as\\na compact way of formulating systems of linear equations so that we can\\nwriteAx=b, see (2.10). Moreover, we deﬁned basic matrix operations,\\nsuch as addition and multiplication of matrices. In the following, we will\\nfocus on solving systems of linear equations and provide an algorithm for\\nﬁnding the inverse of a matrix.\\n2.3.1 Particular and General Solution\\nBefore discussing how to generally solve systems of linear equations, let\\nus have a look at an example. Consider the system of equations\\n[1 0 8−4\\n0 1 2 12]\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\nx3\\nx4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=[42\\n8]\\n. (2.38)\\nThe system has two equations and four unknowns. Therefore, in general\\nwe would expect inﬁnitely many solutions. This system of equations is\\nin a particularly easy form, where the ﬁrst two columns consist of a 1\\nand a 0. Remember that we want to ﬁnd scalars x1,...,x 4, such that∑4\\ni=1xici=b, where we deﬁne cito be theith column of the matrix and\\nbthe right-hand-side of (2.38). A solution to the problem in (2.38) can\\nbe found immediately by taking 42times the ﬁrst column and 8times the\\nsecond column so that\\nb=[42\\n8]\\n= 42[1\\n0]\\n+ 8[0\\n1]\\n. (2.39)\\nTherefore, a solution is [42,8,0,0]⊤. This solution is called a particular particular solution\\nsolution orspecial solution . However, this is not the only solution of this special solution\\nsystem of linear equations. To capture all the other solutions, we need\\nto be creative in generating 0in a non-trivial way using the columns of\\nthe matrix: Adding 0to our special solution does not change the special\\nsolution. To do so, we express the third column using the ﬁrst two columns\\n(which are of this very simple form)\\n[8\\n2]\\n= 8[1\\n0]\\n+ 2[0\\n1]\\n(2.40)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e577a61-3878-4fab-80bb-fe8b5149907c', embedding=None, metadata={'page_label': '28', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 Linear Algebra\\nso that 0= 8c1+ 2c2−1c3+ 0c4and(x1,x2,x3,x4) = (8,2,−1,0). In\\nfact, any scaling of this solution by λ1∈Rproduces the 0vector, i.e.,\\n[1 0 8−4\\n0 1 2 12]\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f08\\n2\\n−1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8=λ1(8c1+ 2c2−c3) =0. (2.41)\\nFollowing the same line of reasoning, we express the fourth column of the\\nmatrix in (2.38) using the ﬁrst two columns and generate another set of\\nnon-trivial versions of 0as\\n[1 0 8−4\\n0 1 2 12]\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edλ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−4\\n12\\n0\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8=λ2(−4c1+ 12c2−c4) =0 (2.42)\\nfor anyλ2∈R. Putting everything together, we obtain all solutions of the\\nequation system in (2.38), which is called the general solution , as the set general solution\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3x∈R4:x=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f042\\n8\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f08\\n2\\n−1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−4\\n12\\n0\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,λ1,λ2∈R\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe.(2.43)\\nRemark. The general approach we followed consisted of the following\\nthree steps:\\n1. Find a particular solution to Ax=b.\\n2. Find all solutions to Ax=0.\\n3. Combine the solutions from steps 1. and 2. to the general solution.\\nNeither the general nor the particular solution is unique. ♦\\nThe system of linear equations in the preceding example was easy to\\nsolve because the matrix in (2.38) has this particularly convenient form,\\nwhich allowed us to ﬁnd the particular and the general solution by in-\\nspection. However, general equation systems are not of this simple form.\\nFortunately, there exists a constructive algorithmic way of transforming\\nany system of linear equations into this particularly simple form: Gaussian\\nelimination. Key to Gaussian elimination are elementary transformations\\nof systems of linear equations, which transform the equation system into\\na simple form. Then, we can apply the three steps to the simple form that\\nwe just discussed in the context of the example in (2.38).\\n2.3.2 Elementary Transformations\\nKey to solving a system of linear equations are elementary transformations elementary\\ntransformations that keep the solution set the same, but that transform the equation system\\ninto a simpler form:\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d6c65e4-0f9b-49f3-a987-24024ce33f22', embedding=None, metadata={'page_label': '29', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Solving Systems of Linear Equations 29\\nExchange of two equations (rows in the matrix representing the system\\nof equations)\\nMultiplication of an equation (row) with a constant λ∈R\\\\{0}\\nAddition of two equations (rows)\\nExample 2.6\\nFora∈R, we seek all solutions of the following system of equations:\\n−2x1+ 4x2−2x3−x4+ 4x5=−3\\n4x1−8x2+ 3x3−3x4+x5= 2\\nx1−2x2+x3−x4+x5= 0\\nx1−2x2−3x4+ 4x5=a. (2.44)\\nWe start by converting this system of equations into the compact matrix\\nnotationAx=b. We no longer mention the variables xexplicitly and\\nbuild the augmented matrix (in the form[A|b]\\n) augmented matrix\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−2 4−2−1 4−3\\n4−8 3−3 1 2\\n1−2 1−1 1 0\\n1−2 0−3 4 a\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbSwap withR3\\nSwap withR1\\nwhere we used the vertical line to separate the left-hand side from the\\nright-hand side in (2.44). We use ⇝to indicate a transformation of the\\naugmented matrix using elementary transformations. The augmented\\nmatrix[\\nA|b]\\ncompactly\\nrepresents the\\nsystem of linear\\nequationsAx=b.Swapping Rows 1and3leads to\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−2 1−1 1 0\\n4−8 3−3 1 2\\n−2 4−2−1 4−3\\n1−2 0−3 4 a\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb−4R1\\n+2R1\\n−R1\\nWhen we now apply the indicated transformations (e.g., subtract Row 1\\nfour times from Row 2), we obtain\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−2 1−1 1 0\\n0 0−1 1−3 2\\n0 0 0 −3 6−3\\n0 0−1−2 3 a\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n−R2−R3\\n⇝\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−2 1−1 1 0\\n0 0−1 1−3 2\\n0 0 0 −3 6−3\\n0 0 0 0 0 a+1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb·(−1)\\n·(−1\\n3)\\n⇝\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−2 1−1 1 0\\n0 0 1 −1 3−2\\n0 0 0 1 −2 1\\n0 0 0 0 0 a+1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab852668-791d-4ff8-90cd-1a8bc1a31524', embedding=None, metadata={'page_label': '30', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 Linear Algebra\\nThis (augmented) matrix is in a convenient form, the row-echelon form row-echelon form\\n(REF). Reverting this compact notation back into the explicit notation with\\nthe variables we seek, we obtain\\nx1−2x2+x3−x4+x5= 0\\nx3−x4+ 3x5=−2\\nx4−2x5= 1\\n0 =a+ 1. (2.45)\\nOnly fora=−1this system can be solved. A particular solution is particular solution\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\nx3\\nx4\\nx5\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n0\\n−1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (2.46)\\nThegeneral solution , which captures the set of all possible solutions, is general solution\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3x∈R5:x=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n0\\n−1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb+λ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n1\\n0\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n0\\n−1\\n2\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, λ 1,λ2∈R\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe.(2.47)\\nIn the following, we will detail a constructive way to obtain a particular\\nand general solution of a system of linear equations.\\nRemark (Pivots and Staircase Structure) .The leading coefﬁcient of a row\\n(ﬁrst nonzero number from the left) is called the pivot and is always pivot\\nstrictly to the right of the pivot of the row above it. Therefore, any equa-\\ntion system in row-echelon form always has a “staircase” structure. ♦\\nDeﬁnition 2.6 (Row-Echelon Form) .A matrix is in row-echelon form if row-echelon form\\nAll rows that contain only zeros are at the bottom of the matrix; corre-\\nspondingly, all rows that contain at least one nonzero element are on\\ntop of rows that contain only zeros.\\nLooking at nonzero rows only, the ﬁrst nonzero number from the left\\n(also called the pivot or the leading coefﬁcient ) is always strictly to the pivot\\nleading coefﬁcient right of the pivot of the row above it.\\nIn other texts, it is\\nsometimes required\\nthat the pivot is 1.Remark (Basic and Free Variables) .The variables corresponding to the\\npivots in the row-echelon form are called basic variables and the other\\nbasic variable variables are free variables . For example, in (2.45), x1,x3,x4are basic\\nfree variable variables, whereas x2,x5are free variables. ♦\\nRemark (Obtaining a Particular Solution) .The row-echelon form makes\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='241a50bd-63e0-4f0b-8dc7-d1d02b61e523', embedding=None, metadata={'page_label': '31', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Solving Systems of Linear Equations 31\\nour lives easier when we need to determine a particular solution. To do\\nthis, we express the right-hand side of the equation system using the pivot\\ncolumns, such that b=∑P\\ni=1λipi, wherepi, i= 1,...,P , are the pivot\\ncolumns. The λiare determined easiest if we start with the rightmost pivot\\ncolumn and work our way to the left.\\nIn the previous example, we would try to ﬁnd λ1,λ2,λ3so that\\nλ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n0\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ3\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n−1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n−2\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.48)\\nFrom here, we ﬁnd relatively directly that λ3= 1,λ2=−1,λ1= 2. When\\nwe put everything together, we must not forget the non-pivot columns\\nfor which we set the coefﬁcients implicitly to 0. Therefore, we get the\\nparticular solution x= [2,0,−1,1,0]⊤. ♦\\nRemark (Reduced Row Echelon Form) .An equation system is in reduced reduced\\nrow-echelon form row-echelon form (also: row-reduced echelon form orrow canonical form ) if\\nIt is in row-echelon form.\\nEvery pivot is 1.\\nThe pivot is the only nonzero entry in its column.\\n♦\\nThe reduced row-echelon form will play an important role later in Sec-\\ntion 2.3.3 because it allows us to determine the general solution of a sys-\\ntem of linear equations in a straightforward way.Gaussian\\nelimination Remark (Gaussian Elimination) . Gaussian elimination is an algorithm that\\nperforms elementary transformations to bring a system of linear equations\\ninto reduced row-echelon form. ♦\\nExample 2.7 (Reduced Row Echelon Form)\\nVerify that the following matrix is in reduced row-echelon form (the pivots\\nare in bold):\\nA=\\uf8ee\\n\\uf8f013 0 0 3\\n0 0 10 9\\n0 0 0 1−4\\uf8f9\\n\\uf8fb. (2.49)\\nThe key idea for ﬁnding the solutions of Ax=0is to look at the non-\\npivot columns , which we will need to express as a (linear) combination of\\nthe pivot columns. The reduced row echelon form makes this relatively\\nstraightforward, and we express the non-pivot columns in terms of sums\\nand multiples of the pivot columns that are on their left: The second col-\\numn is 3times the ﬁrst column (we can ignore the pivot columns on the\\nright of the second column). Therefore, to obtain 0, we need to subtract\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9216be03-a1bf-465b-a986-3ea5d7912240', embedding=None, metadata={'page_label': '32', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='32 Linear Algebra\\nthe second column from three times the ﬁrst column. Now, we look at the\\nﬁfth column, which is our second non-pivot column. The ﬁfth column can\\nbe expressed as 3times the ﬁrst pivot column, 9times the second pivot\\ncolumn, and−4times the third pivot column. We need to keep track of\\nthe indices of the pivot columns and translate this into 3times the ﬁrst col-\\numn, 0times the second column (which is a non-pivot column), 9times\\nthe third column (which is our second pivot column), and −4times the\\nfourth column (which is the third pivot column). Then we need to subtract\\nthe ﬁfth column to obtain 0. In the end, we are still solving a homogeneous\\nequation system.\\nTo summarize, all solutions of Ax=0,x∈R5are given by\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3x∈R5:x=λ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03\\n−1\\n0\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03\\n0\\n9\\n−4\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, λ 1,λ2∈R\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe. (2.50)\\n2.3.3 The Minus-1 Trick\\nIn the following, we introduce a practical trick for reading out the solu-\\ntionsxof a homogeneous system of linear equations Ax=0, where\\nA∈Rk×n,x∈Rn.\\nTo start, we assume that Ais in reduced row-echelon form without any\\nrows that just contain zeros, i.e.,\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00···01∗ ··· ∗ 0∗ ··· ∗ 0∗ ··· ∗\\n......0 0···01∗ ··· ∗.........\\n...............0...............\\n........................0......\\n0···0 0 0···0 0 0···01∗ ··· ∗\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\n(2.51)\\nwhere∗can be an arbitrary real number, with the constraints that the ﬁrst\\nnonzero entry per row must be 1and all other entries in the corresponding\\ncolumn must be 0. The columns j1,...,jkwith the pivots (marked in\\nbold) are the standard unit vectors e1,...,ek∈Rk. We extend this matrix\\nto ann×n-matrix ˜Aby addingn−krows of the form\\n[0···0−1 0···0]\\n(2.52)\\nso that the diagonal of the augmented matrix ˜Acontains either 1or−1.\\nThen, the columns of ˜Athat contain the−1as pivots are solutions of\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='94d9c14b-588e-427b-a2f8-6cf3a916128d', embedding=None, metadata={'page_label': '33', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Solving Systems of Linear Equations 33\\nthe homogeneous equation system Ax=0. To be more precise, these\\ncolumns form a basis (Section 2.6.1) of the solution space of Ax=0,\\nwhich we will later call the kernel ornull space (see Section 2.7.3). kernel\\nnull space\\nExample 2.8 (Minus-1 Trick)\\nLet us revisit the matrix in (2.49), which is already in REF:\\nA=\\uf8ee\\n\\uf8f01 3 0 0 3\\n0 0 1 0 9\\n0 0 0 1−4\\uf8f9\\n\\uf8fb. (2.53)\\nWe now augment this matrix to a 5×5matrix by adding rows of the\\nform (2.52) at the places where the pivots on the diagonal are missing\\nand obtain\\n˜A=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 3 0 0 3\\n0−10 0 0\\n0 0 1 0 9\\n0 0 0 1−4\\n0 0 0 0−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (2.54)\\nFrom this form, we can immediately read out the solutions of Ax=0by\\ntaking the columns of ˜A, which contain−1on the diagonal:\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3x∈R5:x=λ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03\\n−1\\n0\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03\\n0\\n9\\n−4\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, λ 1,λ2∈R\\uf8fc\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8fe, (2.55)\\nwhich is identical to the solution in (2.50) that we obtained by “insight”.\\nCalculating the Inverse\\nTo compute the inverse A−1ofA∈Rn×n, we need to ﬁnd a matrix X\\nthat satisﬁesAX =In. Then,X=A−1. We can write this down as\\na set of simultaneous linear equations AX =In, where we solve for\\nX= [x1|···|xn]. We use the augmented matrix notation for a compact\\nrepresentation of this set of systems of linear equations and obtain\\n[A|In]\\n⇝···⇝[In|A−1]. (2.56)\\nThis means that if we bring the augmented equation system into reduced\\nrow-echelon form, we can read out the inverse on the right-hand side of\\nthe equation system. Hence, determining the inverse of a matrix is equiv-\\nalent to solving systems of linear equations.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1d51bd9-608e-4b8b-98ba-6d71928bfdc1', embedding=None, metadata={'page_label': '34', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 Linear Algebra\\nExample 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)\\nTo determine the inverse of\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 2 0\\n1 1 0 0\\n1 2 0 1\\n1 1 1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(2.57)\\nwe write down the augmented matrix\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 2 0 1 0 0 0\\n1 1 0 0 0 1 0 0\\n1 2 0 1 0 0 1 0\\n1 1 1 1 0 0 0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nand use Gaussian elimination to bring it into reduced row-echelon form\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 0 0−1 2−2 2\\n0 1 0 0 1−1 2−2\\n0 0 1 0 1−1 1−1\\n0 0 0 1−1 0−1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\nsuch that the desired inverse is given as its right-hand side:\\nA−1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1 2−2 2\\n1−1 2−2\\n1−1 1−1\\n−1 0−1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.58)\\nWe can verify that (2.58) is indeed the inverse by performing the multi-\\nplicationAA−1and observing that we recover I4.\\n2.3.4 Algorithms for Solving a System of Linear Equations\\nIn the following, we brieﬂy discuss approaches to solving a system of lin-\\near equations of the form Ax=b. We make the assumption that a solu-\\ntion exists. Should there be no solution, we need to resort to approximate\\nsolutions, which we do not cover in this chapter. One way to solve the ap-\\nproximate problem is using the approach of linear regression, which we\\ndiscuss in detail in Chapter 9.\\nIn special cases, we may be able to determine the inverse A−1, such\\nthat the solution of Ax=bis given asx=A−1b. However, this is\\nonly possible if Ais a square matrix and invertible, which is often not the\\ncase. Otherwise, under mild assumptions (i.e., Aneeds to have linearly\\nindependent columns) we can use the transformation\\nAx=b⇐⇒A⊤Ax=A⊤b⇐⇒x= (A⊤A)−1A⊤b (2.59)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33ef4904-450f-4cf7-8a53-1483437bf001', embedding=None, metadata={'page_label': '35', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Vector Spaces 35\\nand use the Moore-Penrose pseudo-inverse (A⊤A)−1A⊤to determine the Moore-Penrose\\npseudo-inverse solution (2.59) that solves Ax=b, which also corresponds to the mini-\\nmum norm least-squares solution. A disadvantage of this approach is that\\nit requires many computations for the matrix-matrix product and comput-\\ning the inverse of A⊤A. Moreover, for reasons of numerical precision it\\nis generally not recommended to compute the inverse or pseudo-inverse.\\nIn the following, we therefore brieﬂy discuss alternative approaches to\\nsolving systems of linear equations.\\nGaussian elimination plays an important role when computing deter-\\nminants (Section 4.1), checking whether a set of vectors is linearly inde-\\npendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),\\ncomputing the rank of a matrix (Section 2.6.2), and determining a basis\\nof a vector space (Section 2.6.1). Gaussian elimination is an intuitive and\\nconstructive way to solve a system of linear equations with thousands of\\nvariables. However, for systems with millions of variables, it is impracti-\\ncal as the required number of arithmetic operations scales cubically in the\\nnumber of simultaneous equations.\\nIn practice, systems of many linear equations are solved indirectly, by ei-\\nther stationary iterative methods, such as the Richardson method, the Ja-\\ncobi method, the Gauß-Seidel method, and the successive over-relaxation\\nmethod, or Krylov subspace methods, such as conjugate gradients, gener-\\nalized minimal residual, or biconjugate gradients. We refer to the books\\nby Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann\\n(2015) for further details.\\nLetx∗be a solution of Ax=b. The key idea of these iterative methods\\nis to set up an iteration of the form\\nx(k+1)=Cx(k)+d (2.60)\\nfor suitableCanddthat reduces the residual error ∥x(k+1)−x∗∥in every\\niteration and converges to x∗. We will introduce norms ∥·∥, which allow\\nus to compute similarities between vectors, in Section 3.1.\\n2.4 Vector Spaces\\nThus far, we have looked at systems of linear equations and how to solve\\nthem (Section 2.3). We saw that systems of linear equations can be com-\\npactly represented using matrix-vector notation (2.10). In the following,\\nwe will have a closer look at vector spaces, i.e., a structured space in which\\nvectors live.\\nIn the beginning of this chapter, we informally characterized vectors as\\nobjects that can be added together and multiplied by a scalar, and they\\nremain objects of the same type. Now, we are ready to formalize this,\\nand we will start by introducing the concept of a group, which is a set\\nof elements and an operation deﬁned on these elements that keeps some\\nstructure of the set intact.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='edd3907d-ce0b-4076-a247-6f341c017382', embedding=None, metadata={'page_label': '36', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 Linear Algebra\\n2.4.1 Groups\\nGroups play an important role in computer science. Besides providing a\\nfundamental framework for operations on sets, they are heavily used in\\ncryptography, coding theory, and graphics.\\nDeﬁnition 2.7 (Group) .Consider a setGand an operation⊗:G×G→G\\ndeﬁned onG. ThenG:= (G,⊗)is called a group if the following hold: group\\nclosure1.Closure ofGunder⊗:∀x,y∈G:x⊗y∈Gassociativity\\n2.Associativity:∀x,y,z∈G: (x⊗y)⊗z=x⊗(y⊗z)neutral element\\n3.Neutral element:∃e∈G∀x∈G:x⊗e=xande⊗x=x inverse element\\n4.Inverse element:∀x∈G∃y∈G:x⊗y=eandy⊗x=e, whereeis\\nthe neutral element. We often write x−1to denote the inverse element\\nofx.\\nRemark. The inverse element is deﬁned with respect to the operation ⊗\\nand does not necessarily mean1\\nx. ♦\\nIf additionally∀x,y∈G:x⊗y=y⊗x, thenG= (G,⊗)is an Abelian Abelian group\\ngroup (commutative).\\nExample 2.10 (Groups)\\nLet us have a look at some examples of sets with associated operations\\nand see whether they are groups:\\n(Z,+)is an Abelian group.\\n(N0,+)is not a group: Although (N0,+)possesses a neutral element N0:=N∪{0}\\n(0), the inverse elements are missing.\\n(Z,·)is not a group: Although (Z,·)contains a neutral element ( 1), the\\ninverse elements for any z∈Z,z̸=±1, are missing.\\n(R,·)is not a group since 0does not possess an inverse element.\\n(R\\\\{0},·)is Abelian.\\n(Rn,+),(Zn,+),n∈Nare Abelian if +is deﬁned componentwise, i.e.,\\n(x1,···,xn) + (y1,···,yn) = (x1+y1,···,xn+yn). (2.61)\\nThen, (x1,···,xn)−1:= (−x1,···,−xn)is the inverse element and\\ne= (0,···,0)is the neutral element.\\n(Rm×n,+), the set ofm×n-matrices is Abelian (with componentwise\\naddition as deﬁned in (2.61)).\\nLet us have a closer look at (Rn×n,·), i.e., the set of n×n-matrices with\\nmatrix multiplication as deﬁned in (2.13).\\n–Closure and associativity follow directly from the deﬁnition of matrix\\nmultiplication.\\n–Neutral element: The identity matrix Inis the neutral element with\\nrespect to matrix multiplication “ ·” in(Rn×n,·).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80a58b23-8278-4cdd-b2de-89cda6c11a22', embedding=None, metadata={'page_label': '37', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Vector Spaces 37\\n–Inverse element: If the inverse exists ( Ais regular), then A−1is the\\ninverse element of A∈Rn×n, and in exactly this case (Rn×n,·)is a\\ngroup, called the general linear group .\\nDeﬁnition 2.8 (General Linear Group) .The set of regular (invertible)\\nmatricesA∈Rn×nis a group with respect to matrix multiplication as\\ndeﬁned in (2.13) and is called general linear group GL(n,R). However, general linear group\\nsince matrix multiplication is not commutative, the group is not Abelian.\\n2.4.2 Vector Spaces\\nWhen we discussed groups, we looked at sets Gand inner operations on\\nG, i.e., mappingsG×G→G that only operate on elements in G. In the\\nfollowing, we will consider sets that in addition to an inner operation +\\nalso contain an outer operation ·, the multiplication of a vector x∈Gby\\na scalarλ∈R. We can think of the inner operation as a form of addition,\\nand the outer operation as a form of scaling. Note that the inner/outer\\noperations have nothing to do with inner/outer products.\\nDeﬁnition 2.9 (Vector Space) .A real-valued vector space V= (V,+,·)is vector space\\na setVwith two operations\\n+ :V×V→V (2.62)\\n·:R×V→V (2.63)\\nwhere\\n1.(V,+)is an Abelian group\\n2. Distributivity:\\n1.∀λ∈R,x,y∈V:λ·(x+y) =λ·x+λ·y\\n2.∀λ,ψ∈R,x∈V: (λ+ψ)·x=λ·x+ψ·x\\n3. Associativity (outer operation): ∀λ,ψ∈R,x∈V:λ·(ψ·x) = (λψ)·x\\n4. Neutral element with respect to the outer operation: ∀x∈V: 1·x=x\\nThe elements x∈Vare called vectors . The neutral element of (V,+)is vector\\nthe zero vector 0= [0,..., 0]⊤, and the inner operation +is called vector vector addition\\naddition . The elements λ∈Rare called scalars and the outer operation scalar\\n·is amultiplication by scalars . Note that a scalar product is something multiplication by\\nscalars different, and we will get to this in Section 3.2.\\nRemark. A “vector multiplication” ab,a,b∈Rn, is not deﬁned. Theoret-\\nically, we could deﬁne an element-wise multiplication, such that c=ab\\nwithcj=ajbj. This “array multiplication” is common to many program-\\nming languages but makes mathematically limited sense using the stan-\\ndard rules for matrix multiplication: By treating vectors as n×1matrices\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e7d5c21-09f1-4e2a-9c18-bef5c6a37dd5', embedding=None, metadata={'page_label': '38', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='38 Linear Algebra\\n(which we usually do), we can use the matrix multiplication as deﬁned\\nin (2.13). However, then the dimensions of the vectors do not match. Only\\nthe following multiplications for vectors are deﬁned: ab⊤∈Rn×n(outer outer product\\nproduct ),a⊤b∈R(inner/scalar/dot product). ♦\\nExample 2.11 (Vector Spaces)\\nLet us have a look at some important examples:\\nV=Rn,n∈Nis a vector space with operations deﬁned as follows:\\n–Addition:x+y= (x1,...,xn)+(y1,...,yn) = (x1+y1,...,xn+yn)\\nfor allx,y∈Rn\\n–Multiplication by scalars: λx=λ(x1,...,xn) = (λx1,...,λxn)for\\nallλ∈R,x∈Rn\\nV=Rm×n,m,n∈Nis a vector space with\\n–Addition:A+B=\\uf8ee\\n\\uf8ef\\uf8f0a11+b11···a1n+b1n\\n......\\nam1+bm1···amn+bmn\\uf8f9\\n\\uf8fa\\uf8fbis deﬁned ele-\\nmentwise for all A,B∈V\\n–Multiplication by scalars: λA=\\uf8ee\\n\\uf8ef\\uf8f0λa11···λa1n\\n......\\nλam1···λamn\\uf8f9\\n\\uf8fa\\uf8fbas deﬁned in\\nSection 2.2. Remember that Rm×nis equivalent to Rmn.\\nV=C, with the standard deﬁnition of addition of complex numbers.\\nRemark. In the following, we will denote a vector space (V,+,·)byV\\nwhen +and·are the standard vector addition and scalar multiplication.\\nMoreover, we will use the notation x∈Vfor vectors inVto simplify\\nnotation. ♦\\nRemark. The vector spaces Rn,Rn×1,R1×nare only different in the way\\nwe write vectors. In the following, we will not make a distinction between\\nRnandRn×1, which allows us to write n-tuples as column vectors column vector\\nx=\\uf8ee\\n\\uf8ef\\uf8f0x1\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fb. (2.64)\\nThis simpliﬁes the notation regarding vector space operations. However,\\nwe do distinguish between Rn×1andR1×n(therow vectors ) to avoid con- row vector\\nfusion with matrix multiplication. By default, we write xto denote a col-\\numn vector, and a row vector is denoted by x⊤, the transpose ofx.♦ transpose\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16fac463-c866-451c-8d2b-78fb96769685', embedding=None, metadata={'page_label': '39', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Vector Spaces 39\\n2.4.3 Vector Subspaces\\nIn the following, we will introduce vector subspaces. Intuitively, they are\\nsets contained in the original vector space with the property that when\\nwe perform vector space operations on elements within this subspace, we\\nwill never leave it. In this sense, they are “closed”. Vector subspaces are a\\nkey idea in machine learning. For example, Chapter 10 demonstrates how\\nto use vector subspaces for dimensionality reduction.\\nDeﬁnition 2.10 (Vector Subspace) .LetV= (V,+,·)be a vector space\\nandU⊆V ,U̸=∅. ThenU= (U,+,·)is called vector subspace ofV(or vector subspace\\nlinear subspace ) ifUis a vector space with the vector space operations + linear subspace\\nand·restricted toU×U andR×U. We writeU⊆Vto denote a subspace\\nUofV.\\nIfU⊆V andVis a vector space, then Unaturally inherits many prop-\\nerties directly from Vbecause they hold for all x∈V, and in particular for\\nallx∈U⊆V . This includes the Abelian group properties, the distribu-\\ntivity, the associativity and the neutral element. To determine whether\\n(U,+,·)is a subspace of Vwe still do need to show\\n1.U̸=∅, in particular: 0∈U\\n2. Closure of U:\\na. With respect to the outer operation: ∀λ∈R∀x∈U:λx∈U.\\nb. With respect to the inner operation: ∀x,y∈U:x+y∈U.\\nExample 2.12 (Vector Subspaces)\\nLet us have a look at some examples:\\nFor every vector space V, the trivial subspaces are Vitself and{0}.\\nOnly example Din Figure 2.6 is a subspace of R2(with the usual inner/\\nouter operations). In AandC, the closure property is violated; Bdoes\\nnot contain 0.\\nThe solution set of a homogeneous system of linear equations Ax=0\\nwithnunknownsx= [x1,...,xn]⊤is a subspace of Rn.\\nThe solution of an inhomogeneous system of linear equations Ax=\\nb,b̸=0is not a subspace of Rn.\\nThe intersection of arbitrarily many subspaces is a subspace itself.\\nFigure 2.6 Not all\\nsubsets of R2are\\nsubspaces. In Aand\\nC, the closure\\nproperty is violated;\\nBdoes not contain\\n0. OnlyDis a\\nsubspace.0 0 0 0AB\\nCD\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c359ba6-f3dc-4196-b007-ad399ae7412b', embedding=None, metadata={'page_label': '40', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='40 Linear Algebra\\nRemark. Every subspace U⊆(Rn,+,·)is the solution space of a homo-\\ngeneous system of linear equations Ax=0forx∈Rn.♦\\n2.5 Linear Independence\\nIn the following, we will have a close look at what we can do with vectors\\n(elements of the vector space). In particular, we can add vectors together\\nand multiply them with scalars. The closure property guarantees that we\\nend up with another vector in the same vector space. It is possible to ﬁnd\\na set of vectors with which we can represent every vector in the vector\\nspace by adding them together and scaling them. This set of vectors is\\nabasis, and we will discuss them in Section 2.6.1. Before we get there,\\nwe will need to introduce the concepts of linear combinations and linear\\nindependence.\\nDeﬁnition 2.11 (Linear Combination) .Consider a vector space Vand a\\nﬁnite number of vectors x1,...,xk∈V. Then, every v∈Vof the form\\nv=λ1x1+···+λkxk=k∑\\ni=1λixi∈V (2.65)\\nwithλ1,...,λk∈Ris alinear combination of the vectors x1,...,xk. linear combination\\nThe0-vector can always be written as the linear combination of kvec-\\ntorsx1,...,xkbecause 0=∑k\\ni=10xiis always true. In the following,\\nwe are interested in non-trivial linear combinations of a set of vectors to\\nrepresent 0, i.e., linear combinations of vectors x1,...,xk, where not all\\ncoefﬁcients λiin (2.65) are 0.\\nDeﬁnition 2.12 (Linear (In)dependence) .Let us consider a vector space\\nVwithk∈Nandx1,...,xk∈V. If there is a non-trivial linear com-\\nbination, such that 0=∑k\\ni=1λixiwith at least one λi̸= 0, the vectors\\nx1,...,xkarelinearly dependent . If only the trivial solution exists, i.e., linearly dependent\\nλ1=...=λk= 0the vectorsx1,...,xkarelinearly independent . linearly\\nindependent\\nLinear independence is one of the most important concepts in linear\\nalgebra. Intuitively, a set of linearly independent vectors consists of vectors\\nthat have no redundancy, i.e., if we remove any of those vectors from\\nthe set, we will lose something. Throughout the next sections, we will\\nformalize this intuition more.\\nExample 2.13 (Linearly Dependent Vectors)\\nA geographic example may help to clarify the concept of linear indepen-\\ndence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is\\nmight say ,“You can get to Kigali by ﬁrst going 506 km Northwest to Kam-\\npala (Uganda) and then 374 km Southwest.”. This is sufﬁcient information\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8f61843-c66a-443c-b7fb-ddcead492ecf', embedding=None, metadata={'page_label': '41', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5 Linear Independence 41\\nto describe the location of Kigali because the geographic coordinate sys-\\ntem may be considered a two-dimensional vector space (ignoring altitude\\nand the Earth’s curved surface). The person may add, “It is about 751 km\\nWest of here.” Although this last statement is true, it is not necessary to\\nﬁnd Kigali given the previous information (see Figure 2.7 for an illus-\\ntration). In this example, the “ 506 km Northwest” vector (blue) and the\\n“374 km Southwest” vector (purple) are linearly independent. This means\\nthe Southwest vector cannot be described in terms of the Northwest vec-\\ntor, and vice versa. However, the third “ 751 km West” vector (black) is a\\nlinear combination of the other two vectors, and it makes the set of vec-\\ntors linearly dependent. Equivalently, given “ 751 km West” and “ 374 km\\nSouthwest” can be linearly combined to obtain “ 506 km Northwest”.\\nFigure 2.7\\nGeographic example\\n(with crude\\napproximations to\\ncardinal directions)\\nof linearly\\ndependent vectors\\nin a\\ntwo-dimensional\\nspace (plane).\\n506 km Northwest\\n751 km West\\n374 km Southwest\\n374 km Southwest\\nKampala\\nNairobi\\nKigali\\nRemark. The following properties are useful to ﬁnd out whether vectors\\nare linearly independent:\\nkvectors are either linearly dependent or linearly independent. There\\nis no third option.\\nIf at least one of the vectors x1,...,xkis0then they are linearly de-\\npendent. The same holds if two vectors are identical.\\nThe vectors{x1,...,xk:xi̸=0,i= 1,...,k},k⩾2, are linearly\\ndependent if and only if (at least) one of them is a linear combination\\nof the others. In particular, if one vector is a multiple of another vector,\\ni.e.,xi=λxj, λ∈Rthen the set{x1,...,xk:xi̸=0,i= 1,...,k}\\nis linearly dependent.\\nA practical way of checking whether vectors x1,...,xk∈Vare linearly\\nindependent is to use Gaussian elimination: Write all vectors as columns\\nof a matrixAand perform Gaussian elimination until the matrix is in\\nrow echelon form (the reduced row-echelon form is unnecessary here):\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4bc934f3-02de-43ee-9b57-78727401b93d', embedding=None, metadata={'page_label': '42', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='42 Linear Algebra\\n–The pivot columns indicate the vectors, which are linearly indepen-\\ndent of the vectors on the left. Note that there is an ordering of vec-\\ntors when the matrix is built.\\n–The non-pivot columns can be expressed as linear combinations of\\nthe pivot columns on their left. For instance, the row-echelon form\\n[1 3 0\\n0 0 2]\\n(2.66)\\ntells us that the ﬁrst and third columns are pivot columns. The sec-\\nond column is a non-pivot column because it is three times the ﬁrst\\ncolumn.\\nAll column vectors are linearly independent if and only if all columns\\nare pivot columns. If there is at least one non-pivot column, the columns\\n(and, therefore, the corresponding vectors) are linearly dependent.\\n♦\\nExample 2.14\\nConsider R4with\\nx1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n2\\n−3\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,x2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,x3=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n−2\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.67)\\nTo check whether they are linearly dependent, we follow the general ap-\\nproach and solve\\nλ1x1+λ2x2+λ3x3=λ1\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n2\\n−3\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb+λ3\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n−2\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=0 (2.68)\\nforλ1,...,λ 3. We write the vectors xi,i= 1,2,3, as the columns of a\\nmatrix and apply elementary row operations until we identify the pivot\\ncolumns:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1−1\\n2 1−2\\n−3 0 1\\n4 2 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb⇝···⇝\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1−1\\n0 1 0\\n0 0 1\\n0 0 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.69)\\nHere, every column of the matrix is a pivot column. Therefore, there is no\\nnon-trivial solution, and we require λ1= 0,λ2= 0,λ3= 0to solve the\\nequation system. Hence, the vectors x1,x2,x3are linearly independent.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='172e428b-fe1b-422f-b714-6f8b66cb9dce', embedding=None, metadata={'page_label': '43', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.5 Linear Independence 43\\nRemark. Consider a vector space Vwithklinearly independent vectors\\nb1,...,bkandmlinear combinations\\nx1=k∑\\ni=1λi1bi,\\n...\\nxm=k∑\\ni=1λimbi.(2.70)\\nDeﬁningB= [b1,...,bk]as the matrix whose columns are the linearly\\nindependent vectors b1,...,bk, we can write\\nxj=Bλj,λj=\\uf8ee\\n\\uf8ef\\uf8f0λ1j\\n...\\nλkj\\uf8f9\\n\\uf8fa\\uf8fb, j = 1,...,m, (2.71)\\nin a more compact form.\\nWe want to test whether x1,...,xmare linearly independent. For this\\npurpose, we follow the general approach of testing when∑m\\nj=1ψjxj=0.\\nWith (2.71), we obtain\\nm∑\\nj=1ψjxj=m∑\\nj=1ψjBλj=Bm∑\\nj=1ψjλj. (2.72)\\nThis means that{x1,...,xm}are linearly independent if and only if the\\ncolumn vectors{λ1,...,λm}are linearly independent.\\n♦\\nRemark. In a vector space V,mlinear combinations of kvectorsx1,...,xk\\nare linearly dependent if m>k . ♦\\nExample 2.15\\nConsider a set of linearly independent vectors b1,b2,b3,b4∈Rnand\\nx1=b1−2b2+b3−b4\\nx2=−4b1−2b2 + 4b4\\nx3= 2b1 + 3b2−b3−3b4\\nx4= 17b1−10b2+ 11b3+b4. (2.73)\\nAre the vectors x1,...,x4∈Rnlinearly independent? To answer this\\nquestion, we investigate whether the column vectors\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n−2\\n1\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−4\\n−2\\n0\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f02\\n3\\n−1\\n−3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f017\\n−10\\n11\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe(2.74)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='091b23da-7899-4bc4-ac13-9424f79a4372', embedding=None, metadata={'page_label': '44', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='44 Linear Algebra\\nare linearly independent. The reduced row-echelon form of the corre-\\nsponding linear equation system with coefﬁcient matrix\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−4 2 17\\n−2−2 3−10\\n1 0−1 11\\n−1 4−3 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(2.75)\\nis given as\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 0−7\\n0 1 0−15\\n0 0 1−18\\n0 0 0 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.76)\\nWe see that the corresponding linear equation system is non-trivially solv-\\nable: The last column is not a pivot column, and x4=−7x1−15x2−18x3.\\nTherefore,x1,...,x4are linearly dependent as x4can be expressed as a\\nlinear combination of x1,...,x3.\\n2.6 Basis and Rank\\nIn a vector space V, we are particularly interested in sets of vectors Athat\\npossess the property that any vector v∈Vcan be obtained by a linear\\ncombination of vectors in A. These vectors are special vectors, and in the\\nfollowing, we will characterize them.\\n2.6.1 Generating Set and Basis\\nDeﬁnition 2.13 (Generating Set and Span) .Consider a vector space V=\\n(V,+,·)and set of vectors A={x1,...,xk}⊆V . If every vector v∈\\nVcan be expressed as a linear combination of x1,...,xk,Ais called a\\ngenerating set ofV. The set of all linear combinations of vectors in Ais generating set\\ncalled the span ofA. IfAspans the vector space V, we writeV= span[A] span\\norV= span[x1,...,xk].\\nGenerating sets are sets of vectors that span vector (sub)spaces, i.e.,\\nevery vector can be represented as a linear combination of the vectors\\nin the generating set. Now, we will be more speciﬁc and characterize the\\nsmallest generating set that spans a vector (sub)space.\\nDeﬁnition 2.14 (Basis) .Consider a vector space V= (V,+,·)andA⊆\\nV. A generating set AofVis called minimal if there exists no smaller set minimal\\n˜A⊊A⊆V that spansV. Every linearly independent generating set of V\\nis minimal and is called a basis ofV. basis\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdbf3feb-fed6-4220-9e02-1b457cc86742', embedding=None, metadata={'page_label': '45', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6 Basis and Rank 45\\nLetV= (V,+,·)be a vector space and B ⊆ V,B ̸=∅. Then, the\\nfollowing statements are equivalent: A basis is a minimal\\ngenerating set and a\\nmaximal linearly\\nindependent set of\\nvectors.Bis a basis of V.\\nBis a minimal generating set.\\nBis a maximal linearly independent set of vectors in V, i.e., adding any\\nother vector to this set will make it linearly dependent.\\nEvery vectorx∈Vis a linear combination of vectors from B, and every\\nlinear combination is unique, i.e., with\\nx=k∑\\ni=1λibi=k∑\\ni=1ψibi (2.77)\\nandλi,ψi∈R,bi∈Bit follows that λi=ψi, i= 1,...,k .\\nExample 2.16\\nInR3, the canonical/standard basis is canonical basis\\nB=\\uf8f1\\n\\uf8f2\\n\\uf8f3\\uf8ee\\n\\uf8f01\\n0\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n1\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n0\\n1\\uf8f9\\n\\uf8fb\\uf8fc\\n\\uf8fd\\n\\uf8fe. (2.78)\\nDifferent bases in R3are\\nB1=\\uf8f1\\n\\uf8f2\\n\\uf8f3\\uf8ee\\n\\uf8f01\\n0\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01\\n1\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb\\uf8fc\\n\\uf8fd\\n\\uf8fe,B2=\\uf8f1\\n\\uf8f2\\n\\uf8f3\\uf8ee\\n\\uf8f00.5\\n0.8\\n0.4\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01.8\\n0.3\\n0.3\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f0−2.2\\n−1.3\\n3.5\\uf8f9\\n\\uf8fb\\uf8fc\\n\\uf8fd\\n\\uf8fe.(2.79)\\nThe set\\nA=\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n2\\n3\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f02\\n−1\\n0\\n2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n−4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\uf8fc\\n\\uf8f4\\uf8f4\\uf8fd\\n\\uf8f4\\uf8f4\\uf8fe(2.80)\\nis linearly independent, but not a generating set (and no basis) of R4:\\nFor instance, the vector [1,0,0,0]⊤cannot be obtained by a linear com-\\nbination of elements in A.\\nRemark. Every vector space Vpossesses a basisB. The preceding exam-\\nples show that there can be many bases of a vector space V, i.e., there is\\nno unique basis. However, all bases possess the same number of elements,\\nthebasis vectors . ♦ basis vector\\nWe only consider ﬁnite-dimensional vector spaces V. In this case, the\\ndimension ofVis the number of basis vectors of V, and we write dim(V).dimension\\nIfU⊆Vis a subspace of V, then dim(U)⩽dim(V)anddim(U) =\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e54e3f3-632e-4aa9-b152-cd90062ac083', embedding=None, metadata={'page_label': '46', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='46 Linear Algebra\\ndim(V)if and only if U=V. Intuitively, the dimension of a vector space\\ncan be thought of as the number of independent directions in this vector\\nspace. The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors.Remark. The dimension of a vector space is not necessarily the number\\nof elements in a vector. For instance, the vector space V= span[[0\\n1]\\n]is\\none-dimensional, although the basis vector possesses two elements. ♦\\nRemark. A basis of a subspace U= span[x1,...,xm]⊆Rncan be found\\nby executing the following steps:\\n1. Write the spanning vectors as columns of a matrix A\\n2. Determine the row-echelon form of A.\\n3. The spanning vectors associated with the pivot columns are a basis of\\nU.\\n♦\\nExample 2.17 (Determining a Basis)\\nFor a vector subspace U⊆R5, spanned by the vectors\\nx1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\n2\\n−1\\n−1\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,x2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n−1\\n1\\n2\\n−2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,x3=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03\\n−4\\n3\\n5\\n−3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,x4=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−1\\n8\\n−5\\n−6\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈R5,(2.81)\\nwe are interested in ﬁnding out which vectors x1,...,x4are a basis for U.\\nFor this, we need to check whether x1,...,x4are linearly independent.\\nTherefore, we need to solve\\n4∑\\ni=1λixi=0, (2.82)\\nwhich leads to a homogeneous system of equations with matrix\\n[x1,x2,x3,x4]=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 2 3−1\\n2−1−4 8\\n−1 1 3−5\\n−1 2 5−6\\n−1−2−3 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (2.83)\\nWith the basic transformation rules for systems of linear equations, we\\nobtain the row-echelon form\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 2 3−1\\n2−1−4 8\\n−1 1 3−5\\n−1 2 5−6\\n−1−2−3 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb⇝···⇝\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01 2 3−1\\n0 1 2−2\\n0 0 0 1\\n0 0 0 0\\n0 0 0 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='586f245d-6e27-4123-9aa4-09f780a6f01e', embedding=None, metadata={'page_label': '47', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6 Basis and Rank 47\\nSince the pivot columns indicate which set of vectors is linearly indepen-\\ndent, we see from the row-echelon form that x1,x2,x4are linearly inde-\\npendent (because the system of linear equations λ1x1+λ2x2+λ4x4=0\\ncan only be solved with λ1=λ2=λ4= 0). Therefore,{x1,x2,x4}is a\\nbasis ofU.\\n2.6.2 Rank\\nThe number of linearly independent columns of a matrix A∈Rm×n\\nequals the number of linearly independent rows and is called the rank rank\\nofAand is denoted by rk(A).\\nRemark. The rank of a matrix has some important properties:\\nrk(A) = rk(A⊤), i.e., the column rank equals the row rank.\\nThe columns of A∈Rm×nspan a subspace U⊆Rmwith dim(U) =\\nrk(A). Later we will call this subspace the image orrange . A basis of\\nUcan be found by applying Gaussian elimination to Ato identify the\\npivot columns.\\nThe rows ofA∈Rm×nspan a subspace W⊆Rnwith dim(W) =\\nrk(A). A basis ofWcan be found by applying Gaussian elimination to\\nA⊤.\\nFor allA∈Rn×nit holds thatAis regular (invertible) if and only if\\nrk(A) =n.\\nFor allA∈Rm×nand allb∈Rmit holds that the linear equation\\nsystemAx=bcan be solved if and only if rk(A) = rk(A|b), where\\nA|bdenotes the augmented system.\\nForA∈Rm×nthe subspace of solutions for Ax=0possesses dimen-\\nsionn−rk(A). Later, we will call this subspace the kernel or the null kernel\\nnull space space .\\nA matrixA∈Rm×nhasfull rank if its rank equals the largest possible full rank\\nrank for a matrix of the same dimensions. This means that the rank of\\na full-rank matrix is the lesser of the number of rows and columns, i.e.,\\nrk(A) = min(m,n). A matrix is said to be rank deﬁcient if it does not rank deﬁcient\\nhave full rank.\\n♦\\nExample 2.18 (Rank)\\nA=\\uf8ee\\n\\uf8f01 0 1\\n0 1 1\\n0 0 0\\uf8f9\\n\\uf8fb.\\nAhas two linearly independent rows/columns so that rk(A) = 2 .\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2fcc167-004a-47fb-a149-4b3f14f549c2', embedding=None, metadata={'page_label': '48', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='48 Linear Algebra\\nA=\\uf8ee\\n\\uf8f01 2 1\\n−2−3 1\\n3 5 0\\uf8f9\\n\\uf8fb.\\nWe use Gaussian elimination to determine the rank:\\n\\uf8ee\\n\\uf8f01 2 1\\n−2−3 1\\n3 5 0\\uf8f9\\n\\uf8fb⇝···⇝\\uf8ee\\n\\uf8f01 2 1\\n0 1 3\\n0 0 0\\uf8f9\\n\\uf8fb. (2.84)\\nHere, we see that the number of linearly independent rows and columns\\nis 2, such that rk(A) = 2 .\\n2.7 Linear Mappings\\nIn the following, we will study mappings on vector spaces that preserve\\ntheir structure, which will allow us to deﬁne the concept of a coordinate.\\nIn the beginning of the chapter, we said that vectors are objects that can be\\nadded together and multiplied by a scalar, and the resulting object is still\\na vector. We wish to preserve this property when applying the mapping:\\nConsider two real vector spaces V,W . A mapping Φ :V→Wpreserves\\nthe structure of the vector space if\\nΦ(x+y) = Φ(x) + Φ(y) (2.85)\\nΦ(λx) =λΦ(x) (2.86)\\nfor allx,y∈Vandλ∈R. We can summarize this in the following\\ndeﬁnition:\\nDeﬁnition 2.15 (Linear Mapping) .For vector spaces V,W , a mapping\\nΦ :V→Wis called a linear mapping (orvector space homomorphism / linear mapping\\nvector space\\nhomomorphismlinear transformation ) if\\nlinear\\ntransformation∀x,y∈V∀λ,ψ∈R: Φ(λx+ψy) =λΦ(x) +ψΦ(y). (2.87)\\nIt turns out that we can represent linear mappings as matrices (Sec-\\ntion 2.7.1). Recall that we can also collect a set of vectors as columns of a\\nmatrix. When working with matrices, we have to keep in mind what the\\nmatrix represents: a linear mapping or a collection of vectors. We will see\\nmore about linear mappings in Chapter 4. Before we continue, we will\\nbrieﬂy introduce special mappings.\\nDeﬁnition 2.16 (Injective, Surjective, Bijective) .Consider a mapping Φ :\\nV→W , whereV,Wcan be arbitrary sets. Then Φis called\\ninjective\\nInjective if∀x,y∈V: Φ(x) = Φ(y) =⇒x=y.surjective\\nSurjective ifΦ(V) =W.bijective\\nBijective if it is injective and surjective.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3aa8ce9d-071b-4aac-acf5-0f8c35d51582', embedding=None, metadata={'page_label': '49', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 49\\nIfΦis surjective, then every element in Wcan be “reached” from V\\nusing Φ. A bijective Φcan be “undone”, i.e., there exists a mapping Ψ :\\nW→V so that Ψ◦Φ(x) =x. This mapping Ψis then called the inverse\\nofΦand normally denoted by Φ−1.\\nWith these deﬁnitions, we introduce the following special cases of linear\\nmappings between vector spaces VandW:\\nisomorphism\\nIsomorphism: Φ :V→Wlinear and bijective endomorphism\\nEndomorphism: Φ :V→Vlinear automorphism\\nAutomorphism: Φ :V→Vlinear and bijective\\nWe deﬁne idV:V→V,x↦→xas the identity mapping oridentity identity mapping\\nidentity\\nautomorphismautomorphism inV.\\nExample 2.19 (Homomorphism)\\nThe mapping Φ :R2→C,Φ(x) =x1+ix2, is a homomorphism:\\nΦ([x1\\nx2]\\n+[y1\\ny2])\\n= (x1+y1) +i(x2+y2) =x1+ix2+y1+iy2\\n= Φ([x1\\nx2])\\n+ Φ([y1\\ny2])\\nΦ(\\nλ[x1\\nx2])\\n=λx1+λix2=λ(x1+ix2) =λΦ([x1\\nx2])\\n.\\n(2.88)\\nThis also justiﬁes why complex numbers can be represented as tuples in\\nR2: There is a bijective linear mapping that converts the elementwise addi-\\ntion of tuples in R2into the set of complex numbers with the correspond-\\ning addition. Note that we only showed linearity, but not the bijection.\\nTheorem 2.17 (Theorem 3.59 in Axler (2015)) .Finite-dimensional vector\\nspacesVandWare isomorphic if and only if dim(V) = dim(W).\\nTheorem 2.17 states that there exists a linear, bijective mapping be-\\ntween two vector spaces of the same dimension. Intuitively, this means\\nthat vector spaces of the same dimension are kind of the same thing, as\\nthey can be transformed into each other without incurring any loss.\\nTheorem 2.17 also gives us the justiﬁcation to treat Rm×n(the vector\\nspace ofm×n-matrices) and Rmn(the vector space of vectors of length\\nmn) the same, as their dimensions are mn, and there exists a linear, bi-\\njective mapping that transforms one into the other.\\nRemark. Consider vector spaces V,W,X . Then:\\nFor linear mappings Φ :V→WandΨ :W→X, the mapping\\nΨ◦Φ :V→Xis also linear.\\nIfΦ :V→Wis an isomorphism, then Φ−1:W→Vis an isomor-\\nphism, too.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2ec091d7-6aea-4b7d-a817-c8e6d8557903', embedding=None, metadata={'page_label': '50', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='50 Linear Algebra\\nFigure 2.8 Two\\ndifferent coordinate\\nsystems deﬁned by\\ntwo sets of basis\\nvectors. A vector x\\nhas different\\ncoordinate\\nrepresentations\\ndepending on which\\ncoordinate system is\\nchosen.x x\\ne1e2\\nb1b2\\nIfΦ :V→W,Ψ :V→Ware linear, then Φ + Ψ andλΦ, λ∈R, are\\nlinear, too.\\n♦\\n2.7.1 Matrix Representation of Linear Mappings\\nAnyn-dimensional vector space is isomorphic to Rn(Theorem 2.17). We\\nconsider a basis{b1,...,bn}of ann-dimensional vector space V. In the\\nfollowing, the order of the basis vectors will be important. Therefore, we\\nwrite\\nB= (b1,...,bn) (2.89)\\nand call this n-tuple an ordered basis ofV. ordered basis\\nRemark (Notation) .We are at the point where notation gets a bit tricky.\\nTherefore, we summarize some parts here. B= (b1,...,bn)is an ordered\\nbasis,B={b1,...,bn}is an (unordered) basis, and B= [b1,...,bn]is a\\nmatrix whose columns are the vectors b1,...,bn. ♦\\nDeﬁnition 2.18 (Coordinates) .Consider a vector space Vand an ordered\\nbasisB= (b1,...,bn)ofV. For anyx∈Vwe obtain a unique represen-\\ntation (linear combination)\\nx=α1b1+...+αnbn (2.90)\\nofxwith respect to B. Thenα1,...,αnare the coordinates ofxwith coordinate\\nrespect toB, and the vector\\nα=\\uf8ee\\n\\uf8ef\\uf8f0α1\\n...\\nαn\\uf8f9\\n\\uf8fa\\uf8fb∈Rn(2.91)\\nis the coordinate vector /coordinate representation ofxwith respect to the coordinate vector\\ncoordinate\\nrepresentationordered basis B.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6b6c389-3b65-45b4-9147-ce1d4423b8cc', embedding=None, metadata={'page_label': '51', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 51\\nA basis effectively deﬁnes a coordinate system. We are familiar with the\\nCartesian coordinate system in two dimensions, which is spanned by the\\ncanonical basis vectors e1,e2. In this coordinate system, a vector x∈R2\\nhas a representation that tells us how to linearly combine e1ande2to\\nobtainx. However, any basis of R2deﬁnes a valid coordinate system,\\nand the same vector xfrom before may have a different coordinate rep-\\nresentation in the (b1,b2)basis. In Figure 2.8, the coordinates of xwith\\nrespect to the standard basis (e1,e2)is[2,2]⊤. However, with respect to\\nthe basis (b1,b2)the same vector xis represented as [1.09,0.72]⊤, i.e.,\\nx= 1.09b1+ 0.72b2. In the following sections, we will discover how to\\nobtain this representation.\\nExample 2.20\\nLet us have a look at a geometric vector x∈R2with coordinates [2,3]⊤Figure 2.9\\nDifferent coordinate\\nrepresentations of a\\nvectorx, depending\\non the choice of\\nbasis.\\ne1e2b2\\nb1x=−1\\n2b1+5\\n2b2x= 2e1+ 3e2with respect to the standard basis (e1,e2)ofR2. This means, we can write\\nx= 2e1+ 3e2. However, we do not have to choose the standard basis to\\nrepresent this vector. If we use the basis vectors b1= [1,−1]⊤,b2= [1,1]⊤\\nwe will obtain the coordinates1\\n2[−1,5]⊤to represent the same vector with\\nrespect to (b1,b2)(see Figure 2.9).\\nRemark. For ann-dimensional vector space Vand an ordered basis B\\nofV, the mapping Φ :Rn→V,Φ(ei) =bi,i= 1,...,n, is linear\\n(and because of Theorem 2.17 an isomorphism), where (e1,...,en)is\\nthe standard basis of Rn.\\n♦\\nNow we are ready to make an explicit connection between matrices and\\nlinear mappings between ﬁnite-dimensional vector spaces.\\nDeﬁnition 2.19 (Transformation Matrix) .Consider vector spaces V,W\\nwith corresponding (ordered) bases B= (b1,...,bn)andC= (c1,...,cm).\\nMoreover, we consider a linear mapping Φ :V→W. Forj∈{1,...,n},\\nΦ(bj) =α1jc1+···+αmjcm=m∑\\ni=1αijci (2.92)\\nis the unique representation of Φ(bj)with respect to C. Then, we call the\\nm×n-matrixAΦ, whose elements are given by\\nAΦ(i,j) =αij, (2.93)\\nthetransformation matrix ofΦ(with respect to the ordered bases BofV transformation\\nmatrix andCofW).\\nThe coordinates of Φ(bj)with respect to the ordered basis CofW\\nare thej-th column of AΦ. Consider (ﬁnite-dimensional) vector spaces\\nV,W with ordered bases B,C and a linear mapping Φ :V→Wwith\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87aee3f6-8467-4062-adaf-212e6473cab1', embedding=None, metadata={'page_label': '52', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='52 Linear Algebra\\ntransformation matrix AΦ. Ifˆxis the coordinate vector of x∈Vwith\\nrespect toBandˆythe coordinate vector of y= Φ(x)∈Wwith respect\\ntoC, then\\nˆy=AΦˆx. (2.94)\\nThis means that the transformation matrix can be used to map coordinates\\nwith respect to an ordered basis in Vto coordinates with respect to an\\nordered basis in W.\\nExample 2.21 (Transformation Matrix)\\nConsider a homomorphism Φ :V→Wand ordered bases B=\\n(b1,...,b3)ofVandC= (c1,...,c4)ofW. With\\nΦ(b1) =c1−c2+ 3c3−c4\\nΦ(b2) = 2c1+c2+ 7c3+ 2c4\\nΦ(b3) = 3c2+c3+ 4c4(2.95)\\nthe transformation matrix AΦwith respect to BandCsatisﬁes Φ(bk) =∑4\\ni=1αikcifork= 1,..., 3and is given as\\nAΦ= [α1,α2,α3] =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 2 0\\n−1 1 3\\n3 7 1\\n−1 2 4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb, (2.96)\\nwhere theαj, j= 1,2,3,are the coordinate vectors of Φ(bj)with respect\\ntoC.\\nExample 2.22 (Linear Transformations of Vectors)\\nFigure 2.10 Three\\nexamples of linear\\ntransformations of\\nthe vectors shown\\nas dots in (a);\\n(b) Rotation by 45◦;\\n(c) Stretching of the\\nhorizontal\\ncoordinates by 2;\\n(d) Combination of\\nreﬂection, rotation\\nand stretching.\\n(a) Original data.\\n (b) Rotation by 45◦.\\n(c) Stretch along the\\nhorizontal axis.\\n(d) General linear\\nmapping.\\nWe consider three linear transformations of a set of vectors in R2with\\nthe transformation matrices\\nA1=[cos(π\\n4)−sin(π\\n4)\\nsin(π\\n4) cos(π\\n4)]\\n,A2=[2 0\\n0 1]\\n,A3=1\\n2[3−1\\n1−1]\\n.(2.97)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f758e85-5387-4d2b-9fef-a70279f28026', embedding=None, metadata={'page_label': '53', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 53\\nFigure 2.10 gives three examples of linear transformations of a set of vec-\\ntors. Figure 2.10(a) shows 400vectors in R2, each of which is represented\\nby a dot at the corresponding (x1,x2)-coordinates. The vectors are ar-\\nranged in a square. When we use matrix A1in (2.97) to linearly transform\\neach of these vectors, we obtain the rotated square in Figure 2.10(b). If we\\napply the linear mapping represented by A2, we obtain the rectangle in\\nFigure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d)\\nshows the original square from Figure 2.10(a) when linearly transformed\\nusingA3, which is a combination of a reﬂection, a rotation, and a stretch.\\n2.7.2 Basis Change\\nIn the following, we will have a closer look at how transformation matrices\\nof a linear mapping Φ :V→Wchange if we change the bases in Vand\\nW. Consider two ordered bases\\nB= (b1,...,bn),˜B= (˜b1,..., ˜bn) (2.98)\\nofVand two ordered bases\\nC= (c1,...,cm),˜C= (˜c1,..., ˜cm) (2.99)\\nofW. Moreover,AΦ∈Rm×nis the transformation matrix of the linear\\nmapping Φ :V→Wwith respect to the bases BandC, and ˜AΦ∈Rm×n\\nis the corresponding transformation mapping with respect to ˜Band ˜C.\\nIn the following, we will investigate how Aand˜Aare related, i.e., how/\\nwhether we can transform AΦinto ˜AΦif we choose to perform a basis\\nchange from B,C to˜B,˜C.\\nRemark. We effectively get different coordinate representations of the\\nidentity mapping idV. In the context of Figure 2.9, this would mean to\\nmap coordinates with respect to (e1,e2)onto coordinates with respect to\\n(b1,b2)without changing the vector x. By changing the basis and corre-\\nspondingly the representation of vectors, the transformation matrix with\\nrespect to this new basis can have a particularly simple form that allows\\nfor straightforward computation. ♦\\nExample 2.23 (Basis Change)\\nConsider a transformation matrix\\nA=[2 1\\n1 2]\\n(2.100)\\nwith respect to the canonical basis in R2. If we deﬁne a new basis\\nB= ([1\\n1]\\n,[1\\n−1]\\n) (2.101)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='daa4ca6d-4c5f-4a1a-b257-c04d99677eeb', embedding=None, metadata={'page_label': '54', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='54 Linear Algebra\\nwe obtain a diagonal transformation matrix\\n˜A=[3 0\\n0 1]\\n(2.102)\\nwith respect to B, which is easier to work with than A.\\nIn the following, we will look at mappings that transform coordinate\\nvectors with respect to one basis into coordinate vectors with respect to\\na different basis. We will state our main result ﬁrst and then provide an\\nexplanation.\\nTheorem 2.20 (Basis Change) .For a linear mapping Φ :V→W, ordered\\nbases\\nB= (b1,...,bn),˜B= (˜b1,..., ˜bn) (2.103)\\nofVand\\nC= (c1,...,cm),˜C= (˜c1,..., ˜cm) (2.104)\\nofW, and a transformation matrix AΦofΦwith respect to BandC, the\\ncorresponding transformation matrix ˜AΦwith respect to the bases ˜Band˜C\\nis given as\\n˜AΦ=T−1AΦS. (2.105)\\nHere,S∈Rn×nis the transformation matrix of idVthat maps coordinates\\nwith respect to ˜Bonto coordinates with respect to B, andT∈Rm×mis the\\ntransformation matrix of idWthat maps coordinates with respect to ˜Conto\\ncoordinates with respect to C.\\nProof Following Drumm and Weil (2001), we can write the vectors of\\nthe new basis ˜BofVas a linear combination of the basis vectors of B,\\nsuch that\\n˜bj=s1jb1+···+snjbn=n∑\\ni=1sijbi, j = 1,...,n. (2.106)\\nSimilarly, we write the new basis vectors ˜CofWas a linear combination\\nof the basis vectors of C, which yields\\n˜ck=t1kc1+···+tmkcm=m∑\\nl=1tlkcl, k = 1,...,m. (2.107)\\nWe deﬁneS= ((sij))∈Rn×nas the transformation matrix that maps\\ncoordinates with respect to ˜Bonto coordinates with respect to Band\\nT= ((tlk))∈Rm×mas the transformation matrix that maps coordinates\\nwith respect to ˜Conto coordinates with respect to C. In particular, the jth\\ncolumn ofSis the coordinate representation of ˜bjwith respect to Band\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eeb95189-f211-4243-b608-737ae2eb2ea1', embedding=None, metadata={'page_label': '55', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 55\\nthekth column ofTis the coordinate representation of ˜ckwith respect to\\nC. Note that both SandTare regular.\\nWe are going to look at Φ(˜bj)from two perspectives. First, applying the\\nmapping Φ, we get that for all j= 1,...,n\\nΦ(˜bj) =m∑\\nk=1˜akj˜ck\\ued19\\ued18\\ued17\\ued1a\\n∈W(2.107)=m∑\\nk=1˜akjm∑\\nl=1tlkcl=m∑\\nl=1(m∑\\nk=1tlk˜akj)\\ncl,(2.108)\\nwhere we ﬁrst expressed the new basis vectors ˜ck∈Was linear com-\\nbinations of the basis vectors cl∈Wand then swapped the order of\\nsummation.\\nAlternatively, when we express the ˜bj∈Vas linear combinations of\\nbj∈V, we arrive at\\nΦ(˜bj)(2.106)= Φ(n∑\\ni=1sijbi)\\n=n∑\\ni=1sijΦ(bi) =n∑\\ni=1sijm∑\\nl=1alicl(2.109a)\\n=m∑\\nl=1(n∑\\ni=1alisij)\\ncl, j = 1,...,n, (2.109b)\\nwhere we exploited the linearity of Φ. Comparing (2.108) and (2.109b),\\nit follows for all j= 1,...,n andl= 1,...,m that\\nm∑\\nk=1tlk˜akj=n∑\\ni=1alisij (2.110)\\nand, therefore,\\nT˜AΦ=AΦS∈Rm×n, (2.111)\\nsuch that\\n˜AΦ=T−1AΦS, (2.112)\\nwhich proves Theorem 2.20.\\nTheorem 2.20 tells us that with a basis change in V(Bis replaced with\\n˜B) andW(Cis replaced with ˜C), the transformation matrix AΦof a\\nlinear mapping Φ :V→Wis replaced by an equivalent matrix ˜AΦwith\\n˜AΦ=T−1AΦS. (2.113)\\nFigure 2.11 illustrates this relation: Consider a homomorphism Φ :V→\\nWand ordered bases B,˜BofVandC,˜CofW. The mapping ΦCBis an\\ninstantiation of Φand maps basis vectors of Bonto linear combinations\\nof basis vectors of C. Assume that we know the transformation matrix AΦ\\nofΦCBwith respect to the ordered bases B,C . When we perform a basis\\nchange from Bto˜BinVand fromCto˜CinW, we can determine the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57463e3c-46d6-4e90-a6ff-e415c17c2ba0', embedding=None, metadata={'page_label': '56', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='56 Linear Algebra\\nFigure 2.11 For a\\nhomomorphism\\nΦ :V→Wand\\nordered bases B,˜B\\nofVandC,˜CofW\\n(marked in blue),\\nwe can express the\\nmapping Φ˜C˜Bwith\\nrespect to the bases\\n˜B,˜Cequivalently as\\na composition of the\\nhomomorphisms\\nΦ˜C˜B=\\nΞ˜CC◦ΦCB◦ΨB˜B\\nwith respect to the\\nbases in the\\nsubscripts. The\\ncorresponding\\ntransformation\\nmatrices are in red.V W\\nB\\n˜B ˜CCΦ\\nΦCB\\nΦ˜C˜BΨB˜B ΞC˜C S T\\n˜AΦAΦV W\\nB\\n˜B ˜CCΦ\\nΦCB\\nΦ˜C˜BΨB˜B Ξ˜CC= Ξ−1\\nC˜CST−1\\n˜AΦAΦVector spaces\\nOrdered bases\\ncorresponding transformation matrix ˜AΦas follows: First, we ﬁnd the ma-\\ntrix representation of the linear mapping ΨB˜B:V→Vthat maps coordi-\\nnates with respect to the new basis ˜Bonto the (unique) coordinates with\\nrespect to the “old” basis B(inV). Then, we use the transformation ma-\\ntrixAΦofΦCB:V→Wto map these coordinates onto the coordinates\\nwith respect to CinW. Finally, we use a linear mapping Ξ˜CC:W→W\\nto map the coordinates with respect to Conto coordinates with respect to\\n˜C. Therefore, we can express the linear mapping Φ˜C˜Bas a composition of\\nlinear mappings that involve the “old” basis:\\nΦ˜C˜B= Ξ ˜CC◦ΦCB◦ΨB˜B= Ξ−1\\nC˜C◦ΦCB◦ΨB˜B. (2.114)\\nConcretely, we use ΨB˜B= idVandΞC˜C= idW, i.e., the identity mappings\\nthat map vectors onto themselves, but with respect to a different basis.\\nDeﬁnition 2.21 (Equivalence) .Two matrices A,˜A∈Rm×nareequivalent equivalent\\nif there exist regular matrices S∈Rn×nandT∈Rm×m, such that\\n˜A=T−1AS.\\nDeﬁnition 2.22 (Similarity) .Two matrices A,˜A∈Rn×naresimilar if similar\\nthere exists a regular matrix S∈Rn×nwith ˜A=S−1AS\\nRemark. Similar matrices are always equivalent. However, equivalent ma-\\ntrices are not necessarily similar. ♦\\nRemark. Consider vector spaces V,W,X . From the remark that follows\\nTheorem 2.17, we already know that for linear mappings Φ :V→W\\nandΨ :W→Xthe mapping Ψ◦Φ :V→Xis also linear. With\\ntransformation matrices AΦandAΨof the corresponding mappings, the\\noverall transformation matrix is AΨ◦Φ=AΨAΦ. ♦\\nIn light of this remark, we can look at basis changes from the perspec-\\ntive of composing linear mappings:\\nAΦis the transformation matrix of a linear mapping ΦCB:V→W\\nwith respect to the bases B,C .\\n˜AΦis the transformation matrix of the linear mapping Φ˜C˜B:V→W\\nwith respect to the bases ˜B,˜C.\\nSis the transformation matrix of a linear mapping ΨB˜B:V→V\\n(automorphism) that represents ˜Bin terms ofB. Normally, Ψ = idVis\\nthe identity mapping in V.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46460194-fd50-4454-884a-1937fb603bd4', embedding=None, metadata={'page_label': '57', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 57\\nTis the transformation matrix of a linear mapping ΞC˜C:W→W\\n(automorphism) that represents ˜Cin terms ofC. Normally, Ξ = idWis\\nthe identity mapping in W.\\nIf we (informally) write down the transformations just in terms of bases,\\nthenAΦ:B→C,˜AΦ:˜B→˜C,S:˜B→B,T:˜C→Cand\\nT−1:C→˜C, and\\n˜B→˜C=˜B→B→C→˜C (2.115)\\n˜AΦ=T−1AΦS. (2.116)\\nNote that the execution order in (2.116) is from right to left because vec-\\ntors are multiplied at the right-hand side so that x↦→Sx↦→AΦ(Sx)↦→\\nT−1(AΦ(Sx))=˜AΦx.\\nExample 2.24 (Basis Change)\\nConsider a linear mapping Φ :R3→R4whose transformation matrix is\\nAΦ=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 2 0\\n−1 1 3\\n3 7 1\\n−1 2 4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(2.117)\\nwith respect to the standard bases\\nB= (\\uf8ee\\n\\uf8f01\\n0\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n1\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n0\\n1\\uf8f9\\n\\uf8fb), C = (\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n0\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n0\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n0\\n0\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb). (2.118)\\nWe seek the transformation matrix ˜AΦofΦwith respect to the new bases\\n˜B= (\\uf8ee\\n\\uf8f01\\n1\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n1\\n1\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01\\n0\\n1\\uf8f9\\n\\uf8fb)∈R3,˜C= (\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n0\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n0\\n0\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb).(2.119)\\nThen,\\nS=\\uf8ee\\n\\uf8f01 0 1\\n1 1 0\\n0 1 1\\uf8f9\\n\\uf8fb,T=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1 0 1\\n1 0 1 0\\n0 1 1 0\\n0 0 0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb, (2.120)\\nwhere the ith column of Sis the coordinate representation of ˜biin\\nterms of the basis vectors of B. SinceBis the standard basis, the co-\\nordinate representation is straightforward to ﬁnd. For a general basis B,\\nwe would need to solve a linear equation system to ﬁnd the λisuch that\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5afee289-a2a3-424d-9bce-beed0062cca3', embedding=None, metadata={'page_label': '58', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='58 Linear Algebra\\n∑3\\ni=1λibi=˜bj,j= 1,..., 3. Similarly, the jth column ofTis the coordi-\\nnate representation of ˜cjin terms of the basis vectors of C.\\nTherefore, we obtain\\n˜AΦ=T−1AΦS=1\\n2\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1−1−1\\n1−1 1−1\\n−1 1 1 1\\n0 0 0 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f03 2 1\\n0 4 2\\n10 8 4\\n1 6 3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(2.121a)\\n=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−4−4−2\\n6 0 0\\n4 8 4\\n1 6 3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (2.121b)\\nIn Chapter 4, we will be able to exploit the concept of a basis change\\nto ﬁnd a basis with respect to which the transformation matrix of an en-\\ndomorphism has a particularly simple (diagonal) form. In Chapter 10, we\\nwill look at a data compression problem and ﬁnd a convenient basis onto\\nwhich we can project the data while minimizing the compression loss.\\n2.7.3 Image and Kernel\\nThe image and kernel of a linear mapping are vector subspaces with cer-\\ntain important properties. In the following, we will characterize them\\nmore carefully.\\nDeﬁnition 2.23 (Image and Kernel) .\\nForΦ :V→W, we deﬁne the kernel /null space kernel\\nnull space\\nker(Φ) := Φ−1(0W) ={v∈V: Φ(v) =0W} (2.122)\\nand the image /range image\\nrange\\nIm(Φ) := Φ( V) ={w∈W|∃v∈V: Φ(v) =w}. (2.123)\\nWe also call VandWalso the domain andcodomain ofΦ, respectively. domain\\ncodomain\\nIntuitively, the kernel is the set of vectors v∈VthatΦmaps onto the\\nneutral element 0W∈W. The image is the set of vectors w∈Wthat\\ncan be “reached” by Φfrom any vector in V. An illustration is given in\\nFigure 2.12.\\nRemark. Consider a linear mapping Φ :V→W, whereV,W are vector\\nspaces.\\nIt always holds that Φ(0V) =0Wand, therefore, 0V∈ker(Φ) . In\\nparticular, the null space is never empty.\\nIm(Φ)⊆Wis a subspace of W, and ker(Φ)⊆Vis a subspace of V.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6cbe14c5-56a1-4dc8-ba33-0b0fcc6cbec7', embedding=None, metadata={'page_label': '59', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Linear Mappings 59\\nFigure 2.12 Kernel\\nand image of a\\nlinear mapping\\nΦ :V→W.\\nIm(Φ)\\n0Wker(Φ)\\n0VΦ :V→WV W\\nΦis injective (one-to-one) if and only if ker(Φ) ={0}.\\n♦\\nRemark (Null Space and Column Space) .Let us consider A∈Rm×nand\\na linear mapping Φ :Rn→Rm,x↦→Ax.\\nForA= [a1,...,an], whereaiare the columns of A, we obtain\\nIm(Φ) ={Ax:x∈Rn}={n∑\\ni=1xiai:x1,...,xn∈R}\\n(2.124a)\\n= span[a1,...,an]⊆Rm, (2.124b)\\ni.e., the image is the span of the columns of A, also called the column column space\\nspace . Therefore, the column space (image) is a subspace of Rm, where\\nmis the “height” of the matrix.\\nrk(A) = dim(Im(Φ)) .\\nThe kernel/null space ker(Φ) is the general solution to the homoge-\\nneous system of linear equations Ax=0and captures all possible\\nlinear combinations of the elements in Rnthat produce 0∈Rm.\\nThe kernel is a subspace of Rn, wherenis the “width” of the matrix.\\nThe kernel focuses on the relationship among the columns, and we can\\nuse it to determine whether/how we can express a column as a linear\\ncombination of other columns.\\n♦\\nExample 2.25 (Image and Kernel of a Linear Mapping)\\nThe mapping\\nΦ :R4→R2,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\nx3\\nx4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb↦→[1 2−1 0\\n1 0 0 1]\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0x1\\nx2\\nx3\\nx4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb=[x1+ 2x2−x3\\nx1+x4]\\n(2.125a)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cdcab6af-9e16-4335-8309-40188bc1dedd', embedding=None, metadata={'page_label': '60', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='60 Linear Algebra\\n=x1[1\\n1]\\n+x2[2\\n0]\\n+x3[−1\\n0]\\n+x4[0\\n1]\\n(2.125b)\\nis linear. To determine Im(Φ) , we can take the span of the columns of the\\ntransformation matrix and obtain\\nIm(Φ) = span[[1\\n1]\\n,[2\\n0]\\n,[−1\\n0]\\n,[0\\n1]\\n]. (2.126)\\nTo compute the kernel (null space) of Φ, we need to solve Ax=0, i.e.,\\nwe need to solve a homogeneous equation system. To do this, we use\\nGaussian elimination to transform Ainto reduced row-echelon form:\\n[1 2−1 0\\n1 0 0 1]\\n⇝···⇝[1 0 0 1\\n0 1−1\\n2−1\\n2]\\n. (2.127)\\nThis matrix is in reduced row-echelon form, and we can use the Minus-\\n1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively,\\nwe can express the non-pivot columns (columns 3and4) as linear com-\\nbinations of the pivot columns (columns 1and2). The third column a3is\\nequivalent to−1\\n2times the second column a2. Therefore, 0=a3+1\\n2a2. In\\nthe same way, we see that a4=a1−1\\n2a2and, therefore, 0=a1−1\\n2a2−a4.\\nOverall, this gives us the kernel (null space) as\\nker(Φ) = span[\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n2\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n1\\n2\\n0\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb]. (2.128)\\nrank-nullity\\ntheorem Theorem 2.24 (Rank-Nullity Theorem) .For vector spaces V,W and a lin-\\near mapping Φ :V→Wit holds that\\ndim(ker(Φ)) + dim(Im(Φ)) = dim( V). (2.129)\\nThe rank-nullity theorem is also referred to as the fundamental theorem fundamental\\ntheorem of linear\\nmappingsof linear mappings (Axler, 2015, theorem 3.22). The following are direct\\nconsequences of Theorem 2.24:\\nIfdim(Im(Φ)) <dim(V), then ker(Φ) is non-trivial, i.e., the kernel\\ncontains more than 0Vanddim(ker(Φ))⩾1.\\nIfAΦis the transformation matrix of Φwith respect to an ordered basis\\nanddim(Im(Φ)) <dim(V), then the system of linear equations AΦx=\\n0has inﬁnitely many solutions.\\nIfdim(V) = dim(W), then the following three-way equivalence holds:\\n–Φis injective\\n–Φis surjective\\n–Φis bijective\\nsince Im(Φ)⊆W.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3dc4a21a-ef7e-4ba7-9340-b5605bb01f33', embedding=None, metadata={'page_label': '61', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.8 Afﬁne Spaces 61\\n2.8 Afﬁne Spaces\\nIn the following, we will have a closer look at spaces that are offset from\\nthe origin, i.e., spaces that are no longer vector subspaces. Moreover, we\\nwill brieﬂy discuss properties of mappings between these afﬁne spaces,\\nwhich resemble linear mappings.\\nRemark. In the machine learning literature, the distinction between linear\\nand afﬁne is sometimes not clear so that we can ﬁnd references to afﬁne\\nspaces/mappings as linear spaces/mappings. ♦\\n2.8.1 Afﬁne Subspaces\\nDeﬁnition 2.25 (Afﬁne Subspace) .LetVbe a vector space, x0∈Vand\\nU⊆Va subspace. Then the subset\\nL=x0+U:={x0+u:u∈U} (2.130a)\\n={v∈V|∃u∈U:v=x0+u}⊆V (2.130b)\\nis called afﬁne subspace orlinear manifold ofV.Uis called direction or afﬁne subspace\\nlinear manifold\\ndirectiondirection space , andx0is called support point . In Chapter 12, we refer to\\ndirection space\\nsupport pointsuch a subspace as a hyperplane .\\nhyperplaneNote that the deﬁnition of an afﬁne subspace excludes 0ifx0/∈U.\\nTherefore, an afﬁne subspace is not a (linear) subspace (vector subspace)\\nofVforx0/∈U.\\nExamples of afﬁne subspaces are points, lines, and planes in R3, which\\ndo not (necessarily) go through the origin.\\nRemark. Consider two afﬁne subspaces L=x0+Uand˜L=˜x0+˜Uof a\\nvector space V. Then,L⊆˜Lif and only if U⊆˜Uandx0−˜x0∈˜U.\\nAfﬁne subspaces are often described by parameters : Consider a k-dimen-\\nsional afﬁne space L=x0+UofV. If(b1,...,bk)is an ordered basis of\\nU, then every element x∈Lcan be uniquely described as\\nx=x0+λ1b1+...+λkbk, (2.131)\\nwhereλ1,...,λk∈R. This representation is called parametric equation parametric equation\\nofLwith directional vectors b1,...,bkandparametersλ1,...,λk.♦ parameters\\nExample 2.26 (Afﬁne Subspaces)\\nOne-dimensional afﬁne subspaces are called lines and can be written line\\nasy=x0+λb1, whereλ∈RandU= span[b1]⊆Rnis a one-\\ndimensional subspace of Rn. This means that a line is deﬁned by a sup-\\nport pointx0and a vectorb1that deﬁnes the direction. See Figure 2.13\\nfor an illustration.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b425241c-3711-4d0e-b536-9a346ac5702f', embedding=None, metadata={'page_label': '62', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='62 Linear Algebra\\nTwo-dimensional afﬁne subspaces of Rnare called planes . The para- plane\\nmetric equation for planes is y=x0+λ1b1+λ2b2, whereλ1,λ2∈R\\nandU= span[b1,b2]⊆Rn. This means that a plane is deﬁned by a\\nsupport point x0and two linearly independent vectors b1,b2that span\\nthe direction space.\\nInRn, the (n−1)-dimensional afﬁne subspaces are called hyperplanes , hyperplane\\nand the corresponding parametric equation is y=x0+∑n−1\\ni=1λibi,\\nwhereb1,...,bn−1form a basis of an (n−1)-dimensional subspace\\nUofRn. This means that a hyperplane is deﬁned by a support point\\nx0and(n−1)linearly independent vectors b1,...,bn−1that span the\\ndirection space. In R2, a line is also a hyperplane. In R3, a plane is also\\na hyperplane.\\nFigure 2.13 Lines\\nare afﬁne subspaces.\\nVectorsyon a line\\nx0+λb1lie in an\\nafﬁne subspace L\\nwith support point\\nx0and direction b1.\\n0x0\\nb1yL=x0+λb1\\nRemark (Inhomogeneous systems of linear equations and afﬁne subspaces) .\\nForA∈Rm×nandx∈Rm, the solution of the system of linear equa-\\ntionsAλ =xis either the empty set or an afﬁne subspace of Rnof\\ndimensionn−rk(A). In particular, the solution of the linear equation\\nλ1b1+...+λnbn=x, where (λ1,...,λn)̸= (0,..., 0), is a hyperplane\\ninRn.\\nInRn, everyk-dimensional afﬁne subspace is the solution of an inho-\\nmogeneous system of linear equations Ax=b, whereA∈Rm×n,b∈\\nRmandrk(A) =n−k. Recall that for homogeneous equation systems\\nAx=0the solution was a vector subspace, which we can also think of\\nas a special afﬁne space with support point x0=0. ♦\\n2.8.2 Afﬁne Mappings\\nSimilar to linear mappings between vector spaces, which we discussed\\nin Section 2.7, we can deﬁne afﬁne mappings between two afﬁne spaces.\\nLinear and afﬁne mappings are closely related. Therefore, many properties\\nthat we already know from linear mappings, e.g., that the composition of\\nlinear mappings is a linear mapping, also hold for afﬁne mappings.\\nDeﬁnition 2.26 (Afﬁne Mapping) .For two vector spaces V,W , a linear\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4fa1adf-3368-4d69-8156-6a17b79aee95', embedding=None, metadata={'page_label': '63', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.9 Further Reading 63\\nmapping Φ :V→W, anda∈W, the mapping\\nφ:V→W (2.132)\\nx↦→a+ Φ(x) (2.133)\\nis an afﬁne mapping fromVtoW. The vectorais called the translation afﬁne mapping\\ntranslation vector vector ofφ.\\nEvery afﬁne mapping φ:V→Wis also the composition of a linear\\nmapping Φ :V→Wand a translation τ:W→WinW, such that\\nφ=τ◦Φ. The mappings Φandτare uniquely determined.\\nThe composition φ′◦φof afﬁne mappings φ:V→W,φ′:W→Xis\\nafﬁne.\\nAfﬁne mappings keep the geometric structure invariant. They also pre-\\nserve the dimension and parallelism.\\n2.9 Further Reading\\nThere are many resources for learning linear algebra, including the text-\\nbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and\\nMehrmann (2015). There are also several online resources that we men-\\ntioned in the introduction to this chapter. We only covered Gaussian elim-\\nination here, but there are many other approaches for solving systems of\\nlinear equations, and we refer to numerical linear algebra textbooks by\\nStoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and\\nJohnson (2013) for an in-depth discussion.\\nIn this book, we distinguish between the topics of linear algebra (e.g.,\\nvectors, matrices, linear independence, basis) and topics related to the\\ngeometry of a vector space. In Chapter 3, we will introduce the inner\\nproduct, which induces a norm. These concepts allow us to deﬁne angles,\\nlengths and distances, which we will use for orthogonal projections. Pro-\\njections turn out to be key in many machine learning algorithms, such as\\nlinear regression and principal component analysis, both of which we will\\ncover in Chapters 9 and 10, respectively.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b955515-b218-4b86-a059-390c8382792f', embedding=None, metadata={'page_label': '64', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='64 Linear Algebra\\nExercises\\n2.1 We consider (R\\\\{−1},⋆), where\\na⋆b :=ab+a+b, a,b∈R\\\\{−1} (2.134)\\na. Show that (R\\\\{−1},⋆)is an Abelian group.\\nb. Solve\\n3⋆x⋆x = 15\\nin the Abelian group (R\\\\{−1},⋆), where⋆is deﬁned in (2.134).\\n2.2 Letnbe inN\\\\{0}. Letk,xbe inZ. We deﬁne the congruence class ¯kof the\\nintegerkas the set\\nk={x∈Z|x−k= 0 (modn)}\\n={x∈Z|∃a∈Z: (x−k=n·a)}.\\nWe now deﬁne Z/nZ(sometimes written Zn) as the set of all congruence\\nclasses modulo n. Euclidean division implies that this set is a ﬁnite set con-\\ntainingnelements:\\nZn={0,1,...,n−1}\\nFor alla,b∈Zn, we deﬁne\\na⊕b:=a+b\\na. Show that (Zn,⊕)is a group. Is it Abelian?\\nb. We now deﬁne another operation ⊗for allaandbinZnas\\na⊗b=a×b, (2.135)\\nwherea×brepresents the usual multiplication in Z.\\nLetn= 5. Draw the times table of the elements of Z5\\\\{0}under⊗, i.e.,\\ncalculate the products a⊗bfor allaandbinZ5\\\\{0}.\\nHence, show that Z5\\\\{0}is closed under⊗and possesses a neutral\\nelement for⊗. Display the inverse of all elements in Z5\\\\{0}under⊗.\\nConclude that (Z5\\\\{0},⊗)is an Abelian group.\\nc. Show that (Z8\\\\{0},⊗)is not a group.\\nd. We recall that the B ´ezout theorem states that two integers aandbare\\nrelatively prime (i.e., gcd(a,b) = 1 ) if and only if there exist two integers\\nuandvsuch thatau+bv= 1. Show that (Zn\\\\{0},⊗)is a group if and\\nonly ifn∈N\\\\{0}is prime.\\n2.3 Consider the set Gof3×3matrices deﬁned as follows:\\nG=\\uf8f1\\n\\uf8f2\\n\\uf8f3\\uf8ee\\n\\uf8f01x z\\n0 1y\\n0 0 1\\uf8f9\\n\\uf8fb∈R3×3⏐⏐⏐⏐⏐⏐x,y,z∈R\\uf8fc\\n\\uf8fd\\n\\uf8fe\\nWe deﬁne·as the standard matrix multiplication.\\nIs(G,·)a group? If yes, is it Abelian? Justify your answer.\\n2.4 Compute the following matrix products, if possible:\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ceb52df-884e-4e58-9c11-79a86743c8fc', embedding=None, metadata={'page_label': '65', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 65\\na.\\n\\uf8ee\\n\\uf8f01 2\\n4 5\\n7 8\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01 1 0\\n0 1 1\\n1 0 1\\uf8f9\\n\\uf8fb\\nb.\\n\\uf8ee\\n\\uf8f01 2 3\\n4 5 6\\n7 8 9\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01 1 0\\n0 1 1\\n1 0 1\\uf8f9\\n\\uf8fb\\nc.\\n\\uf8ee\\n\\uf8f01 1 0\\n0 1 1\\n1 0 1\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01 2 3\\n4 5 6\\n7 8 9\\uf8f9\\n\\uf8fb\\nd.\\n[\\n1 2 1 2\\n4 1−1−4]\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 3\\n1−1\\n2 1\\n5 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\ne.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 3\\n1−1\\n2 1\\n5 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb[\\n1 2 1 2\\n4 1−1−4]\\n2.5 Find the set Sof all solutions in xof the following inhomogeneous linear\\nsystemsAx=b, whereAandbare deﬁned as follows:\\na.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1−1−1\\n2 5−7−5\\n2−1 1 3\\n5 2−4 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,b=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n−2\\n4\\n6\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nb.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01−1 0 0 1\\n1 1 0−3 0\\n2−1 0 1−1\\n−1 2 0−2−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,b=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f03\\n6\\n5\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n2.6 Using Gaussian elimination, ﬁnd all solutions of the inhomogeneous equa-\\ntion systemAx=bwith\\nA=\\uf8ee\\n\\uf8f00 1 0 0 1 0\\n0 0 0 1 1 0\\n0 1 0 0 0 1\\uf8f9\\n\\uf8fb,b=\\uf8ee\\n\\uf8f02\\n−1\\n1\\uf8f9\\n\\uf8fb.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f8a0348-f00c-4dc5-b715-1e296a905bd5', embedding=None, metadata={'page_label': '66', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='66 Linear Algebra\\n2.7 Find all solutions in x=\\uf8ee\\n\\uf8f0x1\\nx2\\nx3\\uf8f9\\n\\uf8fb∈R3of the equation system Ax= 12x,\\nwhere\\nA=\\uf8ee\\n\\uf8f06 4 3\\n6 0 9\\n0 8 0\\uf8f9\\n\\uf8fb\\nand∑3\\ni=1xi= 1.\\n2.8 Determine the inverses of the following matrices if possible:\\na.\\nA=\\uf8ee\\n\\uf8f02 3 4\\n3 4 5\\n4 5 6\\uf8f9\\n\\uf8fb\\nb.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 1 0\\n0 1 1 0\\n1 1 0 1\\n1 1 1 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n2.9 Which of the following sets are subspaces of R3?\\na.A={(λ,λ+µ3,λ−µ3)|λ,µ∈R}\\nb.B={(λ2,−λ2,0)|λ∈R}\\nc. Letγbe inR.\\nC={(ξ1,ξ2,ξ3)∈R3|ξ1−2ξ2+ 3ξ3=γ}\\nd.D={(ξ1,ξ2,ξ3)∈R3|ξ2∈Z}\\n2.10 Are the following sets of vectors linearly independent?\\na.\\nx1=\\uf8ee\\n\\uf8f02\\n−1\\n3\\uf8f9\\n\\uf8fb,x2=\\uf8ee\\n\\uf8f01\\n1\\n−2\\uf8f9\\n\\uf8fb,x3=\\uf8ee\\n\\uf8f03\\n−3\\n8\\uf8f9\\n\\uf8fb\\nb.\\nx1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\n2\\n1\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,x2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\n1\\n0\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,x3=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\n0\\n0\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n2.11 Write\\ny=\\uf8ee\\n\\uf8f01\\n−2\\n5\\uf8f9\\n\\uf8fb\\nas linear combination of\\nx1=\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb,x2=\\uf8ee\\n\\uf8f01\\n2\\n3\\uf8f9\\n\\uf8fb,x3=\\uf8ee\\n\\uf8f02\\n−1\\n1\\uf8f9\\n\\uf8fb\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3ba43c9-1077-446a-bf88-a074eb47dba4', embedding=None, metadata={'page_label': '67', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 67\\n2.12 Consider two subspaces of R4:\\nU1= span[\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n−3\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f02\\n−1\\n0\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n1\\n−1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb], U 2= span[\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−1\\n−2\\n2\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f02\\n−2\\n0\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−3\\n6\\n−2\\n−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb].\\nDetermine a basis of U1∩U2.\\n2.13 Consider two subspaces U1andU2, whereU1is the solution space of the\\nhomogeneous equation system A1x=0andU2is the solution space of the\\nhomogeneous equation system A2x=0with\\nA1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 1\\n1−2−1\\n2 1 3\\n1 0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,A2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f03−3 0\\n1 2 3\\n7−5 2\\n3−1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.\\na. Determine the dimension of U1,U2.\\nb. Determine bases of U1andU2.\\nc. Determine a basis of U1∩U2.\\n2.14 Consider two subspaces U1andU2, whereU1is spanned by the columns of\\nA1andU2is spanned by the columns of A2with\\nA1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0 1\\n1−2−1\\n2 1 3\\n1 0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb,A2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f03−3 0\\n1 2 3\\n7−5 2\\n3−1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.\\na. Determine the dimension of U1,U2\\nb. Determine bases of U1andU2\\nc. Determine a basis of U1∩U2\\n2.15 LetF={(x,y,z )∈R3|x+y−z= 0}andG={(a−b,a+b,a−3b)|a,b∈R}.\\na. Show that FandGare subspaces of R3.\\nb. Calculate F∩Gwithout resorting to any basis vector.\\nc. Find one basis for Fand one for G, calculateF∩Gusing the basis vectors\\npreviously found and check your result with the previous question.\\n2.16 Are the following mappings linear?\\na. Leta,b∈R.\\nΦ :L1([a,b])→R\\nf↦→Φ(f) =∫b\\naf(x)dx,\\nwhereL1([a,b])denotes the set of integrable functions on [a,b].\\nb.\\nΦ :C1→C0\\nf↦→Φ(f) =f′,\\nwhere fork⩾1,Ckdenotes the set of ktimes continuously differen-\\ntiable functions, and C0denotes the set of continuous functions.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db889910-76b7-4d04-91a9-3be6b48c37d9', embedding=None, metadata={'page_label': '68', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='68 Linear Algebra\\nc.\\nΦ :R→R\\nx↦→Φ(x) = cos(x)\\nd.\\nΦ :R3→R2\\nx↦→[\\n1 2 3\\n1 4 3]\\nx\\ne. Letθbe in [0,2π[and\\nΦ :R2→R2\\nx↦→[\\ncos(θ) sin(θ)\\n−sin(θ) cos(θ)]\\nx\\n2.17 Consider the linear mapping\\nΦ :R3→R4\\nΦ\\uf8eb\\n\\uf8ed\\uf8ee\\n\\uf8f0x1\\nx2\\nx3\\uf8f9\\n\\uf8fb\\uf8f6\\n\\uf8f8=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f03x1+ 2x2+x3\\nx1+x2+x3\\nx1−3x2\\n2x1+ 3x2+x3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nFind the transformation matrix AΦ.\\nDetermine rk(AΦ).\\nCompute the kernel and image of Φ. What are dim(ker(Φ)) anddim(Im(Φ)) ?\\n2.18 LetEbe a vector space. Let fandgbe two automorphisms on Esuch that\\nf◦g= idE(i.e.,f◦gis the identity mapping idE). Show that ker(f) =\\nker(g◦f),Im(g) = Im(g◦f)and that ker(f)∩Im(g) ={0E}.\\n2.19 Consider an endomorphism Φ :R3→R3whose transformation matrix\\n(with respect to the standard basis in R3) is\\nAΦ=\\uf8ee\\n\\uf8f01 1 0\\n1−1 0\\n1 1 1\\uf8f9\\n\\uf8fb.\\na. Determine ker(Φ) andIm(Φ) .\\nb. Determine the transformation matrix ˜AΦwith respect to the basis\\nB= (\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01\\n2\\n1\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f01\\n0\\n0\\uf8f9\\n\\uf8fb),\\ni.e., perform a basis change toward the new basis B.\\n2.20 Let us consider b1,b2,b′\\n1,b′\\n2,4vectors of R2expressed in the standard basis\\nofR2as\\nb1=[\\n2\\n1]\\n,b2=[\\n−1\\n−1]\\n,b′\\n1=[\\n2\\n−2]\\n,b′\\n2=[\\n1\\n1]\\nand let us deﬁne two ordered bases B= (b1,b2)andB′= (b′\\n1,b′\\n2)ofR2.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3363c38-feb9-4533-a151-02c17c7b7a8d', embedding=None, metadata={'page_label': '69', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 69\\na. Show that BandB′are two bases of R2and draw those basis vectors.\\nb. Compute the matrix P1that performs a basis change from B′toB.\\nc. We consider c1,c2,c3, three vectors of R3deﬁned in the standard basis\\nofR3as\\nc1=\\uf8ee\\n\\uf8f01\\n2\\n−1\\uf8f9\\n\\uf8fb,c2=\\uf8ee\\n\\uf8f00\\n−1\\n2\\uf8f9\\n\\uf8fb,c3=\\uf8ee\\n\\uf8f01\\n0\\n−1\\uf8f9\\n\\uf8fb\\nand we deﬁne C= (c1,c2,c3).\\n(i) Show that Cis a basis of R3, e.g., by using determinants (see\\nSection 4.1).\\n(ii) Let us call C′= (c′\\n1,c′\\n2,c′\\n3)the standard basis of R3. Determine\\nthe matrixP2that performs the basis change from CtoC′.\\nd. We consider a homomorphism Φ :R2−→R3, such that\\nΦ(b1+b2) =c2+c3\\nΦ(b1−b2) = 2c1−c2+ 3c3\\nwhereB= (b1,b2)andC= (c1,c2,c3)are ordered bases of R2andR3,\\nrespectively.\\nDetermine the transformation matrix AΦofΦwith respect to the or-\\ndered bases BandC.\\ne. Determine A′, the transformation matrix of Φwith respect to the bases\\nB′andC′.\\nf. Let us consider the vector x∈R2whose coordinates in B′are[2,3]⊤.\\nIn other words, x= 2b′\\n1+ 3b′\\n2.\\n(i) Calculate the coordinates of xinB.\\n(ii) Based on that, compute the coordinates of Φ(x)expressed in C.\\n(iii) Then, write Φ(x)in terms ofc′\\n1,c′\\n2,c′\\n3.\\n(iv) Use the representation of xinB′and the matrix A′to ﬁnd this\\nresult directly.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1ae26df-fadc-4f22-ad38-8c0d4d6cd762', embedding=None, metadata={'page_label': '70', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3\\nAnalytic Geometry\\nIn Chapter 2, we studied vectors, vector spaces, and linear mappings at\\na general but abstract level. In this chapter, we will add some geomet-\\nric interpretation and intuition to all of these concepts. In particular, we\\nwill look at geometric vectors and compute their lengths and distances\\nor angles between two vectors. To be able to do this, we equip the vec-\\ntor space with an inner product that induces the geometry of the vector\\nspace. Inner products and their corresponding norms and metrics capture\\nthe intuitive notions of similarity and distances, which we use to develop\\nthe support vector machine in Chapter 12. We will then use the concepts\\nof lengths and angles between vectors to discuss orthogonal projections,\\nwhich will play a central role when we discuss principal component anal-\\nysis in Chapter 10 and regression via maximum likelihood estimation in\\nChapter 9. Figure 3.1 gives an overview of how concepts in this chapter\\nare related and how they are connected to other chapters of the book.\\nFigure 3.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.Inner product\\nNorm\\nLengthsOrthogonal\\nprojectionAngles Rotations\\nChapter 4\\nMatrix\\ndecompositionChapter 10\\nDimensionality\\nreductionChapter 9\\nRegressionChapter 12\\nClassiﬁcationinduces\\n70\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f07ffb8-04be-434f-ad76-4dbf5849475a', embedding=None, metadata={'page_label': '71', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.1 Norms 71\\nFigure 3.3 For\\ndifferent norms, the\\nred lines indicate\\nthe set of vectors\\nwith norm 1. Left:\\nManhattan norm;\\nRight: Euclidean\\ndistance.\\n1\\n1\\n1\\n1\\n∥x∥1= 1\\n∥x∥2= 1\\n3.1 Norms\\nWhen we think of geometric vectors, i.e., directed line segments that start\\nat the origin, then intuitively the length of a vector is the distance of the\\n“end” of this directed line segment from the origin. In the following, we\\nwill discuss the notion of the length of vectors using the concept of a norm.\\nDeﬁnition 3.1 (Norm) .Anorm on a vector space Vis a function norm\\n∥·∥:V→R, (3.1)\\nx↦→∥x∥, (3.2)\\nwhich assigns each vector xitslength∥x∥∈R, such that for all λ∈R length\\nandx,y∈Vthe following hold:\\nabsolutely\\nhomogeneous Absolutely homogeneous: ∥λx∥=|λ|∥x∥\\ntriangle inequality Triangle inequality: ∥x+y∥⩽∥x∥+∥y∥\\npositive deﬁnite Positive deﬁnite:∥x∥⩾0and∥x∥= 0⇐⇒x=0\\nFigure 3.2 Triangle\\ninequality.\\na\\nb\\nc≤a+b In geometric terms, the triangle inequality states that for any triangle,\\nthe sum of the lengths of any two sides must be greater than or equal\\nto the length of the remaining side; see Figure 3.2 for an illustration.\\nDeﬁnition 3.1 is in terms of a general vector space V(Section 2.4), but\\nin this book we will only consider a ﬁnite-dimensional vector space Rn.\\nRecall that for a vector x∈Rnwe denote the elements of the vector using\\na subscript, that is, xiis theithelement of the vector x.\\nExample 3.1 (Manhattan Norm)\\nThe Manhattan norm onRnis deﬁned for x∈Rnas Manhattan norm\\n∥x∥1:=n∑\\ni=1|xi|, (3.3)\\nwhere|·|is the absolute value. The left panel of Figure 3.3 shows all\\nvectorsx∈R2with∥x∥1= 1. The Manhattan norm is also called ℓ1ℓ1norm\\nnorm .\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='72f4da7c-4951-4f78-91f4-388ba2a4518b', embedding=None, metadata={'page_label': '72', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='72 Analytic Geometry\\nExample 3.2 (Euclidean Norm)\\nThe Euclidean norm ofx∈Rnis deﬁned as Euclidean norm\\n∥x∥2:=\\ued6a\\ued6b\\ued6b√n∑\\ni=1x2\\ni=√\\nx⊤x (3.4)\\nand computes the Euclidean distance ofxfrom the origin. The right panel Euclidean distance\\nof Figure 3.3 shows all vectors x∈R2with∥x∥2= 1. The Euclidean\\nnorm is also called ℓ2norm . ℓ2norm\\nRemark. Throughout this book, we will use the Euclidean norm (3.4) by\\ndefault if not stated otherwise. ♦\\n3.2 Inner Products\\nInner products allow for the introduction of intuitive geometrical con-\\ncepts, such as the length of a vector and the angle or distance between\\ntwo vectors. A major purpose of inner products is to determine whether\\nvectors are orthogonal to each other.\\n3.2.1 Dot Product\\nWe may already be familiar with a particular type of inner product, the\\nscalar product /dot product inRn, which is given by scalar product\\ndot product\\nx⊤y=n∑\\ni=1xiyi. (3.5)\\nWe will refer to this particular inner product as the dot product in this\\nbook. However, inner products are more general concepts with speciﬁc\\nproperties, which we will now introduce.\\n3.2.2 General Inner Products\\nRecall the linear mapping from Section 2.7, where we can rearrange the\\nmapping with respect to addition and multiplication with a scalar. A bi- bilinear mapping\\nlinear mapping Ωis a mapping with two arguments, and it is linear in\\neach argument, i.e., when we look at a vector space Vthen it holds that\\nfor allx,y,z∈V, λ,ψ∈Rthat\\nΩ(λx+ψy,z) =λΩ(x,z) +ψΩ(y,z) (3.6)\\nΩ(x,λy+ψz) =λΩ(x,y) +ψΩ(x,z). (3.7)\\nHere, (3.6) asserts that Ωis linear in the ﬁrst argument, and (3.7) asserts\\nthatΩis linear in the second argument (see also (2.87)).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc459794-be9e-4dc2-87df-e08251b8b2bd', embedding=None, metadata={'page_label': '73', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.2 Inner Products 73\\nDeﬁnition 3.2. LetVbe a vector space and Ω :V×V→Rbe a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\nΩis called symmetric ifΩ(x,y) = Ω(y,x)for allx,y∈V, i.e., the symmetric\\norder of the arguments does not matter.\\nΩis called positive deﬁnite if positive deﬁnite\\n∀x∈V\\\\{0}: Ω(x,x)>0,Ω(0,0) = 0. (3.8)\\nDeﬁnition 3.3. LetVbe a vector space and Ω :V×V→Rbe a bilinear\\nmapping that takes two vectors and maps them onto a real number. Then\\nA positive deﬁnite, symmetric bilinear mapping Ω :V×V→Ris called\\naninner product onV. We typically write ⟨x,y⟩instead of Ω(x,y). inner product\\nThe pair (V,⟨·,·⟩)is called an inner product space or (real) vector space inner product space\\nvector space with\\ninner productwith inner product . If we use the dot product deﬁned in (3.5), we call\\n(V,⟨·,·⟩)aEuclidean vector space .\\nEuclidean vector\\nspace We will refer to these spaces as inner product spaces in this book.\\nExample 3.3 (Inner Product That Is Not the Dot Product)\\nConsiderV=R2. If we deﬁne\\n⟨x,y⟩:=x1y1−(x1y2+x2y1) + 2x2y2 (3.9)\\nthen⟨·,·⟩is an inner product but different from the dot product. The proof\\nwill be an exercise.\\n3.2.3 Symmetric, Positive Deﬁnite Matrices\\nSymmetric, positive deﬁnite matrices play an important role in machine\\nlearning, and they are deﬁned via the inner product. In Section 4.3, we\\nwill return to symmetric, positive deﬁnite matrices in the context of matrix\\ndecompositions. The idea of symmetric positive semideﬁnite matrices is\\nkey in the deﬁnition of kernels (Section 12.4).\\nConsider an n-dimensional vector space Vwith an inner product ⟨·,·⟩:\\nV×V→R(see Deﬁnition 3.3) and an ordered basis B= (b1,...,bn)of\\nV. Recall from Section 2.6.1 that any vectors x,y∈Vcan be written as\\nlinear combinations of the basis vectors so that x=∑n\\ni=1ψibi∈Vand\\ny=∑n\\nj=1λjbj∈Vfor suitable ψi,λj∈R. Due to the bilinearity of the\\ninner product, it holds for all x,y∈Vthat\\n⟨x,y⟩=⟨n∑\\ni=1ψibi,n∑\\nj=1λjbj⟩\\n=n∑\\ni=1n∑\\nj=1ψi⟨bi,bj⟩λj=ˆx⊤Aˆy,(3.10)\\nwhereAij:=⟨bi,bj⟩andˆx,ˆyare the coordinates of xandywith respect\\nto the basis B. This implies that the inner product ⟨·,·⟩is uniquely deter-\\nmined through A. The symmetry of the inner product also means that A\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce9ead01-ca2c-4a47-9c80-d7f15d4d59b9', embedding=None, metadata={'page_label': '74', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='74 Analytic Geometry\\nis symmetric. Furthermore, the positive deﬁniteness of the inner product\\nimplies that\\n∀x∈V\\\\{0}:x⊤Ax>0. (3.11)\\nDeﬁnition 3.4 (Symmetric, Positive Deﬁnite Matrix) .A symmetric matrix\\nA∈Rn×nthat satisﬁes (3.11) is called symmetric, positive deﬁnite , or symmetric, positive\\ndeﬁnite justpositive deﬁnite . If only⩾holds in (3.11), then Ais called symmetric,\\npositive deﬁnite\\nsymmetric, positive\\nsemideﬁnitepositive semideﬁnite .\\nExample 3.4 (Symmetric, Positive Deﬁnite Matrices)\\nConsider the matrices\\nA1=[9 6\\n6 5]\\n,A2=[9 6\\n6 3]\\n. (3.12)\\nA1is positive deﬁnite because it is symmetric and\\nx⊤A1x=[x1x2][9 6\\n6 5][x1\\nx2]\\n(3.13a)\\n= 9x2\\n1+ 12x1x2+ 5x2\\n2= (3x1+ 2x2)2+x2\\n2>0 (3.13b)\\nfor allx∈V\\\\{0}. In contrast,A2is symmetric but not positive deﬁnite\\nbecausex⊤A2x= 9x2\\n1+ 12x1x2+ 3x2\\n2= (3x1+ 2x2)2−x2\\n2can be less\\nthan 0, e.g., forx= [2,−3]⊤.\\nIfA∈Rn×nis symmetric, positive deﬁnite, then\\n⟨x,y⟩=ˆx⊤Aˆy (3.14)\\ndeﬁnes an inner product with respect to an ordered basis B, where ˆxand\\nˆyare the coordinate representations of x,y∈Vwith respect to B.\\nTheorem 3.5. For a real-valued, ﬁnite-dimensional vector space Vand an\\nordered basis BofV, it holds that⟨·,·⟩:V×V→Ris an inner product if\\nand only if there exists a symmetric, positive deﬁnite matrix A∈Rn×nwith\\n⟨x,y⟩=ˆx⊤Aˆy. (3.15)\\nThe following properties hold if A∈Rn×nis symmetric and positive\\ndeﬁnite:\\nThe null space (kernel) of Aconsists only of 0becausex⊤Ax>0for\\nallx̸=0. This implies that Ax̸=0ifx̸=0.\\nThe diagonal elements aiiofAare positive because aii=e⊤\\niAei>0,\\nwhereeiis theith vector of the standard basis in Rn.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5bdc4f54-bf14-4dfa-aa63-f587a2ec20d6', embedding=None, metadata={'page_label': '75', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3 Lengths and Distances 75\\n3.3 Lengths and Distances\\nIn Section 3.1, we already discussed norms that we can use to compute\\nthe length of a vector. Inner products and norms are closely related in the\\nsense that any inner product induces a norm Inner products\\ninduce norms.\\n∥x∥:=√\\n⟨x,x⟩ (3.16)\\nin a natural way, such that we can compute lengths of vectors using the in-\\nner product. However, not every norm is induced by an inner product. The\\nManhattan norm (3.3) is an example of a norm without a corresponding\\ninner product. In the following, we will focus on norms that are induced\\nby inner products and introduce geometric concepts, such as lengths, dis-\\ntances, and angles.\\nRemark (Cauchy-Schwarz Inequality) .For an inner product vector space\\n(V,⟨·,·⟩)the induced norm ∥·∥satisﬁes the Cauchy-Schwarz inequality Cauchy-Schwarz\\ninequality\\n|⟨x,y⟩|⩽∥x∥∥y∥. (3.17)\\n♦\\nExample 3.5 (Lengths of Vectors Using Inner Products)\\nIn geometry, we are often interested in lengths of vectors. We can now use\\nan inner product to compute them using (3.16). Let us take x= [1,1]⊤∈\\nR2. If we use the dot product as the inner product, with (3.16) we obtain\\n∥x∥=√\\nx⊤x=√\\n12+ 12=√\\n2 (3.18)\\nas the length of x. Let us now choose a different inner product:\\n⟨x,y⟩:=x⊤[1−1\\n2\\n−1\\n21]\\ny=x1y1−1\\n2(x1y2+x2y1) +x2y2.(3.19)\\nIf we compute the norm of a vector, then this inner product returns smaller\\nvalues than the dot product if x1andx2have the same sign (and x1x2>\\n0); otherwise, it returns greater values than the dot product. With this\\ninner product, we obtain\\n⟨x,x⟩=x2\\n1−x1x2+x2\\n2= 1−1 + 1 = 1 =⇒ ∥x∥=√\\n1 = 1,(3.20)\\nsuch thatxis “shorter” with this inner product than with the dot product.\\nDeﬁnition 3.6 (Distance and Metric) .Consider an inner product space\\n(V,⟨·,·⟩). Then\\nd(x,y) :=∥x−y∥=√\\n⟨x−y,x−y⟩ (3.21)\\nis called the distance betweenxandyforx,y∈V. If we use the dot distance\\nproduct as the inner product, then the distance is called Euclidean distance .Euclidean distance\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53befe91-006d-47ef-add0-05301abb4572', embedding=None, metadata={'page_label': '76', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='76 Analytic Geometry\\nThe mapping\\nd:V×V→R (3.22)\\n(x,y)↦→d(x,y) (3.23)\\nis called a metric . metric\\nRemark. Similar to the length of a vector, the distance between vectors\\ndoes not require an inner product: a norm is sufﬁcient. If we have a norm\\ninduced by an inner product, the distance may vary depending on the\\nchoice of the inner product. ♦\\nA metricdsatisﬁes the following:\\n1.dispositive deﬁnite , i.e.,d(x,y)⩾0for allx,y∈Vandd(x,y) = positive deﬁnite\\n0⇐⇒x=y.\\n2.dissymmetric , i.e.,d(x,y) =d(y,x)for allx,y∈V. symmetric\\ntriangle inequality 3.Triangle inequality: d(x,z)⩽d(x,y) +d(y,z)for allx,y,z∈V.\\nRemark. At ﬁrst glance, the lists of properties of inner products and met-\\nrics look very similar. However, by comparing Deﬁnition 3.3 with Deﬁni-\\ntion 3.6 we observe that ⟨x,y⟩andd(x,y)behave in opposite directions.\\nVery similarxandywill result in a large value for the inner product and\\na small value for the metric. ♦\\n3.4 Angles and Orthogonality\\nFigure 3.4 When\\nrestricted to [0,π]\\nthenf(ω) = cos(ω)\\nreturns a unique\\nnumber in the\\ninterval [−1,1].\\n0π/2π\\nω−101cos(ω)In addition to enabling the deﬁnition of lengths of vectors, as well as the\\ndistance between two vectors, inner products also capture the geometry\\nof a vector space by deﬁning the angle ωbetween two vectors. We use\\nthe Cauchy-Schwarz inequality (3.17) to deﬁne angles ωin inner prod-\\nuct spaces between two vectors x,y, and this notion coincides with our\\nintuition in R2andR3. Assume that x̸=0,y̸=0. Then\\n−1⩽⟨x,y⟩\\n∥x∥∥y∥⩽1. (3.24)\\nTherefore, there exists a unique ω∈[0,π], illustrated in Figure 3.4, with\\ncosω=⟨x,y⟩\\n∥x∥∥y∥. (3.25)\\nThe number ωis the angle between the vectors xandy. Intuitively, the angle\\nangle between two vectors tells us how similar their orientations are. For\\nexample, using the dot product, the angle between xandy= 4x, i.e.,y\\nis a scaled version of x, is0: Their orientation is the same.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec7e4094-3662-4efb-be71-b584304d887d', embedding=None, metadata={'page_label': '77', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Angles and Orthogonality 77\\nExample 3.6 (Angle between Vectors)\\nLet us compute the angle between x= [1,1]⊤∈R2andy= [1,2]⊤∈R2;Figure 3.5 The\\nangleωbetween\\ntwo vectorsx,yis\\ncomputed using the\\ninner product.\\ny\\nx\\n1 01\\nωsee Figure 3.5, where we use the dot product as the inner product. Then\\nwe get\\ncosω=⟨x,y⟩√\\n⟨x,x⟩⟨y,y⟩=x⊤y√\\nx⊤xy⊤y=3√\\n10, (3.26)\\nand the angle between the two vectors is arccos(3√\\n10)≈0.32 rad , which\\ncorresponds to about 18◦.\\nA key feature of the inner product is that it also allows us to characterize\\nvectors that are orthogonal.\\nDeﬁnition 3.7 (Orthogonality) .Two vectorsxandyareorthogonal if and orthogonal\\nonly if⟨x,y⟩= 0, and we write x⊥y. If additionally∥x∥= 1 =∥y∥,\\ni.e., the vectors are unit vectors, then xandyareorthonormal . orthonormal\\nAn implication of this deﬁnition is that the 0-vector is orthogonal to\\nevery vector in the vector space.\\nRemark. Orthogonality is the generalization of the concept of perpendic-\\nularity to bilinear forms that do not have to be the dot product. In our\\ncontext, geometrically, we can think of orthogonal vectors as having a\\nright angle with respect to a speciﬁc inner product. ♦\\nExample 3.7 (Orthogonal Vectors)\\nFigure 3.6 The\\nangleωbetween\\ntwo vectorsx,ycan\\nchange depending\\non the inner\\nproduct.y x\\n−1 1 01\\nω\\nConsider two vectors x= [1,1]⊤,y= [−1,1]⊤∈R2; see Figure 3.6.\\nWe are interested in determining the angle ωbetween them using two\\ndifferent inner products. Using the dot product as the inner product yields\\nan angleωbetweenxandyof90◦, such thatx⊥y. However, if we\\nchoose the inner product\\n⟨x,y⟩=x⊤[2 0\\n0 1]\\ny, (3.27)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93064c34-7518-48df-b7cf-9cd10fa63f91', embedding=None, metadata={'page_label': '78', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='78 Analytic Geometry\\nwe get that the angle ωbetweenxandyis given by\\ncosω=⟨x,y⟩\\n∥x∥∥y∥=−1\\n3=⇒ω≈1.91 rad≈109.5◦, (3.28)\\nandxandyare not orthogonal. Therefore, vectors that are orthogonal\\nwith respect to one inner product do not have to be orthogonal with re-\\nspect to a different inner product.\\nDeﬁnition 3.8 (Orthogonal Matrix) .A square matrix A∈Rn×nis an\\northogonal matrix if and only if its columns are orthonormal so that orthogonal matrix\\nAA⊤=I=A⊤A, (3.29)\\nwhich implies that\\nA−1=A⊤, (3.30)\\ni.e., the inverse is obtained by simply transposing the matrix. It is convention to\\ncall these matrices\\n“orthogonal” but a\\nmore precise\\ndescription would\\nbe “orthonormal”.Transformations by orthogonal matrices are special because the length\\nof a vectorxis not changed when transforming it using an orthogonal\\nmatrixA. For the dot product, we obtain\\nTransformations\\nwith orthogonal\\nmatrices preserve\\ndistances and\\nangles.∥Ax∥2= (Ax)⊤(Ax) =x⊤A⊤Ax=x⊤Ix=x⊤x=∥x∥2.(3.31)\\nMoreover, the angle between any two vectors x,y, as measured by their\\ninner product, is also unchanged when transforming both of them using\\nan orthogonal matrix A. Assuming the dot product as the inner product,\\nthe angle of the images AxandAyis given as\\ncosω=(Ax)⊤(Ay)\\n∥Ax∥∥Ay∥=x⊤A⊤Ay√\\nx⊤A⊤Axy⊤A⊤Ay=x⊤y\\n∥x∥∥y∥,(3.32)\\nwhich gives exactly the angle between xandy. This means that orthog-\\nonal matrices AwithA⊤=A−1preserve both angles and distances. It\\nturns out that orthogonal matrices deﬁne transformations that are rota-\\ntions (with the possibility of ﬂips). In Section 3.9, we will discuss more\\ndetails about rotations.\\n3.5 Orthonormal Basis\\nIn Section 2.6.1, we characterized properties of basis vectors and found\\nthat in ann-dimensional vector space, we need nbasis vectors, i.e., n\\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\\ninner products to compute the length of vectors and the angle between\\nvectors. In the following, we will discuss the special case where the basis\\nvectors are orthogonal to each other and where the length of each basis\\nvector is 1. We will call this basis then an orthonormal basis.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbd47487-5d7a-4801-8377-5c573b7d7a2e', embedding=None, metadata={'page_label': '79', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.6 Orthogonal Complement 79\\nLet us introduce this more formally.\\nDeﬁnition 3.9 (Orthonormal Basis) .Consider an n-dimensional vector\\nspaceVand a basis{b1,...,bn}ofV. If\\n⟨bi,bj⟩= 0 fori̸=j (3.33)\\n⟨bi,bi⟩= 1 (3.34)\\nfor alli,j= 1,...,n then the basis is called an orthonormal basis (ONB). orthonormal basis\\nONB If only (3.33) is satisﬁed, then the basis is called an orthogonal basis . Note\\northogonal basisthat (3.34) implies that every basis vector has length/norm 1.\\nRecall from Section 2.6.1 that we can use Gaussian elimination to ﬁnd a\\nbasis for a vector space spanned by a set of vectors. Assume we are given\\na set{˜b1,..., ˜bn}of non-orthogonal and unnormalized basis vectors. We\\nconcatenate them into a matrix ˜B= [˜b1,..., ˜bn]and apply Gaussian elim-\\nination to the augmented matrix (Section 2.3.2) [˜B˜B⊤|˜B]to obtain an\\northonormal basis. This constructive way to iteratively build an orthonor-\\nmal basis{b1,...,bn}is called the Gram-Schmidt process (Strang, 2003).\\nExample 3.8 (Orthonormal Basis)\\nThe canonical/standard basis for a Euclidean vector space Rnis an or-\\nthonormal basis, where the inner product is the dot product of vectors.\\nInR2, the vectors\\nb1=1√\\n2[1\\n1]\\n,b2=1√\\n2[1\\n−1]\\n(3.35)\\nform an orthonormal basis since b⊤\\n1b2= 0and∥b1∥= 1 =∥b2∥.\\nWe will exploit the concept of an orthonormal basis in Chapter 12 and\\nChapter 10 when we discuss support vector machines and principal com-\\nponent analysis.\\n3.6 Orthogonal Complement\\nHaving deﬁned orthogonality, we will now look at vector spaces that are\\northogonal to each other. This will play an important role in Chapter 10,\\nwhen we discuss linear dimensionality reduction from a geometric per-\\nspective.\\nConsider aD-dimensional vector space Vand anM-dimensional sub-\\nspaceU⊆V. Then its orthogonal complement U⊥is a(D−M)-dimensional orthogonal\\ncomplement subspace of Vand contains all vectors in Vthat are orthogonal to every\\nvector inU. Furthermore, U∩U⊥={0}so that any vector x∈Vcan be\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4730ad0b-aec0-4341-b5d9-235ba38a705c', embedding=None, metadata={'page_label': '80', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='80 Analytic Geometry\\nFigure 3.7 A plane\\nUin a\\nthree-dimensional\\nvector space can be\\ndescribed by its\\nnormal vector,\\nwhich spans its\\northogonal\\ncomplement U⊥.e3\\ne1e2w\\nU\\nuniquely decomposed into\\nx=M∑\\nm=1λmbm+D−M∑\\nj=1ψjb⊥\\nj, λm, ψj∈R, (3.36)\\nwhere (b1,...,bM)is a basis of Uand(b⊥\\n1,...,b⊥\\nD−M)is a basis of U⊥.\\nTherefore, the orthogonal complement can also be used to describe a\\nplaneU(two-dimensional subspace) in a three-dimensional vector space.\\nMore speciﬁcally, the vector wwith∥w∥= 1, which is orthogonal to the\\nplaneU, is the basis vector of U⊥. Figure 3.7 illustrates this setting. All\\nvectors that are orthogonal to wmust (by construction) lie in the plane\\nU. The vectorwis called the normal vector ofU. normal vector\\nGenerally, orthogonal complements can be used to describe hyperplanes\\ninn-dimensional vector and afﬁne spaces.\\n3.7 Inner Product of Functions\\nThus far, we looked at properties of inner products to compute lengths,\\nangles and distances. We focused on inner products of ﬁnite-dimensional\\nvectors. In the following, we will look at an example of inner products of\\na different type of vectors: inner products of functions.\\nThe inner products we discussed so far were deﬁned for vectors with a\\nﬁnite number of entries. We can think of a vector x∈Rnas a function\\nwithnfunction values. The concept of an inner product can be generalized\\nto vectors with an inﬁnite number of entries (countably inﬁnite) and also\\ncontinuous-valued functions (uncountably inﬁnite). Then the sum over\\nindividual components of vectors (see Equation (3.5) for example) turns\\ninto an integral.\\nAn inner product of two functions u:R→Randv:R→Rcan be\\ndeﬁned as the deﬁnite integral\\n⟨u,v⟩:=∫b\\nau(x)v(x)dx (3.37)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15fae326-3e8e-444e-aec0-0a424ca5b5a5', embedding=None, metadata={'page_label': '81', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Orthogonal Projections 81\\nfor lower and upper limits a,b<∞, respectively. As with our usual inner\\nproduct, we can deﬁne norms and orthogonality by looking at the inner\\nproduct. If (3.37) evaluates to 0, the functions uandvare orthogonal. To\\nmake the preceding inner product mathematically precise, we need to take\\ncare of measures and the deﬁnition of integrals, leading to the deﬁnition of\\na Hilbert space. Furthermore, unlike inner products on ﬁnite-dimensional\\nvectors, inner products on functions may diverge (have inﬁnite value). All\\nthis requires diving into some more intricate details of real and functional\\nanalysis, which we do not cover in this book.\\nExample 3.9 (Inner Product of Functions)\\nIf we choose u= sin(x)andv= cos(x), the integrand f(x) =u(x)v(x)Figure 3.8f(x) =\\nsin(x) cos(x).\\n−2.5 0.0 2.5\\nx−0.50.00.5sin(x) cos(x) of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e.,\\nf(−x) =−f(x). Therefore, the integral with limits a=−π,b=πof this\\nproduct evaluates to 0. Therefore, sinandcosare orthogonal functions.\\nRemark. It also holds that the collection of functions\\n{1,cos(x),cos(2x),cos(3x),...} (3.38)\\nis orthogonal if we integrate from −πtoπ, i.e., any pair of functions are\\northogonal to each other. The collection of functions in (3.38) spans a\\nlarge subspace of the functions that are even and periodic on [−π,π), and\\nprojecting functions onto this subspace is the fundamental idea behind\\nFourier series. ♦\\nIn Section 6.4.6, we will have a look at a second type of unconventional\\ninner products: the inner product of random variables.\\n3.8 Orthogonal Projections\\nProjections are an important class of linear transformations (besides rota-\\ntions and reﬂections) and play an important role in graphics, coding the-\\nory, statistics and machine learning. In machine learning, we often deal\\nwith data that is high-dimensional. High-dimensional data is often hard\\nto analyze or visualize. However, high-dimensional data quite often pos-\\nsesses the property that only a few dimensions contain most information,\\nand most other dimensions are not essential to describe key properties\\nof the data. When we compress or visualize high-dimensional data, we\\nwill lose information. To minimize this compression loss, we ideally ﬁnd\\nthe most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a\\ncommon expression\\nfor data\\nrepresentation.data can be represented as vectors, and in this chapter, we will discuss\\nsome of the fundamental tools for data compression. More speciﬁcally, we\\ncan project the original high-dimensional data onto a lower-dimensional\\nfeature space and work in this lower-dimensional space to learn more\\nabout the dataset and extract relevant patterns. For example, machine\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e494f6e-bf3e-4a91-8fb8-545a36c4322a', embedding=None, metadata={'page_label': '82', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='82 Analytic Geometry\\nFigure 3.9\\nOrthogonal\\nprojection (orange\\ndots) of a\\ntwo-dimensional\\ndataset (blue dots)\\nonto a\\none-dimensional\\nsubspace (straight\\nline).\\n−4−2 0 2 4\\nx1−2−1012x2\\nlearning algorithms, such as principal component analysis (PCA) by Pear-\\nson (1901) and Hotelling (1933) and deep neural networks (e.g., deep\\nauto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension-\\nality reduction. In the following, we will focus on orthogonal projections,\\nwhich we will use in Chapter 10 for linear dimensionality reduction and\\nin Chapter 12 for classiﬁcation. Even linear regression, which we discuss\\nin Chapter 9, can be interpreted using orthogonal projections. For a given\\nlower-dimensional subspace, orthogonal projections of high-dimensional\\ndata retain as much information as possible and minimize the difference/\\nerror between the original data and the corresponding projection. An il-\\nlustration of such an orthogonal projection is given in Figure 3.9. Before\\nwe detail how to obtain these projections, let us deﬁne what a projection\\nactually is.\\nDeﬁnition 3.10 (Projection) .LetVbe a vector space and U⊆Va\\nsubspace of V. A linear mapping π:V→Uis called a projection if projection\\nπ2=π◦π=π.\\nSince linear mappings can be expressed by transformation matrices (see\\nSection 2.7), the preceding deﬁnition applies equally to a special kind\\nof transformation matrices, the projection matrices Pπ, which exhibit the projection matrix\\nproperty that P2\\nπ=Pπ.\\nIn the following, we will derive orthogonal projections of vectors in the\\ninner product space (Rn,⟨·,·⟩)onto subspaces. We will start with one-\\ndimensional subspaces, which are also called lines. If not mentioned oth- line\\nerwise, we assume the dot product ⟨x,y⟩=x⊤yas the inner product.\\n3.8.1 Projection onto One-Dimensional Subspaces (Lines)\\nAssume we are given a line (one-dimensional subspace) through the ori-\\ngin with basis vector b∈Rn. The line is a one-dimensional subspace\\nU⊆Rnspanned byb. When we project x∈RnontoU, we seek the\\nvectorπU(x)∈Uthat is closest to x. Using geometric arguments, let\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b1e9185-47de-4304-b62f-bd103ca499c0', embedding=None, metadata={'page_label': '83', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Orthogonal Projections 83\\nFigure 3.10\\nExamples of\\nprojections onto\\none-dimensional\\nsubspaces.\\nbx\\nπU(x)\\nω\\n(a) Projection of x∈R2onto a subspace U\\nwith basis vector b.cosωωsinω\\nbx\\n(b) Projection of a two-dimensional vector\\nxwith∥x∥= 1 onto a one-dimensional\\nsubspace spanned by b.\\nus characterize some properties of the projection πU(x)(Figure 3.10(a)\\nserves as an illustration):\\nThe projection πU(x)is closest tox, where “closest” implies that the\\ndistance∥x−πU(x)∥is minimal. It follows that the segment πU(x)−x\\nfromπU(x)toxis orthogonal to U, and therefore the basis vector bof\\nU. The orthogonality condition yields ⟨πU(x)−x,b⟩= 0since angles\\nbetween vectors are deﬁned via the inner product.λis then the\\ncoordinate of πU(x)\\nwith respect to b.The projection πU(x)ofxontoUmust be an element of Uand, there-\\nfore, a multiple of the basis vector bthat spansU. Hence,πU(x) =λb,\\nfor someλ∈R.\\nIn the following three steps, we determine the coordinate λ, the projection\\nπU(x)∈U, and the projection matrix Pπthat maps any x∈RnontoU:\\n1. Finding the coordinate λ. The orthogonality condition yields\\n⟨x−πU(x),b⟩= 0πU(x)=λb⇐⇒ ⟨x−λb,b⟩= 0. (3.39)\\nWe can now exploit the bilinearity of the inner product and arrive at With a general inner\\nproduct, we get\\nλ=⟨x,b⟩if\\n∥b∥= 1. ⟨x,b⟩−λ⟨b,b⟩= 0⇐⇒λ=⟨x,b⟩\\n⟨b,b⟩=⟨b,x⟩\\n∥b∥2. (3.40)\\nIn the last step, we exploited the fact that inner products are symmet-\\nric. If we choose⟨·,·⟩to be the dot product, we obtain\\nλ=b⊤x\\nb⊤b=b⊤x\\n∥b∥2. (3.41)\\nIf∥b∥= 1, then the coordinate λof the projection is given by b⊤x.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='644b46c5-94c6-4694-8ce1-12181dd638b0', embedding=None, metadata={'page_label': '84', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='84 Analytic Geometry\\n2. Finding the projection point πU(x)∈U. SinceπU(x) =λb, we imme-\\ndiately obtain with (3.40) that\\nπU(x) =λb=⟨x,b⟩\\n∥b∥2b=b⊤x\\n∥b∥2b, (3.42)\\nwhere the last equality holds for the dot product only. We can also\\ncompute the length of πU(x)by means of Deﬁnition 3.1 as\\n∥πU(x)∥=∥λb∥=|λ|∥b∥. (3.43)\\nHence, our projection is of length |λ|times the length of b. This also\\nadds the intuition that λis the coordinate of πU(x)with respect to the\\nbasis vectorbthat spans our one-dimensional subspace U.\\nIf we use the dot product as an inner product, we get\\n∥πU(x)∥(3.42)=|b⊤x|\\n∥b∥2∥b∥(3.25)=|cosω|∥x∥∥b∥∥b∥\\n∥b∥2=|cosω|∥x∥.\\n(3.44)\\nHere,ωis the angle between xandb. This equation should be familiar\\nfrom trigonometry: If ∥x∥= 1, thenxlies on the unit circle. It follows\\nthat the projection onto the horizontal axis spanned by bis exactly The horizontal axis\\nis a one-dimensional\\nsubspace.cosω, and the length of the corresponding vector πU(x) =|cosω|. An\\nillustration is given in Figure 3.10(b).\\n3. Finding the projection matrix Pπ. We know that a projection is a lin-\\near mapping (see Deﬁnition 3.10). Therefore, there exists a projection\\nmatrixPπ, such that πU(x) =Pπx. With the dot product as inner\\nproduct and\\nπU(x) =λb=bλ=bb⊤x\\n∥b∥2=bb⊤\\n∥b∥2x, (3.45)\\nwe immediately see that\\nPπ=bb⊤\\n∥b∥2. (3.46)\\nNote thatbb⊤(and, consequently, Pπ) is a symmetric matrix (of rank Projection matrices\\nare always\\nsymmetric.1), and∥b∥2=⟨b,b⟩is a scalar.\\nThe projection matrix Pπprojects any vector x∈Rnonto the line through\\nthe origin with direction b(equivalently, the subspace Uspanned byb).\\nRemark. The projection πU(x)∈Rnis still ann-dimensional vector and\\nnot a scalar. However, we no longer require ncoordinates to represent the\\nprojection, but only a single one if we want to express it with respect to\\nthe basis vector bthat spans the subspace U:λ. ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='658e37dd-865a-4ed1-8c32-e95fd7fbafcc', embedding=None, metadata={'page_label': '85', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Orthogonal Projections 85\\nFigure 3.11\\nProjection onto a\\ntwo-dimensional\\nsubspaceUwith\\nbasisb1,b2. The\\nprojectionπU(x)of\\nx∈R3ontoUcan\\nbe expressed as a\\nlinear combination\\nofb1,b2and the\\ndisplacement vector\\nx−πU(x)is\\northogonal to both\\nb1andb2.\\n0x\\nb1b2U\\nπU(x)x−πU(x)\\nExample 3.10 (Projection onto a Line)\\nFind the projection matrix Pπonto the line through the origin spanned\\nbyb=[1 2 2]⊤.bis a direction and a basis of the one-dimensional\\nsubspace (line through origin).\\nWith (3.46), we obtain\\nPπ=bb⊤\\nb⊤b=1\\n9\\uf8ee\\n\\uf8f01\\n2\\n2\\uf8f9\\n\\uf8fb[1 2 2]=1\\n9\\uf8ee\\n\\uf8f01 2 2\\n2 4 4\\n2 4 4\\uf8f9\\n\\uf8fb. (3.47)\\nLet us now choose a particular xand see whether it lies in the subspace\\nspanned byb. Forx=[1 1 1]⊤, the projection is\\nπU(x) =Pπx=1\\n9\\uf8ee\\n\\uf8f01 2 2\\n2 4 4\\n2 4 4\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb=1\\n9\\uf8ee\\n\\uf8f05\\n10\\n10\\uf8f9\\n\\uf8fb∈span[\\uf8ee\\n\\uf8f01\\n2\\n2\\uf8f9\\n\\uf8fb].(3.48)\\nNote that the application of PπtoπU(x)does not change anything, i.e.,\\nPππU(x) =πU(x). This is expected because according to Deﬁnition 3.10,\\nwe know that a projection matrix PπsatisﬁesP2\\nπx=Pπxfor allx.\\nRemark. With the results from Chapter 4, we can show that πU(x)is an\\neigenvector of Pπ, and the corresponding eigenvalue is 1.♦\\n3.8.2 Projection onto General Subspaces\\nIfUis given by a set\\nof spanning vectors,\\nwhich are not a\\nbasis, make sure\\nyou determine a\\nbasisb1,...,bm\\nbefore proceeding.In the following, we look at orthogonal projections of vectors x∈Rn\\nonto lower-dimensional subspaces U⊆Rnwith dim(U) =m⩾1. An\\nillustration is given in Figure 3.11.\\nAssume that (b1,...,bm)is an ordered basis of U. Any projection πU(x)\\nontoUis necessarily an element of U. Therefore, they can be represented\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d1cda78-9e8f-4be0-a1c9-e41e8bdd17c3', embedding=None, metadata={'page_label': '86', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='86 Analytic Geometry\\nas linear combinations of the basis vectors b1,...,bmofU, such that\\nπU(x) =∑m\\ni=1λibi. The basis vectors\\nform the columns of\\nB∈Rn×m, where\\nB= [b1,...,bm].As in the 1D case, we follow a three-step procedure to ﬁnd the projec-\\ntionπU(x)and the projection matrix Pπ:\\n1. Find the coordinates λ1,...,λmof the projection (with respect to the\\nbasis ofU), such that the linear combination\\nπU(x) =m∑\\ni=1λibi=Bλ, (3.49)\\nB= [b1,...,bm]∈Rn×m,λ= [λ1,...,λm]⊤∈Rm, (3.50)\\nis closest tox∈Rn. As in the 1D case, “closest” means “minimum\\ndistance”, which implies that the vector connecting πU(x)∈Uand\\nx∈Rnmust be orthogonal to all basis vectors of U. Therefore, we\\nobtainmsimultaneous conditions (assuming the dot product as the\\ninner product)\\n⟨b1,x−πU(x)⟩=b⊤\\n1(x−πU(x)) = 0 (3.51)\\n...\\n⟨bm,x−πU(x)⟩=b⊤\\nm(x−πU(x)) = 0 (3.52)\\nwhich, with πU(x) =Bλ, can be written as\\nb⊤\\n1(x−Bλ) = 0 (3.53)\\n...\\nb⊤\\nm(x−Bλ) = 0 (3.54)\\nsuch that we obtain a homogeneous linear equation system\\n\\uf8ee\\n\\uf8ef\\uf8f0b⊤\\n1...\\nb⊤\\nm\\uf8f9\\n\\uf8fa\\uf8fb\\uf8ee\\n\\uf8f0x−Bλ\\uf8f9\\n\\uf8fb=0⇐⇒B⊤(x−Bλ) =0 (3.55)\\n⇐⇒B⊤Bλ=B⊤x. (3.56)\\nThe last expression is called normal equation . Sinceb1,...,bmare a normal equation\\nbasis ofUand, therefore, linearly independent, B⊤B∈Rm×mis reg-\\nular and can be inverted. This allows us to solve for the coefﬁcients/\\ncoordinates\\nλ= (B⊤B)−1B⊤x. (3.57)\\nThe matrix (B⊤B)−1B⊤is also called the pseudo-inverse ofB, which pseudo-inverse\\ncan be computed for non-square matrices B. It only requires that B⊤B\\nis positive deﬁnite, which is the case if Bis full rank. In practical ap-\\nplications (e.g., linear regression), we often add a “jitter term” ϵIto\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44a32f6a-6229-4583-aef3-c8558cea76c1', embedding=None, metadata={'page_label': '87', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Orthogonal Projections 87\\nB⊤Bto guarantee increased numerical stability and positive deﬁnite-\\nness. This “ridge” can be rigorously derived using Bayesian inference.\\nSee Chapter 9 for details.\\n2. Find the projection πU(x)∈U. We already established that πU(x) =\\nBλ. Therefore, with (3.57)\\nπU(x) =B(B⊤B)−1B⊤x. (3.58)\\n3. Find the projection matrix Pπ. From (3.58), we can immediately see\\nthat the projection matrix that solves Pπx=πU(x)must be\\nPπ=B(B⊤B)−1B⊤. (3.59)\\nRemark. The solution for projecting onto general subspaces includes the\\n1D case as a special case: If dim(U) = 1 , thenB⊤B∈Ris a scalar and\\nwe can rewrite the projection matrix in (3.59) Pπ=B(B⊤B)−1B⊤as\\nPπ=BB⊤\\nB⊤B, which is exactly the projection matrix in (3.46). ♦\\nExample 3.11 (Projection onto a Two-dimensional Subspace)\\nFor a subspace U= span[\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n1\\n2\\uf8f9\\n\\uf8fb]⊆R3andx=\\uf8ee\\n\\uf8f06\\n0\\n0\\uf8f9\\n\\uf8fb∈R3ﬁnd the\\ncoordinatesλofxin terms of the subspace U, the projection point πU(x)\\nand the projection matrix Pπ.\\nFirst, we see that the generating set of Uis a basis (linear indepen-\\ndence) and write the basis vectors of Uinto a matrixB=\\uf8ee\\n\\uf8f01 0\\n1 1\\n1 2\\uf8f9\\n\\uf8fb.\\nSecond, we compute the matrix B⊤Band the vector B⊤xas\\nB⊤B=[1 1 1\\n0 1 2]\\uf8ee\\n\\uf8f01 0\\n1 1\\n1 2\\uf8f9\\n\\uf8fb=[3 3\\n3 5]\\n,B⊤x=[1 1 1\\n0 1 2]\\uf8ee\\n\\uf8f06\\n0\\n0\\uf8f9\\n\\uf8fb=[6\\n0]\\n.\\n(3.60)\\nThird, we solve the normal equation B⊤Bλ=B⊤xto ﬁndλ:\\n[3 3\\n3 5][λ1\\nλ2]\\n=[6\\n0]\\n⇐⇒λ=[5\\n−3]\\n. (3.61)\\nFourth, the projection πU(x)ofxontoU, i.e., into the column space of\\nB, can be directly computed via\\nπU(x) =Bλ=\\uf8ee\\n\\uf8f05\\n2\\n−1\\uf8f9\\n\\uf8fb. (3.62)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ded9eb01-ca55-4e09-9c09-c7ce1961ec95', embedding=None, metadata={'page_label': '88', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='88 Analytic Geometry\\nThe corresponding projection error is the norm of the difference vector projection error\\nbetween the original vector and its projection onto U, i.e., The projection error\\nis also called the\\nreconstruction error . ∥x−πU(x)∥=\\ued79\\ued79\\ued79[1−2 1]⊤\\ued79\\ued79\\ued79=√\\n6. (3.63)\\nFifth, the projection matrix (for any x∈R3) is given by\\nPπ=B(B⊤B)−1B⊤=1\\n6\\uf8ee\\n\\uf8f05 2−1\\n2 2 2\\n−1 2 5\\uf8f9\\n\\uf8fb. (3.64)\\nTo verify the results, we can (a) check whether the displacement vector\\nπU(x)−xis orthogonal to all basis vectors of U, and (b) verify that\\nPπ=P2\\nπ(see Deﬁnition 3.10).\\nRemark. The projections πU(x)are still vectors in Rnalthough they lie in\\nanm-dimensional subspace U⊆Rn. However, to represent a projected\\nvector we only need the mcoordinates λ1,...,λmwith respect to the\\nbasis vectorsb1,...,bmofU. ♦\\nRemark. In vector spaces with general inner products, we have to pay\\nattention when computing angles and distances, which are deﬁned by\\nmeans of the inner product. ♦We can ﬁnd\\napproximate\\nsolutions to\\nunsolvable linear\\nequation systems\\nusing projections.Projections allow us to look at situations where we have a linear system\\nAx=bwithout a solution. Recall that this means that bdoes not lie in\\nthe span ofA, i.e., the vector bdoes not lie in the subspace spanned by\\nthe columns of A. Given that the linear equation cannot be solved exactly,\\nwe can ﬁnd an approximate solution . The idea is to ﬁnd the vector in the\\nsubspace spanned by the columns of Athat is closest to b, i.e., we compute\\nthe orthogonal projection of bonto the subspace spanned by the columns\\nofA. This problem arises often in practice, and the solution is called the\\nleast-squares solution (assuming the dot product as the inner product) of least-squares\\nsolution an overdetermined system. This is discussed further in Section 9.4. Using\\nreconstruction errors (3.63) is one possible approach to derive principal\\ncomponent analysis (Section 10.3).\\nRemark. We just looked at projections of vectors xonto a subspace Uwith\\nbasis vectors{b1,...,bk}. If this basis is an ONB, i.e., (3.33) and (3.34)\\nare satisﬁed, the projection equation (3.58) simpliﬁes greatly to\\nπU(x) =BB⊤x (3.65)\\nsinceB⊤B=Iwith coordinates\\nλ=B⊤x. (3.66)\\nThis means that we no longer have to compute the inverse from (3.58),\\nwhich saves computation time. ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac249e2f-4bf6-4fe4-bc58-1039f17928cc', embedding=None, metadata={'page_label': '89', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Orthogonal Projections 89\\n3.8.3 Gram-Schmidt Orthogonalization\\nProjections are at the core of the Gram-Schmidt method that allows us to\\nconstructively transform any basis (b1,...,bn)of ann-dimensional vector\\nspaceVinto an orthogonal/orthonormal basis (u1,...,un)ofV. This\\nbasis always exists (Liesen and Mehrmann, 2015) and span[b1,...,bn] =\\nspan[u1,...,un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt\\northogonalization constructs an orthogonal basis (u1,...,un)from any basis (b1,...,bn)of\\nVas follows:\\nu1:=b1 (3.67)\\nuk:=bk−πspan[u1,...,uk−1](bk), k = 2,...,n. (3.68)\\nIn (3.68), the kth basis vector bkis projected onto the subspace spanned\\nby the ﬁrst k−1constructed orthogonal vectors u1,...,uk−1; see Sec-\\ntion 3.8.2. This projection is then subtracted from bkand yields a vector\\nukthat is orthogonal to the (k−1)-dimensional subspace spanned by\\nu1,...,uk−1. Repeating this procedure for all nbasis vectors b1,...,bn\\nyields an orthogonal basis (u1,...,un)ofV. If we normalize the uk, we\\nobtain an ONB where ∥uk∥= 1fork= 1,...,n .\\nExample 3.12 (Gram-Schmidt Orthogonalization)\\nFigure 3.12\\nGram-Schmidt\\northogonalization.\\n(a) non-orthogonal\\nbasis (b1,b2)ofR2;\\n(b) ﬁrst constructed\\nbasis vectoru1and\\northogonal\\nprojection ofb2\\nonto span[u1];\\n(c) orthogonal basis\\n(u1,u2)ofR2.b1b2\\n0\\n(a) Original non-orthogonal\\nbasis vectorsb1,b2.u1b2\\n0πspan[u1](b2)\\n(b) First new basis vector\\nu1=b1and projection of b2\\nonto the subspace spanned by\\nu1.u1b2\\n0πspan[u1](b2)u2\\n(c) Orthogonal basis vectors u1\\nandu2=b2−πspan[u1](b2).\\nConsider a basis (b1,b2)ofR2, where\\nb1=[2\\n0]\\n,b2=[1\\n1]\\n; (3.69)\\nsee also Figure 3.12(a). Using the Gram-Schmidt method, we construct an\\northogonal basis (u1,u2)ofR2as follows (assuming the dot product as\\nthe inner product):\\nu1:=b1=[2\\n0]\\n, (3.70)\\nu2:=b2−πspan[u1](b2)(3.45)=b2−u1u⊤\\n1\\n∥u1∥2b2=[1\\n1]\\n−[1 0\\n0 0][1\\n1]\\n=[0\\n1]\\n.\\n(3.71)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6115686e-7956-40bb-bffd-0c9e3b76d163', embedding=None, metadata={'page_label': '90', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='90 Analytic Geometry\\nFigure 3.13\\nProjection onto an\\nafﬁne space.\\n(a) original setting;\\n(b) setting shifted\\nby−x0so that\\nx−x0can be\\nprojected onto the\\ndirection space U;\\n(c) projection is\\ntranslated back to\\nx0+πU(x−x0),\\nwhich gives the ﬁnal\\northogonal\\nprojectionπL(x).L\\nx0x\\nb2\\nb1 0\\n(a) Setting.b1 0x−x0\\nU=L−x0\\nπU(x−x0)b2\\n(b) Reduce problem to pro-\\njectionπUonto vector sub-\\nspace.L\\nx0x\\nb2\\nb1 0πL(x)\\n(c) Add support point back in\\nto get afﬁne projection πL.\\nThese steps are illustrated in Figures 3.12(b) and (c). We immediately see\\nthatu1andu2are orthogonal, i.e., u⊤\\n1u2= 0.\\n3.8.4 Projection onto Afﬁne Subspaces\\nThus far, we discussed how to project a vector onto a lower-dimensional\\nsubspaceU. In the following, we provide a solution to projecting a vector\\nonto an afﬁne subspace.\\nConsider the setting in Figure 3.13(a). We are given an afﬁne space L=\\nx0+U, whereb1,b2are basis vectors of U. To determine the orthogonal\\nprojectionπL(x)ofxontoL, we transform the problem into a problem\\nthat we know how to solve: the projection onto a vector subspace. In\\norder to get there, we subtract the support point x0fromxand fromL,\\nso thatL−x0=Uis exactly the vector subspace U. We can now use the\\northogonal projections onto a subspace we discussed in Section 3.8.2 and\\nobtain the projection πU(x−x0), which is illustrated in Figure 3.13(b).\\nThis projection can now be translated back into Lby addingx0, such that\\nwe obtain the orthogonal projection onto an afﬁne space Las\\nπL(x) =x0+πU(x−x0), (3.72)\\nwhereπU(·)is the orthogonal projection onto the subspace U, i.e., the\\ndirection space of L; see Figure 3.13(c).\\nFrom Figure 3.13, it is also evident that the distance of xfrom the afﬁne\\nspaceLis identical to the distance of x−x0fromU, i.e.,\\nd(x,L) =∥x−πL(x)∥=∥x−(x0+πU(x−x0))∥ (3.73a)\\n=d(x−x0,πU(x−x0)) =d(x−x0,U). (3.73b)\\nWe will use projections onto an afﬁne subspace to derive the concept of\\na separating hyperplane in Section 12.1.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aefef10d-9756-4db5-9048-6692191639d7', embedding=None, metadata={'page_label': '91', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.9 Rotations 91\\nFigure 3.14 A\\nrotation rotates\\nobjects in a plane\\nabout the origin. If\\nthe rotation angle is\\npositive, we rotate\\ncounterclockwise.\\nOriginal\\nRotated by 112.5◦\\nFigure 3.15 The\\nrobotic arm needs to\\nrotate its joints in\\norder to pick up\\nobjects or to place\\nthem correctly.\\nFigure taken\\nfrom (Deisenroth\\net al., 2015).\\n3.9 Rotations\\nLength and angle preservation, as discussed in Section 3.4, are the two\\ncharacteristics of linear mappings with orthogonal transformation matri-\\nces. In the following, we will have a closer look at speciﬁc orthogonal\\ntransformation matrices, which describe rotations.\\nArotation is a linear mapping (more speciﬁcally, an automorphism of rotation\\na Euclidean vector space) that rotates a plane by an angle θabout the\\norigin, i.e., the origin is a ﬁxed point. For a positive angle θ >0, by com-\\nmon convention, we rotate in a counterclockwise direction. An example is\\nshown in Figure 3.14, where the transformation matrix is\\nR=[−0.38−0.92\\n0.92−0.38]\\n. (3.74)\\nImportant application areas of rotations include computer graphics and\\nrobotics. For example, in robotics, it is often important to know how to\\nrotate the joints of a robotic arm in order to pick up or place an object,\\nsee Figure 3.15.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b8d7b50-219a-404b-b8ad-72f3db85e7d7', embedding=None, metadata={'page_label': '92', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='92 Analytic Geometry\\nFigure 3.16\\nRotation of the\\nstandard basis in R2\\nby an angle θ.\\ne1e2\\nθθΦ(e2) = [−sinθ,cosθ]⊤\\nΦ(e1) = [cosθ,sinθ]⊤\\ncosθsinθ\\n−sinθcosθ\\n3.9.1 Rotations in R2\\nConsider the standard basis{\\ne1=[1\\n0]\\n,e2=[0\\n1]}\\nofR2, which deﬁnes\\nthe standard coordinate system in R2. We aim to rotate this coordinate\\nsystem by an angle θas illustrated in Figure 3.16. Note that the rotated\\nvectors are still linearly independent and, therefore, are a basis of R2. This\\nmeans that the rotation performs a basis change.\\nRotations Φare linear mappings so that we can express them by a\\nrotation matrix R(θ). Trigonometry (see Figure 3.16) allows us to de- rotation matrix\\ntermine the coordinates of the rotated axes (the image of Φ) with respect\\nto the standard basis in R2. We obtain\\nΦ(e1) =[cosθ\\nsinθ]\\n,Φ(e2) =[−sinθ\\ncosθ]\\n. (3.75)\\nTherefore, the rotation matrix that performs the basis change into the\\nrotated coordinates R(θ)is given as\\nR(θ) =[Φ(e1) Φ(e2)]=[cosθ−sinθ\\nsinθcosθ]\\n. (3.76)\\n3.9.2 Rotations in R3\\nIn contrast to the R2case, in R3we can rotate any two-dimensional plane\\nabout a one-dimensional axis. The easiest way to specify the general rota-\\ntion matrix is to specify how the images of the standard basis e1,e2,e3are\\nsupposed to be rotated, and making sure these images Re1,Re2,Re3are\\northonormal to each other. We can then obtain a general rotation matrix\\nRby combining the images of the standard basis.\\nTo have a meaningful rotation angle, we have to deﬁne what “coun-\\nterclockwise” means when we operate in more than two dimensions. We\\nuse the convention that a “counterclockwise” (planar) rotation about an\\naxis refers to a rotation about an axis when we look at the axis “head on,\\nfrom the end toward the origin”. In R3, there are therefore three (planar)\\nrotations about the three standard basis vectors (see Figure 3.17):\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d6535c0-90b1-4ecf-beda-137f23eae12d', embedding=None, metadata={'page_label': '93', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.9 Rotations 93\\nFigure 3.17\\nRotation of a vector\\n(gray) in R3by an\\nangleθabout the\\ne3-axis. The rotated\\nvector is shown in\\nblue.\\ne1e2e3\\nθ\\nRotation about the e1-axis\\nR1(θ) =[Φ(e1) Φ(e2) Φ(e3)]=\\uf8ee\\n\\uf8f01 0 0\\n0 cosθ−sinθ\\n0 sinθcosθ\\uf8f9\\n\\uf8fb.(3.77)\\nHere, thee1coordinate is ﬁxed, and the counterclockwise rotation is\\nperformed in the e2e3plane.\\nRotation about the e2-axis\\nR2(θ) =\\uf8ee\\n\\uf8f0cosθ0 sinθ\\n0 1 0\\n−sinθ0 cosθ\\uf8f9\\n\\uf8fb. (3.78)\\nIf we rotate the e1e3plane about the e2axis, we need to look at the e2\\naxis from its “tip” toward the origin.\\nRotation about the e3-axis\\nR3(θ) =\\uf8ee\\n\\uf8f0cosθ−sinθ0\\nsinθcosθ0\\n0 0 1\\uf8f9\\n\\uf8fb. (3.79)\\nFigure 3.17 illustrates this.\\n3.9.3 Rotations in nDimensions\\nThe generalization of rotations from 2D and 3D to n-dimensional Eu-\\nclidean vector spaces can be intuitively described as ﬁxing n−2dimen-\\nsions and restrict the rotation to a two-dimensional plane in the n-dimen-\\nsional space. As in the three-dimensional case, we can rotate any plane\\n(two-dimensional subspace of Rn).\\nDeﬁnition 3.11 (Givens Rotation) .LetVbe ann-dimensional Euclidean\\nvector space and Φ :V→Van automorphism with transformation ma-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69b3b89e-3c20-498a-8536-f5bed3da0581', embedding=None, metadata={'page_label': '94', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='94 Analytic Geometry\\ntrix\\nRij(θ) :=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0Ii−10··· ··· 0\\n0 cosθ 0−sinθ0\\n0 0Ij−i−1 0 0\\n0 sinθ 0 cosθ 0\\n0··· ··· 0In−j\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rn×n,(3.80)\\nfor1⩽i < j⩽nandθ∈R. ThenRij(θ)is called a Givens rotation . Givens rotation\\nEssentially,Rij(θ)is the identity matrix Inwith\\nrii= cosθ, rij=−sinθ, rji= sinθ, rjj= cosθ. (3.81)\\nIn two dimensions (i.e., n= 2), we obtain (3.76) as a special case.\\n3.9.4 Properties of Rotations\\nRotations exhibit a number of useful properties, which can be derived by\\nconsidering them as orthogonal matrices (Deﬁnition 3.8):\\nRotations preserve distances, i.e., ∥x−y∥=∥Rθ(x)−Rθ(y)∥. In other\\nwords, rotations leave the distance between any two points unchanged\\nafter the transformation.\\nRotations preserve angles, i.e., the angle between RθxandRθyequals\\nthe angle between xandy.\\nRotations in three (or more) dimensions are generally not commuta-\\ntive. Therefore, the order in which rotations are applied is important,\\neven if they rotate about the same point. Only in two dimensions vector\\nrotations are commutative, such that R(φ)R(θ) =R(θ)R(φ)for all\\nφ,θ∈[0,2π). They form an Abelian group (with multiplication) only if\\nthey rotate about the same point (e.g., the origin).\\n3.10 Further Reading\\nIn this chapter, we gave a brief overview of some of the important concepts\\nof analytic geometry, which we will use in later chapters of the book.\\nFor a broader and more in-depth overview of some of the concepts we\\npresented, we refer to the following excellent books: Axler (2015) and\\nBoyd and Vandenberghe (2018).\\nInner products allow us to determine speciﬁc bases of vector (sub)spaces,\\nwhere each vector is orthogonal to all others (orthogonal bases) using the\\nGram-Schmidt method. These bases are important in optimization and\\nnumerical algorithms for solving linear equation systems. For instance,\\nKrylov subspace methods, such as conjugate gradients or the generalized\\nminimal residual method (GMRES), minimize residual errors that are or-\\nthogonal to each other (Stoer and Burlirsch, 2002).\\nIn machine learning, inner products are important in the context of\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f8eaf3e-d498-4c09-8414-8c764ac6c87b', embedding=None, metadata={'page_label': '95', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.10 Further Reading 95\\nkernel methods (Sch ¨olkopf and Smola, 2002). Kernel methods exploit the\\nfact that many linear algorithms can be expressed purely by inner prod-\\nuct computations. Then, the “kernel trick” allows us to compute these\\ninner products implicitly in a (potentially inﬁnite-dimensional) feature\\nspace, without even knowing this feature space explicitly. This allowed the\\n“non-linearization” of many algorithms used in machine learning, such as\\nkernel-PCA (Sch ¨olkopf et al., 1997) for dimensionality reduction. Gaus-\\nsian processes (Rasmussen and Williams, 2006) also fall into the category\\nof kernel methods and are the current state of the art in probabilistic re-\\ngression (ﬁtting curves to data points). The idea of kernels is explored\\nfurther in Chapter 12.\\nProjections are often used in computer graphics, e.g., to generate shad-\\nows. In optimization, orthogonal projections are often used to (iteratively)\\nminimize residual errors. This also has applications in machine learning,\\ne.g., in linear regression where we want to ﬁnd a (linear) function that\\nminimizes the residual errors, i.e., the lengths of the orthogonal projec-\\ntions of the data onto the linear function (Bishop, 2006). We will investi-\\ngate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\\nuses projections to reduce the dimensionality of high-dimensional data.\\nWe will discuss this in more detail in Chapter 10.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8cb663a0-40b5-458d-8b58-4dd9b52ef64e', embedding=None, metadata={'page_label': '96', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='96 Analytic Geometry\\nExercises\\n3.1 Show that⟨·,·⟩deﬁned for all x= [x1,x2]⊤∈R2andy= [y1,y2]⊤∈R2by\\n⟨x,y⟩:=x1y1−(x1y2+x2y1) + 2(x2y2)\\nis an inner product.\\n3.2 Consider R2with⟨·,·⟩deﬁned for all xandyinR2as\\n⟨x,y⟩:=x⊤[\\n2 0\\n1 2]\\n\\ued19\\ued18\\ued17\\ued1a\\n=:Ay.\\nIs⟨·,·⟩an inner product?\\n3.3 Compute the distance between\\nx=\\uf8ee\\n\\uf8f01\\n2\\n3\\uf8f9\\n\\uf8fb,y=\\uf8ee\\n\\uf8f0−1\\n−1\\n0\\uf8f9\\n\\uf8fb\\nusing\\na.⟨x,y⟩:=x⊤y\\nb.⟨x,y⟩:=x⊤Ay,A:=\\uf8ee\\n\\uf8f02 1 0\\n1 3−1\\n0−1 2\\uf8f9\\n\\uf8fb\\n3.4 Compute the angle between\\nx=[\\n1\\n2]\\n,y=[\\n−1\\n−1]\\nusing\\na.⟨x,y⟩:=x⊤y\\nb.⟨x,y⟩:=x⊤By,B:=[\\n2 1\\n1 3]\\n3.5 Consider the Euclidean vector space R5with the dot product. A subspace\\nU⊆R5andx∈R5are given by\\nU= span[\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00\\n−1\\n2\\n0\\n2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\n−3\\n1\\n−1\\n2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−3\\n4\\n1\\n2\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb,\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−1\\n−3\\n5\\n0\\n7\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb],x=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−1\\n−9\\n−1\\n4\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\na. Determine the orthogonal projection πU(x)ofxontoU\\nb. Determine the distance d(x,U)\\n3.6 Consider R3with the inner product\\n⟨x,y⟩:=x⊤\\uf8ee\\n\\uf8f02 1 0\\n1 2−1\\n0−1 2\\uf8f9\\n\\uf8fby.\\nFurthermore, we deﬁne e1,e2,e3as the standard/canonical basis in R3.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59cc6fa0-2346-4368-b338-30c4260077ae', embedding=None, metadata={'page_label': '97', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 97\\na. Determine the orthogonal projection πU(e2)ofe2onto\\nU= span[e1,e3].\\nHint: Orthogonality is deﬁned through the inner product.\\nb. Compute the distance d(e2,U).\\nc. Draw the scenario: standard basis vectors and πU(e2)\\n3.7 LetVbe a vector space and πan endomorphism of V.\\na. Prove that πis a projection if and only if idV−πis a projection, where\\nidVis the identity endomorphism on V.\\nb. Assume now that πis a projection. Calculate Im(idV−π)andker(idV−π)\\nas a function of Im(π)andker(π).\\n3.8 Using the Gram-Schmidt method, turn the basis B= (b1,b2)of a two-\\ndimensional subspace U⊆R3into an ONB C= (c1,c2)ofU, where\\nb1:=\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb,b2:=\\uf8ee\\n\\uf8f0−1\\n2\\n0\\uf8f9\\n\\uf8fb.\\n3.9 Letn∈Nand letx1,...,xn>0benpositive real numbers so that x1+\\n...+xn= 1. Use the Cauchy-Schwarz inequality and show that\\na.∑n\\ni=1x2\\ni⩾1\\nn\\nb.∑n\\ni=11\\nxi⩾n2\\nHint: Think about the dot product on Rn. Then, choose speciﬁc vectors\\nx,y∈Rnand apply the Cauchy-Schwarz inequality.\\n3.10 Rotate the vectors\\nx1:=[\\n2\\n3]\\n,x2:=[\\n0\\n−1]\\nby30◦.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d251587f-95e2-4ad6-a3e7-98ea079eae4e', embedding=None, metadata={'page_label': '98', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4\\nMatrix Decompositions\\nIn Chapters 2 and 3, we studied ways to manipulate and measure vectors,\\nprojections of vectors, and linear mappings. Mappings and transforma-\\ntions of vectors can be conveniently described as operations performed by\\nmatrices. Moreover, data is often represented in matrix form as well, e.g.,\\nwhere the rows of the matrix represent different people and the columns\\ndescribe different features of the people, such as weight, height, and socio-\\neconomic status. In this chapter, we present three aspects of matrices: how\\nto summarize matrices, how matrices can be decomposed, and how these\\ndecompositions can be used for matrix approximations.\\nWe ﬁrst consider methods that allow us to describe matrices with just\\na few numbers that characterize the overall properties of matrices. We\\nwill do this in the sections on determinants (Section 4.1) and eigenval-\\nues (Section 4.2) for the important special case of square matrices. These\\ncharacteristic numbers have important mathematical consequences and\\nallow us to quickly grasp what useful properties a matrix has. From here\\nwe will proceed to matrix decomposition methods: An analogy for ma-\\ntrix decomposition is the factoring of numbers, such as the factoring of\\n21into prime numbers 7·3. For this reason matrix decomposition is also\\noften referred to as matrix factorization . Matrix decompositions are used matrix factorization\\nto describe a matrix by means of a different representation using factors\\nof interpretable matrices.\\nWe will ﬁrst cover a square-root-like operation for symmetric, positive\\ndeﬁnite matrices, the Cholesky decomposition (Section 4.3). From here\\nwe will look at two related methods for factorizing matrices into canoni-\\ncal forms. The ﬁrst one is known as matrix diagonalization (Section 4.4),\\nwhich allows us to represent the linear mapping using a diagonal trans-\\nformation matrix if we choose an appropriate basis. The second method,\\nsingular value decomposition (Section 4.5), extends this factorization to\\nnon-square matrices, and it is considered one of the fundamental concepts\\nin linear algebra. These decompositions are helpful, as matrices represent-\\ning numerical data are often very large and hard to analyze. We conclude\\nthe chapter with a systematic overview of the types of matrices and the\\ncharacteristic properties that distinguish them in the form of a matrix tax-\\nonomy (Section 4.7).\\nThe methods that we cover in this chapter will become important in\\n98\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53cfa29d-b8dd-45d7-84df-faaafe62f713', embedding=None, metadata={'page_label': '99', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.1 Determinant and Trace 99\\nFigure 4.1 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhere they are used\\nin other parts of the\\nbook.Determinant Invertibility Cholesky\\nEigenvalues\\nEigenvectors Orthogonal matrix Diagonalization\\nSVDChapter 6\\nProbability\\n& distributions\\nChapter 10\\nDimensionality\\nreductiontests used inused in\\nused in determines\\nused in\\nused in\\nused inconstructs used in\\nused inused in\\nboth subsequent mathematical chapters, such as Chapter 6, but also in\\napplied chapters, such as dimensionality reduction in Chapters 10 or den-\\nsity estimation in Chapter 11. This chapter’s overall structure is depicted\\nin the mind map of Figure 4.1.\\n4.1 Determinant and TraceThe determinant\\nnotation|A|must\\nnot be confused\\nwith the absolute\\nvalue.Determinants are important concepts in linear algebra. A determinant is\\na mathematical object in the analysis and solution of systems of linear\\nequations. Determinants are only deﬁned for square matrices A∈Rn×n,\\ni.e., matrices with the same number of rows and columns. In this book,\\nwe write the determinant as det(A)or sometimes as|A|so that\\ndet(A) =⏐⏐⏐⏐⏐⏐⏐⏐⏐a11a12... a 1n\\na21a22... a 2n\\n.........\\nan1an2... ann⏐⏐⏐⏐⏐⏐⏐⏐⏐. (4.1)\\nThedeterminant of a square matrix A∈Rn×nis a function that maps A determinant\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9560d48-08f4-4c14-9c85-dde68b28ed2a', embedding=None, metadata={'page_label': '100', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='100 Matrix Decompositions\\nonto a real number. Before providing a deﬁnition of the determinant for\\ngeneraln×nmatrices, let us have a look at some motivating examples,\\nand deﬁne determinants for some special matrices.\\nExample 4.1 (Testing for Matrix Invertibility)\\nLet us begin with exploring if a square matrix Ais invertible (see Sec-\\ntion 2.2.2). For the smallest cases, we already know when a matrix\\nis invertible. If Ais a 1×1matrix, i.e., it is a scalar number, then\\nA=a=⇒A−1=1\\na. Thusa1\\na= 1holds, if and only if a̸= 0.\\nFor2×2matrices, by the deﬁnition of the inverse (Deﬁnition 2.3), we\\nknow thatAA−1=I. Then, with (2.24), the inverse of Ais\\nA−1=1\\na11a22−a12a21[a22−a12\\n−a21a11]\\n. (4.2)\\nHence,Ais invertible if and only if\\na11a22−a12a21̸= 0. (4.3)\\nThis quantity is the determinant of A∈R2×2, i.e.,\\ndet(A) =⏐⏐⏐⏐⏐a11a12\\na21a22⏐⏐⏐⏐⏐=a11a22−a12a21. (4.4)\\nExample 4.1 points already at the relationship between determinants\\nand the existence of inverse matrices. The next theorem states the same\\nresult forn×nmatrices.\\nTheorem 4.1. For any square matrix A∈Rn×nit holds thatAis invertible\\nif and only if det(A)̸= 0.\\nWe have explicit (closed-form) expressions for determinants of small\\nmatrices in terms of the elements of the matrix. For n= 1,\\ndet(A) = det(a11) =a11. (4.5)\\nForn= 2,\\ndet(A) =⏐⏐⏐⏐a11a12\\na21a22⏐⏐⏐⏐=a11a22−a12a21, (4.6)\\nwhich we have observed in the preceding example.\\nForn= 3(known as Sarrus’ rule),\\n⏐⏐⏐⏐⏐⏐a11a12a13\\na21a22a23\\na31a32a33⏐⏐⏐⏐⏐⏐=a11a22a33+a21a32a13+a31a12a23 (4.7)\\n−a31a22a13−a11a32a23−a21a12a33.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2c642c8-81e5-4064-a604-0c5b44c926d4', embedding=None, metadata={'page_label': '101', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.1 Determinant and Trace 101\\nFor a memory aid of the product terms in Sarrus’ rule, try tracing the\\nelements of the triple products in the matrix.\\nWe call a square matrix Tanupper-triangular matrix ifTij= 0 for upper-triangular\\nmatrix i>j , i.e., the matrix is zero below its diagonal. Analogously, we deﬁne a\\nlower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\\nmatrix angular matrix T∈Rn×n, the determinant is the product of the diagonal\\nelements, i.e.,\\ndet(T) =n∏\\ni=1Tii. (4.8)\\nThe determinant is\\nthe signed volume\\nof the parallelepiped\\nformed by the\\ncolumns of the\\nmatrix.\\nFigure 4.2 The area\\nof the parallelogram\\n(shaded region)\\nspanned by the\\nvectorsbandgis\\n|det([b,g])|.\\nb\\ng\\nFigure 4.3 The\\nvolume of the\\nparallelepiped\\n(shaded volume)\\nspanned by vectors\\nr,b,gis\\n|det([r,b,g])|.\\nb\\ngrExample 4.2 (Determinants as Measures of Volume)\\nThe notion of a determinant is natural when we consider it as a mapping\\nfrom a set of nvectors spanning an object in Rn. It turns out that the de-\\nterminant det(A)is the signed volume of an n-dimensional parallelepiped\\nformed by columns of the matrix A.\\nForn= 2, the columns of the matrix form a parallelogram; see Fig-\\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\\nogram shrinks, too. Consider two vectors b,gthat form the columns of a\\nmatrixA= [b,g]. Then, the absolute value of the determinant of Ais the\\narea of the parallelogram with vertices 0,b,g,b+g. In particular, if b,g\\nare linearly dependent so that b=λgfor someλ∈R, they no longer\\nform a two-dimensional parallelogram. Therefore, the corresponding area\\nis0. On the contrary, if b,gare linearly independent and are multiples of\\nthe canonical basis vectors e1,e2then they can be written as b=[b\\n0]\\nand\\ng=[0\\ng]\\n, and the determinant is⏐⏐⏐⏐b0\\n0g⏐⏐⏐⏐=bg−0 =bg.\\nThe sign of the determinant indicates the orientation of the spanning\\nvectorsb,gwith respect to the standard basis (e1,e2). In our ﬁgure, ﬂip-\\nping the order to g,bswaps the columns of Aand reverses the orientation\\nof the shaded area. This becomes the familiar formula: area =height×\\nlength. This intuition extends to higher dimensions. In R3, we consider\\nthree vectors r,b,g∈R3spanning the edges of a parallelepiped, i.e., a\\nsolid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\\ndeterminant\\nindicates the\\norientation of the\\nspanning vectors.solute value of the determinant of the 3×3matrix [r,b,g]is the volume\\nof the solid. Thus, the determinant acts as a function that measures the\\nsigned volume formed by column vectors composed in a matrix.\\nConsider the three linearly independent vectors r,g,b∈R3given as\\nr=\\uf8ee\\n\\uf8f02\\n0\\n−8\\uf8f9\\n\\uf8fb,g=\\uf8ee\\n\\uf8f06\\n1\\n0\\uf8f9\\n\\uf8fb,b=\\uf8ee\\n\\uf8f01\\n4\\n−1\\uf8f9\\n\\uf8fb. (4.9)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b33cf2dd-3da7-41d3-8612-660b1bbb9d86', embedding=None, metadata={'page_label': '102', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='102 Matrix Decompositions\\nWriting these vectors as the columns of a matrix\\nA= [r,g,b] =\\uf8ee\\n\\uf8f02 6 1\\n0 1 4\\n−8 0−1\\uf8f9\\n\\uf8fb (4.10)\\nallows us to compute the desired volume as\\nV=|det(A)|= 186. (4.11)\\nComputing the determinant of an n×nmatrix requires a general algo-\\nrithm to solve the cases for n>3, which we are going to explore in the fol-\\nlowing. Theorem 4.2 below reduces the problem of computing the deter-\\nminant of an n×nmatrix to computing the determinant of (n−1)×(n−1)\\nmatrices. By recursively applying the Laplace expansion (Theorem 4.2),\\nwe can therefore compute determinants of n×nmatrices by ultimately\\ncomputing determinants of 2×2matrices.\\nLaplace expansion\\nTheorem 4.2 (Laplace Expansion) .Consider a matrix A∈Rn×n. Then,\\nfor allj= 1,...,n :\\n1. Expansion along column j det(Ak,j)is called\\naminor and\\n(−1)k+jdet(Ak,j)\\nacofactor .det(A) =n∑\\nk=1(−1)k+jakjdet(Ak,j). (4.12)\\n2. Expansion along row j\\ndet(A) =n∑\\nk=1(−1)k+jajkdet(Aj,k). (4.13)\\nHereAk,j∈R(n−1)×(n−1)is the submatrix of Athat we obtain when delet-\\ning rowkand column j.\\nExample 4.3 (Laplace Expansion)\\nLet us compute the determinant of\\nA=\\uf8ee\\n\\uf8f01 2 3\\n3 1 2\\n0 0 1\\uf8f9\\n\\uf8fb (4.14)\\nusing the Laplace expansion along the ﬁrst row. Applying (4.13) yields\\n⏐⏐⏐⏐⏐⏐1 2 3\\n3 1 2\\n0 0 1⏐⏐⏐⏐⏐⏐= (−1)1+1·1⏐⏐⏐⏐1 2\\n0 1⏐⏐⏐⏐\\n+ (−1)1+2·2⏐⏐⏐⏐3 2\\n0 1⏐⏐⏐⏐+ (−1)1+3·3⏐⏐⏐⏐3 1\\n0 0⏐⏐⏐⏐.(4.15)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e08ca804-e923-4db5-baa8-412646b0b0b1', embedding=None, metadata={'page_label': '103', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.1 Determinant and Trace 103\\nWe use (4.6) to compute the determinants of all 2×2matrices and obtain\\ndet(A) = 1(1−0)−2(3−0) + 3(0−0) =−5. (4.16)\\nFor completeness we can compare this result to computing the determi-\\nnant using Sarrus’ rule (4.7):\\ndet(A) = 1·1·1+3·0·3+0·2·2−0·1·3−1·0·2−3·2·1 = 1−6 =−5.(4.17)\\nForA∈Rn×nthe determinant exhibits the following properties:\\nThe determinant of a matrix product is the product of the corresponding\\ndeterminants, det(AB) = det(A)det(B).\\nDeterminants are invariant to transposition, i.e., det(A) = det(A⊤).\\nIfAis regular (invertible), then det(A−1) =1\\ndet(A).\\nSimilar matrices (Deﬁnition 2.22) possess the same determinant. There-\\nfore, for a linear mapping Φ :V→Vall transformation matrices AΦ\\nofΦhave the same determinant. Thus, the determinant is invariant to\\nthe choice of basis of a linear mapping.\\nAdding a multiple of a column/row to another one does not change\\ndet(A).\\nMultiplication of a column/row with λ∈Rscales det(A)byλ. In\\nparticular, det(λA) =λndet(A).\\nSwapping two rows/columns changes the sign of det(A).\\nBecause of the last three properties, we can use Gaussian elimination (see\\nSection 2.1) to compute det(A)by bringingAinto row-echelon form.\\nWe can stop Gaussian elimination when we have Ain a triangular form\\nwhere the elements below the diagonal are all 0. Recall from (4.8) that the\\ndeterminant of a triangular matrix is the product of the diagonal elements.\\nTheorem 4.3. A square matrix A∈Rn×nhasdet(A)̸= 0if and only if\\nrk(A) =n. In other words, Ais invertible if and only if it is full rank.\\nWhen mathematics was mainly performed by hand, the determinant\\ncalculation was considered an essential way to analyze matrix invertibil-\\nity. However, contemporary approaches in machine learning use direct\\nnumerical methods that superseded the explicit calculation of the deter-\\nminant. For example, in Chapter 2, we learned that inverse matrices can\\nbe computed by Gaussian elimination. Gaussian elimination can thus be\\nused to compute the determinant of a matrix.\\nDeterminants will play an important theoretical role for the following\\nsections, especially when we learn about eigenvalues and eigenvectors\\n(Section 4.2) through the characteristic polynomial.\\nDeﬁnition 4.4. Thetrace of a square matrix A∈Rn×nis deﬁned as trace\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5157c48-201a-4384-94e2-e0c98c90c917', embedding=None, metadata={'page_label': '104', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='104 Matrix Decompositions\\ntr(A) :=n∑\\ni=1aii, (4.18)\\ni.e. , the trace is the sum of the diagonal elements of A.\\nThe trace satisﬁes the following properties:\\ntr(A+B) =tr(A) +tr(B)forA,B∈Rn×n\\ntr(αA) =αtr(A),α∈RforA∈Rn×n\\ntr(In) =n\\ntr(AB) =tr(BA)forA∈Rn×k,B∈Rk×n\\nIt can be shown that only one function satisﬁes these four properties to-\\ngether – the trace (Gohberg et al., 2012).\\nThe properties of the trace of matrix products are more general. Specif-\\nically, the trace is invariant under cyclic permutations, i.e., The trace is\\ninvariant under\\ncyclic permutations. tr(AKL ) =tr(KLA ) (4.19)\\nfor matricesA∈Ra×k,K∈Rk×l,L∈Rl×a. This property generalizes to\\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\\nfollows that for two vectors x,y∈Rn\\ntr(xy⊤) =tr(y⊤x) =y⊤x∈R. (4.20)\\nGiven a linear mapping Φ :V→V, whereVis a vector space, we\\ndeﬁne the trace of this map by using the trace of matrix representation\\nofΦ. For a given basis of V, we can describe Φby means of the transfor-\\nmation matrix A. Then the trace of Φis the trace of A. For a different\\nbasis ofV, it holds that the corresponding transformation matrix BofΦ\\ncan be obtained by a basis change of the form S−1ASfor suitableS(see\\nSection 2.7.2). For the corresponding trace of Φ, this means\\ntr(B) =tr(S−1AS)(4.19)=tr(ASS−1) =tr(A). (4.21)\\nHence, while matrix representations of linear mappings are basis depen-\\ndent the trace of a linear mapping Φis independent of the basis.\\nIn this section, we covered determinants and traces as functions char-\\nacterizing a square matrix. Taking together our understanding of determi-\\nnants and traces we can now deﬁne an important equation describing a\\nmatrixAin terms of a polynomial, which we will use extensively in the\\nfollowing sections.\\nDeﬁnition 4.5 (Characteristic Polynomial) .Forλ∈Rand a square ma-\\ntrixA∈Rn×n\\npA(λ) := det(A−λI) (4.22a)\\n=c0+c1λ+c2λ2+···+cn−1λn−1+ (−1)nλn, (4.22b)\\nc0,...,cn−1∈R, is the characteristic polynomial ofA. In particular, characteristic\\npolynomial\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cce249dd-53da-4d0e-a8cb-c70c0a47ae4a', embedding=None, metadata={'page_label': '105', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Eigenvalues and Eigenvectors 105\\nc0= det(A), (4.23)\\ncn−1= (−1)n−1tr(A). (4.24)\\nThe characteristic polynomial (4.22a) will allow us to compute eigen-\\nvalues and eigenvectors, covered in the next section.\\n4.2 Eigenvalues and Eigenvectors\\nWe will now get to know a new way to characterize a matrix and its associ-\\nated linear mapping. Recall from Section 2.7.1 that every linear mapping\\nhas a unique transformation matrix given an ordered basis. We can in-\\nterpret linear mappings and their associated transformation matrices by\\nperforming an “eigen” analysis. As we will see, the eigenvalues of a lin- Eigen is a German\\nword meaning\\n“characteristic”,\\n“self”, or “own”.ear mapping will tell us how a special set of vectors, the eigenvectors, is\\ntransformed by the linear mapping.\\nDeﬁnition 4.6. LetA∈Rn×nbe a square matrix. Then λ∈Ris an\\neigenvalue ofAandx∈Rn\\\\{0}is the corresponding eigenvector ofAif eigenvalue\\neigenvectorAx=λx. (4.25)\\nWe call (4.25) the eigenvalue equation . eigenvalue equation\\nRemark. In the linear algebra literature and software, it is often a conven-\\ntion that eigenvalues are sorted in descending order, so that the largest\\neigenvalue and associated eigenvector are called the ﬁrst eigenvalue and\\nits associated eigenvector, and the second largest called the second eigen-\\nvalue and its associated eigenvector, and so on. However, textbooks and\\npublications may have different or no notion of orderings. We do not want\\nto presume an ordering in this book if not stated explicitly. ♦\\nThe following statements are equivalent:\\nλis an eigenvalue of A∈Rn×n.\\nThere exists an x∈Rn\\\\{0}withAx=λx, or equivalently, (A−\\nλIn)x=0can be solved non-trivially, i.e., x̸=0.\\nrk(A−λIn)<n.\\ndet(A−λIn) = 0 .\\nDeﬁnition 4.7 (Collinearity and Codirection) .Two vectors that point in\\nthe same direction are called codirected . Two vectors are collinear if they codirected\\ncollinear point in the same or the opposite direction.\\nRemark (Non-uniqueness of eigenvectors) .Ifxis an eigenvector of A\\nassociated with eigenvalue λ, then for any c∈R\\\\{0}it holds that cxis\\nan eigenvector of Awith the same eigenvalue since\\nA(cx) =cAx=cλx=λ(cx). (4.26)\\nThus, all vectors that are collinear to xare also eigenvectors of A.\\n♦\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c02e4c11-70dd-4aa4-839a-9854a7ab251a', embedding=None, metadata={'page_label': '106', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='106 Matrix Decompositions\\nTheorem 4.8. λ∈Ris an eigenvalue of A∈Rn×nif and only if λis a\\nroot of the characteristic polynomial pA(λ)ofA.\\nDeﬁnition 4.9. Let a square matrix Ahave an eigenvalue λi. The algebraic algebraic\\nmultiplicity multiplicity ofλiis the number of times the root appears in the character-\\nistic polynomial.\\nDeﬁnition 4.10 (Eigenspace and Eigenspectrum) .ForA∈Rn×n, the set\\nof all eigenvectors of Aassociated with an eigenvalue λspans a subspace\\nofRn, which is called the eigenspace ofAwith respect to λand is denoted eigenspace\\nbyEλ. The set of all eigenvalues of Ais called the eigenspectrum , or just eigenspectrum\\nspectrum , ofA. spectrum\\nIfλis an eigenvalue of A∈Rn×n, then the corresponding eigenspace\\nEλis the solution space of the homogeneous system of linear equations\\n(A−λI)x=0. Geometrically, the eigenvector corresponding to a nonzero\\neigenvalue points in a direction that is stretched by the linear mapping.\\nThe eigenvalue is the factor by which it is stretched. If the eigenvalue is\\nnegative, the direction of the stretching is ﬂipped.\\nExample 4.4 (The Case of the Identity Matrix)\\nThe identity matrix I∈Rn×nhas characteristic polynomial pI(λ) =\\ndet(I−λI) = (1−λ)n= 0, which has only one eigenvalue λ= 1that oc-\\ncursntimes. Moreover, Ix=λx= 1xholds for all vectors x∈Rn\\\\{0}.\\nBecause of this, the sole eigenspace E1of the identity matrix spans ndi-\\nmensions, and all nstandard basis vectors of Rnare eigenvectors of I.\\nUseful properties regarding eigenvalues and eigenvectors include the\\nfollowing:\\nA matrixAand its transpose A⊤possess the same eigenvalues, but not\\nnecessarily the same eigenvectors.\\nThe eigenspace Eλis the null space of A−λIsince\\nAx=λx⇐⇒Ax−λx=0 (4.27a)\\n⇐⇒ (A−λI)x=0⇐⇒x∈ker(A−λI).(4.27b)\\nSimilar matrices (see Deﬁnition 2.22) possess the same eigenvalues.\\nTherefore, a linear mapping Φhas eigenvalues that are independent of\\nthe choice of basis of its transformation matrix. This makes eigenvalues,\\ntogether with the determinant and the trace, key characteristic param-\\neters of a linear mapping as they are all invariant under basis change.\\nSymmetric, positive deﬁnite matrices always have positive, real eigen-\\nvalues.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7cd3b08c-0391-4a00-928c-404865fec8fc', embedding=None, metadata={'page_label': '107', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Eigenvalues and Eigenvectors 107\\nExample 4.5 (Computing Eigenvalues, Eigenvectors, and\\nEigenspaces)\\nLet us ﬁnd the eigenvalues and eigenvectors of the 2×2matrix\\nA=[4 2\\n1 3]\\n. (4.28)\\nStep 1: Characteristic Polynomial. From our deﬁnition of the eigen-\\nvectorx̸=0and eigenvalue λofA, there will be a vector such that\\nAx=λx, i.e., (A−λI)x=0. Sincex̸=0, this requires that the kernel\\n(null space) of A−λIcontains more elements than just 0. This means\\nthatA−λIis not invertible and therefore det(A−λI) = 0 . Hence, we\\nneed to compute the roots of the characteristic polynomial (4.22a) to ﬁnd\\nthe eigenvalues.\\nStep 2: Eigenvalues. The characteristic polynomial is\\npA(λ) = det(A−λI) (4.29a)\\n= det([4 2\\n1 3]\\n−[λ0\\n0λ])\\n=⏐⏐⏐⏐4−λ 2\\n1 3−λ⏐⏐⏐⏐(4.29b)\\n= (4−λ)(3−λ)−2·1. (4.29c)\\nWe factorize the characteristic polynomial and obtain\\np(λ) = (4−λ)(3−λ)−2·1 = 10−7λ+λ2= (2−λ)(5−λ)(4.30)\\ngiving the roots λ1= 2andλ2= 5.\\nStep 3: Eigenvectors and Eigenspaces. We ﬁnd the eigenvectors that\\ncorrespond to these eigenvalues by looking at vectors xsuch that\\n[4−λ 2\\n1 3−λ]\\nx=0. (4.31)\\nForλ= 5we obtain\\n[4−5 2\\n1 3−5][x1\\nx2]\\n=[−1 2\\n1−2][x1\\nx2]\\n=0. (4.32)\\nWe solve this homogeneous system and obtain a solution space\\nE5= span[[2\\n1]\\n]. (4.33)\\nThis eigenspace is one-dimensional as it possesses a single basis vector.\\nAnalogously, we ﬁnd the eigenvector for λ= 2by solving the homoge-\\nneous system of equations\\n[4−2 2\\n1 3−2]\\nx=[2 2\\n1 1]\\nx=0. (4.34)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7888611-b25f-4040-b247-f77dc3759de6', embedding=None, metadata={'page_label': '108', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='108 Matrix Decompositions\\nThis means any vector x=[x1\\nx2]\\n, wherex2=−x1, such as[1\\n−1]\\n, is an\\neigenvector with eigenvalue 2. The corresponding eigenspace is given as\\nE2= span[[1\\n−1]\\n]. (4.35)\\nThe two eigenspaces E5andE2in Example 4.5 are one-dimensional\\nas they are each spanned by a single vector. However, in other cases\\nwe may have multiple identical eigenvalues (see Deﬁnition 4.9) and the\\neigenspace may have more than one dimension.\\nDeﬁnition 4.11. Letλibe an eigenvalue of a square matrix A. Then the\\ngeometric multiplicity ofλiis the number of linearly independent eigen- geometric\\nmultiplicity vectors associated with λi. In other words, it is the dimensionality of the\\neigenspace spanned by the eigenvectors associated with λi.\\nRemark. A speciﬁc eigenvalue’s geometric multiplicity must be at least\\none because every eigenvalue has at least one associated eigenvector. An\\neigenvalue’s geometric multiplicity cannot exceed its algebraic multiplic-\\nity, but it may be lower. ♦\\nExample 4.6\\nThe matrixA=[2 1\\n0 2]\\nhas two repeated eigenvalues λ1=λ2= 2and an\\nalgebraic multiplicity of 2. The eigenvalue has, however, only one distinct\\nunit eigenvector x1=[1\\n0]\\nand, thus, geometric multiplicity 1.\\nGraphical Intuition in Two Dimensions\\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\\nues using different linear mappings. Figure 4.4 depicts ﬁve transformation\\nmatricesA1,...,A5and their impact on a square grid of points, centered\\nat the origin: In geometry, the\\narea-preserving\\nproperties of this\\ntype of shearing\\nparallel to an axis is\\nalso known as\\nCavalieri’s principle\\nof equal areas for\\nparallelograms\\n(Katz, 2004).A1=[1\\n20\\n0 2]\\n. The direction of the two eigenvectors correspond to the\\ncanonical basis vectors in R2, i.e., to two cardinal axes. The vertical axis\\nis extended by a factor of 2(eigenvalue λ1= 2), and the horizontal axis\\nis compressed by factor1\\n2(eigenvalue λ2=1\\n2). The mapping is area\\npreserving ( det(A1) = 1 = 2·1\\n2).\\nA2=[11\\n2\\n0 1]\\ncorresponds to a shearing mapping , i.e., it shears the\\npoints along the horizontal axis to the right if they are on the positive\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5fd7ce0-bedf-4446-a380-e48f6db905ee', embedding=None, metadata={'page_label': '109', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Eigenvalues and Eigenvectors 109\\nFigure 4.4\\nDeterminants and\\neigenspaces.\\nOverview of ﬁve\\nlinear mappings and\\ntheir associated\\ntransformation\\nmatrices\\nAi∈R2×2\\nprojecting 400\\ncolor-coded points\\nx∈R2(left\\ncolumn) onto target\\npointsAix(right\\ncolumn). The\\ncentral column\\ndepicts the ﬁrst\\neigenvector,\\nstretched by its\\nassociated\\neigenvalueλ1, and\\nthe second\\neigenvector\\nstretched by its\\neigenvalueλ2. Each\\nrow depicts the\\neffect of one of ﬁve\\ntransformation\\nmatricesAiwith\\nrespect to the\\nstandard basis.\\ndet(A) = 1.0λ1= 2.0\\nλ2= 0.5\\ndet(A) = 1.0λ1= 1.0\\nλ2= 1.0\\ndet(A) = 1.0λ1= (0.87-0.5j)\\nλ2= (0.87+0.5j)\\ndet(A) = 0.0λ1= 0.0\\nλ2= 2.0\\ndet(A) = 0.75λ1= 0.5\\nλ2= 1.5\\nhalf of the vertical axis, and to the left vice versa. This mapping is area\\npreserving ( det(A2) = 1 ). The eigenvalue λ1= 1 =λ2is repeated\\nand the eigenvectors are collinear (drawn here for emphasis in two\\nopposite directions). This indicates that the mapping acts only along\\none direction (the horizontal axis).\\nA3=[cos(π\\n6)−sin(π\\n6)\\nsin(π\\n6) cos(π\\n6)]\\n=1\\n2[√\\n3−1\\n1√\\n3]\\nThe matrixA3rotates the\\npoints byπ\\n6rad = 30◦counter-clockwise and has only complex eigen-\\nvalues, reﬂecting that the mapping is a rotation (hence, no eigenvectors\\nare drawn). A rotation has to be volume preserving, and so the deter-\\nminant is 1. For more details on rotations, we refer to Section 3.9.\\nA4=[1−1\\n−1 1]\\nrepresents a mapping in the standard basis that col-\\nlapses a two-dimensional domain onto one dimension. Since one eigen-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7eab8a13-e3d7-43d8-acbc-e5a75e9a9951', embedding=None, metadata={'page_label': '110', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='110 Matrix Decompositions\\nvalue is 0, the space in direction of the (blue) eigenvector corresponding\\ntoλ1= 0 collapses, while the orthogonal (red) eigenvector stretches\\nspace by a factor λ2= 2. Therefore, the area of the image is 0.\\nA5=[11\\n21\\n21]\\nis a shear-and-stretch mapping that scales space by 75%\\nsince|det(A5)|=3\\n4. It stretches space along the (red) eigenvector\\nofλ2by a factor 1.5and compresses it along the orthogonal (blue)\\neigenvector by a factor 0.5.\\nExample 4.7 (Eigenspectrum of a Biological Neural Network)\\nFigure 4.5\\nCaenorhabditis\\nelegans neural\\nnetwork (Kaiser and\\nHilgetag,\\n2006).(a) Sym-\\nmetrized\\nconnectivity matrix;\\n(b) Eigenspectrum.\\n0 50 100 150 200 250\\nneuron index0\\n50\\n100\\n150\\n200\\n250neuron index\\n(a) Connectivity matrix.\\n0 100 200\\nindex of sorted eigenvalue−10−50510152025eigenvalue\\n (b) Eigenspectrum.\\nMethods to analyze and learn from network data are an essential com-\\nponent of machine learning methods. The key to understanding networks\\nis the connectivity between network nodes, especially if two nodes are\\nconnected to each other or not. In data science applications, it is often\\nuseful to study the matrix that captures this connectivity data.\\nWe build a connectivity/adjacency matrix A∈R277×277of the complete\\nneural network of the worm C.Elegans . Each row/column represents one\\nof the 277neurons of this worm’s brain. The connectivity matrix Ahas\\na value ofaij= 1 if neuronitalks to neuron jthrough a synapse, and\\naij= 0 otherwise. The connectivity matrix is not symmetric, which im-\\nplies that eigenvalues may not be real valued. Therefore, we compute a\\nsymmetrized version of the connectivity matrix as Asym:=A+A⊤. This\\nnew matrixAsymis shown in Figure 4.5(a) and has a nonzero value aijif\\nand only if two neurons are connected (white pixels), irrespective of the\\ndirection of the connection. In Figure 4.5(b), we show the correspond-\\ning eigenspectrum of Asym. The horizontal axis shows the index of the\\neigenvalues, sorted in descending order. The vertical axis shows the corre-\\nsponding eigenvalue. The S-like shape of this eigenspectrum is typical for\\nmany biological neural networks. The underlying mechanism responsible\\nfor this is an area of active neuroscience research.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e41630cb-1e40-4adb-bc62-67d36983dd56', embedding=None, metadata={'page_label': '111', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Eigenvalues and Eigenvectors 111\\nTheorem 4.12. The eigenvectors x1,...,xnof a matrixA∈Rn×nwithn\\ndistinct eigenvalues λ1,...,λnare linearly independent.\\nThis theorem states that eigenvectors of a matrix with ndistinct eigen-\\nvalues form a basis of Rn.\\nDeﬁnition 4.13. A square matrix A∈Rn×nisdefective if it possesses defective\\nfewer thannlinearly independent eigenvectors.\\nA non-defective matrix A∈Rn×ndoes not necessarily require ndis-\\ntinct eigenvalues, but it does require that the eigenvectors form a basis of\\nRn. Looking at the eigenspaces of a defective matrix, it follows that the\\nsum of the dimensions of the eigenspaces is less than n. Speciﬁcally, a de-\\nfective matrix has at least one eigenvalue λiwith an algebraic multiplicity\\nm> 1and a geometric multiplicity of less than m.\\nRemark. A defective matrix cannot have ndistinct eigenvalues, as distinct\\neigenvalues have linearly independent eigenvectors (Theorem 4.12). ♦\\nTheorem 4.14. Given a matrix A∈Rm×n, we can always obtain a sym-\\nmetric, positive semideﬁnite matrix S∈Rn×nby deﬁning\\nS:=A⊤A. (4.36)\\nRemark. Ifrk(A) =n, thenS:=A⊤Ais symmetric, positive deﬁnite.\\n♦\\nUnderstanding why Theorem 4.14 holds is insightful for how we can\\nuse symmetrized matrices: Symmetry requires S=S⊤, and by insert-\\ning (4.36) we obtain S=A⊤A=A⊤(A⊤)⊤= (A⊤A)⊤=S⊤. More-\\nover, positive semideﬁniteness (Section 3.2.3) requires that x⊤Sx⩾0\\nand inserting (4.36) we obtain x⊤Sx=x⊤A⊤Ax= (x⊤A⊤)(Ax) =\\n(Ax)⊤(Ax)⩾0, because the dot product computes a sum of squares\\n(which are themselves non-negative).\\nspectral theorem\\nTheorem 4.15 (Spectral Theorem) .IfA∈Rn×nis symmetric, there ex-\\nists an orthonormal basis of the corresponding vector space Vconsisting of\\neigenvectors of A, and each eigenvalue is real.\\nA direct implication of the spectral theorem is that the eigendecompo-\\nsition of a symmetric matrix Aexists (with real eigenvalues), and that\\nwe can ﬁnd an ONB of eigenvectors so that A=PDP⊤, whereDis\\ndiagonal and the columns of Pcontain the eigenvectors.\\nExample 4.8\\nConsider the matrix\\nA=\\uf8ee\\n\\uf8f03 2 2\\n2 3 2\\n2 2 3\\uf8f9\\n\\uf8fb. (4.37)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87d19ea3-1ba2-4a25-aeb6-8a486ce22b6d', embedding=None, metadata={'page_label': '112', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='112 Matrix Decompositions\\nThe characteristic polynomial of Ais\\npA(λ) =−(λ−1)2(λ−7), (4.38)\\nso that we obtain the eigenvalues λ1= 1 andλ2= 7, whereλ1is a\\nrepeated eigenvalue. Following our standard procedure for computing\\neigenvectors, we obtain the eigenspaces\\nE1= span[\\uf8ee\\n\\uf8f0−1\\n1\\n0\\uf8f9\\n\\uf8fb\\n\\ued19\\ued18\\ued17\\ued1a\\n=:x1,\\uf8ee\\n\\uf8f0−1\\n0\\n1\\uf8f9\\n\\uf8fb\\n\\ued19\\ued18\\ued17\\ued1a\\n=:x2], E 7= span[\\uf8ee\\n\\uf8f01\\n1\\n1\\uf8f9\\n\\uf8fb\\n\\ued19\\ued18\\ued17\\ued1a\\n=:x3]. (4.39)\\nWe see thatx3is orthogonal to both x1andx2. However, since x⊤\\n1x2=\\n1̸= 0, they are not orthogonal. The spectral theorem (Theorem 4.15)\\nstates that there exists an orthogonal basis, but the one we have is not\\northogonal. However, we can construct one.\\nTo construct such a basis, we exploit the fact that x1,x2are eigenvec-\\ntors associated with the same eigenvalue λ. Therefore, for any α,β∈Rit\\nholds that\\nA(αx1+βx2) =Ax1α+Ax2β=λ(αx1+βx2), (4.40)\\ni.e., any linear combination of x1andx2is also an eigenvector of Aas-\\nsociated with λ. The Gram-Schmidt algorithm (Section 3.8.3) is a method\\nfor iteratively constructing an orthogonal/orthonormal basis from a set of\\nbasis vectors using such linear combinations. Therefore, even if x1andx2\\nare not orthogonal, we can apply the Gram-Schmidt algorithm and ﬁnd\\neigenvectors associated with λ1= 1 that are orthogonal to each other\\n(and tox3). In our example, we will obtain\\nx′\\n1=\\uf8ee\\n\\uf8f0−1\\n1\\n0\\uf8f9\\n\\uf8fb,x′\\n2=1\\n2\\uf8ee\\n\\uf8f0−1\\n−1\\n2\\uf8f9\\n\\uf8fb, (4.41)\\nwhich are orthogonal to each other, orthogonal to x3, and eigenvectors of\\nAassociated with λ1= 1.\\nBefore we conclude our considerations of eigenvalues and eigenvectors\\nit is useful to tie these matrix characteristics together with the concepts of\\nthe determinant and the trace.\\nTheorem 4.16. The determinant of a matrix A∈Rn×nis the product of\\nits eigenvalues, i.e.,\\ndet(A) =n∏\\ni=1λi, (4.42)\\nwhereλi∈Care (possibly repeated) eigenvalues of A.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88f43aea-0523-4dca-88b2-646bd5d5fb1d', embedding=None, metadata={'page_label': '113', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.2 Eigenvalues and Eigenvectors 113\\nFigure 4.6\\nGeometric\\ninterpretation of\\neigenvalues. The\\neigenvectors of A\\nget stretched by the\\ncorresponding\\neigenvalues. The\\narea of the unit\\nsquare changes by\\n|λ1λ2|, the\\nperimeter changes\\nby a factor of\\n1\\n2(|λ1|+|λ2|).x1x2\\nv1v2A\\nTheorem 4.17. The trace of a matrix A∈Rn×nis the sum of its eigenval-\\nues, i.e.,\\ntr(A) =n∑\\ni=1λi, (4.43)\\nwhereλi∈Care (possibly repeated) eigenvalues of A.\\nLet us provide a geometric intuition of these two theorems. Consider\\na matrixA∈R2×2that possesses two linearly independent eigenvectors\\nx1,x2. For this example, we assume (x1,x2)are an ONB of R2so that they\\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\\nFrom Section 4.1, we know that the determinant computes the change of\\narea of unit square under the transformation A. In this example, we can\\ncompute the change of area explicitly: Mapping the eigenvectors using\\nAgives us vectors v1=Ax1=λ1x1andv2=Ax2=λ2x2, i.e., the\\nnew vectorsviare scaled versions of the eigenvectors xi, and the scaling\\nfactors are the corresponding eigenvalues λi.v1,v2are still orthogonal,\\nand the area of the rectangle they span is |λ1λ2|.\\nGiven thatx1,x2(in our example) are orthonormal, we can directly\\ncompute the perimeter of the unit square as 2(1 + 1) . Mapping the eigen-\\nvectors using Acreates a rectangle whose perimeter is 2(|λ1|+|λ2|).\\nTherefore, the sum of the absolute values of the eigenvalues tells us how\\nthe perimeter of the unit square changes under the transformation matrix\\nA.\\nExample 4.9 (Google’s PageRank – Webpages as Eigenvectors)\\nGoogle uses the eigenvector corresponding to the maximal eigenvalue of\\na matrixAto determine the rank of a page for search. The idea for the\\nPageRank algorithm, developed at Stanford University by Larry Page and\\nSergey Brin in 1996, was that the importance of any web page can be ap-\\nproximated by the importance of pages that link to it. For this, they write\\ndown all web sites as a huge directed graph that shows which page links\\nto which. PageRank computes the weight (importance) xi⩾0of a web\\nsiteaiby counting the number of pages pointing to ai. Moreover, PageR-\\nank takes into account the importance of the web sites that link to ai. The\\nnavigation behavior of a user is then modeled by a transition matrix Aof\\nthis graph that tells us with what (click) probability somebody will end up\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f0d20aa-d265-4f3b-b3a0-eaad09d47911', embedding=None, metadata={'page_label': '114', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='114 Matrix Decompositions\\non a different web site. The matrix Ahas the property that for any ini-\\ntial rank/importance vector xof a web site the sequence x,Ax,A2x,...\\nconverges to a vector x∗. This vector is called the PageRank and satisﬁes PageRank\\nAx∗=x∗, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\\nA. After normalizing x∗, such that∥x∗∥= 1, we can interpret the entries\\nas probabilities. More details and different perspectives on PageRank can\\nbe found in the original technical report (Page et al., 1999).\\n4.3 Cholesky Decomposition\\nThere are many ways to factorize special types of matrices that we en-\\ncounter often in machine learning. In the positive real numbers, we have\\nthe square-root operation that gives us a decomposition of the number\\ninto identical components, e.g., 9 = 3·3. For matrices, we need to be\\ncareful that we compute a square-root-like operation on positive quanti-\\nties. For symmetric, positive deﬁnite matrices (see Section 3.2.3), we can\\nchoose from a number of square-root equivalent operations. The Cholesky Cholesky\\ndecomposition decomposition /Cholesky factorization provides a square-root equivalent op-\\nCholesky\\nfactorizationeration on symmetric, positive deﬁnite matrices that is useful in practice.\\nTheorem 4.18 (Cholesky Decomposition) .A symmetric, positive deﬁnite\\nmatrixAcan be factorized into a product A=LL⊤, whereLis a lower-\\ntriangular matrix with positive diagonal elements:\\n\\uf8ee\\n\\uf8ef\\uf8f0a11···a1n\\n.........\\nan1···ann\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8f0l11··· 0\\n.........\\nln1···lnn\\uf8f9\\n\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8f0l11···ln1\\n.........\\n0···lnn\\uf8f9\\n\\uf8fa\\uf8fb. (4.44)\\nLis called the Cholesky factor of A, andLis unique. Cholesky factor\\nExample 4.10 (Cholesky Factorization)\\nConsider a symmetric, positive deﬁnite matrix A∈R3×3. We are inter-\\nested in ﬁnding its Cholesky factorization A=LL⊤, i.e.,\\nA=\\uf8ee\\n\\uf8f0a11a21a31\\na21a22a32\\na31a32a33\\uf8f9\\n\\uf8fb=LL⊤=\\uf8ee\\n\\uf8f0l110 0\\nl21l220\\nl31l32l33\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f0l11l21l31\\n0l22l32\\n0 0l33\\uf8f9\\n\\uf8fb.(4.45)\\nMultiplying out the right-hand side yields\\nA=\\uf8ee\\n\\uf8f0l2\\n11l21l11 l31l11\\nl21l11l2\\n21+l2\\n22l31l21+l32l22\\nl31l11l31l21+l32l22l2\\n31+l2\\n32+l2\\n33\\uf8f9\\n\\uf8fb. (4.46)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d70067e7-5a67-44e5-85b4-94ade4577ca7', embedding=None, metadata={'page_label': '115', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Eigendecomposition and Diagonalization 115\\nComparing the left-hand side of (4.45) and the right-hand side of (4.46)\\nshows that there is a simple pattern in the diagonal elements lii:\\nl11=√a11, l 22=√\\na22−l2\\n21, l 33=√\\na33−(l2\\n31+l2\\n32).(4.47)\\nSimilarly for the elements below the diagonal ( lij, wherei > j ), there is\\nalso a repeating pattern:\\nl21=1\\nl11a21, l 31=1\\nl11a31, l 32=1\\nl22(a32−l31l21). (4.48)\\nThus, we constructed the Cholesky decomposition for any symmetric, pos-\\nitive deﬁnite 3×3matrix. The key realization is that we can backward\\ncalculate what the components lijfor theLshould be, given the values\\naijforAand previously computed values of lij.\\nThe Cholesky decomposition is an important tool for the numerical\\ncomputations underlying machine learning. Here, symmetric positive def-\\ninite matrices require frequent manipulation, e.g., the covariance matrix\\nof a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\\ndeﬁnite. The Cholesky factorization of this covariance matrix allows us to\\ngenerate samples from a Gaussian distribution. It also allows us to perform\\na linear transformation of random variables, which is heavily exploited\\nwhen computing gradients in deep stochastic models, such as the varia-\\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\\n2014). The Cholesky decomposition also allows us to compute determi-\\nnants very efﬁciently. Given the Cholesky decomposition A=LL⊤, we\\nknow that det(A) = det(L) det(L⊤) = det(L)2. SinceLis a triangular\\nmatrix, the determinant is simply the product of its diagonal entries so\\nthatdet(A) =∏\\nil2\\nii. Thus, many numerical software packages use the\\nCholesky decomposition to make computations more efﬁcient.\\n4.4 Eigendecomposition and Diagonalization\\nAdiagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\\nments, i.e., they are of the form\\nD=\\uf8ee\\n\\uf8ef\\uf8f0c1··· 0\\n.........\\n0···cn\\uf8f9\\n\\uf8fa\\uf8fb. (4.49)\\nThey allow fast computation of determinants, powers, and inverses. The\\ndeterminant is the product of its diagonal entries, a matrix power Dkis\\ngiven by each diagonal element raised to the power k, and the inverse\\nD−1is the reciprocal of its diagonal elements if all of them are nonzero.\\nIn this section, we will discuss how to transform matrices into diagonal\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15397e83-344f-4e77-ac9c-c2ea4bcc9edf', embedding=None, metadata={'page_label': '116', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='116 Matrix Decompositions\\nform. This is an important application of the basis change we discussed in\\nSection 2.7.2 and eigenvalues from Section 4.2.\\nRecall that two matrices A,Dare similar (Deﬁnition 2.22) if there ex-\\nists an invertible matrix P, such thatD=P−1AP. More speciﬁcally, we\\nwill look at matrices Athat are similar to diagonal matrices Dthat con-\\ntain the eigenvalues of Aon the diagonal.\\nDeﬁnition 4.19 (Diagonalizable) .A matrixA∈Rn×nisdiagonalizable diagonalizable\\nif it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\\nP∈Rn×nsuch thatD=P−1AP.\\nIn the following, we will see that diagonalizing a matrix A∈Rn×nis\\na way of expressing the same linear mapping but in another basis (see\\nSection 2.6.1), which will turn out to be a basis that consists of the eigen-\\nvectors ofA.\\nLetA∈Rn×n, letλ1,...,λnbe a set of scalars, and let p1,...,pnbe a\\nset of vectors in Rn. We deﬁneP:= [p1,...,pn]and letD∈Rn×nbe a\\ndiagonal matrix with diagonal entries λ1,...,λn. Then we can show that\\nAP=PD (4.50)\\nif and only if λ1,...,λnare the eigenvalues of Aandp1,...,pnare cor-\\nresponding eigenvectors of A.\\nWe can see that this statement holds because\\nAP=A[p1,...,pn] = [Ap1,...,Apn], (4.51)\\nPD = [p1,...,pn]\\uf8ee\\n\\uf8ef\\uf8f0λ1 0\\n...\\n0λn\\uf8f9\\n\\uf8fa\\uf8fb= [λ1p1,...,λnpn]. (4.52)\\nThus, (4.50) implies that\\nAp1=λ1p1 (4.53)\\n...\\nApn=λnpn. (4.54)\\nTherefore, the columns of Pmust be eigenvectors of A.\\nOur deﬁnition of diagonalization requires that P∈Rn×nis invertible,\\ni.e.,Phas full rank (Theorem 4.3). This requires us to have nlinearly\\nindependent eigenvectors p1,...,pn, i.e., thepiform a basis of Rn.\\nTheorem 4.20 (Eigendecomposition) .A square matrix A∈Rn×ncan be\\nfactored into\\nA=PDP−1, (4.55)\\nwhereP∈Rn×nandDis a diagonal matrix whose diagonal entries are\\nthe eigenvalues of A, if and only if the eigenvectors of Aform a basis of Rn.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a90c9fd7-3177-463c-9653-efc57105a401', embedding=None, metadata={'page_label': '117', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Eigendecomposition and Diagonalization 117\\nFigure 4.7 Intuition\\nbehind the\\neigendecomposition\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left:P−1\\nperforms a basis\\nchange (here drawn\\ninR2and depicted\\nas a rotation-like\\noperation) from the\\nstandard basis into\\nthe eigenbasis.\\nBottom-left to\\nbottom-right: D\\nperforms a scaling\\nalong the remapped\\northogonal\\neigenvectors,\\ndepicted here by a\\ncircle being\\nstretched to an\\nellipse. Bottom-right\\nto top-right:P\\nundoes the basis\\nchange (depicted as\\na reverse rotation)\\nand restores the\\noriginal coordinate\\nframe.\\ne1e2\\np1p2\\np1p2e1e2\\np1p2\\nλ1p1λ2p2\\ne1e2\\nAe 1Ae 2P−1\\nDPA\\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\\nized and that the columns of Pare theneigenvectors of A. For symmetric\\nmatrices we can obtain even stronger outcomes for the eigenvalue decom-\\nposition.\\nTheorem 4.21. A symmetric matrix S∈Rn×ncan always be diagonalized.\\nTheorem 4.21 follows directly from the spectral theorem 4.15. More-\\nover, the spectral theorem states that we can ﬁnd an ONB of eigenvectors\\nofRn. This makesPan orthogonal matrix so that D=P⊤AP.\\nRemark. The Jordan normal form of a matrix offers a decomposition that\\nworks for defective matrices (Lang, 1987) but is beyond the scope of this\\nbook. ♦\\nGeometric Intuition for the Eigendecomposition\\nWe can interpret the eigendecomposition of a matrix as follows (see also\\nFigure 4.7): Let Abe the transformation matrix of a linear mapping with\\nrespect to the standard basis. P−1performs a basis change from the stan-\\ndard basis into the eigenbasis. This identiﬁes the eigenvectors pi(red and\\norange arrows in Figure 4.7) onto the standard basis vectors ei. Then, the\\ndiagonalDscales the vectors along these axes by the eigenvalues λi. Fi-\\nnally,Ptransforms these scaled vectors back into the standard/canonical\\ncoordinates yielding λipi.\\nExample 4.11 (Eigendecomposition)\\nLet us compute the eigendecomposition of A=1\\n2[5−2\\n−2 5]\\n.\\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d37da45-e3e8-4549-9b4c-4a1857b108f8', embedding=None, metadata={'page_label': '118', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='118 Matrix Decompositions\\npolynomial of Ais\\ndet(A−λI) = det([5\\n2−λ−1\\n−15\\n2−λ])\\n(4.56a)\\n= (5\\n2−λ)2−1 =λ2−5λ+21\\n4= (λ−7\\n2)(λ−3\\n2). (4.56b)\\nTherefore, the eigenvalues of Aareλ1=7\\n2andλ2=3\\n2(the roots of the\\ncharacteristic polynomial), and the associated (normalized) eigenvectors\\nare obtained via\\n[2 1\\n1 2]\\np1=7\\n2p1,[2 1\\n1 2]\\np2=3\\n2p2. (4.57)\\nThis yields\\np1=1√\\n2[1\\n−1]\\n,p2=1√\\n2[1\\n1]\\n. (4.58)\\nStep 2: Check for existence. The eigenvectors p1,p2form a basis of R2.\\nTherefore,Acan be diagonalized.\\nStep 3: Construct the matrix Pto diagonalize A.We collect the eigen-\\nvectors ofAinPso that\\nP= [p1,p2] =1√\\n2[1 1\\n−1 1]\\n. (4.59)\\nWe then obtain\\nP−1AP=[7\\n20\\n03\\n2]\\n=D. (4.60)\\nEquivalently, we get (exploiting that P−1=P⊤since the eigenvectors Figure 4.7 visualizes\\nthe\\neigendecomposition\\nofA=[5−2\\n−2 5]\\nas a sequence of\\nlinear\\ntransformations.p1andp2in this example form an ONB)\\n1\\n2[5−2\\n−2 5]\\n\\ued19\\ued18\\ued17\\ued1a\\nA=1√\\n2[1 1\\n−1 1]\\n\\ued19\\ued18\\ued17\\ued1a\\nP[7\\n20\\n03\\n2]\\n\\ued19\\ued18\\ued17\\ued1a\\nD1√\\n2[1−1\\n1 1]\\n\\ued19\\ued18\\ued17\\ued1a\\nP−1. (4.61)\\nDiagonal matrices Dcan efﬁciently be raised to a power. Therefore,\\nwe can ﬁnd a matrix power for a matrix A∈Rn×nvia the eigenvalue\\ndecomposition (if it exists) so that\\nAk= (PDP−1)k=PDkP−1. (4.62)\\nComputingDkis efﬁcient because we apply this operation individually\\nto any diagonal element.\\nAssume that the eigendecomposition A=PDP−1exists. Then,\\ndet(A) = det(PDP−1) = det(P) det(D) det(P−1) (4.63a)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90fceb68-f025-4ef7-9cf1-c79ca7d4f983', embedding=None, metadata={'page_label': '119', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Singular Value Decomposition 119\\n= det(D) =∏\\nidii (4.63b)\\nallows for an efﬁcient computation of the determinant of A.\\nThe eigenvalue decomposition requires square matrices. It would be\\nuseful to perform a decomposition on general matrices. In the next sec-\\ntion, we introduce a more general matrix decomposition technique, the\\nsingular value decomposition.\\n4.5 Singular Value Decomposition\\nThe singular value decomposition (SVD) of a matrix is a central matrix\\ndecomposition method in linear algebra. It has been referred to as the\\n“fundamental theorem of linear algebra” (Strang, 1993) because it can be\\napplied to all matrices, not only to square matrices, and it always exists.\\nMoreover, as we will explore in the following, the SVD of a matrix A,\\nwhich represents a linear mapping Φ :V→W, quantiﬁes the change\\nbetween the underlying geometry of these two vector spaces. We recom-\\nmend the work by Kalman (1996) and Roy and Banerjee (2014) for a\\ndeeper overview of the mathematics of the SVD.\\nSVD theorem\\nTheorem 4.22 (SVD Theorem) .LetAm×nbe a rectangular matrix of rank\\nr∈[0,min(m,n)]. The SVD ofAis a decomposition of the form SVD\\nsingular value\\ndecomposition\\n=UA V⊤Σmn\\nmm\\nmn\\nnn\\n(4.64)\\nwith an orthogonal matrix U∈Rm×mwith column vectors ui,i= 1,...,m ,\\nand an orthogonal matrix V∈Rn×nwith column vectors vj,j= 1,...,n .\\nMoreover, Σis anm×nmatrix with Σii=σi⩾0andΣij= 0, i̸=j.\\nThe diagonal entries σi,i= 1,...,r , ofΣare called the singular values ,singular values\\nuiare called the left-singular vectors , andvjare called the right-singular left-singular vectors\\nright-singular\\nvectorsvectors . By convention, the singular values are ordered, i.e., σ1⩾σ2⩾\\nσr⩾0.\\nThesingular value matrix Σis unique, but it requires some attention. singular value\\nmatrix Observe that the Σ∈Rm×nis rectangular. In particular, Σis of the same\\nsize asA. This means that Σhas a diagonal submatrix that contains the\\nsingular values and needs additional zero padding. Speciﬁcally, if m>n ,\\nthen the matrix Σhas diagonal structure up to row nand then consists of\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdfaeab4-8c5b-437b-805a-ff5bf256f971', embedding=None, metadata={'page_label': '120', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='120 Matrix Decompositions\\nFigure 4.8 Intuition\\nbehind the SVD of a\\nmatrixA∈R3×2\\nas sequential\\ntransformations.\\nTop-left to\\nbottom-left:V⊤\\nperforms a basis\\nchange in R2.\\nBottom-left to\\nbottom-right: Σ\\nscales and maps\\nfromR2toR3. The\\nellipse in the\\nbottom-right lives in\\nR3. The third\\ndimension is\\northogonal to the\\nsurface of the\\nelliptical disk.\\nBottom-right to\\ntop-right:U\\nperforms a basis\\nchange within R3.v2\\nv1\\n σ2u2\\nσ1u1\\ne2\\ne1σ2e2\\nσ1e1A\\nV⊤\\nΣU\\n0⊤row vectors from n+ 1tombelow so that\\nΣ=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0σ10 0\\n0...0\\n0 0σn\\n0... 0\\n......\\n0... 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (4.65)\\nIfm < n , the matrix Σhas a diagonal structure up to column mand\\ncolumns that consist of 0fromm+ 1ton:\\nΣ=\\uf8ee\\n\\uf8ef\\uf8f0σ10 0 0 ...0\\n0...0 0 0\\n0 0σm0...0\\uf8f9\\n\\uf8fa\\uf8fb. (4.66)\\nRemark. The SVD exists for any matrix A∈Rm×n. ♦\\n4.5.1 Geometric Intuitions for the SVD\\nThe SVD offers geometric intuitions to describe a transformation matrix\\nA. In the following, we will discuss the SVD as sequential linear trans-\\nformations performed on the bases. In Example 4.12, we will then apply\\ntransformation matrices of the SVD to a set of vectors in R2, which allows\\nus to visualize the effect of each transformation more clearly.\\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\\nsponding linear mapping (recall Section 2.7.1) Φ :Rn→Rminto three\\noperations; see Figure 4.8. The SVD intuition follows superﬁcially a simi-\\nlar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\\nspeaking, the SVD performs a basis change via V⊤followed by a scal-\\ning and augmentation (or reduction) in dimensionality via the singular\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f4ec426-f776-47cc-b65f-8bfe0ffeca95', embedding=None, metadata={'page_label': '121', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Singular Value Decomposition 121\\nvalue matrix Σ. Finally, it performs a second basis change via U. The SVD\\nentails a number of important details and caveats, which is why we will\\nreview our intuition in more detail. It is useful to revise\\nbasis changes\\n(Section 2.7.2),\\northogonal matrices\\n(Deﬁnition 3.8) and\\northonormal bases\\n(Section 3.5).Assume we are given a transformation matrix of a linear mapping Φ :\\nRn→Rmwith respect to the standard bases BandCofRnandRm,\\nrespectively. Moreover, assume a second basis ˜BofRnand˜CofRm. Then\\n1. The matrix Vperforms a basis change in the domain Rnfrom ˜B(rep-\\nresented by the red and orange vectors v1andv2in the top-left of Fig-\\nure 4.8) to the standard basis B.V⊤=V−1performs a basis change\\nfromBto˜B. The red and orange vectors are now aligned with the\\ncanonical basis in the bottom-left of Figure 4.8.\\n2. Having changed the coordinate system to ˜B,Σscales the new coordi-\\nnates by the singular values σi(and adds or deletes dimensions), i.e.,\\nΣis the transformation matrix of Φwith respect to ˜Band ˜C, rep-\\nresented by the red and orange vectors being stretched and lying in\\nthee1-e2plane, which is now embedded in a third dimension in the\\nbottom-right of Figure 4.8.\\n3.Uperforms a basis change in the codomain Rmfrom ˜Cinto the canoni-\\ncal basis of Rm, represented by a rotation of the red and orange vectors\\nout of thee1-e2plane. This is shown in the top-right of Figure 4.8.\\nThe SVD expresses a change of basis in both the domain and codomain.\\nThis is in contrast with the eigendecomposition that operates within the\\nsame vector space, where the same basis change is applied and then un-\\ndone. What makes the SVD special is that these two different bases are\\nsimultaneously linked by the singular value matrix Σ.\\nExample 4.12 (Vectors and the SVD)\\nConsider a mapping of a square grid of vectors X∈R2that ﬁt in a box of\\nsize2×2centered at the origin. Using the standard basis, we map these\\nvectors using\\nA=\\uf8ee\\n\\uf8f01−0.8\\n0 1\\n1 0\\uf8f9\\n\\uf8fb=UΣV⊤(4.67a)\\n=\\uf8ee\\n\\uf8f0−0.79 0−0.62\\n0.38−0.78−0.49\\n−0.48−0.62 0.62\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8f01.62 0\\n0 1.0\\n0 0\\uf8f9\\n\\uf8fb[−0.78 0.62\\n−0.62−0.78]\\n.(4.67b)\\nWe start with a set of vectors X(colored dots; see top-left panel of Fig-\\nure 4.9) arranged in a grid. We then apply V⊤∈R2×2, which rotatesX.\\nThe rotated vectors are shown in the bottom-left panel of Figure 4.9. We\\nnow map these vectors using the singular value matrix Σto the codomain\\nR3(see the bottom-right panel in Figure 4.9). Note that all vectors lie in\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea35de81-f907-43a4-a427-d2e2b01b2699', embedding=None, metadata={'page_label': '122', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='122 Matrix Decompositions\\nthex1-x2plane. The third coordinate is always 0. The vectors in the x1-x2\\nplane have been stretched by the singular values.\\nThe direct mapping of the vectors XbyAto the codomain R3equals\\nthe transformation of XbyUΣV⊤, whereUperforms a rotation within\\nthe codomain R3so that the mapped vectors are no longer restricted to\\nthex1-x2plane; they still are on a plane as shown in the top-right panel\\nof Figure 4.9.\\nFigure 4.9 SVD and\\nmapping of vectors\\n(represented by\\ndiscs). The panels\\nfollow the same\\nanti-clockwise\\nstructure of\\nFigure 4.8.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx1−1.5−1.0−0.50.00.51.01.5x2\\nx1-1.5-0.5\\n0.5\\n1.5x2\\n-1.5-0.50.51.5x3\\n-1.0-0.50.00.51.0\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx1−1.5−1.0−0.50.00.51.01.5x2\\nx1-1.5-0.50.51.5x2\\n-1.5-0.50.51.5x3\\n0\\n4.5.2 Construction of the SVD\\nWe will next discuss why the SVD exists and show how to compute it\\nin detail. The SVD of a general matrix shares some similarities with the\\neigendecomposition of a square matrix.\\nRemark. Compare the eigendecomposition of an SPD matrix\\nS=S⊤=PDP⊤(4.68)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='086efadb-d937-4d00-b423-7e945626e4e1', embedding=None, metadata={'page_label': '123', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Singular Value Decomposition 123\\nwith the corresponding SVD\\nS=UΣV⊤. (4.69)\\nIf we set\\nU=P=V,D=Σ, (4.70)\\nwe see that the SVD of SPD matrices is their eigendecomposition. ♦\\nIn the following, we will explore why Theorem 4.22 holds and how\\nthe SVD is constructed. Computing the SVD of A∈Rm×nis equivalent\\nto ﬁnding two sets of orthonormal bases U= (u1,...,um)andV=\\n(v1,...,vn)of the codomain Rmand the domain Rn, respectively. From\\nthese ordered bases, we will construct the matrices UandV.\\nOur plan is to start with constructing the orthonormal set of right-\\nsingular vectors v1,...,vn∈Rn. We then construct the orthonormal set\\nof left-singular vectors u1,...,um∈Rm. Thereafter, we will link the two\\nand require that the orthogonality of the viis preserved under the trans-\\nformation ofA. This is important because we know that the images Avi\\nform a set of orthogonal vectors. We will then normalize these images by\\nscalar factors, which will turn out to be the singular values.\\nLet us begin with constructing the right-singular vectors. The spectral\\ntheorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\\nmatrix form an ONB, which also means it can be diagonalized. More-\\nover, from Theorem 4.14 we can always construct a symmetric, positive\\nsemideﬁnite matrix A⊤A∈Rn×nfrom any rectangular matrix A∈\\nRm×n. Thus, we can always diagonalize A⊤Aand obtain\\nA⊤A=PDP⊤=P\\uf8ee\\n\\uf8ef\\uf8f0λ1··· 0\\n.........\\n0···λn\\uf8f9\\n\\uf8fa\\uf8fbP⊤, (4.71)\\nwherePis an orthogonal matrix, which is composed of the orthonormal\\neigenbasis. The λi⩾0are the eigenvalues of A⊤A. Let us assume the\\nSVD ofAexists and inject (4.64) into (4.71). This yields\\nA⊤A= (UΣV⊤)⊤(UΣV⊤) =VΣ⊤U⊤UΣV⊤, (4.72)\\nwhereU,Vare orthogonal matrices. Therefore, with U⊤U=Iwe ob-\\ntain\\nA⊤A=VΣ⊤ΣV⊤=V\\uf8ee\\n\\uf8ef\\uf8f0σ2\\n10 0\\n0...0\\n0 0σ2\\nn\\uf8f9\\n\\uf8fa\\uf8fbV⊤. (4.73)\\nComparing now (4.71) and (4.73), we identify\\nV⊤=P⊤, (4.74)\\nσ2\\ni=λi. (4.75)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1fa523bf-f4be-44b4-b976-6e4fdc5c6acb', embedding=None, metadata={'page_label': '124', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='124 Matrix Decompositions\\nTherefore, the eigenvectors of A⊤Athat compose Pare the right-singular\\nvectorsVofA(see (4.74)). The eigenvalues of A⊤Aare the squared\\nsingular values of Σ(see (4.75)).\\nTo obtain the left-singular vectors U, we follow a similar procedure.\\nWe start by computing the SVD of the symmetric matrix AA⊤∈Rm×m\\n(instead of the previous A⊤A∈Rn×n). The SVD of Ayields\\nAA⊤= (UΣV⊤)(UΣV⊤)⊤=UΣV⊤VΣ⊤U⊤(4.76a)\\n=U\\uf8ee\\n\\uf8ef\\uf8f0σ2\\n10 0\\n0...0\\n0 0σ2\\nm\\uf8f9\\n\\uf8fa\\uf8fbU⊤. (4.76b)\\nThe spectral theorem tells us that AA⊤=SDS⊤can be diagonalized\\nand we can ﬁnd an ONB of eigenvectors of AA⊤, which are collected in\\nS. The orthonormal eigenvectors of AA⊤are the left-singular vectors U\\nand form an orthonormal basis set in the codomain of the SVD.\\nThis leaves the question of the structure of the matrix Σ. SinceAA⊤\\nandA⊤Ahave the same nonzero eigenvalues (see page 106) the nonzero\\nentries of the Σmatrices in the SVD for both cases have to be the same.\\nThe last step is to link up all the parts we touched upon so far. We have\\nan orthonormal set of right-singular vectors in V. To ﬁnish the construc-\\ntion of the SVD, we connect them with the orthonormal vectors U. To\\nreach this goal, we use the fact the images of the viunderAhave to be\\northogonal, too. We can show this by using the results from Section 3.4.\\nWe require that the inner product between AviandAvjmust be 0for\\ni̸=j. For any two orthogonal eigenvectors vi,vj,i̸=j, it holds that\\n(Avi)⊤(Avj) =v⊤\\ni(A⊤A)vj=v⊤\\ni(λjvj) =λjv⊤\\nivj= 0. (4.77)\\nFor the case m⩾r, it holds that{Av1,...,Avr}is a basis of an r-\\ndimensional subspace of Rm.\\nTo complete the SVD construction, we need left-singular vectors that\\nare ortho normal : We normalize the images of the right-singular vectors\\nAviand obtain\\nui:=Avi\\n∥Avi∥=1√λiAvi=1\\nσiAvi, (4.78)\\nwhere the last equality was obtained from (4.75) and (4.76b), showing\\nus that the eigenvalues of AA⊤are such that σ2\\ni=λi.\\nTherefore, the eigenvectors of A⊤A, which we know are the right-\\nsingular vectors vi, and their normalized images under A, the left-singular\\nvectorsui, form two self-consistent ONBs that are connected through the\\nsingular value matrix Σ.\\nLet us rearrange (4.78) to obtain the singular value equation singular value\\nequation\\nAvi=σiui, i= 1,...,r. (4.79)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='043f4a75-0ed8-4c1b-b5a9-2b73227d632e', embedding=None, metadata={'page_label': '125', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Singular Value Decomposition 125\\nThis equation closely resembles the eigenvalue equation (4.25), but the\\nvectors on the left- and the right-hand sides are not the same.\\nForn>m , (4.79) holds only for i⩽mand (4.79) says nothing about\\ntheuifori > m . However, we know by construction that they are or-\\nthonormal. Conversely, for m>n , (4.79) holds only for i⩽n. Fori>n ,\\nwe haveAvi=0and we still know that the viform an orthonormal set.\\nThis means that the SVD also supplies an orthonormal basis of the kernel\\n(null space) of A, the set of vectors xwithAx=0(see Section 2.7.3).\\nMoreover, concatenating the vias the columns of Vand theuias the\\ncolumns ofUyields\\nAV=UΣ, (4.80)\\nwhere Σhas the same dimensions as Aand a diagonal structure for rows\\n1,...,r . Hence, right-multiplying with V⊤yieldsA=UΣV⊤, which is\\nthe SVD ofA.\\nExample 4.13 (Computing the SVD)\\nLet us ﬁnd the singular value decomposition of\\nA=[1 0 1\\n−2 1 0]\\n. (4.81)\\nThe SVD requires us to compute the right-singular vectors vj, the singular\\nvaluesσk, and the left-singular vectors ui.\\nStep 1: Right-singular vectors as the eigenbasis of A⊤A.\\nWe start by computing\\nA⊤A=\\uf8ee\\n\\uf8f01−2\\n0 1\\n1 0\\uf8f9\\n\\uf8fb[1 0 1\\n−2 1 0]\\n=\\uf8ee\\n\\uf8f05−2 1\\n−2 1 0\\n1 0 1\\uf8f9\\n\\uf8fb. (4.82)\\nWe compute the singular values and right-singular vectors vjthrough\\nthe eigenvalue decomposition of A⊤A, which is given as\\nA⊤A=\\uf8ee\\n\\uf8ef\\uf8f05√\\n300−1√\\n6\\n−2√\\n301√\\n5−2√\\n6\\n1√\\n302√\\n51√\\n6\\uf8f9\\n\\uf8fa\\uf8fb\\uf8ee\\n\\uf8f06 0 0\\n0 1 0\\n0 0 0\\uf8f9\\n\\uf8fb\\uf8ee\\n\\uf8ef\\uf8f05√\\n30−2√\\n301√\\n30\\n01√\\n52√\\n5\\n−1√\\n6−2√\\n61√\\n6\\uf8f9\\n\\uf8fa\\uf8fb=PDP⊤,\\n(4.83)\\nand we obtain the right-singular vectors as the columns of Pso that\\nV=P=\\uf8ee\\n\\uf8ef\\uf8f05√\\n300−1√\\n6\\n−2√\\n301√\\n5−2√\\n6\\n1√\\n302√\\n51√\\n6\\uf8f9\\n\\uf8fa\\uf8fb. (4.84)\\nStep 2: Singular-value matrix.\\nAs the singular values σiare the square roots of the eigenvalues of\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='52c48d12-41f5-400a-8559-8171f10ca56a', embedding=None, metadata={'page_label': '126', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='126 Matrix Decompositions\\nA⊤Awe obtain them straight from D. Since rk(A) = 2 , there are only\\ntwo nonzero singular values: σ1=√\\n6andσ2= 1. The singular value\\nmatrix must be the same size as A, and we obtain\\nΣ=[√\\n6 0 0\\n0 1 0]\\n. (4.85)\\nStep 3: Left-singular vectors as the normalized image of the right-\\nsingular vectors.\\nWe ﬁnd the left-singular vectors by computing the image of the right-\\nsingular vectors under Aand normalizing them by dividing them by their\\ncorresponding singular value. We obtain\\nu1=1\\nσ1Av1=1√\\n6[1 0 1\\n−2 1 0]\\uf8ee\\n\\uf8ef\\uf8f05√\\n30\\n−2√\\n30\\n1√\\n30\\uf8f9\\n\\uf8fa\\uf8fb=[\\n1√\\n5\\n−2√\\n5]\\n, (4.86)\\nu2=1\\nσ2Av2=1\\n1[1 0 1\\n−2 1 0]\\uf8ee\\n\\uf8f00\\n1√\\n5\\n2√\\n5\\uf8f9\\n\\uf8fb=[\\n2√\\n5\\n1√\\n5]\\n, (4.87)\\nU= [u1,u2] =1√\\n5[1 2\\n−2 1]\\n. (4.88)\\nNote that on a computer the approach illustrated here has poor numerical\\nbehavior, and the SVD of Ais normally computed without resorting to the\\neigenvalue decomposition of A⊤A.\\n4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition\\nLet us consider the eigendecomposition A=PDP−1and the SVDA=\\nUΣV⊤and review the core elements of the past sections.\\nThe SVD always exists for any matrix Rm×n. The eigendecomposition is\\nonly deﬁned for square matrices Rn×nand only exists if we can ﬁnd a\\nbasis of eigenvectors of Rn.\\nThe vectors in the eigendecomposition matrix Pare not necessarily\\northogonal, i.e., the change of basis is not a simple rotation and scaling.\\nOn the other hand, the vectors in the matrices UandVin the SVD are\\northonormal, so they do represent rotations.\\nBoth the eigendecomposition and the SVD are compositions of three\\nlinear mappings:\\n1. Change of basis in the domain\\n2. Independent scaling of each new basis vector and mapping from do-\\nmain to codomain\\n3. Change of basis in the codomain\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22e998aa-976c-4b19-a12e-2711d36ee5d5', embedding=None, metadata={'page_label': '127', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Singular Value Decomposition 127\\nFigure 4.10 Movie\\nratings of three\\npeople for four\\nmovies and its SVD\\ndecomposition. 5 4 1\\n5 5 0\\n0 0 5\\n1 0 4\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbAli\\nBeatrix\\nChandra\\nStar Wars\\nBlade Runner\\nAmelie\\nDelicatessen=−0.6710 0.0236 0.4647−0.5774\\n−0.7197 0.2054−0.4759 0.4619\\n−0.0939−0.7705−0.5268−0.3464\\n−0.1515−0.6030 0.5293−0.5774\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n9.6438 0 0\\n06.3639 0\\n0 00.7056\\n0 0 0\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n−0.7367−0.6515−0.1811\\n0.0852 0.1762−0.9807\\n0.6708−0.7379−0.0743\\uf8ee\\n\\uf8ef\\uf8f0\\uf8f9\\n\\uf8fa\\uf8fb\\nA key difference between the eigendecomposition and the SVD is that\\nin the SVD, domain and codomain can be vector spaces of different\\ndimensions.\\nIn the SVD, the left- and right-singular vector matrices UandVare\\ngenerally not inverse of each other (they perform basis changes in dif-\\nferent vector spaces). In the eigendecomposition, the basis change ma-\\ntricesPandP−1are inverses of each other.\\nIn the SVD, the entries in the diagonal matrix Σare all real and non-\\nnegative, which is not generally true for the diagonal matrix in the\\neigendecomposition.\\nThe SVD and the eigendecomposition are closely related through their\\nprojections\\n–The left-singular vectors of Aare eigenvectors of AA⊤\\n–The right-singular vectors of Aare eigenvectors of A⊤A.\\n–The nonzero singular values of Aare the square roots of the nonzero\\neigenvalues of AA⊤and are equal to the nonzero eigenvalues of\\nA⊤A.\\nFor symmetric matrices A∈Rn×n, the eigenvalue decomposition and\\nthe SVD are one and the same, which follows from the spectral theo-\\nrem 4.15.\\nExample 4.14 (Finding Structure in Movie Ratings and Consumers)\\nLet us add a practical interpretation of the SVD by analyzing data on\\npeople and their preferred movies. Consider three viewers (Ali, Beatrix,\\nChandra) rating four different movies ( Star Wars ,Blade Runner ,Amelie ,\\nDelicatessen ). Their ratings are values between 0(worst) and 5(best) and\\nencoded in a data matrix A∈R4×3as shown in Figure 4.10. Each row\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da04058a-8db4-4e01-beb1-4f056e222ca4', embedding=None, metadata={'page_label': '128', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='128 Matrix Decompositions\\nrepresents a movie and each column a user. Thus, the column vectors of\\nmovie ratings, one for each viewer, are xAli,xBeatrix ,xChandra .\\nFactoringAusing the SVD offers us a way to capture the relationships\\nof how people rate movies, and especially if there is a structure linking\\nwhich people like which movies. Applying the SVD to our data matrix A\\nmakes a number of assumptions:\\n1. All viewers rate movies consistently using the same linear mapping.\\n2. There are no errors or noise in the ratings.\\n3. We interpret the left-singular vectors uias stereotypical movies and\\nthe right-singular vectors vjas stereotypical viewers.\\nWe then make the assumption that any viewer’s speciﬁc movie preferences\\ncan be expressed as a linear combination of the vj. Similarly, any movie’s\\nlike-ability can be expressed as a linear combination of the ui. Therefore,\\na vector in the domain of the SVD can be interpreted as a viewer in the\\n“space” of stereotypical viewers, and a vector in the codomain of the SVD\\ncorrespondingly as a movie in the “space” of stereotypical movies. Let us These two “spaces”\\nare only\\nmeaningfully\\nspanned by the\\nrespective viewer\\nand movie data if\\nthe data itself covers\\na sufﬁcient diversity\\nof viewers and\\nmovies.inspect the SVD of our movie-user matrix. The ﬁrst left-singular vector u1\\nhas large absolute values for the two science ﬁction movies and a large\\nﬁrst singular value (red shading in Figure 4.10). Thus, this groups a type\\nof users with a speciﬁc set of movies (science ﬁction theme). Similarly, the\\nﬁrst right-singular v1shows large absolute values for Ali and Beatrix, who\\ngive high ratings to science ﬁction movies (green shading in Figure 4.10).\\nThis suggests that v1reﬂects the notion of a science ﬁction lover.\\nSimilarly,u2, seems to capture a French art house ﬁlm theme, and v2\\nindicates that Chandra is close to an idealized lover of such movies. An\\nidealized science ﬁction lover is a purist and only loves science ﬁction\\nmovies, so a science ﬁction lover v1gives a rating of zero to everything\\nbut science ﬁction themed – this logic is implied the diagonal substructure\\nfor the singular value matrix Σ. A speciﬁc movie is therefore represented\\nby how it decomposes (linearly) into its stereotypical movies. Likewise, a\\nperson would be represented by how they decompose (via linear combi-\\nnation) into movie themes.\\nIt is worth, to brieﬂy discuss SVD terminology and conventions, as there\\nare different versions used in the literature. The mathematics remains in-\\nvariant to these differences, but these differences can be confusing.\\nFor convenience in notation and abstraction, we use an SVD notation\\nwhere the SVD is described as having two square left- and right-singular\\nvector matrices, but a non-square singular value matrix. Our deﬁni-\\ntion (4.64) for the SVD is sometimes called the full SVD . full SVD\\nSome authors deﬁne the SVD a bit differently and focus on square sin-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6636b7a8-ab82-4e68-ac04-910edc17a38e', embedding=None, metadata={'page_label': '129', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.6 Matrix Approximation 129\\ngular matrices. Then, for A∈Rm×nandm⩾n,\\nA\\nm×n=U\\nm×nΣ\\nn×nV⊤\\nn×n. (4.89)\\nSometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD\\northeSVD (e.g., Press et al. (2007)). This alternative format changes\\nmerely how the matrices are constructed but leaves the mathematical\\nstructure of the SVD unchanged. The convenience of this alternative\\nformulation is that Σis diagonal, as in the eigenvalue decomposition.\\nIn Section 4.6, we will learn about matrix approximation techniques\\nusing the SVD, which is also called the truncated SVD . truncated SVD\\nIt is possible to deﬁne the SVD of a rank- rmatrixAso thatUis an\\nm×rmatrix, Σa diagonal matrix r×r, andVanr×nmatrix.\\nThis construction is very similar to our deﬁnition, and ensures that the\\ndiagonal matrix Σhas only nonzero entries along the diagonal. The\\nmain convenience of this alternative notation is that Σis diagonal, as\\nin the eigenvalue decomposition.\\nA restriction that the SVD for Aonly applies to m×nmatrices with\\nm>n is practically unnecessary. When m<n , the SVD decomposition\\nwill yield Σwith more zero columns than rows and, consequently, the\\nsingular values σm+1,...,σnare0.\\nThe SVD is used in a variety of applications in machine learning from\\nleast-squares problems in curve ﬁtting to solving systems of linear equa-\\ntions. These applications harness various important properties of the SVD,\\nits relation to the rank of a matrix, and its ability to approximate matrices\\nof a given rank with lower-rank matrices. Substituting a matrix with its\\nSVD has often the advantage of making calculation more robust to nu-\\nmerical rounding errors. As we will explore in the next section, the SVD’s\\nability to approximate matrices with “simpler” matrices in a principled\\nmanner opens up machine learning applications ranging from dimension-\\nality reduction and topic modeling to data compression and clustering.\\n4.6 Matrix Approximation\\nWe considered the SVD as a way to factorize A=UΣV⊤∈Rm×ninto\\nthe product of three matrices, where U∈Rm×mandV∈Rn×nare or-\\nthogonal and Σcontains the singular values on its main diagonal. Instead\\nof doing the full SVD factorization, we will now investigate how the SVD\\nallows us to represent a matrix Aas a sum of simpler (low-rank) matrices\\nAi, which lends itself to a matrix approximation scheme that is cheaper\\nto compute than the full SVD.\\nWe construct a rank- 1matrixAi∈Rm×nas\\nAi:=uiv⊤\\ni, (4.90)\\nwhich is formed by the outer product of the ith orthogonal column vector\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02ff3a26-e1fa-4a91-8949-38578399282d', embedding=None, metadata={'page_label': '130', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='130 Matrix Decompositions\\nFigure 4.11 Image\\nprocessing with the\\nSVD. (a) The\\noriginal grayscale\\nimage is a\\n1,432×1,910\\nmatrix of values\\nbetween 0(black)\\nand1(white).\\n(b)–(f) Rank- 1\\nmatrices\\nA1,...,A5and\\ntheir corresponding\\nsingular values\\nσ1,...,σ 5. The\\ngrid-like structure of\\neach rank- 1matrix\\nis imposed by the\\nouter-product of the\\nleft and\\nright-singular\\nvectors.\\n(a) Original image A.\\n (b)A1, σ1≈228,052.\\n (c)A2, σ2≈40,647.\\n(d)A3, σ3≈26,125.\\n (e)A4, σ4≈20,232.\\n (f)A5, σ5≈15,436.\\nofUandV. Figure 4.11 shows an image of Stonehenge, which can be\\nrepresented by a matrix A∈R1432×1910, and some outer products Ai, as\\ndeﬁned in (4.90).\\nA matrixA∈Rm×nof rankrcan be written as a sum of rank-1 matrices\\nAiso that\\nA=r∑\\ni=1σiuiv⊤\\ni=r∑\\ni=1σiAi, (4.91)\\nwhere the outer-product matrices Aiare weighted by the ith singular\\nvalueσi. We can see why (4.91) holds: The diagonal structure of the\\nsingular value matrix Σmultiplies only matching left- and right-singular\\nvectorsuiv⊤\\niand scales them by the corresponding singular value σi. All\\nterms Σijuiv⊤\\njvanish fori̸=jbecause Σis a diagonal matrix. Any terms\\ni>r vanish because the corresponding singular values are 0.\\nIn (4.90), we introduced rank- 1matricesAi. We summed up the rin-\\ndividual rank- 1matrices to obtain a rank- rmatrixA; see (4.91). If the\\nsum does not run over all matrices Ai,i= 1,...,r , but only up to an\\nintermediate value k<r , we obtain a rank-kapproximation rank-k\\napproximation\\nˆA(k) :=k∑\\ni=1σiuiv⊤\\ni=k∑\\ni=1σiAi (4.92)\\nofAwith rk(ˆA(k)) =k. Figure 4.12 shows low-rank approximations\\nˆA(k)of an original image Aof Stonehenge. The shape of the rocks be-\\ncomes increasingly visible and clearly recognizable in the rank- 5approx-\\nimation. While the original image requires 1,432·1,910 = 2,735,120\\nnumbers, the rank- 5approximation requires us only to store the ﬁve sin-\\ngular values and the ﬁve left- and right-singular vectors ( 1,432and1,910-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='281209b0-d237-4093-a5c7-f4051324ffb9', embedding=None, metadata={'page_label': '131', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.6 Matrix Approximation 131\\nFigure 4.12 Image\\nreconstruction with\\nthe SVD. (a)\\nOriginal image.\\n(b)–(f) Image\\nreconstruction using\\nthe low-rank\\napproximation of\\nthe SVD, where the\\nrank-k\\napproximation is\\ngiven byˆA(k) =∑k\\ni=1σiAi.\\n(a) Original image A.\\n (b) Rank-1 approximation ˆA(1).\\n(c) Rank-2 approximation ˆA(2).\\n(d) Rank-3 approximation ˆA(3).\\n(e) Rank-4 approximation ˆA(4).\\n(f) Rank-5 approximation ˆA(5).\\ndimensional each) for a total of 5·(1,432+1,910+1) = 16 ,715numbers\\n– just above 0.6%of the original.\\nTo measure the difference (error) between Aand its rank- kapproxima-\\ntionˆA(k), we need the notion of a norm. In Section 3.1, we already used\\nnorms on vectors that measure the length of a vector. By analogy we can\\nalso deﬁne norms on matrices.\\nDeﬁnition 4.23 (Spectral Norm of a Matrix) .Forx∈Rn\\\\{0}, the spectral spectral norm\\nnorm of a matrixA∈Rm×nis deﬁned as\\n∥A∥2:= max\\nx∥Ax∥2\\n∥x∥2. (4.93)\\nWe introduce the notation of a subscript in the matrix norm (left-hand\\nside), similar to the Euclidean norm for vectors (right-hand side), which\\nhas subscript 2. The spectral norm (4.93) determines how long any vector\\nxcan at most become when multiplied by A.\\nTheorem 4.24. The spectral norm of Ais its largest singular value σ1.\\nWe leave the proof of this theorem as an exercise.\\nEckart-Young\\ntheorem Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)) .Con-\\nsider a matrix A∈Rm×nof rankrand letB∈Rm×nbe a matrix of rank\\nk. For anyk⩽rwithˆA(k) =∑k\\ni=1σiuiv⊤\\niit holds that\\nˆA(k) = argminrk(B)=k∥A−B∥2, (4.94)\\ued79\\ued79\\ued79A−ˆA(k)\\ued79\\ued79\\ued79\\n2=σk+1. (4.95)\\nThe Eckart-Young theorem states explicitly how much error we intro-\\nduce by approximating Ausing a rank- kapproximation. We can inter-\\npret the rank- kapproximation obtained with the SVD as a projection of\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='78a549a0-5121-4399-b065-53ce613cda98', embedding=None, metadata={'page_label': '132', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='132 Matrix Decompositions\\nthe full-rank matrix Aonto a lower-dimensional space of rank-at-most- k\\nmatrices. Of all possible projections, the SVD minimizes the error (with\\nrespect to the spectral norm) between Aand any rank- kapproximation.\\nWe can retrace some of the steps to understand why (4.95) should hold.\\nWe observe that the difference between A−ˆA(k)is a matrix containing\\nthe sum of the remaining rank- 1matrices\\nA−ˆA(k) =r∑\\ni=k+1σiuiv⊤\\ni. (4.96)\\nBy Theorem 4.24, we immediately obtain σk+1as the spectral norm of the\\ndifference matrix. Let us have a closer look at (4.94). If we assume that\\nthere is another matrix Bwith rk(B)⩽k, such that\\n∥A−B∥2<\\ued79\\ued79\\ued79A−ˆA(k)\\ued79\\ued79\\ued79\\n2, (4.97)\\nthen there exists an at least ( n−k)-dimensional null space Z⊆Rn, such\\nthatx∈Zimplies thatBx=0. Then it follows that\\n∥Ax∥2=∥(A−B)x∥2, (4.98)\\nand by using a version of the Cauchy-Schwartz inequality (3.17) that en-\\ncompasses norms of matrices, we obtain\\n∥Ax∥2⩽∥A−B∥2∥x∥2<σk+1∥x∥2. (4.99)\\nHowever, there exists a (k+ 1)-dimensional subspace where ∥Ax∥2⩾\\nσk+1∥x∥2, which is spanned by the right-singular vectors vj,j⩽k+ 1of\\nA. Adding up dimensions of these two spaces yields a number greater than\\nn, as there must be a nonzero vector in both spaces. This is a contradiction\\nof the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\\nThe Eckart-Young theorem implies that we can use SVD to reduce a\\nrank-rmatrixAto a rank-kmatrixˆAin a principled, optimal (in the\\nspectral norm sense) manner. We can interpret the approximation of Aby\\na rank-kmatrix as a form of lossy compression. Therefore, the low-rank\\napproximation of a matrix appears in many machine learning applications,\\ne.g., image processing, noise ﬁltering, and regularization of ill-posed prob-\\nlems. Furthermore, it plays a key role in dimensionality reduction and\\nprincipal component analysis, as we will see in Chapter 10.\\nExample 4.15 (Finding Structure in Movie Ratings and Consumers\\n(continued))\\nComing back to our movie-rating example, we can now apply the con-\\ncept of low-rank approximations to approximate the original data matrix.\\nRecall that our ﬁrst singular value captures the notion of science ﬁction\\ntheme in movies and science ﬁction lovers. Thus, by using only the ﬁrst\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d14c6183-fa69-4653-a562-409775e7d379', embedding=None, metadata={'page_label': '133', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.6 Matrix Approximation 133\\nsingular value term in a rank- 1decomposition of the movie-rating matrix,\\nwe obtain the predicted ratings\\nA1=u1v⊤\\n1=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0−0.6710\\n−0.7197\\n−0.0939\\n−0.1515\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb[−0.7367−0.6515−0.1811]\\n(4.100a)\\n=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00.4943 0.4372 0.1215\\n0.5302 0.4689 0.1303\\n0.0692 0.0612 0.0170\\n0.1116 0.0987 0.0274\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (4.100b)\\nThis ﬁrst rank- 1approximation A1is insightful: it tells us that Ali and\\nBeatrix like science ﬁction movies, such as Star Wars and Bladerunner\\n(entries have values >0.4), but fails to capture the ratings of the other\\nmovies by Chandra. This is not surprising, as Chandra’s type of movies is\\nnot captured by the ﬁrst singular value. The second singular value gives\\nus a better rank- 1approximation for those movie-theme lovers:\\nA2=u2v⊤\\n2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00.0236\\n0.2054\\n−0.7705\\n−0.6030\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb[0.0852 0.1762−0.9807]\\n(4.101a)\\n=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00.0020 0.0042−0.0231\\n0.0175 0.0362−0.2014\\n−0.0656−0.1358 0.7556\\n−0.0514−0.1063 0.5914\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (4.101b)\\nIn this second rank- 1approximation A2, we capture Chandra’s ratings\\nand movie types well, but not the science ﬁction movies. This leads us to\\nconsider the rank- 2approximation ˆA(2), where we combine the ﬁrst two\\nrank- 1approximations\\nˆA(2) =σ1A1+σ2A2=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f04.7801 4.2419 1.0244\\n5.2252 4.7522−0.0250\\n0.2493−0.2743 4.9724\\n0.7495 0.2756 4.0278\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.(4.102)\\nˆA(2)is similar to the original movie ratings table\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f05 4 1\\n5 5 0\\n0 0 5\\n1 0 4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb, (4.103)\\nand this suggests that we can ignore the contribution of A3. We can in-\\nterpret this so that in the data table there is no evidence of a third movie-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc19237a-aa99-4c9f-858a-3e7923114ba6', embedding=None, metadata={'page_label': '134', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='134 Matrix Decompositions\\nFigure 4.13 A\\nfunctional\\nphylogeny of\\nmatrices\\nencountered in\\nmachine learning.Real matrices\\n∃Pseudo-inverse\\n∃SVD\\nSquare\\n∃Determinant\\n∃TraceNonsquare\\nDefectiveSingular\\nNon-defective\\n(diagonalizable)Singular\\nNormal Non-normal\\nSymmetric\\neigenvalues∈R\\nPositive deﬁnite\\nCholesky\\neigenvalues >0Diagonal\\nIdentity\\nmatrix∃Inverse Matrix\\nRegular\\n(invertible)\\nOrthogonal RotationRn×nRn×m\\nNo basis of\\neigenvectors\\nBasis of\\neigenvectors\\nA⊤A=AA⊤A⊤A̸=AA⊤\\nColumns are\\northogonal\\neigenvectorsA⊤A=AA\\n⊤\\n=Idet\\n̸= 0det\\n̸= 0det = 0\\ntheme/movie-lovers category. This also means that the entire space of\\nmovie-themes/movie-lovers in our example is a two-dimensional space\\nspanned by science ﬁction and French art house movies and lovers.\\n4.7 Matrix Phylogeny\\nThe word\\n“phylogenetic”\\ndescribes how we\\ncapture the\\nrelationships among\\nindividuals or\\ngroups and derived\\nfrom the Greek\\nwords for “tribe”\\nand “source”.In Chapters 2 and 3, we covered the basics of linear algebra and analytic\\ngeometry. In this chapter, we looked at fundamental characteristics of ma-\\ntrices and linear mappings. Figure 4.13 depicts the phylogenetic tree of\\nrelationships between different types of matrices (black arrows indicating\\n“is a subset of”) and the covered operations we can perform on them (in\\nblue). We consider all real matricesA∈Rn×m. For non-square matrices\\n(wheren̸=m), the SVD always exists, as we saw in this chapter. Focus-\\ning on square matrices A∈Rn×n, the determinant informs us whether a\\nsquare matrix possesses an inverse matrix , i.e., whether it belongs to the\\nclass of regular, invertible matrices. If the square n×nmatrix possesses n\\nlinearly independent eigenvectors, then the matrix is non-defective and an\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='767910f2-5fb0-4934-a194-c31de76d1235', embedding=None, metadata={'page_label': '135', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.8 Further Reading 135\\neigendecomposition exists (Theorem 4.12). We know that repeated eigen-\\nvalues may result in defective matrices, which cannot be diagonalized.\\nNon-singular and non-defective matrices are not the same. For exam-\\nple, a rotation matrix will be invertible (determinant is nonzero) but not\\ndiagonalizable in the real numbers (eigenvalues are not guaranteed to be\\nreal numbers).\\nWe dive further into the branch of non-defective square n×nmatrices.\\nAisnormal if the condition A⊤A=AA⊤holds. Moreover, if the more\\nrestrictive condition holds that A⊤A=AA⊤=I, thenAis called or-\\nthogona l (see Deﬁnition 3.8). The set of orthogonal matrices is a subset of\\nthe regular (invertible) matrices and satisﬁes A⊤=A−1.\\nNormal matrices have a frequently encountered subset, the symmetric\\nmatricesS∈Rn×n, which satisfy S=S⊤. Symmetric matrices have only\\nreal eigenvalues. A subset of the symmetric matrices consists of the pos-\\nitive deﬁnite matrices Pthat satisfy the condition of x⊤Px>0for all\\nx∈Rn\\\\{0}. In this case, a unique Cholesky decomposition exists (Theo-\\nrem 4.18). Positive deﬁnite matrices have only positive eigenvalues and\\nare always invertible (i.e., have a nonzero determinant).\\nAnother subset of symmetric matrices consists of the diagonal matrices\\nD. Diagonal matrices are closed under multiplication and addition, but do\\nnot necessarily form a group (this is only the case if all diagonal entries\\nare nonzero so that the matrix is invertible). A special diagonal matrix is\\nthe identity matrix I.\\n4.8 Further Reading\\nMost of the content in this chapter establishes underlying mathematics\\nand connects them to methods for studying mappings, many of which are\\nat the heart of machine learning at the level of underpinning software so-\\nlutions and building blocks for almost all machine learning theory. Matrix\\ncharacterization using determinants, eigenspectra, and eigenspaces pro-\\nvides fundamental features and conditions for categorizing and analyzing\\nmatrices. This extends to all forms of representations of data and map-\\npings involving data, as well as judging the numerical stability of compu-\\ntational operations on such matrices (Press et al., 2007).\\nDeterminants are fundamental tools in order to invert matrices and\\ncompute eigenvalues “by hand”. However, for almost all but the smallest\\ninstances, numerical computation by Gaussian elimination outperforms\\ndeterminants (Press et al., 2007). Determinants remain nevertheless a\\npowerful theoretical concept, e.g., to gain intuition about the orientation\\nof a basis based on the sign of the determinant. Eigenvectors can be used\\nto perform basis changes to transform data into the coordinates of mean-\\ningful orthogonal, feature vectors. Similarly, matrix decomposition meth-\\nods, such as the Cholesky decomposition, reappear often when we com-\\npute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f99d3bd9-67a1-489a-bedb-54ffb074fa1b', embedding=None, metadata={'page_label': '136', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='136 Matrix Decompositions\\nthe Cholesky decomposition enables us to compute the reparametrization\\ntrick where we want to perform continuous differentiation over random\\nvariables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;\\nKingma and Welling, 2014).\\nEigendecomposition is fundamental in enabling us to extract mean-\\ningful and interpretable information that characterizes linear mappings.\\nTherefore, the eigendecomposition underlies a general class of machine\\nlearning algorithms called spectral methods that perform eigendecomposi-\\ntion of a positive-deﬁnite kernel. These spectral decomposition methods\\nencompass classical approaches to statistical data analysis, such as the\\nfollowing:\\nprincipal component\\nanalysis Principal component analysis (PCA (Pearson, 1901), see also Chapter 10),\\nin which a low-dimensional subspace, which explains most of the vari-\\nability in the data, is sought. Fisher discriminant\\nanalysis Fisher discriminant analysis , which aims to determine a separating hy-\\nperplane for data classiﬁcation (Mika et al., 1999). multidimensional\\nscaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\\nThe computational efﬁciency of these methods typically comes from ﬁnd-\\ning the best rank- kapproximation to a symmetric, positive semideﬁnite\\nmatrix. More contemporary examples of spectral methods have different\\norigins, but each of them requires the computation of the eigenvectors\\nand eigenvalues of a positive-deﬁnite kernel, such as Isomap (Tenenbaum Isomap\\net al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), Hessian Laplacian\\neigenmaps\\nHessian eigenmapseigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\\nspectral clusteringMalik, 2000). The core computations of these are generally underpinned\\nby low-rank matrix approximation techniques (Belabbas and Wolfe, 2009)\\nas we encountered here via the SVD.\\nThe SVD allows us to discover some of the same kind of information as\\nthe eigendecomposition. However, the SVD is more generally applicable\\nto non-square matrices and data tables. These matrix factorization meth-\\nods become relevant whenever we want to identify heterogeneity in data\\nwhen we want to perform data compression by approximation, e.g., in-\\nstead of storing n×mvalues just storing (n+m)kvalues, or when we want\\nto perform data pre-processing, e.g., to decorrelate predictor variables of\\na design matrix (Ormoneit et al., 2001). The SVD operates on matrices,\\nwhich we can interpret as rectangular arrays with two indices (rows and\\ncolumns). The extension of matrix-like structure to higher-dimensional\\narrays are called tensors. It turns out that the SVD is the special case of\\na more general family of decompositions that operate on such tensors\\n(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-\\ntions on tensors are, for example, the Tucker decomposition (Tucker, 1966) Tucker\\ndecomposition or the CP decomposition (Carroll and Chang, 1970).\\nCP decomposition The SVD low-rank approximation is frequently used in machine learn-\\ning for computational efﬁciency reasons. This is because it reduces the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4887e1a-33ec-4fa6-b25c-8eae897521aa', embedding=None, metadata={'page_label': '137', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 137\\namount of memory and operations with nonzero multiplications we need\\nto perform on potentially very large matrices of data (Trefethen and Bau III,\\n1997). Moreover, low-rank approximations are used to operate on ma-\\ntrices that may contain missing values as well as for purposes of lossy\\ncompression and dimensionality reduction (Moonen and De Moor, 1995;\\nMarkovsky, 2011).\\nExercises\\n4.1 Compute the determinant using the Laplace expansion (using the ﬁrst row)\\nand the Sarrus rule for\\nA=\\uf8ee\\n\\uf8f01 3 5\\n2 4 6\\n0 2 4\\uf8f9\\n\\uf8fb.\\n4.2 Compute the following determinant efﬁciently:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02 0 1 2 0\\n2−1 0 1 1\\n0 1 2 1 2\\n−2 0 2−1 2\\n2 0 0 1 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb.\\n4.3 Compute the eigenspaces of\\na.\\nA:=[\\n1 0\\n1 1]\\nb.\\nB:=[\\n−2 2\\n2 1]\\n4.4 Compute all eigenspaces of\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00−1 1 1\\n−1 1−2 3\\n2−1 0 0\\n1−1 1 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb.\\n4.5 Diagonalizability of a matrix is unrelated to its invertibility. Determine for\\nthe following four matrices whether they are diagonalizable and/or invert-\\nible\\n[\\n1 0\\n0 1]\\n,[\\n1 0\\n0 0]\\n,[\\n1 1\\n0 1]\\n,[\\n0 1\\n0 0]\\n.\\n4.6 Compute the eigenspaces of the following transformation matrices. Are they\\ndiagonalizable?\\na. For\\nA=\\uf8ee\\n\\uf8f02 3 0\\n1 4 3\\n0 0 1\\uf8f9\\n\\uf8fb\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69a76201-cb36-45de-9f7a-0dcd2f152d20', embedding=None, metadata={'page_label': '138', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='138 Matrix Decompositions\\nb. For\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 1 0 0\\n0 0 0 0\\n0 0 0 0\\n0 0 0 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n4.7 Are the following matrices diagonalizable? If yes, determine their diagonal\\nform and a basis with respect to which the transformation matrices are di-\\nagonal. If no, give reasons why they are not diagonalizable.\\na.\\nA=[\\n0 1\\n−8 4]\\nb.\\nA=\\uf8ee\\n\\uf8f01 1 1\\n1 1 1\\n1 1 1\\uf8f9\\n\\uf8fb\\nc.\\nA=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f05 4 2 1\\n0 1−1−1\\n−1−1 3 0\\n1 1−1 2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nd.\\nA=\\uf8ee\\n\\uf8f05−6−6\\n−1 4 2\\n3−6−4\\uf8f9\\n\\uf8fb\\n4.8 Find the SVD of the matrix\\nA=[\\n3 2 2\\n2 3−2]\\n.\\n4.9 Find the singular value decomposition of\\nA=[\\n2 2\\n−1 1]\\n.\\n4.10 Find the rank-1 approximation of\\nA=[\\n3 2 2\\n2 3−2]\\n4.11 Show that for any A∈Rm×nthe matrices A⊤AandAA⊤possess the\\nsame nonzero eigenvalues.\\n4.12 Show that for x̸=0Theorem 4.24 holds, i.e., show that\\nmax\\nx∥Ax∥2\\n∥x∥2=σ1,\\nwhereσ1is the largest singular value of A∈Rm×n.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34c03956-9920-4940-985d-5c11c3c07fba', embedding=None, metadata={'page_label': '139', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5\\nVector Calculus\\nMany algorithms in machine learning optimize an objective function with\\nrespect to a set of desired model parameters that control how well a model\\nexplains the data: Finding good parameters can be phrased as an opti-\\nmization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-\\near regression (see Chapter 9), where we look at curve-ﬁtting problems\\nand optimize linear weight parameters to maximize the likelihood; (ii)\\nneural-network auto-encoders for dimensionality reduction and data com-\\npression, where the parameters are the weights and biases of each layer,\\nand where we minimize a reconstruction error by repeated application of\\nthe chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\\nmodeling data distributions, where we optimize the location and shape\\nparameters of each mixture component to maximize the likelihood of the\\nmodel. Figure 5.1 illustrates some of these problems, which we typically\\nsolve by using optimization algorithms that exploit gradient information\\n(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-\\nter are related and how they are connected to other chapters of the book.\\nCentral to this chapter is the concept of a function. A function fis\\na quantity that relates two quantities to each other. In this book, these\\nquantities are typically inputs x∈RDand targets (function values) f(x),\\nwhich we assume are real-valued if not stated otherwise. Here RDis the\\ndomain off, and the function values f(x)are the image/codomain off.domain\\nimage/codomain\\nFigure 5.1 Vector\\ncalculus plays a\\ncentral role in (a)\\nregression (curve\\nﬁtting) and (b)\\ndensity estimation,\\ni.e., modeling data\\ndistributions.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a) Regression problem: Find parameters,\\nsuch that the curve explains the observations\\n(crosses) well.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(b) Density estimation with a Gaussian mixture\\nmodel: Find means and covariances, such that\\nthe data (dots) can be explained well.\\n139\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18c48d8d-1fc0-4688-845c-30947ff9638b', embedding=None, metadata={'page_label': '140', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='140 Vector Calculus\\nFigure 5.2 A mind\\nmap of the concepts\\nintroduced in this\\nchapter, along with\\nwhen they are used\\nin other parts of the\\nbook.Difference quotient\\nPartial derivatives\\nJacobian\\nHessian\\nTaylor seriesChapter 7\\nOptimization\\nChapter 6\\nProbabilityChapter 9\\nRegression\\nChapter 10\\nDimensionality\\nreduction\\nChapter 11\\nDensity estimation\\nChapter 12\\nClassiﬁcation\\ndeﬁnes collected in used inused inused in\\nused inused in\\nused inused in\\nSection 2.7.3 provides much more detailed discussion in the context of\\nlinear functions. We often write\\nf:RD→R (5.1a)\\nx↦→f(x) (5.1b)\\nto specify a function, where (5.1a) speciﬁes that fis a mapping from\\nRDtoRand (5.1b) speciﬁes the explicit assignment of an input xto\\na function value f(x). A function fassigns every input xexactly one\\nfunction value f(x).\\nExample 5.1\\nRecall the dot product as a special case of an inner product (Section 3.2).\\nIn the previous notation, the function f(x) =x⊤x,x∈R2, would be\\nspeciﬁed as\\nf:R2→R (5.2a)\\nx↦→x2\\n1+x2\\n2. (5.2b)\\nIn this chapter, we will discuss how to compute gradients of functions,\\nwhich is often essential to facilitate learning in machine learning models\\nsince the gradient points in the direction of steepest ascent. Therefore,\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e30c8b1f-466b-4c62-8a55-5f030efbfd2c', embedding=None, metadata={'page_label': '141', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.1 Differentiation of Univariate Functions 141\\nFigure 5.3 The\\naverage incline of a\\nfunctionfbetween\\nx0andx0+δxis\\nthe incline of the\\nsecant (blue)\\nthroughf(x0)and\\nf(x0+δx)and\\ngiven byδy/δx .\\nδy\\nδxf(x)\\nxy\\nf(x0)f(x0+δx)\\nvector calculus is one of the fundamental mathematical tools we need in\\nmachine learning. Throughout this book, we assume that functions are\\ndifferentiable. With some additional technical deﬁnitions, which we do\\nnot cover here, many of the approaches presented can be extended to\\nsub-differentials (functions that are continuous but not differentiable at\\ncertain points). We will look at an extension to the case of functions with\\nconstraints in Chapter 7.\\n5.1 Differentiation of Univariate Functions\\nIn the following, we brieﬂy revisit differentiation of a univariate function,\\nwhich may be familiar from high school mathematics. We start with the\\ndifference quotient of a univariate function y=f(x), x,y∈R, which we\\nwill subsequently use to deﬁne derivatives.\\nDeﬁnition 5.1 (Difference Quotient) .Thedifference quotient difference quotient\\nδy\\nδx:=f(x+δx)−f(x)\\nδx(5.3)\\ncomputes the slope of the secant line through two points on the graph of\\nf. In Figure 5.3, these are the points with x-coordinates x0andx0+δx.\\nThe difference quotient can also be considered the average slope of f\\nbetweenxandx+δxif we assume fto be a linear function. In the limit\\nforδx→0, we obtain the tangent of fatx, iffis differentiable. The\\ntangent is then the derivative of fatx.\\nDeﬁnition 5.2 (Derivative) .More formally, for h >0thederivative off derivative\\natxis deﬁned as the limit\\ndf\\ndx:= lim\\nh→0f(x+h)−f(x)\\nh, (5.4)\\nand the secant in Figure 5.3 becomes a tangent.\\nThe derivative of fpoints in the direction of steepest ascent of f.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7b1f75a-42a0-4629-ad7d-509959142b1a', embedding=None, metadata={'page_label': '142', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='142 Vector Calculus\\nExample 5.2 (Derivative of a Polynomial)\\nWe want to compute the derivative of f(x) =xn,n∈N. We may already\\nknow that the answer will be nxn−1, but we want to derive this result\\nusing the deﬁnition of the derivative as the limit of the difference quotient.\\nUsing the deﬁnition of the derivative in (5.4), we obtain\\ndf\\ndx= lim\\nh→0f(x+h)−f(x)\\nh(5.5a)\\n= lim\\nh→0(x+h)n−xn\\nh(5.5b)\\n= lim\\nh→0∑n\\ni=0(n\\ni)xn−ihi−xn\\nh. (5.5c)\\nWe see that xn=(n\\n0)xn−0h0. By starting the sum at 1, thexn-term cancels,\\nand we obtain\\ndf\\ndx= lim\\nh→0∑n\\ni=1(n\\ni)xn−ihi\\nh(5.6a)\\n= lim\\nh→0n∑\\ni=1(\\nn\\ni)\\nxn−ihi−1(5.6b)\\n= lim\\nh→0(\\nn\\n1)\\nxn−1+n∑\\ni=2(\\nn\\ni)\\nxn−ihi−1\\n\\ued19\\ued18\\ued17\\ued1a\\n→0ash→0(5.6c)\\n=n!\\n1!(n−1)!xn−1=nxn−1. (5.6d)\\n5.1.1 Taylor Series\\nThe Taylor series is a representation of a function fas an inﬁnite sum of\\nterms. These terms are determined using derivatives of fevaluated at x0.\\nDeﬁnition 5.3 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\\nf:R→Ratx0is deﬁned as We deﬁnet0:= 1\\nfor allt∈R.\\nTn(x) :=n∑\\nk=0f(k)(x0)\\nk!(x−x0)k, (5.7)\\nwheref(k)(x0)is thekth derivative of fatx0(which we assume exists)\\nandf(k)(x0)\\nk!are the coefﬁcients of the polynomial.\\nDeﬁnition 5.4 (Taylor Series) .For a smooth function f∈C∞,f:R→R,\\ntheTaylor series offatx0is deﬁned as Taylor series\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c98e1d6a-5ef4-43b3-8738-03f1a34d7c51', embedding=None, metadata={'page_label': '143', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.1 Differentiation of Univariate Functions 143\\nT∞(x) =∞∑\\nk=0f(k)(x0)\\nk!(x−x0)k. (5.8)\\nForx0= 0, we obtain the Maclaurin series as a special instance of the f∈C∞means that\\nfis continuously\\ndifferentiable\\ninﬁnitely many\\ntimes.\\nMaclaurin seriesTaylor series. If f(x) =T∞(x), thenfis called analytic .\\nanalyticRemark. In general, a Taylor polynomial of degree nis an approximation\\nof a function, which does not need to be a polynomial. The Taylor poly-\\nnomial is similar to fin a neighborhood around x0. However, a Taylor\\npolynomial of degree nis an exact representation of a polynomial fof\\ndegreek⩽nsince all derivatives f(i),i>k vanish. ♦\\nExample 5.3 (Taylor Polynomial)\\nWe consider the polynomial\\nf(x) =x4(5.9)\\nand seek the Taylor polynomial T6, evaluated at x0= 1. We start by com-\\nputing the coefﬁcients f(k)(1)fork= 0,..., 6:\\nf(1) = 1 (5.10)\\nf′(1) = 4 (5.11)\\nf′′(1) = 12 (5.12)\\nf(3)(1) = 24 (5.13)\\nf(4)(1) = 24 (5.14)\\nf(5)(1) = 0 (5.15)\\nf(6)(1) = 0 (5.16)\\nTherefore, the desired Taylor polynomial is\\nT6(x) =6∑\\nk=0f(k)(x0)\\nk!(x−x0)k(5.17a)\\n= 1 + 4(x−1) + 6(x−1)2+ 4(x−1)3+ (x−1)4+ 0.(5.17b)\\nMultiplying out and re-arranging yields\\nT6(x) = (1−4 + 6−4 + 1) +x(4−12 + 12−4)\\n+x2(6−12 + 6) +x3(4−4) +x4(5.18a)\\n=x4=f(x), (5.18b)\\ni.e., we obtain an exact representation of the original function.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00c4108d-7bec-4890-9593-d23e8841bd3d', embedding=None, metadata={'page_label': '144', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='144 Vector Calculus\\nFigure 5.4 Taylor\\npolynomials. The\\noriginal function\\nf(x) =\\nsin(x) + cos(x)\\n(black, solid) is\\napproximated by\\nTaylor polynomials\\n(dashed) around\\nx0= 0.\\nHigher-order Taylor\\npolynomials\\napproximate the\\nfunctionfbetter\\nand more globally.\\nT10is already\\nsimilar tofin\\n[−4,4].\\n−4−2 0 2 4\\nx−2024yf\\nT0\\nT1\\nT5\\nT10\\nExample 5.4 (Taylor Series)\\nConsider the function in Figure 5.4 given by\\nf(x) = sin(x) + cos(x)∈C∞. (5.19)\\nWe seek a Taylor series expansion of fatx0= 0, which is the Maclaurin\\nseries expansion of f. We obtain the following derivatives:\\nf(0) = sin(0) + cos(0) = 1 (5.20)\\nf′(0) = cos(0)−sin(0) = 1 (5.21)\\nf′′(0) =−sin(0)−cos(0) =−1 (5.22)\\nf(3)(0) =−cos(0) + sin(0) = −1 (5.23)\\nf(4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\\n...\\nWe can see a pattern here: The coefﬁcients in our Taylor series are only\\n±1(since sin(0) = 0 ), each of which occurs twice before switching to the\\nother one. Furthermore, f(k+4)(0) =f(k)(0).\\nTherefore, the full Taylor series expansion of fatx0= 0is given by\\nT∞(x) =∞∑\\nk=0f(k)(x0)\\nk!(x−x0)k(5.25a)\\n= 1 +x−1\\n2!x2−1\\n3!x3+1\\n4!x4+1\\n5!x5−··· (5.25b)\\n= 1−1\\n2!x2+1\\n4!x4∓··· +x−1\\n3!x3+1\\n5!x5∓··· (5.25c)\\n=∞∑\\nk=0(−1)k1\\n(2k)!x2k+∞∑\\nk=0(−1)k 1\\n(2k+ 1)!x2k+1(5.25d)\\n= cos(x) + sin(x), (5.25e)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f647c7ad-b683-475c-8f93-b5ab9c83c3d5', embedding=None, metadata={'page_label': '145', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.1 Differentiation of Univariate Functions 145\\nwhere we used the power series representations power series\\nrepresentation\\ncos(x) =∞∑\\nk=0(−1)k1\\n(2k)!x2k, (5.26)\\nsin(x) =∞∑\\nk=0(−1)k 1\\n(2k+ 1)!x2k+1. (5.27)\\nFigure 5.4 shows the corresponding ﬁrst Taylor polynomials Tnforn=\\n0,1,5,10.\\nRemark. A Taylor series is a special case of a power series\\nf(x) =∞∑\\nk=0ak(x−c)k(5.28)\\nwhereakare coefﬁcients and cis a constant, which has the special form\\nin Deﬁnition 5.4. ♦\\n5.1.2 Differentiation Rules\\nIn the following, we brieﬂy state basic differentiation rules, where we\\ndenote the derivative of fbyf′.\\nProduct rule: (f(x)g(x))′=f′(x)g(x) +f(x)g′(x) (5.29)\\nQuotient rule:(f(x)\\ng(x))′\\n=f′(x)g(x)−f(x)g′(x)\\n(g(x))2(5.30)\\nSum rule: (f(x) +g(x))′=f′(x) +g′(x) (5.31)\\nChain rule:(g(f(x)))′= (g◦f)′(x) =g′(f(x))f′(x) (5.32)\\nHere,g◦fdenotes function composition x↦→f(x)↦→g(f(x)).\\nExample 5.5 (Chain Rule)\\nLet us compute the derivative of the function h(x) = (2x+ 1)4using the\\nchain rule. With\\nh(x) = (2x+ 1)4=g(f(x)), (5.33)\\nf(x) = 2x+ 1, (5.34)\\ng(f) =f4, (5.35)\\nwe obtain the derivatives of fandgas\\nf′(x) = 2, (5.36)\\ng′(f) = 4f3, (5.37)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce8eb06e-88e3-4212-aa36-5bbc7c274774', embedding=None, metadata={'page_label': '146', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='146 Vector Calculus\\nsuch that the derivative of his given as\\nh′(x) =g′(f)f′(x) = (4f3)·2(5.34)= 4(2x+ 1)3·2 = 8(2x+ 1)3,(5.38)\\nwhere we used the chain rule (5.32) and substituted the deﬁnition of f\\nin (5.34) in g′(f).\\n5.2 Partial Differentiation and Gradients\\nDifferentiation as discussed in Section 5.1 applies to functions fof a\\nscalar variable x∈R. In the following, we consider the general case\\nwhere the function fdepends on one or more variables x∈Rn, e.g.,\\nf(x) =f(x1,x2). The generalization of the derivative to functions of sev-\\neral variables is the gradient .\\nWe ﬁnd the gradient of the function fwith respect to xbyvarying one\\nvariable at a time and keeping the others constant. The gradient is then\\nthe collection of these partial derivatives .\\nDeﬁnition 5.5 (Partial Derivative) .For a function f:Rn→R,x↦→\\nf(x),x∈Rnofnvariablesx1,...,xnwe deﬁne the partial derivatives as partial derivative\\n∂f\\n∂x1= lim\\nh→0f(x1+h,x 2,...,xn)−f(x)\\nh\\n...\\n∂f\\n∂xn= lim\\nh→0f(x1,...,xn−1,xn+h)−f(x)\\nh(5.39)\\nand collect them in the row vector\\n∇xf= gradf=df\\ndx=[∂f(x)\\n∂x1∂f(x)\\n∂x2···∂f(x)\\n∂xn]\\n∈R1×n,(5.40)\\nwherenis the number of variables and 1is the dimension of the image/\\nrange/codomain of f. Here, we deﬁned the column vector x= [x1,...,xn]⊤\\n∈Rn. The row vector in (5.40) is called the gradient offor the Jacobian gradient\\nJacobian and is the generalization of the derivative from Section 5.1.\\nRemark. This deﬁnition of the Jacobian is a special case of the general\\ndeﬁnition of the Jacobian for vector-valued functions as the collection of\\npartial derivatives. We will get back to this in Section 5.3. ♦We can use results\\nfrom scalar\\ndifferentiation: Each\\npartial derivative is\\na derivative with\\nrespect to a scalar.Example 5.6 (Partial Derivatives Using the Chain Rule)\\nForf(x,y) = (x+ 2y3)2, we obtain the partial derivatives\\n∂f(x,y)\\n∂x= 2(x+ 2y3)∂\\n∂x(x+ 2y3) = 2(x+ 2y3), (5.41)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb210983-4bbf-4c1a-bdbc-ec183d66686d', embedding=None, metadata={'page_label': '147', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 Partial Differentiation and Gradients 147\\n∂f(x,y)\\n∂y= 2(x+ 2y3)∂\\n∂y(x+ 2y3) = 12(x+ 2y3)y2. (5.42)\\nwhere we used the chain rule (5.32) to compute the partial derivatives.\\nRemark (Gradient as a Row Vector) .It is not uncommon in the literature\\nto deﬁne the gradient vector as a column vector, following the conven-\\ntion that vectors are generally column vectors. The reason why we deﬁne\\nthe gradient vector as a row vector is twofold: First, we can consistently\\ngeneralize the gradient to vector-valued functions f:Rn→Rm(then\\nthe gradient becomes a matrix). Second, we can immediately apply the\\nmulti-variate chain rule without paying attention to the dimension of the\\ngradient. We will discuss both points in Section 5.3. ♦\\nExample 5.7 (Gradient)\\nForf(x1,x2) =x2\\n1x2+x1x3\\n2∈R, the partial derivatives (i.e., the deriva-\\ntives offwith respect to x1andx2) are\\n∂f(x1,x2)\\n∂x1= 2x1x2+x3\\n2 (5.43)\\n∂f(x1,x2)\\n∂x2=x2\\n1+ 3x1x2\\n2 (5.44)\\nand the gradient is then\\ndf\\ndx=[∂f(x1,x2)\\n∂x1∂f(x1,x2)\\n∂x2]\\n=[2x1x2+x3\\n2x2\\n1+ 3x1x2\\n2]∈R1×2.\\n(5.45)\\n5.2.1 Basic Rules of Partial Differentiation\\nProduct rule:\\n(fg)′=f′g+fg′,\\nSum rule:\\n(f+g)′=f′+g′,\\nChain rule:\\n(g(f))′=g′(f)f′In the multivariate case, where x∈Rn, the basic differentiation rules that\\nwe know from school (e.g., sum rule, product rule, chain rule; see also\\nSection 5.1.2) still apply. However, when we compute derivatives with re-\\nspect to vectors x∈Rnwe need to pay attention: Our gradients now\\ninvolve vectors and matrices, and matrix multiplication is not commuta-\\ntive (Section 2.2.1), i.e., the order matters.\\nHere are the general product rule, sum rule, and chain rule:\\nProduct rule:∂\\n∂x(f(x)g(x))=∂f\\n∂xg(x) +f(x)∂g\\n∂x(5.46)\\nSum rule:∂\\n∂x(f(x) +g(x))=∂f\\n∂x+∂g\\n∂x(5.47)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fcfa01a1-ff0d-4c5a-93c4-32b13090827b', embedding=None, metadata={'page_label': '148', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='148 Vector Calculus\\nChain rule:∂\\n∂x(g◦f)(x) =∂\\n∂x(g(f(x)))=∂g\\n∂f∂f\\n∂x(5.48)\\nLet us have a closer look at the chain rule. The chain rule (5.48) resem- This is only an\\nintuition, but not\\nmathematically\\ncorrect since the\\npartial derivative is\\nnot a fraction.bles to some degree the rules for matrix multiplication where we said that\\nneighboring dimensions have to match for matrix multiplication to be de-\\nﬁned; see Section 2.2.1. If we go from left to right, the chain rule exhibits\\nsimilar properties: ∂fshows up in the “denominator” of the ﬁrst factor\\nand in the “numerator” of the second factor. If we multiply the factors to-\\ngether, multiplication is deﬁned, i.e., the dimensions of ∂fmatch, and ∂f\\n“cancels”, such that ∂g/∂xremains.\\n5.2.2 Chain Rule\\nConsider a function f:R2→Rof two variables x1,x2. Furthermore,\\nx1(t)andx2(t)are themselves functions of t. To compute the gradient of\\nfwith respect to t, we need to apply the chain rule (5.48) for multivariate\\nfunctions as\\ndf\\ndt=[\\n∂f\\n∂x1∂f\\n∂x2][\\n∂x1(t)\\n∂t∂x2(t)\\n∂t]\\n=∂f\\n∂x1∂x1\\n∂t+∂f\\n∂x2∂x2\\n∂t, (5.49)\\nwhere ddenotes the gradient and ∂partial derivatives.\\nExample 5.8\\nConsiderf(x1,x2) =x2\\n1+ 2x2, wherex1= sintandx2= cost, then\\ndf\\ndt=∂f\\n∂x1∂x1\\n∂t+∂f\\n∂x2∂x2\\n∂t(5.50a)\\n= 2 sint∂sint\\n∂t+ 2∂cost\\n∂t(5.50b)\\n= 2 sintcost−2 sint= 2 sint(cost−1) (5.50c)\\nis the corresponding derivative of fwith respect to t.\\nIff(x1,x2)is a function of x1andx2, wherex1(s,t)andx2(s,t)are\\nthemselves functions of two variables sandt, the chain rule yields the\\npartial derivatives\\n∂f\\n∂s=∂f\\n∂x1∂x1\\n∂s+∂f\\n∂x2∂x2\\n∂s, (5.51)\\n∂f\\n∂t=∂f\\n∂x1∂x1\\n∂t+∂f\\n∂x2∂x2\\n∂t, (5.52)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a952a30a-034f-470e-9697-845eb3a5bbb1', embedding=None, metadata={'page_label': '149', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Gradients of Vector-Valued Functions 149\\nand the gradient is obtained by the matrix multiplication\\ndf\\nd(s,t)=∂f\\n∂x∂x\\n∂(s,t)=[∂f\\n∂x1∂f\\n∂x2]\\n\\ued19\\ued18\\ued17\\ued1a\\n=∂f\\n∂x\\uf8ee\\n\\uf8ef\\uf8f0∂x1\\n∂s∂x1\\n∂t\\n∂x2\\n∂s∂x2\\n∂t\\uf8f9\\n\\uf8fa\\uf8fb\\n\\ued19\\ued18\\ued17\\ued1a\\n=∂x\\n∂(s,t). (5.53)\\nThis compact way of writing the chain rule as a matrix multiplication only The chain rule can\\nbe written as a\\nmatrix\\nmultiplication.makes sense if the gradient is deﬁned as a row vector. Otherwise, we will\\nneed to start transposing gradients for the matrix dimensions to match.\\nThis may still be straightforward as long as the gradient is a vector or a\\nmatrix; however, when the gradient becomes a tensor (we will discuss this\\nin the following), the transpose is no longer a triviality.\\nRemark (Verifying the Correctness of a Gradient Implementation) .The\\ndeﬁnition of the partial derivatives as the limit of the corresponding dif-\\nference quotient (see (5.39)) can be exploited when numerically checking\\nthe correctness of gradients in computer programs: When we compute Gradient checking\\ngradients and implement them, we can use ﬁnite differences to numer-\\nically test our computation and implementation: We choose the value h\\nto be small (e.g., h= 10−4) and compare the ﬁnite-difference approxima-\\ntion from (5.39) with our (analytic) implementation of the gradient. If the\\nerror is small, our gradient implementation is probably correct. “Small”\\ncould mean that√∑\\ni(dhi−dfi)2\\n∑\\ni(dhi+dfi)2<10−6, wheredhiis the ﬁnite-difference\\napproximation and dfiis the analytic gradient of fwith respect to the ith\\nvariablexi. ♦\\n5.3 Gradients of Vector-Valued Functions\\nThus far, we discussed partial derivatives and gradients of functions f:\\nRn→Rmapping to the real numbers. In the following, we will generalize\\nthe concept of the gradient to vector-valued functions (vector ﬁelds) f:\\nRn→Rm, wheren⩾1andm> 1.\\nFor a function f:Rn→Rmand a vectorx= [x1,...,xn]⊤∈Rn, the\\ncorresponding vector of function values is given as\\nf(x) =\\uf8ee\\n\\uf8ef\\uf8f0f1(x)\\n...\\nfm(x)\\uf8f9\\n\\uf8fa\\uf8fb∈Rm. (5.54)\\nWriting the vector-valued function in this way allows us to view a vector-\\nvalued function f:Rn→Rmas a vector of functions [f1,...,fm]⊤,\\nfi:Rn→Rthat map onto R. The differentiation rules for every fiare\\nexactly the ones we discussed in Section 5.2.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8676d692-8261-4bfb-aca3-781811f885d8', embedding=None, metadata={'page_label': '150', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='150 Vector Calculus\\nTherefore, the partial derivative of a vector-valued function f:Rn→\\nRmwith respect to xi∈R,i= 1,...n , is given as the vector\\n∂f\\n∂xi=\\uf8ee\\n\\uf8ef\\uf8f0∂f1\\n∂xi...\\n∂fm\\n∂xi\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8f0limh→0f1(x1,...,x i−1,xi+h,xi+1,...xn)−f1(x)\\nh...\\nlimh→0fm(x1,...,x i−1,xi+h,xi+1,...xn)−fm(x)\\nh\\uf8f9\\n\\uf8fa\\uf8fb∈Rm.\\n(5.55)\\nFrom (5.40), we know that the gradient of fwith respect to a vector is\\nthe row vector of the partial derivatives. In (5.55), every partial derivative\\n∂f/∂xiis itself a column vector. Therefore, we obtain the gradient of f:\\nRn→Rmwith respect to x∈Rnby collecting these partial derivatives:\\ndf(x)\\ndx=∂f(x)\\n∂x1···∂f(x)\\n∂xn[ ]\\n(5.56a)\\n=∂f1(x)\\n∂x1···∂f1(x)\\n∂xn\\n......\\n∂fm(x)\\n∂x1···∂fm(x)\\n∂xn\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rm×n.(5.56b)\\nDeﬁnition 5.6 (Jacobian) .The collection of all ﬁrst-order partial deriva-\\ntives of a vector-valued function f:Rn→Rmis called the Jacobian . The Jacobian\\nJacobianJis anm×nmatrix, which we deﬁne and arrange as follows: The gradient of a\\nfunction\\nf:Rn→Rmis a\\nmatrix of size\\nm×n.J=∇xf=df(x)\\ndx=[∂f(x)\\n∂x1···∂f(x)\\n∂xn]\\n(5.57)\\n=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0∂f1(x)\\n∂x1···∂f1(x)\\n∂xn......\\n∂fm(x)\\n∂x1···∂fm(x)\\n∂xn\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb, (5.58)\\nx=\\uf8ee\\n\\uf8ef\\uf8f0x1\\n...\\nxn\\uf8f9\\n\\uf8fa\\uf8fb, J(i,j) =∂fi\\n∂xj. (5.59)\\nAs a special case of (5.58), a function f:Rn→R1, which maps a\\nvectorx∈Rnonto a scalar (e.g., f(x) =∑n\\ni=1xi), possesses a Jacobian\\nthat is a row vector (matrix of dimension 1×n); see (5.40).\\nRemark. In this book, we use the numerator layout of the derivative, i.e., numerator layout\\nthe derivative df/dxoff∈Rmwith respect to x∈Rnis anm×\\nnmatrix, where the elements of fdeﬁne the rows and the elements of\\nxdeﬁne the columns of the corresponding Jacobian; see (5.58). There\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f630c771-c316-4246-8c11-1919628ef000', embedding=None, metadata={'page_label': '151', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Gradients of Vector-Valued Functions 151\\nFigure 5.5 The\\ndeterminant of the\\nJacobian offcan\\nbe used to compute\\nthe magniﬁer\\nbetween the blue\\nand orange area.b1b2 c1 c2f(·)\\nexists also the denominator layout , which is the transpose of the numerator denominator layout\\nlayout. In this book, we will use the numerator layout. ♦\\nWe will see how the Jacobian is used in the change-of-variable method\\nfor probability distributions in Section 6.7. The amount of scaling due to\\nthe transformation of a variable is provided by the determinant.\\nIn Section 4.1, we saw that the determinant can be used to compute\\nthe area of a parallelogram. If we are given two vectors b1= [1,0]⊤,\\nb2= [0,1]⊤as the sides of the unit square (blue; see Figure 5.5), the area\\nof this square is\\n⏐⏐⏐⏐det([1 0\\n0 1])⏐⏐⏐⏐= 1. (5.60)\\nIf we take a parallelogram with the sides c1= [−2,1]⊤,c2= [1,1]⊤\\n(orange in Figure 5.5), its area is given as the absolute value of the deter-\\nminant (see Section 4.1)\\n⏐⏐⏐⏐det([−2 1\\n1 1])⏐⏐⏐⏐=|−3|= 3, (5.61)\\ni.e., the area of this is exactly three times the area of the unit square.\\nWe can ﬁnd this scaling factor by ﬁnding a mapping that transforms the\\nunit square into the other square. In linear algebra terms, we effectively\\nperform a variable transformation from (b1,b2)to(c1,c2). In our case,\\nthe mapping is linear and the absolute value of the determinant of this\\nmapping gives us exactly the scaling factor we are looking for.\\nWe will describe two approaches to identify this mapping. First, we ex-\\nploit that the mapping is linear so that we can use the tools from Chapter 2\\nto identify this mapping. Second, we will ﬁnd the mapping using partial\\nderivatives using the tools we have been discussing in this chapter.\\nApproach 1 To get started with the linear algebra approach, we\\nidentify both{b1,b2}and{c1,c2}as bases of R2(see Section 2.6.1 for a\\nrecap). What we effectively perform is a change of basis from (b1,b2)to\\n(c1,c2), and we are looking for the transformation matrix that implements\\nthe basis change. Using results from Section 2.7.2, we identify the desired\\nbasis change matrix as\\nJ=[−2 1\\n1 1]\\n, (5.62)\\nsuch thatJb1=c1andJb2=c2. The absolute value of the determi-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a1216a9-5e6d-4ba3-b35a-f015f77b2276', embedding=None, metadata={'page_label': '152', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='152 Vector Calculus\\nnant ofJ, which yields the scaling factor we are looking for, is given as\\n|det(J)|= 3, i.e., the area of the square spanned by (c1,c2)is three times\\ngreater than the area spanned by (b1,b2).\\nApproach 2 The linear algebra approach works for linear trans-\\nformations; for nonlinear transformations (which become relevant in Sec-\\ntion 6.7), we follow a more general approach using partial derivatives.\\nFor this approach, we consider a function f:R2→R2that performs a\\nvariable transformation. In our example, fmaps the coordinate represen-\\ntation of any vector x∈R2with respect to (b1,b2)onto the coordinate\\nrepresentation y∈R2with respect to (c1,c2). We want to identify the\\nmapping so that we can compute how an area (or volume) changes when\\nit is being transformed by f. For this, we need to ﬁnd out how f(x)\\nchanges if we modify xa bit. This question is exactly answered by the\\nJacobian matrixdf\\ndx∈R2×2. Since we can write\\ny1=−2x1+x2 (5.63)\\ny2=x1+x2 (5.64)\\nwe obtain the functional relationship between xandy, which allows us\\nto get the partial derivatives\\n∂y1\\n∂x1=−2,∂y1\\n∂x2= 1,∂y2\\n∂x1= 1,∂y2\\n∂x2= 1 (5.65)\\nand compose the Jacobian as\\nJ=\\uf8ee\\n\\uf8ef\\uf8f0∂y1\\n∂x1∂y1\\n∂x2∂y2\\n∂x1∂y2\\n∂x2\\uf8f9\\n\\uf8fa\\uf8fb=[−2 1\\n1 1]\\n. (5.66)\\nThe Jacobian represents the coordinate transformation we are looking Geometrically, the\\nJacobian\\ndeterminant gives\\nthe magniﬁcation/\\nscaling factor when\\nwe transform an\\narea or volume.for. It is exact if the coordinate transformation is linear (as in our case),\\nand (5.66) recovers exactly the basis change matrix in (5.62). If the co-\\nordinate transformation is nonlinear, the Jacobian approximates this non-\\nlinear transformation locally with a linear one. The absolute value of the\\nJacobian determinant |det(J)|is the factor by which areas or volumes are\\nJacobian\\ndeterminantscaled when coordinates are transformed. Our case yields |det(J)|= 3.\\nThe Jacobian determinant and variable transformations will become\\nrelevant in Section 6.7 when we transform random variables and prob-\\nability distributions. These transformations are extremely relevant in ma- Figure 5.6\\nDimensionality of\\n(partial) derivatives.\\nf(x)\\nx\\n∂f\\n∂xchine learning in the context of training deep neural networks using the\\nreparametrization trick , also called inﬁnite perturbation analysis .\\nIn this chapter, we encountered derivatives of functions. Figure 5.6 sum-\\nmarizes the dimensions of those derivatives. If f:R→Rthe gradient is\\nsimply a scalar (top-left entry). For f:RD→Rthe gradient is a 1×D\\nrow vector (top-right entry). For f:R→RE, the gradient is an E×1\\ncolumn vector, and for f:RD→REthe gradient is an E×Dmatrix.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4004b1a8-7921-44be-9995-80e7b159e411', embedding=None, metadata={'page_label': '153', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Gradients of Vector-Valued Functions 153\\nExample 5.9 (Gradient of a Vector-Valued Function)\\nWe are given\\nf(x) =Ax,f(x)∈RM,A∈RM×N,x∈RN.\\nTo compute the gradient df/dxwe ﬁrst determine the dimension of\\ndf/dx: Sincef:RN→RM, it follows that df/dx∈RM×N. Second,\\nto compute the gradient we determine the partial derivatives of fwith\\nrespect to every xj:\\nfi(x) =N∑\\nj=1Aijxj=⇒∂fi\\n∂xj=Aij (5.67)\\nWe collect the partial derivatives in the Jacobian and obtain the gradient\\ndf\\ndx=\\uf8ee\\n\\uf8ef\\uf8f0∂f1\\n∂x1···∂f1\\n∂xN......\\n∂fM\\n∂x1···∂fM\\n∂xN\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8f0A11···A1N\\n......\\nAM1···AMN\\uf8f9\\n\\uf8fa\\uf8fb=A∈RM×N.(5.68)\\nExample 5.10 (Chain Rule)\\nConsider the function h:R→R,h(t) = (f◦g)(t)with\\nf:R2→R (5.69)\\ng:R→R2(5.70)\\nf(x) = exp(x1x2\\n2), (5.71)\\nx=[x1\\nx2]\\n=g(t) =[tcost\\ntsint]\\n(5.72)\\nand compute the gradient of hwith respect to t. Sincef:R2→Rand\\ng:R→R2we note that\\n∂f\\n∂x∈R1×2,∂g\\n∂t∈R2×1. (5.73)\\nThe desired gradient is computed by applying the chain rule:\\ndh\\ndt=∂f\\n∂x∂x\\n∂t=[∂f\\n∂x1∂f\\n∂x2]\\uf8ee\\n\\uf8ef\\uf8f0∂x1\\n∂t∂x2\\n∂t\\uf8f9\\n\\uf8fa\\uf8fb (5.74a)\\n=[exp(x1x2\\n2)x2\\n22 exp(x1x2\\n2)x1x2][cost−tsint\\nsint+tcost]\\n(5.74b)\\n= exp(x1x2\\n2)(x2\\n2(cost−tsint) + 2x1x2(sint+tcost)),(5.74c)\\nwherex1=tcostandx2=tsint; see (5.72).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b769f079-f82b-413c-a486-a8ae6c409e05', embedding=None, metadata={'page_label': '154', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='154 Vector Calculus\\nExample 5.11 (Gradient of a Least-Squares Loss in a Linear Model)\\nLet us consider the linear model We will discuss this\\nmodel in much\\nmore detail in\\nChapter 9 in the\\ncontext of linear\\nregression, where\\nwe need derivatives\\nof the least-squares\\nlossLwith respect\\nto the parameters θ.y=Φθ, (5.75)\\nwhereθ∈RDis a parameter vector, Φ∈RN×Dare input features and\\ny∈RNare the corresponding observations. We deﬁne the functions\\nL(e) :=∥e∥2, (5.76)\\ne(θ) :=y−Φθ. (5.77)\\nWe seek∂L\\n∂θ, and we will use the chain rule for this purpose. Lis called a\\nleast-squares loss function. least-squares loss\\nBefore we start our calculation, we determine the dimensionality of the\\ngradient as\\n∂L\\n∂θ∈R1×D. (5.78)\\nThe chain rule allows us to compute the gradient as\\n∂L\\n∂θ=∂L\\n∂e∂e\\n∂θ, (5.79)\\nwhere thedth element is given by dLdtheta =\\nnp.einsum(\\n’n,nd’,\\ndLde,dedtheta)∂L\\n∂θ[1,d] =N∑\\nn=1∂L\\n∂e[n]∂e\\n∂θ[n,d]. (5.80)\\nWe know that∥e∥2=e⊤e(see Section 3.2) and determine\\n∂L\\n∂e= 2e⊤∈R1×N. (5.81)\\nFurthermore, we obtain\\n∂e\\n∂θ=−Φ∈RN×D, (5.82)\\nsuch that our desired derivative is\\n∂L\\n∂θ=−2e⊤Φ(5.77)=−2(y⊤−θ⊤Φ⊤)\\ued19\\ued18\\ued17\\ued1a\\n1×NΦ\\ued19\\ued18\\ued17\\ued1a\\nN×D∈R1×D. (5.83)\\nRemark. We would have obtained the same result without using the chain\\nrule by immediately looking at the function\\nL2(θ) :=∥y−Φθ∥2= (y−Φθ)⊤(y−Φθ). (5.84)\\nThis approach is still practical for simple functions like L2but becomes\\nimpractical for deep function compositions. ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0fffca0-08d6-4925-af31-40d5e1349e61', embedding=None, metadata={'page_label': '155', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4 Gradients of Matrices 155\\nFigure 5.7\\nVisualization of\\ngradient\\ncomputation of a\\nmatrix with respect\\nto a vector. We are\\ninterested in\\ncomputing the\\ngradient of\\nA∈R4×2with\\nrespect to a vector\\nx∈R3. We know\\nthat gradient\\ndA\\ndx∈R4×2×3. We\\nfollow two\\nequivalent\\napproaches to arrive\\nthere: (a) collating\\npartial derivatives\\ninto a Jacobian\\ntensor;\\n(b) ﬂattening of the\\nmatrix into a vector,\\ncomputing the\\nJacobian matrix,\\nre-shaping into a\\nJacobian tensor.A∈R4×2x∈R3\\n∂A\\n∂x1∈R4×2∂A\\n∂x2∈R4×2∂A\\n∂x3∈R4×2x1\\nx2\\nx3\\ndA\\ndx∈R4×2×3\\n4\\n23Partial derivatives:\\ncollate\\n(a) Approach 1: We compute the partial derivative\\n∂A\\n∂x1,∂A\\n∂x2,∂A\\n∂x3, each of which is a 4×2matrix, and col-\\nlate them in a 4×2×3tensor.\\nA∈R4×2x∈R3\\nx1\\nx2\\nx3\\ndA\\ndx∈R4×2×3\\nre-shape re-shape gradientA∈R4×2 ˜A∈R8d˜A\\ndx∈R8×3\\n(b) Approach 2: We re-shape (ﬂatten) A∈R4×2into a vec-\\ntor˜A∈R8. Then, we compute the gradientd˜A\\ndx∈R8×3.\\nWe obtain the gradient tensor by re-shaping this gradient as\\nillustrated above.\\n5.4 Gradients of MatricesWe can think of a\\ntensor as a\\nmultidimensional\\narray.We will encounter situations where we need to take gradients of matrices\\nwith respect to vectors (or other matrices), which results in a multidimen-\\nsional tensor. We can think of this tensor as a multidimensional array that\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c1aad32-c788-4737-aa19-be9cdc090f0b', embedding=None, metadata={'page_label': '156', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='156 Vector Calculus\\ncollects partial derivatives. For example, if we compute the gradient of an\\nm×nmatrixAwith respect to a p×qmatrixB, the resulting Jacobian\\nwould be (m×n)×(p×q), i.e., a four-dimensional tensor J, whose entries\\nare given as Jijkl=∂Aij/∂Bkl.\\nSince matrices represent linear mappings, we can exploit the fact that\\nthere is a vector-space isomorphism (linear, invertible mapping) between\\nthe space Rm×nofm×nmatrices and the space Rmnofmnvectors.\\nTherefore, we can re-shape our matrices into vectors of lengths mnand\\npq, respectively. The gradient using these mnvectors results in a Jacobian\\nof sizemn×pq. Figure 5.7 visualizes both approaches. In practical ap- Matrices can be\\ntransformed into\\nvectors by stacking\\nthe columns of the\\nmatrix\\n(“ﬂattening”).plications, it is often desirable to re-shape the matrix into a vector and\\ncontinue working with this Jacobian matrix: The chain rule (5.48) boils\\ndown to simple matrix multiplication, whereas in the case of a Jacobian\\ntensor, we will need to pay more attention to what dimensions we need\\nto sum out.\\nExample 5.12 (Gradient of Vectors with Respect to Matrices)\\nLet us consider the following example, where\\nf=Ax,f∈RM,A∈RM×N,x∈RN(5.85)\\nand where we seek the gradient df/dA. Let us start again by determining\\nthe dimension of the gradient as\\ndf\\ndA∈RM×(M×N). (5.86)\\nBy deﬁnition, the gradient is the collection of the partial derivatives:\\ndf\\ndA=\\uf8ee\\n\\uf8ef\\uf8f0∂f1\\n∂A...\\n∂fM\\n∂A\\uf8f9\\n\\uf8fa\\uf8fb,∂fi\\n∂A∈R1×(M×N). (5.87)\\nTo compute the partial derivatives, it will be helpful to explicitly write out\\nthe matrix vector multiplication:\\nfi=N∑\\nj=1Aijxj, i= 1,...,M, (5.88)\\nand the partial derivatives are then given as\\n∂fi\\n∂Aiq=xq. (5.89)\\nThis allows us to compute the partial derivatives of fiwith respect to a\\nrow ofA, which is given as\\n∂fi\\n∂Ai,:=x⊤∈R1×1×N, (5.90)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='def4483b-5bc8-468d-858f-2a4632608da4', embedding=None, metadata={'page_label': '157', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.4 Gradients of Matrices 157\\n∂fi\\n∂Ak̸=i,:=0⊤∈R1×1×N(5.91)\\nwhere we have to pay attention to the correct dimensionality. Since fi\\nmaps onto Rand each row of Ais of size 1×N, we obtain a 1×1×N-\\nsized tensor as the partial derivative of fiwith respect to a row of A.\\nWe stack the partial derivatives (5.91) and get the desired gradient\\nin (5.87) via\\n∂fi\\n∂A=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00⊤\\n...\\n0⊤\\nx⊤\\n0⊤\\n...\\n0⊤\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈R1×(M×N). (5.92)\\nExample 5.13 (Gradient of Matrices with Respect to Matrices)\\nConsider a matrix R∈RM×Nandf:RM×N→RN×Nwith\\nf(R) =R⊤R=:K∈RN×N, (5.93)\\nwhere we seek the gradient dK/dR.\\nTo solve this hard problem, let us ﬁrst write down what we already\\nknow: The gradient has the dimensions\\ndK\\ndR∈R(N×N)×(M×N), (5.94)\\nwhich is a tensor. Moreover,\\ndKpq\\ndR∈R1×M×N(5.95)\\nforp,q= 1,...,N , whereKpqis the (p,q)th entry ofK=f(R). De-\\nnoting theith column of Rbyri, every entry of Kis given by the dot\\nproduct of two columns of R, i.e.,\\nKpq=r⊤\\nprq=M∑\\nm=1RmpRmq. (5.96)\\nWhen we now compute the partial derivative∂Kpq\\n∂Rijwe obtain\\n∂Kpq\\n∂Rij=M∑\\nm=1∂\\n∂RijRmpRmq=∂pqij, (5.97)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06836039-8bc7-4ca4-a67c-cfea5061f34c', embedding=None, metadata={'page_label': '158', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='158 Vector Calculus\\n∂pqij=\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3Riq ifj=p, p̸=q\\nRip ifj=q, p̸=q\\n2Riqifj=p, p=q\\n0 otherwise. (5.98)\\nFrom (5.94), we know that the desired gradient has the dimension (N×\\nN)×(M×N), and every single entry of this tensor is given by ∂pqij\\nin (5.98), where p,q,j = 1,...,N andi= 1,...,M .\\n5.5 Useful Identities for Computing Gradients\\nIn the following, we list some useful gradients that are frequently required\\nin a machine learning context (Petersen and Pedersen, 2012). Here, we\\nuse tr (·)as the trace (see Deﬁnition 4.4), det(·)as the determinant (see\\nSection 4.1) and f(X)−1as the inverse of f(X), assuming it exists.\\n∂\\n∂Xf(X)⊤=(∂f(X)\\n∂X)⊤\\n(5.99)\\n∂\\n∂Xtr(f(X)) = tr(∂f(X)\\n∂X)\\n(5.100)\\n∂\\n∂Xdet(f(X)) = det(f(X))tr(\\nf(X)−1∂f(X)\\n∂X)\\n(5.101)\\n∂\\n∂Xf(X)−1=−f(X)−1∂f(X)\\n∂Xf(X)−1(5.102)\\n∂a⊤X−1b\\n∂X=−(X−1)⊤ab⊤(X−1)⊤(5.103)\\n∂x⊤a\\n∂x=a⊤(5.104)\\n∂a⊤x\\n∂x=a⊤(5.105)\\n∂a⊤Xb\\n∂X=ab⊤(5.106)\\n∂x⊤Bx\\n∂x=x⊤(B+B⊤) (5.107)\\n∂\\n∂s(x−As)⊤W(x−As) =−2(x−As)⊤WA for symmetric W\\n(5.108)\\nRemark. In this book, we only cover traces and transposes of matrices.\\nHowever, we have seen that derivatives can be higher-dimensional ten-\\nsors, in which case the usual trace and transpose are not deﬁned. In these\\ncases, the trace of a D×D×E×Ftensor would be an E×F-dimensional\\nmatrix. This is a special case of a tensor contraction. Similarly, when we\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e3c4ad2-b781-4b9e-9ecd-464551448160', embedding=None, metadata={'page_label': '159', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Backpropagation and Automatic Differentiation 159\\n“transpose” a tensor, we mean swapping the ﬁrst two dimensions. Specif-\\nically, in (5.99) through (5.102), we require tensor-related computations\\nwhen we work with multivariate functions f(·)and compute derivatives\\nwith respect to matrices (and choose not to vectorize them as discussed in\\nSection 5.4). ♦\\n5.6 Backpropagation and Automatic Differentiation\\nA good discussion\\nabout\\nbackpropagation\\nand the chain rule is\\navailable at a blog\\nby Tim Viera at\\nhttps://tinyurl.\\ncom/ycfm2yrw .In many machine learning applications, we ﬁnd good model parameters\\nby performing gradient descent (Section 7.1), which relies on the fact\\nthat we can compute the gradient of a learning objective with respect\\nto the parameters of the model. For a given objective function, we can\\nobtain the gradient with respect to the model parameters using calculus\\nand applying the chain rule; see Section 5.2.2. We already had a taste in\\nSection 5.3 when we looked at the gradient of a squared loss with respect\\nto the parameters of a linear regression model.\\nConsider the function\\nf(x) =√\\nx2+ exp(x2) + cos(x2+ exp(x2)). (5.109)\\nBy application of the chain rule, and noting that differentiation is linear,\\nwe compute the gradient\\ndf\\ndx=2x+ 2xexp(x2)\\n2√\\nx2+ exp(x2)−sin(x2+ exp(x2))(2x+ 2xexp(x2))\\n= 2x(\\n1\\n2√\\nx2+ exp(x2)−sin(x2+ exp(x2)))\\n(1 + exp(x2)).\\n(5.110)\\nWriting out the gradient in this explicit way is often impractical since it\\noften results in a very lengthy expression for a derivative. In practice,\\nit means that, if we are not careful, the implementation of the gradient\\ncould be signiﬁcantly more expensive than computing the function, which\\nimposes unnecessary overhead. For training deep neural network mod-\\nels, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\\n1962; Rumelhart et al., 1986) is an efﬁcient way to compute the gradient\\nof an error function with respect to the parameters of the model.\\n5.6.1 Gradients in a Deep Network\\nAn area where the chain rule is used to an extreme is deep learning, where\\nthe function value yis computed as a many-level function composition\\ny= (fK◦fK−1◦···◦f1)(x) =fK(fK−1(···(f1(x))···)),(5.111)\\nwherexare the inputs (e.g., images), yare the observations (e.g., class\\nlabels), and every function fi,i= 1,...,K , possesses its own parameters.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be67f15a-4598-4905-b65b-ee58aecb26de', embedding=None, metadata={'page_label': '160', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='160 Vector Calculus\\nFigure 5.8 Forward\\npass in a multi-layer\\nneural network to\\ncompute the loss L\\nas a function of the\\ninputsxand the\\nparametersAi,bi.x fK\\nA0,b0 AK−1,bK−1L fK−1\\nAK−2,bK−2f1\\nA1,b1\\nIn neural networks with multiple layers, we have functions fi(xi−1) = We discuss the case,\\nwhere the activation\\nfunctions are\\nidentical in each\\nlayer to unclutter\\nnotation.σ(Ai−1xi−1+bi−1)in theith layer. Here xi−1is the output of layer i−1\\nandσan activation function, such as the logistic sigmoid1\\n1+e−x,tanh or a\\nrectiﬁed linear unit (ReLU). In order to train these models, we require the\\ngradient of a loss function Lwith respect to all model parameters Aj,bj\\nforj= 1,...,K . This also requires us to compute the gradient of Lwith\\nrespect to the inputs of each layer. For example, if we have inputs xand\\nobservationsyand a network structure deﬁned by\\nf0:=x (5.112)\\nfi:=σi(Ai−1fi−1+bi−1), i= 1,...,K, (5.113)\\nsee also Figure 5.8 for a visualization, we may be interested in ﬁnding\\nAj,bjforj= 0,...,K−1, such that the squared loss\\nL(θ) =∥y−fK(θ,x)∥2(5.114)\\nis minimized, where θ={A0,b0,...,AK−1,bK−1}.\\nTo obtain the gradients with respect to the parameter set θ, we require\\nthe partial derivatives of Lwith respect to the parameters θj={Aj,bj}\\nof each layer j= 0,...,K−1. The chain rule allows us to determine the\\npartial derivatives as A more in-depth\\ndiscussion about\\ngradients of neural\\nnetworks can be\\nfound in Justin\\nDomke’s lecture\\nnotes\\nhttps://tinyurl.\\ncom/yalcxgtv .∂L\\n∂θK−1=∂L\\n∂fK∂fK\\n∂θK−1(5.115)\\n∂L\\n∂θK−2=∂L\\n∂fK∂fK\\n∂fK−1∂fK−1\\n∂θK−2(5.116)\\n∂L\\n∂θK−3=∂L\\n∂fK∂fK\\n∂fK−1∂fK−1\\n∂fK−2∂fK−2\\n∂θK−3(5.117)\\n∂L\\n∂θi=∂L\\n∂fK∂fK\\n∂fK−1···∂fi+2\\n∂fi+1∂fi+1\\n∂θi(5.118)\\nTheorange terms are partial derivatives of the output of a layer with\\nrespect to its inputs, whereas the blue terms are partial derivatives of\\nthe output of a layer with respect to its parameters. Assuming, we have\\nalready computed the partial derivatives ∂L/∂θi+1, then most of the com-\\nputation can be reused to compute ∂L/∂θi. The additional terms that we\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61ae7a5b-a725-4f98-a0c4-3fb0b23a5c13', embedding=None, metadata={'page_label': '161', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Backpropagation and Automatic Differentiation 161\\nFigure 5.9\\nBackward pass in a\\nmulti-layer neural\\nnetwork to compute\\nthe gradients of the\\nloss function.x fK\\nA0,b0 AK−1,bK−1L fK−1\\nAK−2,bK−2f1\\nA1,b1\\nFigure 5.10 Simple\\ngraph illustrating\\nthe ﬂow of data\\nfromxtoyvia\\nsome intermediate\\nvariablesa,b.xaby\\nneed to compute are indicated by the boxes. Figure 5.9 visualizes that the\\ngradients are passed backward through the network.\\n5.6.2 Automatic Differentiation\\nIt turns out that backpropagation is a special case of a general technique\\nin numerical analysis called automatic differentiation . We can think of au- automatic\\ndifferentiation tomatic differentation as a set of techniques to numerically (in contrast to\\nsymbolically) evaluate the exact (up to machine precision) gradient of a\\nfunction by working with intermediate variables and applying the chain\\nrule. Automatic differentiation applies a series of elementary arithmetic Automatic\\ndifferentiation is\\ndifferent from\\nsymbolic\\ndifferentiation and\\nnumerical\\napproximations of\\nthe gradient, e.g., by\\nusing ﬁnite\\ndifferences.operations, e.g., addition and multiplication and elementary functions,\\ne.g.,sin,cos,exp,log. By applying the chain rule to these operations, the\\ngradient of quite complicated functions can be computed automatically.\\nAutomatic differentiation applies to general computer programs and has\\nforward and reverse modes. Baydin et al. (2018) give a great overview of\\nautomatic differentiation in machine learning.\\nFigure 5.10 shows a simple graph representing the data ﬂow from in-\\nputsxto outputsyvia some intermediate variables a,b. If we were to\\ncompute the derivative dy/dx, we would apply the chain rule and obtain\\ndy\\ndx=dy\\ndbdb\\ndada\\ndx. (5.119)\\nIntuitively, the forward and reverse mode differ in the order of multipli- In the general case,\\nwe work with\\nJacobians, which\\ncan be vectors,\\nmatrices, or tensors.cation. Due to the associativity of matrix multiplication, we can choose\\nbetween\\ndy\\ndx=(dy\\ndbdb\\nda)da\\ndx, (5.120)\\ndy\\ndx=dy\\ndb(db\\ndada\\ndx)\\n. (5.121)\\nEquation (5.120) would be the reverse mode because gradients are prop- reverse mode\\nagated backward through the graph, i.e., reverse to the data ﬂow. Equa-\\ntion (5.121) would be the forward mode , where the gradients ﬂow with forward mode\\nthe data from left to right through the graph.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='82508231-ca4d-4b63-91af-de49e6895ce7', embedding=None, metadata={'page_label': '162', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='162 Vector Calculus\\nIn the following, we will focus on reverse mode automatic differentia-\\ntion, which is backpropagation. In the context of neural networks, where\\nthe input dimensionality is often much higher than the dimensionality of\\nthe labels, the reverse mode is computationally signiﬁcantly cheaper than\\nthe forward mode. Let us start with an instructive example.\\nExample 5.14\\nConsider the function\\nf(x) =√\\nx2+ exp(x2) + cos(x2+ exp(x2))\\n(5.122)\\nfrom (5.109). If we were to implement a function fon a computer, we\\nwould be able to save some computation by using intermediate variables : intermediate\\nvariables\\na=x2, (5.123)\\nb= exp(a), (5.124)\\nc=a+b, (5.125)\\nd=√c, (5.126)\\ne= cos(c), (5.127)\\nf=d+e. (5.128)\\nFigure 5.11\\nComputation graph\\nwith inputsx,\\nfunction values f,\\nand intermediate\\nvariablesa,b,c,d,e .x (·)2aexp(·)b\\n+c√·\\ncos(·)d\\ne+f\\nThis is the same kind of thinking process that occurs when applying\\nthe chain rule. Note that the preceding set of equations requires fewer\\noperations than a direct implementation of the function f(x)as deﬁned\\nin (5.109). The corresponding computation graph in Figure 5.11 shows\\nthe ﬂow of data and computations required to obtain the function value\\nf.\\nThe set of equations that include intermediate variables can be thought\\nof as a computation graph, a representation that is widely used in imple-\\nmentations of neural network software libraries. We can directly compute\\nthe derivatives of the intermediate variables with respect to their corre-\\nsponding inputs by recalling the deﬁnition of the derivative of elementary\\nfunctions. We obtain the following:\\n∂a\\n∂x= 2x (5.129)\\n∂b\\n∂a= exp(a) (5.130)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b20a3270-65a7-481f-ad89-c4b3b19e7bdb', embedding=None, metadata={'page_label': '163', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Backpropagation and Automatic Differentiation 163\\n∂c\\n∂a= 1 =∂c\\n∂b(5.131)\\n∂d\\n∂c=1\\n2√c(5.132)\\n∂e\\n∂c=−sin(c) (5.133)\\n∂f\\n∂d= 1 =∂f\\n∂e. (5.134)\\nBy looking at the computation graph in Figure 5.11, we can compute\\n∂f/∂x by working backward from the output and obtain\\n∂f\\n∂c=∂f\\n∂d∂d\\n∂c+∂f\\n∂e∂e\\n∂c(5.135)\\n∂f\\n∂b=∂f\\n∂c∂c\\n∂b(5.136)\\n∂f\\n∂a=∂f\\n∂b∂b\\n∂a+∂f\\n∂c∂c\\n∂a(5.137)\\n∂f\\n∂x=∂f\\n∂a∂a\\n∂x. (5.138)\\nNote that we implicitly applied the chain rule to obtain ∂f/∂x . By substi-\\ntuting the results of the derivatives of the elementary functions, we get\\n∂f\\n∂c= 1·1\\n2√c+ 1·(−sin(c)) (5.139)\\n∂f\\n∂b=∂f\\n∂c·1 (5.140)\\n∂f\\n∂a=∂f\\n∂bexp(a) +∂f\\n∂c·1 (5.141)\\n∂f\\n∂x=∂f\\n∂a·2x. (5.142)\\nBy thinking of each of the derivatives above as a variable, we observe\\nthat the computation required for calculating the derivative is of similar\\ncomplexity as the computation of the function itself. This is quite counter-\\nintuitive since the mathematical expression for the derivative∂f\\n∂x(5.110)\\nis signiﬁcantly more complicated than the mathematical expression of the\\nfunctionf(x)in (5.109).\\nAutomatic differentiation is a formalization of Example 5.14. Let x1,...,xd\\nbe the input variables to the function, xd+1,...,xD−1be the intermediate\\nvariables, and xDthe output variable. Then the computation graph can be\\nexpressed as follows:\\nFori=d+ 1,...,D :xi=gi(xPa(xi)), (5.143)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b092946-d1c7-4846-8aa4-db4227af3051', embedding=None, metadata={'page_label': '164', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='164 Vector Calculus\\nwhere thegi(·)are elementary functions and xPa(xi)are the parent nodes\\nof the variable xiin the graph. Given a function deﬁned in this way, we\\ncan use the chain rule to compute the derivative of the function in a step-\\nby-step fashion. Recall that by deﬁnition f=xDand hence\\n∂f\\n∂xD= 1. (5.144)\\nFor other variables xi, we apply the chain rule\\n∂f\\n∂xi=∑\\nxj:xi∈Pa(xj)∂f\\n∂xj∂xj\\n∂xi=∑\\nxj:xi∈Pa(xj)∂f\\n∂xj∂gj\\n∂xi, (5.145)\\nwhere Pa(xj)is the set of parent nodes of xjin the computation graph.\\nEquation (5.143) is the forward propagation of a function, whereas (5.145) Auto-differentiation\\nin reverse mode\\nrequires a parse\\ntree.is the backpropagation of the gradient through the computation graph.\\nFor neural network training, we backpropagate the error of the prediction\\nwith respect to the label.\\nThe automatic differentiation approach above works whenever we have\\na function that can be expressed as a computation graph, where the ele-\\nmentary functions are differentiable. In fact, the function may not even be\\na mathematical function but a computer program. However, not all com-\\nputer programs can be automatically differentiated, e.g., if we cannot ﬁnd\\ndifferential elementary functions. Programming structures, such as for\\nloops and ifstatements, require more care as well.\\n5.7 Higher-Order Derivatives\\nSo far, we have discussed gradients, i.e., ﬁrst-order derivatives. Some-\\ntimes, we are interested in derivatives of higher order, e.g., when we want\\nto use Newton’s Method for optimization, which requires second-order\\nderivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed\\nthe Taylor series to approximate functions using polynomials. In the mul-\\ntivariate case, we can do exactly the same. In the following, we will do\\nexactly this. But let us start with some notation.\\nConsider a function f:R2→Rof two variables x,y. We use the\\nfollowing notation for higher-order partial derivatives (and for gradients):\\n∂2f\\n∂x2is the second partial derivative of fwith respect to x.\\n∂nf\\n∂xnis thenth partial derivative of fwith respect to x.\\n∂2f\\n∂y∂x=∂\\n∂y(∂f\\n∂x)\\nis the partial derivative obtained by ﬁrst partial differ-\\nentiating with respect to xand then with respect to y.\\n∂2f\\n∂x∂yis the partial derivative obtained by ﬁrst partial differentiating by\\nyand thenx.\\nTheHessian is the collection of all second-order partial derivatives. Hessian\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4693db0-e178-40b5-aff7-9d58674a0d43', embedding=None, metadata={'page_label': '165', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Linearization and Multivariate Taylor Series 165\\nFigure 5.12 Linear\\napproximation of a\\nfunction. The\\noriginal function f\\nis linearized at\\nx0=−2using a\\nﬁrst-order Taylor\\nseries expansion.\\n−4−2 0 2 4\\nx−2−101f(x)f(x)\\nf(x0)f(x0) +f′(x0)(x−x0)\\nIff(x,y)is a twice (continuously) differentiable function, then\\n∂2f\\n∂x∂y=∂2f\\n∂y∂x, (5.146)\\ni.e., the order of differentiation does not matter, and the corresponding\\nHessian matrix Hessian matrix\\nH=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0∂2f\\n∂x2∂2f\\n∂x∂y\\n∂2f\\n∂x∂y∂2f\\n∂y2\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(5.147)\\nis symmetric. The Hessian is denoted as ∇2\\nx,yf(x,y). Generally, for x∈Rn\\nandf:Rn→R, the Hessian is an n×nmatrix. The Hessian measures\\nthe curvature of the function locally around (x,y).\\nRemark (Hessian of a Vector Field) .Iff:Rn→Rmis a vector ﬁeld, the\\nHessian is an (m×n×n)-tensor. ♦\\n5.8 Linearization and Multivariate Taylor Series\\nThe gradient∇fof a function fis often used for a locally linear approxi-\\nmation offaroundx0:\\nf(x)≈f(x0) + (∇xf)(x0)(x−x0). (5.148)\\nHere (∇xf)(x0)is the gradient of fwith respect to x, evaluated at x0.\\nFigure 5.12 illustrates the linear approximation of a function fat an input\\nx0. The original function is approximated by a straight line. This approx-\\nimation is locally accurate, but the farther we move away from x0the\\nworse the approximation gets. Equation (5.148) is a special case of a mul-\\ntivariate Taylor series expansion of fatx0, where we consider only the\\nﬁrst two terms. We discuss the more general case in the following, which\\nwill allow for better approximations.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c484df6e-ec28-4332-9162-b8e6ed5ab9f3', embedding=None, metadata={'page_label': '166', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='166 Vector Calculus\\nFigure 5.13\\nVisualizing outer\\nproducts. Outer\\nproducts of vectors\\nincrease the\\ndimensionality of\\nthe array by 1 per\\nterm. (a) The outer\\nproduct of two\\nvectors results in a\\nmatrix; (b) the\\nouter product of\\nthree vectors yields\\na third-order tensor.\\n(a) Given a vector δ∈R4, we obtain the outer product δ2:=δ⊗δ=δδ⊤∈\\nR4×4as a matrix.\\n(b) An outer product δ3:=δ⊗δ⊗δ∈R4×4×4results in a third-order tensor (“three-\\ndimensional matrix”), i.e., an array with three indexes.\\nDeﬁnition 5.7 (Multivariate Taylor Series) .We consider a function\\nf:RD→R (5.149)\\nx↦→f(x),x∈RD, (5.150)\\nthat is smooth at x0. When we deﬁne the difference vector δ:=x−x0,\\nthemultivariate Taylor series offat(x0)is deﬁned as multivariate Taylor\\nseries\\nf(x) =∞∑\\nk=0Dk\\nxf(x0)\\nk!δk, (5.151)\\nwhereDk\\nxf(x0)is thek-th (total) derivative of fwith respect to x, eval-\\nuated atx0.\\nDeﬁnition 5.8 (Taylor Polynomial) .TheTaylor polynomial of degreenof Taylor polynomial\\nfatx0contains the ﬁrst n+ 1components of the series in (5.151) and is\\ndeﬁned as\\nTn(x) =n∑\\nk=0Dk\\nxf(x0)\\nk!δk. (5.152)\\nIn (5.151) and (5.152), we used the slightly sloppy notation of δk,\\nwhich is not deﬁned for vectors x∈RD, D > 1,andk > 1. Note that\\nbothDk\\nxfandδkarek-th order tensors, i.e., k-dimensional arrays. The A vector can be\\nimplemented as a\\none-dimensional\\narray, a matrix as a\\ntwo-dimensional\\narray.kth-order tensor δk∈Rktimes\\ued17\\ued1a\\ued19\\ued18\\nD×D×...×Dis obtained as a k-fold outer product,\\ndenoted by⊗, of the vector δ∈RD. For example,\\nδ2:=δ⊗δ=δδ⊤,δ2[i,j] =δ[i]δ[j] (5.153)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1b3604a-a28d-4df4-8d08-a42eee77fa61', embedding=None, metadata={'page_label': '167', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Linearization and Multivariate Taylor Series 167\\nδ3:=δ⊗δ⊗δ,δ3[i,j,k ] =δ[i]δ[j]δ[k]. (5.154)\\nFigure 5.13 visualizes two such outer products. In general, we obtain the\\nterms\\nDk\\nxf(x0)δk=D∑\\ni1=1···D∑\\nik=1Dk\\nxf(x0)[i1,...,ik]δ[i1]···δ[ik](5.155)\\nin the Taylor series, where Dk\\nxf(x0)δkcontainsk-th order polynomials.\\nNow that we deﬁned the Taylor series for vector ﬁelds, let us explicitly\\nwrite down the ﬁrst terms Dk\\nxf(x0)δkof the Taylor series expansion for\\nk= 0,..., 3andδ:=x−x0:np.einsum(\\n’i,i’,Df1,d)\\nnp.einsum(\\n’ij,i,j’,\\nDf2,d,d)\\nnp.einsum(\\n’ijk,i,j,k’,\\nDf3,d,d,d)k= 0 :D0\\nxf(x0)δ0=f(x0)∈R (5.156)\\nk= 1 :D1\\nxf(x0)δ1=∇xf(x0)\\ued19\\ued18\\ued17\\ued1a\\n1×Dδ\\ued19\\ued18\\ued17\\ued1a\\nD×1=D∑\\ni=1∇xf(x0)[i]δ[i]∈R(5.157)\\nk= 2 :D2\\nxf(x0)δ2=tr(H(x0)\\ued19\\ued18\\ued17\\ued1a\\nD×Dδ\\ued19\\ued18\\ued17\\ued1a\\nD×1δ⊤\\n\\ued19\\ued18\\ued17\\ued1a\\n1×D)=δ⊤H(x0)δ (5.158)\\n=D∑\\ni=1D∑\\nj=1H[i,j]δ[i]δ[j]∈R (5.159)\\nk= 3 :D3\\nxf(x0)δ3=D∑\\ni=1D∑\\nj=1D∑\\nk=1D3\\nxf(x0)[i,j,k ]δ[i]δ[j]δ[k]∈R\\n(5.160)\\nHere,H(x0)is the Hessian of fevaluated atx0.\\nExample 5.15 (Taylor Series Expansion of a Function with Two Vari-\\nables)\\nConsider the function\\nf(x,y) =x2+ 2xy+y3. (5.161)\\nWe want to compute the Taylor series expansion of fat(x0,y0) = (1,2).\\nBefore we start, let us discuss what to expect: The function in (5.161) is\\na polynomial of degree 3. We are looking for a Taylor series expansion,\\nwhich itself is a linear combination of polynomials. Therefore, we do not\\nexpect the Taylor series expansion to contain terms of fourth or higher\\norder to express a third-order polynomial. This means that it should be\\nsufﬁcient to determine the ﬁrst four terms of (5.151) for an exact alterna-\\ntive representation of (5.161).\\nTo determine the Taylor series expansion, we start with the constant\\nterm and the ﬁrst-order derivatives, which are given by\\nf(1,2) = 13 (5.162)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f463fbde-49e1-47c3-8627-e9349740f51d', embedding=None, metadata={'page_label': '168', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='168 Vector Calculus\\n∂f\\n∂x= 2x+ 2y=⇒∂f\\n∂x(1,2) = 6 (5.163)\\n∂f\\n∂y= 2x+ 3y2=⇒∂f\\n∂y(1,2) = 14. (5.164)\\nTherefore, we obtain\\nD1\\nx,yf(1,2) =∇x,yf(1,2) =[\\n∂f\\n∂x(1,2)∂f\\n∂y(1,2)]\\n=[6 14]∈R1×2\\n(5.165)\\nsuch that\\nD1\\nx,yf(1,2)\\n1!δ=[6 14][x−1\\ny−2]\\n= 6(x−1) + 14(y−2).(5.166)\\nNote thatD1\\nx,yf(1,2)δcontains only linear terms, i.e., ﬁrst-order polyno-\\nmials.\\nThe second-order partial derivatives are given by\\n∂2f\\n∂x2= 2 =⇒∂2f\\n∂x2(1,2) = 2 (5.167)\\n∂2f\\n∂y2= 6y=⇒∂2f\\n∂y2(1,2) = 12 (5.168)\\n∂2f\\n∂y∂x= 2 =⇒∂2f\\n∂y∂x(1,2) = 2 (5.169)\\n∂2f\\n∂x∂y= 2 =⇒∂2f\\n∂x∂y(1,2) = 2. (5.170)\\nWhen we collect the second-order partial derivatives, we obtain the Hes-\\nsian\\nH=[∂2f\\n∂x2∂2f\\n∂x∂y\\n∂2f\\n∂y∂x∂2f\\n∂y2]\\n=[2 2\\n2 6y]\\n, (5.171)\\nsuch that\\nH(1,2) =[2 2\\n2 12]\\n∈R2×2. (5.172)\\nTherefore, the next term of the Taylor-series expansion is given by\\nD2\\nx,yf(1,2)\\n2!δ2=1\\n2δ⊤H(1,2)δ (5.173a)\\n=1\\n2[x−1y−2][2 2\\n2 12][x−1\\ny−2]\\n(5.173b)\\n= (x−1)2+ 2(x−1)(y−2) + 6(y−2)2.(5.173c)\\nHere,D2\\nx,yf(1,2)δ2contains only quadratic terms, i.e., second-order poly-\\nnomials.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93169138-5aba-4e30-83c6-bd79b6b6b38c', embedding=None, metadata={'page_label': '169', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.8 Linearization and Multivariate Taylor Series 169\\nThe third-order derivatives are obtained as\\nD3\\nx,yf=[\\n∂H\\n∂x∂H\\n∂y]\\n∈R2×2×2, (5.174)\\nD3\\nx,yf[:,:,1] =∂H\\n∂x=[∂3f\\n∂x3∂3f\\n∂x2∂y\\n∂3f\\n∂x∂y∂x∂3f\\n∂x∂y2]\\n, (5.175)\\nD3\\nx,yf[:,:,2] =∂H\\n∂y=[∂3f\\n∂y∂x2∂3f\\n∂y∂x∂y\\n∂3f\\n∂y2∂x∂3f\\n∂y3]\\n. (5.176)\\nSince most second-order partial derivatives in the Hessian in (5.171) are\\nconstant, the only nonzero third-order partial derivative is\\n∂3f\\n∂y3= 6 =⇒∂3f\\n∂y3(1,2) = 6. (5.177)\\nHigher-order derivatives and the mixed derivatives of degree 3 (e.g.,\\n∂f3\\n∂x2∂y) vanish, such that\\nD3\\nx,yf[:,:,1] =[0 0\\n0 0]\\n, D3\\nx,yf[:,:,2] =[0 0\\n0 6]\\n(5.178)\\nand\\nD3\\nx,yf(1,2)\\n3!δ3= (y−2)3, (5.179)\\nwhich collects all cubic terms of the Taylor series. Overall, the (exact)\\nTaylor series expansion of fat(x0,y0) = (1,2)is\\nf(x) =f(1,2) +D1\\nx,yf(1,2)δ+D2\\nx,yf(1,2)\\n2!δ2+D3\\nx,yf(1,2)\\n3!δ3\\n(5.180a)\\n=f(1,2) +∂f(1,2)\\n∂x(x−1) +∂f(1,2)\\n∂y(y−2)\\n+1\\n2!(∂2f(1,2)\\n∂x2(x−1)2+∂2f(1,2)\\n∂y2(y−2)2\\n+ 2∂2f(1,2)\\n∂x∂y(x−1)(y−2))\\n+1\\n6∂3f(1,2)\\n∂y3(y−2)3(5.180b)\\n= 13 + 6(x−1) + 14(y−2)\\n+ (x−1)2+ 6(y−2)2+ 2(x−1)(y−2) + (y−2)3.(5.180c)\\nIn this case, we obtained an exact Taylor series expansion of the polyno-\\nmial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\\npolynomial in (5.161). In this particular example, this result is not sur-\\nprising since the original function was a third-order polynomial, which\\nwe expressed through a linear combination of constant terms, ﬁrst-order,\\nsecond-order, and third-order polynomials in (5.180c).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='889158f6-cb13-411c-b53d-8e8b7a0cf79b', embedding=None, metadata={'page_label': '170', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='170 Vector Calculus\\n5.9 Further Reading\\nFurther details of matrix differentials, along with a short review of the\\nrequired linear algebra, can be found in Magnus and Neudecker (2007).\\nAutomatic differentiation has had a long history, and we refer to Griewank\\nand Walther (2003), Griewank and Walther (2008), and Elliott (2009)\\nand the references therein.\\nIn machine learning (and other disciplines), we often need to compute\\nexpectations, i.e., we need to solve integrals of the form\\nEx[f(x)] =∫\\nf(x)p(x)dx. (5.181)\\nEven ifp(x)is in a convenient form (e.g., Gaussian), this integral gen-\\nerally cannot be solved analytically. The Taylor series expansion of fis\\none way of ﬁnding an approximate solution: Assuming p(x) =N(µ,Σ)\\nis Gaussian, then the ﬁrst-order Taylor series expansion around µlocally\\nlinearizes the nonlinear function f. For linear functions, we can compute\\nthe mean (and the covariance) exactly if p(x)is Gaussian distributed (see\\nSection 6.5). This property is heavily exploited by the extended Kalman extended Kalman\\nﬁlter ﬁlter (Maybeck, 1979) for online state estimation in nonlinear dynami-\\ncal systems (also called “state-space models”). Other deterministic ways\\nto approximate the integral in (5.181) are the unscented transform (Julier unscented transform\\nand Uhlmann, 1997), which does not require any gradients, or the Laplace Laplace\\napproximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses\\na second-order Taylor series expansion (requiring the Hessian) for a local\\nGaussian approximation of p(x)around its mode.\\nExercises\\n5.1 Compute the derivative f′(x)for\\nf(x) = log(x4) sin(x3).\\n5.2 Compute the derivative f′(x)of the logistic sigmoid\\nf(x) =1\\n1 + exp(−x).\\n5.3 Compute the derivative f′(x)of the function\\nf(x) = exp(−1\\n2σ2(x−µ)2),\\nwhereµ, σ∈Rare constants.\\n5.4 Compute the Taylor polynomials Tn,n= 0,..., 5off(x) = sin(x) + cos(x)\\natx0= 0.\\n5.5 Consider the following functions:\\nf1(x) = sin(x1) cos(x2),x∈R2\\nf2(x,y) =x⊤y,x,y∈Rn\\nf3(x) =xx⊤,x∈Rn\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac738c35-3867-4843-be68-9e0e787cf4b9', embedding=None, metadata={'page_label': '171', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 171\\na. What are the dimensions of∂fi\\n∂x?\\nb. Compute the Jacobians.\\n5.6 Differentiate fwith respect to tandgwith respect to X, where\\nf(t) = sin(log(t⊤t)), t∈RD\\ng(X) =tr(AXB ),A∈RD×E,X∈RE×F,B∈RF×D,\\nwhere tr (·)denotes the trace.\\n5.7 Compute the derivatives df/dxof the following functions by using the chain\\nrule. Provide the dimensions of every single partial derivative. Describe your\\nsteps in detail.\\na.\\nf(z) = log(1 + z), z =x⊤x,x∈RD\\nb.\\nf(z) = sin(z),z=Ax+b,A∈RE×D,x∈RD,b∈RE\\nwhere sin(·)is applied to every element of z.\\n5.8 Compute the derivatives df/dxof the following functions. Describe your\\nsteps in detail.\\na. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive.\\nf(z) = exp(−1\\n2z)\\nz=g(y) =y⊤S−1y\\ny=h(x) =x−µ\\nwherex,µ∈RD,S∈RD×D.\\nb.\\nf(x) =tr(xx⊤+σ2I),x∈RD\\nHere tr (A)is the trace of A, i.e., the sum of the diagonal elements Aii.\\nHint: Explicitly write out the outer product.\\nc. Use the chain rule. Provide the dimensions of every single partial deriva-\\ntive. You do not need to compute the product of the partial derivatives\\nexplicitly.\\nf= tanh(z)∈RM\\nz=Ax+b,x∈RN,A∈RM×N,b∈RM.\\nHere, tanh is applied to every component of z.\\n5.9 We deﬁne\\ng(z,ν) := logp(x,z)−logq(z,ν)\\nz:=t(ϵ,ν)\\nfor differentiable functions p,q,t , andx∈RD,z∈RE,ν∈RF,ϵ∈RG. By\\nusing the chain rule, compute the gradient\\nd\\ndνg(z,ν).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90315bc8-8863-4759-b7db-089e8b164ce9', embedding=None, metadata={'page_label': '172', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6\\nProbability and Distributions\\nProbability, loosely speaking, concerns the study of uncertainty. Probabil-\\nity can be thought of as the fraction of times an event occurs, or as a degree\\nof belief about an event. We then would like to use this probability to mea-\\nsure the chance of something occurring in an experiment. As mentioned\\nin Chapter 1, we often quantify uncertainty in the data, uncertainty in the\\nmachine learning model, and uncertainty in the predictions produced by\\nthe model. Quantifying uncertainty requires the idea of a random variable , random variable\\nwhich is a function that maps outcomes of random experiments to a set of\\nproperties that we are interested in. Associated with the random variable\\nis a function that measures the probability that a particular outcome (or\\nset of outcomes) will occur; this is called the probability distribution . probability\\ndistribution Probability distributions are used as a building block for other con-\\ncepts, such as probabilistic modeling (Section 8.4), graphical models (Sec-\\ntion 8.5), and model selection (Section 8.6). In the next section, we present\\nthe three concepts that deﬁne a probability space (the sample space, the\\nevents, and the probability of an event) and how they are related to a\\nfourth concept called the random variable. The presentation is deliber-\\nately slightly hand wavy since a rigorous presentation may occlude the\\nintuition behind the concepts. An outline of the concepts presented in this\\nchapter are shown in Figure 6.1.\\n6.1 Construction of a Probability Space\\nThe theory of probability aims at deﬁning a mathematical structure to\\ndescribe random outcomes of experiments. For example, when tossing a\\nsingle coin, we cannot determine the outcome, but by doing a large num-\\nber of coin tosses, we can observe a regularity in the average outcome.\\nUsing this mathematical structure of probability, the goal is to perform\\nautomated reasoning, and in this sense, probability generalizes logical\\nreasoning (Jaynes, 2003).\\n6.1.1 Philosophical Issues\\nWhen constructing automated reasoning systems, classical Boolean logic\\ndoes not allow us to express certain forms of plausible reasoning. Consider\\n172\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='910ce95a-c7f0-4548-aada-8057d2530cf6', embedding=None, metadata={'page_label': '173', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 Construction of a Probability Space 173\\nFigure 6.1 A mind\\nmap of the concepts\\nrelated to random\\nvariables and\\nprobability\\ndistributions, as\\ndescribed in this\\nchapter.\\nRandom variable\\n& distributionSum rule Product ruleBayes’ Theorem\\nSummary statisticsMean Variance\\nTransformations\\nIndependence\\nInner productGaussian\\nBernoulli\\nBetaSufﬁcient statistics\\nExponential familyChapter 9\\nRegression\\nChapter 10\\nDimensionality\\nreduction\\nChapter 11\\nDensity estimationProperty\\nSimilarityExample\\nExampleConjugate\\nProperty Finite\\nthe following scenario: We observe that Ais false. We ﬁnd Bbecomes\\nless plausible, although no conclusion can be drawn from classical logic.\\nWe observe that Bis true. It seems Abecomes more plausible. We use\\nthis form of reasoning daily. We are waiting for a friend, and consider\\nthree possibilities: H1, she is on time; H2, she has been delayed by trafﬁc;\\nand H3, she has been abducted by aliens. When we observe our friend\\nis late, we must logically rule out H1. We also tend to consider H2 to be\\nmore likely, though we are not logically required to do so. Finally, we may\\nconsider H3 to be possible, but we continue to consider it quite unlikely.\\nHow do we conclude H2 is the most plausible answer? Seen in this way, “For plausible\\nreasoning it is\\nnecessary to extend\\nthe discrete true and\\nfalse values of truth\\nto continuous\\nplausibilities”\\n(Jaynes, 2003).probability theory can be considered a generalization of Boolean logic. In\\nthe context of machine learning, it is often applied in this way to formalize\\nthe design of automated reasoning systems. Further arguments about how\\nprobability theory is the foundation of reasoning systems can be found\\nin Pearl (1988).\\nThe philosophical basis of probability and how it should be somehow\\nrelated to what we think should be true (in the logical sense) was studied\\nby Cox (Jaynes, 2003). Another way to think about it is that if we are\\nprecise about our common sense we end up constructing probabilities.\\nE. T. Jaynes (1922–1998) identiﬁed three mathematical criteria, which\\nmust apply to all plausibilities:\\n1. The degrees of plausibility are represented by real numbers.\\n2. These numbers must be based on the rules of common sense.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a63a5c87-5543-4ca5-8953-f5df2e0fded0', embedding=None, metadata={'page_label': '174', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='174 Probability and Distributions\\n3. The resulting reasoning must be consistent, with the three following\\nmeanings of the word “consistent”:\\n(a) Consistency or non-contradiction: When the same result can be\\nreached through different means, the same plausibility value must\\nbe found in all cases.\\n(b) Honesty: All available data must be taken into account.\\n(c) Reproducibility: If our state of knowledge about two problems are\\nthe same, then we must assign the same degree of plausibility to\\nboth of them.\\nThe Cox–Jaynes theorem proves these plausibilities to be sufﬁcient to\\ndeﬁne the universal mathematical rules that apply to plausibility p, up to\\ntransformation by an arbitrary monotonic function. Crucially, these rules\\narethe rules of probability.\\nRemark. In machine learning and statistics, there are two major interpre-\\ntations of probability: the Bayesian and frequentist interpretations (Bishop,\\n2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil-\\nity to specify the degree of uncertainty that the user has about an event. It\\nis sometimes referred to as “subjective probability” or “degree of belief”.\\nThe frequentist interpretation considers the relative frequencies of events\\nof interest to the total number of events that occurred. The probability of\\nan event is deﬁned as the relative frequency of the event in the limit when\\none has inﬁnite data. ♦\\nSome machine learning texts on probabilistic models use lazy notation\\nand jargon, which is confusing. This text is no exception. Multiple distinct\\nconcepts are all referred to as “probability distribution”, and the reader\\nhas to often disentangle the meaning from the context. One trick to help\\nmake sense of probability distributions is to check whether we are trying\\nto model something categorical (a discrete random variable) or some-\\nthing continuous (a continuous random variable). The kinds of questions\\nwe tackle in machine learning are closely related to whether we are con-\\nsidering categorical or continuous models.\\n6.1.2 Probability and Random Variables\\nThere are three distinct ideas that are often confused when discussing\\nprobabilities. First is the idea of a probability space, which allows us to\\nquantify the idea of a probability. However, we mostly do not work directly\\nwith this basic probability space. Instead, we work with random variables\\n(the second idea), which transfers the probability to a more convenient\\n(often numerical) space. The third idea is the idea of a distribution or law\\nassociated with a random variable. We will introduce the ﬁrst two ideas\\nin this section and expand on the third idea in Section 6.2.\\nModern probability is based on a set of axioms proposed by Kolmogorov\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d6f028f-a500-4a25-a384-c04481d3a2c4', embedding=None, metadata={'page_label': '175', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 Construction of a Probability Space 175\\n(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-\\ncepts of sample space, event space, and probability measure. The prob-\\nability space models a real-world process (referred to as an experiment)\\nwith random outcomes.\\nThe sample space Ω\\nThesample space is the set of all possible outcomes of the experiment, sample space\\nusually denoted by Ω. For example, two successive coin tosses have\\na sample space of {hh, tt, ht, th}, where “h” denotes “heads” and “t”\\ndenotes “tails”.\\nThe event spaceA\\nTheevent space is the space of potential results of the experiment. A event space\\nsubsetAof the sample space Ωis in the event space Aif at the end\\nof the experiment we can observe whether a particular outcome ω∈Ω\\nis inA. The event space Ais obtained by considering the collection of\\nsubsets of Ω, and for discrete probability distributions (Section 6.2.1)\\nAis often the power set of Ω.\\nThe probability P\\nWith each event A∈A, we associate a number P(A)that measures the\\nprobability or degree of belief that the event will occur. P(A)is called\\ntheprobability ofA. probability\\nThe probability of a single event must lie in the interval [0,1], and the\\ntotal probability over all outcomes in the sample space Ωmust be 1, i.e.,\\nP(Ω) = 1 . Given a probability space (Ω,A,P), we want to use it to model\\nsome real-world phenomenon. In machine learning, we often avoid explic-\\nitly referring to the probability space, but instead refer to probabilities on\\nquantities of interest, which we denote by T. In this book, we refer to T\\nas the target space and refer to elements of Tas states. We introduce a target space\\nfunctionX: Ω→T that takes an element of Ω(an outcome) and returns\\na particular quantity of interest x, a value inT. This association/mapping\\nfrom ΩtoTis called a random variable . For example, in the case of tossing random variable\\ntwo coins and counting the number of heads, a random variable Xmaps\\nto the three possible outcomes: X(hh) = 2 ,X(ht) = 1 ,X(th) = 1 , and\\nX(tt) = 0 . In this particular case, T={0,1,2}, and it is the probabilities\\non elements ofTthat we are interested in. For a ﬁnite sample space Ωand The name “random\\nvariable” is a great\\nsource of\\nmisunderstanding\\nas it is neither\\nrandom nor is it a\\nvariable. It is a\\nfunction.ﬁniteT, the function corresponding to a random variable is essentially a\\nlookup table. For any subset S⊆T , we associate PX(S)∈[0,1](the\\nprobability) to a particular event occurring corresponding to the random\\nvariableX. Example 6.1 provides a concrete illustration of the terminol-\\nogy.\\nRemark. The aforementioned sample space Ωunfortunately is referred\\nto by different names in different books. Another common name for Ω\\nis “state space” (Jacod and Protter, 2004), but state space is sometimes\\nreserved for referring to states in a dynamical system (Hasselblatt and\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e921b5e-34fa-465d-8483-56bedf3dacb6', embedding=None, metadata={'page_label': '176', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='176 Probability and Distributions\\nKatok, 2003). Other names sometimes used to describe Ωare: “sample\\ndescription space”, “possibility space,” and “event space”. ♦\\nExample 6.1\\nWe assume that the reader is already familiar with computing probabil- This toy example is\\nessentially a biased\\ncoin ﬂip example.ities of intersections and unions of sets of events. A gentler introduction\\nto probability with many examples can be found in chapter 2 of Walpole\\net al. (2011).\\nConsider a statistical experiment where we model a funfair game con-\\nsisting of drawing two coins from a bag (with replacement). There are\\ncoins from USA (denoted as $) and UK (denoted as £) in the bag, and\\nsince we draw two coins from the bag, there are four outcomes in total.\\nThe state space or sample space Ωof this experiment is then ($, $), ($,\\n£), (£, $), ( £,£). Let us assume that the composition of the bag of coins is\\nsuch that a draw returns at random a $ with probability 0.3.\\nThe event we are interested in is the total number of times the repeated\\ndraw returns $. Let us deﬁne a random variable Xthat maps the sample\\nspace ΩtoT, which denotes the number of times we draw $ out of the\\nbag. We can see from the preceding sample space we can get zero $, one $,\\nor two $s, and therefore T={0,1,2}. The random variable X(a function\\nor lookup table) can be represented as a table like the following:\\nX(($,$)) = 2 (6.1)\\nX(($,£)) = 1 (6.2)\\nX((£,$)) = 1 (6.3)\\nX((£,£)) = 0. (6.4)\\nSince we return the ﬁrst coin we draw before drawing the second, this\\nimplies that the two draws are independent of each other, which we will\\ndiscuss in Section 6.4.5. Note that there are two experimental outcomes,\\nwhich map to the same event, where only one of the draws returns $.\\nTherefore, the probability mass function (Section 6.2.1) of Xis given by\\nP(X= 2) =P(($,$))\\n=P($)·P($)\\n= 0.3·0.3 = 0.09 (6.5)\\nP(X= 1) =P(($,£)∪(£,$))\\n=P(($,£)) +P((£,$))\\n= 0.3·(1−0.3) + (1−0.3)·0.3 = 0.42 (6.6)\\nP(X= 0) =P((£,£))\\n=P(£)·P(£)\\n= (1−0.3)·(1−0.3) = 0.49. (6.7)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6357a96e-2fea-46cf-a435-d89e1ad3d8a4', embedding=None, metadata={'page_label': '177', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.1 Construction of a Probability Space 177\\nIn the calculation, we equated two different concepts, the probability\\nof the output of Xand the probability of the samples in Ω. For example,\\nin (6.7) we say P(X= 0) =P((£,£)). Consider the random variable\\nX: Ω→T and a subset S⊆T (for example, a single element of T,\\nsuch as the outcome that one head is obtained when tossing two coins).\\nLetX−1(S)be the pre-image of SbyX, i.e., the set of elements of Ωthat\\nmap toSunderX;{ω∈Ω :X(ω)∈S}. One way to understand the\\ntransformation of probability from events in Ωvia the random variable\\nXis to associate it with the probability of the pre-image of S(Jacod and\\nProtter, 2004). For S⊆T, we have the notation\\nPX(S) =P(X∈S) =P(X−1(S)) =P({ω∈Ω :X(ω)∈S}).(6.8)\\nThe left-hand side of (6.8) is the probability of the set of possible outcomes\\n(e.g., number of $= 1) that we are interested in. Via the random variable\\nX, which maps states to outcomes, we see in the right-hand side of (6.8)\\nthat this is the probability of the set of states (in Ω) that have the property\\n(e.g., $£,£$). We say that a random variable Xis distributed according\\nto a particular probability distribution PX, which deﬁnes the probability\\nmapping between the event and the probability of the outcome of the\\nrandom variable. In other words, the function PXor equivalently P◦X−1\\nis the lawordistribution of random variable X. law\\ndistributionRemark. The target space, that is, the range Tof the random variable X,\\nis used to indicate the kind of probability space, i.e., a Trandom variable.\\nWhenTis ﬁnite or countably inﬁnite, this is called a discrete random\\nvariable (Section 6.2.1). For continuous random variables (Section 6.2.2),\\nwe only considerT=RorT=RD. ♦\\n6.1.3 Statistics\\nProbability theory and statistics are often presented together, but they con-\\ncern different aspects of uncertainty. One way of contrasting them is by the\\nkinds of problems that are considered. Using probability, we can consider\\na model of some process, where the underlying uncertainty is captured\\nby random variables, and we use the rules of probability to derive what\\nhappens. In statistics, we observe that something has happened and try\\nto ﬁgure out the underlying process that explains the observations. In this\\nsense, machine learning is close to statistics in its goals to construct a\\nmodel that adequately represents the process that generated the data. We\\ncan use the rules of probability to obtain a “best-ﬁtting” model for some\\ndata.\\nAnother aspect of machine learning systems is that we are interested\\nin generalization error (see Chapter 8). This means that we are actually\\ninterested in the performance of our system on instances that we will\\nobserve in future, which are not identical to the instances that we have\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2a92ebc-8a12-4358-9ae6-a24fb5965bfe', embedding=None, metadata={'page_label': '178', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='178 Probability and Distributions\\nseen so far. This analysis of future performance relies on probability and\\nstatistics, most of which is beyond what will be presented in this chapter.\\nThe interested reader is encouraged to look at the books by Boucheron\\net al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more\\nabout statistics in Chapter 8.\\n6.2 Discrete and Continuous Probabilities\\nLet us focus our attention on ways to describe the probability of an event\\nas introduced in Section 6.1. Depending on whether the target space is dis-\\ncrete or continuous, the natural way to refer to distributions is different.\\nWhen the target space Tis discrete, we can specify the probability that a\\nrandom variable Xtakes a particular value x∈T, denoted as P(X=x).\\nThe expression P(X=x)for a discrete random variable Xis known as\\ntheprobability mass function . When the target space Tis continuous, e.g., probability mass\\nfunction the real line R, it is more natural to specify the probability that a random\\nvariableXis in an interval, denoted by P(a⩽X⩽b)fora<b . By con-\\nvention, we specify the probability that a random variable Xis less than\\na particular value x, denoted by P(X⩽x). The expression P(X⩽x)for\\na continuous random variable Xis known as the cumulative distribution cumulative\\ndistribution function function . We will discuss continuous random variables in Section 6.2.2.\\nWe will revisit the nomenclature and contrast discrete and continuous\\nrandom variables in Section 6.2.3.\\nRemark. We will use the phrase univariate distribution to refer to distribu- univariate\\ntions of a single random variable (whose states are denoted by non-bold\\nx). We will refer to distributions of more than one random variable as\\nmultivariate distributions, and will usually consider a vector of random multivariate\\nvariables (whose states are denoted by bold x). ♦\\n6.2.1 Discrete Probabilities\\nWhen the target space is discrete, we can imagine the probability distri-\\nbution of multiple random variables as ﬁlling out a (multidimensional)\\narray of numbers. Figure 6.2 shows an example. The target space of the\\njoint probability is the Cartesian product of the target spaces of each of\\nthe random variables. We deﬁne the joint probability as the entry of both joint probability\\nvalues jointly\\nP(X=xi,Y=yj) =nij\\nN, (6.9)\\nwherenijis the number of events with state xiandyjandNthe total\\nnumber of events. The joint probability is the probability of the intersec-\\ntion of both events, that is, P(X=xi,Y=yj) =P(X=xi∩Y=yj).\\nFigure 6.2 illustrates the probability mass function (pmf) of a discrete prob- probability mass\\nfunction ability distribution. For two random variables XandY, the probability\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5207142e-94fa-46d6-ba61-e7457a7a9261', embedding=None, metadata={'page_label': '179', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.2 Discrete and Continuous Probabilities 179\\nFigure 6.2\\nVisualization of a\\ndiscrete bivariate\\nprobability mass\\nfunction, with\\nrandom variables X\\nandY. This\\ndiagram is adapted\\nfrom Bishop (2006).\\nXx1x2x3x4x5Y\\ny3y2y1\\nnij}\\nrjci\\ued17\\ued1a\\ued19\\ued18\\nthatX=xandY=yis (lazily) written as p(x,y)and is called the joint\\nprobability. One can think of a probability as a function that takes state\\nxandyand returns a real number, which is the reason we write p(x,y).\\nThemarginal probability thatXtakes the value xirrespective of the value marginal probability\\nof random variable Yis (lazily) written as p(x). We writeX∼p(x)to\\ndenote that the random variable Xis distributed according to p(x). If we\\nconsider only the instances where X=x, then the fraction of instances\\n(theconditional probability ) for whichY=yis written (lazily) as p(y|x).conditional\\nprobability\\nExample 6.2\\nConsider two random variables XandY, whereXhas ﬁve possible states\\nandYhas three possible states, as shown in Figure 6.2. We denote by nij\\nthe number of events with state X=xiandY=yj, and denote by\\nNthe total number of events. The value ciis the sum of the individual\\nfrequencies for the ith column, that is, ci=∑3\\nj=1nij. Similarly, the value\\nrjis the row sum, that is, rj=∑5\\ni=1nij. Using these deﬁnitions, we can\\ncompactly express the distribution of XandY.\\nThe probability distribution of each random variable, the marginal\\nprobability, can be seen as the sum over a row or column\\nP(X=xi) =ci\\nN=∑3\\nj=1nij\\nN(6.10)\\nand\\nP(Y=yj) =rj\\nN=∑5\\ni=1nij\\nN, (6.11)\\nwhereciandrjare theith column and jth row of the probability table,\\nrespectively. By convention, for discrete random variables with a ﬁnite\\nnumber of events, we assume that probabilties sum up to one, that is,\\n5∑\\ni=1P(X=xi) = 1 and3∑\\nj=1P(Y=yj) = 1. (6.12)\\nThe conditional probability is the fraction of a row or column in a par-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0b7bc83-16fe-4d20-bc7f-349122f18d2e', embedding=None, metadata={'page_label': '180', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='180 Probability and Distributions\\nticular cell. For example, the conditional probability of YgivenXis\\nP(Y=yj|X=xi) =nij\\nci, (6.13)\\nand the conditional probability of XgivenYis\\nP(X=xi|Y=yj) =nij\\nrj. (6.14)\\nIn machine learning, we use discrete probability distributions to model\\ncategorical variables , i.e., variables that take a ﬁnite set of unordered val- categorical variable\\nues. They could be categorical features, such as the degree taken at uni-\\nversity when used for predicting the salary of a person, or categorical la-\\nbels, such as letters of the alphabet when doing handwriting recognition.\\nDiscrete distributions are also often used to construct probabilistic models\\nthat combine a ﬁnite number of continuous distributions (Chapter 11).\\n6.2.2 Continuous Probabilities\\nWe consider real-valued random variables in this section, i.e., we consider\\ntarget spaces that are intervals of the real line R. In this book, we pretend\\nthat we can perform operations on real random variables as if we have dis-\\ncrete probability spaces with ﬁnite states. However, this simpliﬁcation is\\nnot precise for two situations: when we repeat something inﬁnitely often,\\nand when we want to draw a point from an interval. The ﬁrst situation\\narises when we discuss generalization errors in machine learning (Chap-\\nter 8). The second situation arises when we want to discuss continuous\\ndistributions, such as the Gaussian (Section 6.5). For our purposes, the\\nlack of precision allows for a briefer introduction to probability.\\nRemark. In continuous spaces, there are two additional technicalities,\\nwhich are counterintuitive. First, the set of all subsets (used to deﬁne\\nthe event spaceAin Section 6.1) is not well behaved enough. Aneeds\\nto be restricted to behave well under set complements, set intersections,\\nand set unions. Second, the size of a set (which in discrete spaces can be\\nobtained by counting the elements) turns out to be tricky. The size of a\\nset is called its measure . For example, the cardinality of discrete sets, the measure\\nlength of an interval in R, and the volume of a region in Rdare all mea-\\nsures. Sets that behave well under set operations and additionally have\\na topology are called a Borelσ-algebra . Betancourt details a careful con- Borelσ-algebra\\nstruction of probability spaces from set theory without being bogged down\\nin technicalities; see https://tinyurl.com/yb3t6mfd . For a more pre-\\ncise construction, we refer to Billingsley (1995) and Jacod and Protter\\n(2004).\\nIn this book, we consider real-valued random variables with their cor-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b55f17e8-e40c-4928-8b19-e407e5da44f0', embedding=None, metadata={'page_label': '181', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.2 Discrete and Continuous Probabilities 181\\nresponding Borel σ-algebra. We consider random variables with values in\\nRDto be a vector of real-valued random variables. ♦\\nDeﬁnition 6.1 (Probability Density Function) .A functionf:RD→Ris\\ncalled a probability density function (pdf) if probability density\\nfunction\\npdf 1.∀x∈RD:f(x)⩾0\\n2. Its integral exists and\\n∫\\nRDf(x)dx= 1. (6.15)\\nFor probability mass functions (pmf) of discrete random variables, the\\nintegral in (6.15) is replaced with a sum (6.12).\\nObserve that the probability density function is any function fthat is\\nnon-negative and integrates to one. We associate a random variable X\\nwith this function fby\\nP(a⩽X⩽b) =∫b\\naf(x)dx, (6.16)\\nwherea,b∈Randx∈Rare outcomes of the continuous random vari-\\nableX. Statesx∈RDare deﬁned analogously by considering a vector\\nofx∈R. This association (6.16) is called the lawordistribution of the law\\nrandom variable X.P(X=x)is a set of\\nmeasure zero. Remark. In contrast to discrete random variables, the probability of a con-\\ntinuous random variable Xtaking a particular value P(X=x)is zero.\\nThis is like trying to specify an interval in (6.16) where a=b.♦\\nDeﬁnition 6.2 (Cumulative Distribution Function) .Acumulative distribu- cumulative\\ndistribution function tion function (cdf) of a multivariate real-valued random variable Xwith\\nstatesx∈RDis given by\\nFX(x) =P(X1⩽x1,...,XD⩽xD), (6.17)\\nwhereX= [X1,...,XD]⊤,x= [x1,...,xD]⊤, and the right-hand side\\nrepresents the probability that random variable Xitakes the value smaller\\nthan or equal to xi.\\nThere are cdfs,\\nwhich do not have\\ncorresponding pdfs.The cdf can be expressed also as the integral of the probability density\\nfunctionf(x)so that\\nFX(x) =∫x1\\n−∞···∫xD\\n−∞f(z1,...,zD)dz1···dzD. (6.18)\\nRemark. We reiterate that there are in fact two distinct concepts when\\ntalking about distributions. First is the idea of a pdf (denoted by f(x)),\\nwhich is a nonnegative function that sums to one. Second is the law of a\\nrandom variable X, that is, the association of a random variable Xwith\\nthe pdff(x). ♦\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e74d45a-84a1-4a6a-b696-81751676a95b', embedding=None, metadata={'page_label': '182', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='182 Probability and Distributions\\nFigure 6.3\\nExamples of\\n(a) discrete and\\n(b) continuous\\nuniform\\ndistributions. See\\nExample 6.3 for\\ndetails of the\\ndistributions.\\n−1 0 1 2\\nz0.00.51.01.52.0P(Z=z)\\n(a) Discrete distribution\\n−1 0 1 2\\nx0.00.51.01.52.0p(x) (b) Continuous distribution\\nFor most of this book, we will not use the notation f(x)andFX(x)as\\nwe mostly do not need to distinguish between the pdf and cdf. However,\\nwe will need to be careful about pdfs and cdfs in Section 6.7.\\n6.2.3 Contrasting Discrete and Continuous Distributions\\nRecall from Section 6.1.2 that probabilities are positive and the total prob-\\nability sums up to one. For discrete random variables (see (6.12)), this\\nimplies that the probability of each state must lie in the interval [0,1].\\nHowever, for continuous random variables the normalization (see (6.15))\\ndoes not imply that the value of the density is less than or equal to 1for\\nall values. We illustrate this in Figure 6.3 using the uniform distribution uniform distribution\\nfor both discrete and continuous random variables.\\nExample 6.3\\nWe consider two examples of the uniform distribution, where each state is\\nequally likely to occur. This example illustrates some differences between\\ndiscrete and continuous probability distributions.\\nLetZbe a discrete uniform random variable with three states {z=\\n−1.1,z= 0.3,z= 1.5}. The probability mass function can be represented The actual values of\\nthese states are not\\nmeaningful here,\\nand we deliberately\\nchose numbers to\\ndrive home the\\npoint that we do not\\nwant to use (and\\nshould ignore) the\\nordering of the\\nstates.as a table of probability values:\\nz\\nP(Z=z)−1.1\\n1\\n30.3\\n1\\n31.5\\n1\\n3\\nAlternatively, we can think of this as a graph (Figure 6.3(a)), where we\\nuse the fact that the states can be located on the x-axis, and the y-axis\\nrepresents the probability of a particular state. The y-axis in Figure 6.3(a)\\nis deliberately extended so that is it the same as in Figure 6.3(b).\\nLetXbe a continuous random variable taking values in the range 0.9⩽\\nX⩽1.6, as represented by Figure 6.3(b). Observe that the height of the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e8058ea-eba9-4556-a5e2-b2c784c2252b', embedding=None, metadata={'page_label': '183', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183\\nTable 6.1\\nNomenclature for\\nprobability\\ndistributions.Type “Point probability” “Interval probability”\\nDiscrete P(X=x) Not applicable\\nProbability mass function\\nContinuous p(x) P(X⩽x)\\nProbability density function Cumulative distribution function\\ndensity can be greater than 1. However, it needs to hold that\\n∫1.6\\n0.9p(x)dx= 1. (6.19)\\nRemark. There is an additional subtlety with regards to discrete prob-\\nability distributions. The states z1,...,zddo not in principle have any\\nstructure, i.e., there is usually no way to compare them, for example\\nz1= red,z2= green,z3= blue . However, in many machine learning\\napplications discrete states take numerical values, e.g., z1=−1.1,z2=\\n0.3,z3= 1.5, where we could say z1< z 2< z 3. Discrete states that as-\\nsume numerical values are particularly useful because we often consider\\nexpected values (Section 6.4.1) of random variables. ♦\\nUnfortunately, machine learning literature uses notation and nomen-\\nclature that hides the distinction between the sample space Ω, the target\\nspaceT, and the random variable X. For a value xof the set of possible\\noutcomes of the random variable X, i.e.,x∈T,p(x)denotes the prob- We think of the\\noutcomexas the\\nargument that\\nresults in the\\nprobabilityp(x).ability that random variable Xhas the outcome x. For discrete random\\nvariables, this is written as P(X=x), which is known as the probabil-\\nity mass function. The pmf is often referred to as the “distribution”. For\\ncontinuous variables, p(x)is called the probability density function (often\\nreferred to as a density). To muddy things even further, the cumulative\\ndistribution function P(X⩽x)is often also referred to as the “distribu-\\ntion”. In this chapter, we will use the notation Xto refer to both univariate\\nand multivariate random variables, and denote the states by xandxre-\\nspectively. We summarize the nomenclature in Table 6.1.\\nRemark. We will be using the expression “probability distribution” not\\nonly for discrete probability mass functions but also for continuous proba-\\nbility density functions, although this is technically incorrect. In line with\\nmost machine learning literature, we also rely on context to distinguish\\nthe different uses of the phrase probability distribution. ♦\\n6.3 Sum Rule, Product Rule, and Bayes’ Theorem\\nWe think of probability theory as an extension to logical reasoning. As we\\ndiscussed in Section 6.1.1, the rules of probability presented here follow\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2315067-2c94-4da8-b480-6c26d3e04000', embedding=None, metadata={'page_label': '184', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='184 Probability and Distributions\\nnaturally from fulﬁlling the desiderata (Jaynes, 2003, chapter 2). Prob-\\nabilistic modeling (Section 8.4) provides a principled foundation for de-\\nsigning machine learning methods. Once we have deﬁned probability dis-\\ntributions (Section 6.2) corresponding to the uncertainties of the data and\\nour problem, it turns out that there are only two fundamental rules, the\\nsum rule and the product rule.\\nRecall from (6.9) that p(x,y)is the joint distribution of the two ran-\\ndom variables x,y. The distributions p(x)andp(y)are the correspond-\\ning marginal distributions, and p(y|x)is the conditional distribution of y\\ngivenx. Given the deﬁnitions of the marginal and conditional probability\\nfor discrete and continuous random variables in Section 6.2, we can now\\npresent the two fundamental rules in probability theory. These two rules\\narise\\nnaturally (Jaynes,\\n2003) from the\\nrequirements we\\ndiscussed in\\nSection 6.1.1.The ﬁrst rule, the sum rule , states that\\nsum rulep(x) =\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3∑\\ny∈Yp(x,y) ifyis discrete\\n∫\\nYp(x,y)dy ifyis continuous, (6.20)\\nwhereYare the states of the target space of random variable Y. This\\nmeans that we sum out (or integrate out) the set of states yof the random\\nvariableY. The sum rule is also known as the marginalization property . marginalization\\nproperty The sum rule relates the joint distribution to a marginal distribution. In\\ngeneral, when the joint distribution contains more than two random vari-\\nables, the sum rule can be applied to any subset of the random variables,\\nresulting in a marginal distribution of potentially more than one random\\nvariable. More concretely, if x= [x1,...,xD]⊤, we obtain the marginal\\np(xi) =∫\\np(x1,...,xD)dx\\\\i (6.21)\\nby repeated application of the sum rule where we integrate/sum out all\\nrandom variables except xi, which is indicated by \\\\i, which reads “all\\nexcepti.”\\nRemark. Many of the computational challenges of probabilistic modeling\\nare due to the application of the sum rule. When there are many variables\\nor discrete variables with many states, the sum rule boils down to per-\\nforming a high-dimensional sum or integral. Performing high-dimensional\\nsums or integrals is generally computationally hard, in the sense that there\\nis no known polynomial-time algorithm to calculate them exactly. ♦\\nThe second rule, known as the product rule , relates the joint distribution product rule\\nto the conditional distribution via\\np(x,y) =p(y|x)p(x). (6.22)\\nThe product rule can be interpreted as the fact that every joint distribu-\\ntion of two random variables can be factorized (written as a product)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1904b344-2c1c-4fac-81d2-4d6df6afcccc', embedding=None, metadata={'page_label': '185', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 Sum Rule, Product Rule, and Bayes’ Theorem 185\\nof two other distributions. The two factors are the marginal distribu-\\ntion of the ﬁrst random variable p(x), and the conditional distribution\\nof the second random variable given the ﬁrst p(y|x). Since the ordering\\nof random variables is arbitrary in p(x,y), the product rule also implies\\np(x,y) =p(x|y)p(y). To be precise, (6.22) is expressed in terms of the\\nprobability mass functions for discrete random variables. For continuous\\nrandom variables, the product rule is expressed in terms of the probability\\ndensity functions (Section 6.2.3).\\nIn machine learning and Bayesian statistics, we are often interested in\\nmaking inferences of unobserved (latent) random variables given that we\\nhave observed other random variables. Let us assume we have some prior\\nknowledgep(x)about an unobserved random variable xand some rela-\\ntionshipp(y|x)betweenxand a second random variable y, which we\\ncan observe. If we observe y, we can use Bayes’ theorem to draw some\\nconclusions about xgiven the observed values of y.Bayes’ theorem (also Bayes’ theorem\\nBayes’ rule orBayes’ law ) Bayes’ rule\\nBayes’ law\\np(x|y)\\ued19\\ued18\\ued17\\ued1a\\nposterior=likelihood\\ued17\\ued1a\\ued19\\ued18\\np(y|x)prior\\ued17\\ued1a\\ued19\\ued18\\np(x)\\np(y)\\ued19\\ued18\\ued17\\ued1a\\nevidence(6.23)\\nis a direct consequence of the product rule in (6.22) since\\np(x,y) =p(x|y)p(y) (6.24)\\nand\\np(x,y) =p(y|x)p(x) (6.25)\\nso that\\np(x|y)p(y) =p(y|x)p(x)⇐⇒p(x|y) =p(y|x)p(x)\\np(y).(6.26)\\nIn (6.23),p(x)is the prior , which encapsulates our subjective prior prior\\nknowledge of the unobserved (latent) variable xbefore observing any\\ndata. We can choose any prior that makes sense to us, but it is critical to\\nensure that the prior has a nonzero pdf (or pmf) on all plausible x, even\\nif they are very rare.\\nThelikelihoodp(y|x)describes how xandyare related, and in the likelihood\\nThe likelihood is\\nsometimes also\\ncalled the\\n“measurement\\nmodel”.case of discrete probability distributions, it is the probability of the data y\\nif we were to know the latent variable x. Note that the likelihood is not a\\ndistribution in x, but only iny. We callp(y|x)either the “likelihood of\\nx(giveny)” or the “probability of ygivenx” but never the likelihood of\\ny(MacKay, 2003).\\nTheposteriorp(x|y)is the quantity of interest in Bayesian statistics posterior\\nbecause it expresses exactly what we are interested in, i.e., what we know\\naboutxafter having observed y.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7102264b-b378-453c-a499-2563a11763e0', embedding=None, metadata={'page_label': '186', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='186 Probability and Distributions\\nThe quantity\\np(y) :=∫\\np(y|x)p(x)dx=EX[p(y|x)] (6.27)\\nis the marginal likelihood /evidence . The right-hand side of (6.27) uses the marginal likelihood\\nevidence expectation operator which we deﬁne in Section 6.4.1. By deﬁnition, the\\nmarginal likelihood integrates the numerator of (6.23) with respect to the\\nlatent variable x. Therefore, the marginal likelihood is independent of\\nx, and it ensures that the posterior p(x|y)is normalized. The marginal\\nlikelihood can also be interpreted as the expected likelihood where we\\ntake the expectation with respect to the prior p(x). Beyond normalization\\nof the posterior, the marginal likelihood also plays an important role in\\nBayesian model selection, as we will discuss in Section 8.6. Due to the\\nintegration in (8.44), the evidence is often hard to compute. Bayes’ theorem is\\nalso called the\\n“probabilistic\\ninverse.”Bayes’ theorem (6.23) allows us to invert the relationship between x\\nandygiven by the likelihood. Therefore, Bayes’ theorem is sometimes\\ncalled the probabilistic inverse . We will discuss Bayes’ theorem further inprobabilistic inverse\\nSection 8.4.\\nRemark. In Bayesian statistics, the posterior distribution is the quantity\\nof interest as it encapsulates all available information from the prior and\\nthe data. Instead of carrying the posterior around, it is possible to focus\\non some statistic of the posterior, such as the maximum of the posterior,\\nwhich we will discuss in Section 8.3. However, focusing on some statistic\\nof the posterior leads to loss of information. If we think in a bigger con-\\ntext, then the posterior can be used within a decision-making system, and\\nhaving the full posterior can be extremely useful and lead to decisions that\\nare robust to disturbances. For example, in the context of model-based re-\\ninforcement learning, Deisenroth et al. (2015) show that using the full\\nposterior distribution of plausible transition functions leads to very fast\\n(data/sample efﬁcient) learning, whereas focusing on the maximum of\\nthe posterior leads to consistent failures. Therefore, having the full pos-\\nterior can be very useful for a downstream task. In Chapter 9, we will\\ncontinue this discussion in the context of linear regression. ♦\\n6.4 Summary Statistics and Independence\\nWe are often interested in summarizing sets of random variables and com-\\nparing pairs of random variables. A statistic of a random variable is a de-\\nterministic function of that random variable. The summary statistics of a\\ndistribution provide one useful view of how a random variable behaves,\\nand as the name suggests, provide numbers that summarize and charac-\\nterize the distribution. We describe the mean and the variance, two well-\\nknown summary statistics. Then we discuss two ways to compare a pair\\nof random variables: ﬁrst, how to say that two random variables are inde-\\npendent; and second, how to compute an inner product between them.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f41a465a-ad98-4272-b18e-01e1f951a04c', embedding=None, metadata={'page_label': '187', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Summary Statistics and Independence 187\\n6.4.1 Means and Covariances\\nMean and (co)variance are often useful to describe properties of probabil-\\nity distributions (expected values and spread). We will see in Section 6.6\\nthat there is a useful family of distributions (called the exponential fam-\\nily), where the statistics of the random variable capture all possible infor-\\nmation.\\nThe concept of the expected value is central to machine learning, and\\nthe foundational concepts of probability itself can be derived from the\\nexpected value (Whittle, 2000).\\nDeﬁnition 6.3 (Expected Value) .Theexpected value of a function g:R→ expected value\\nRof a univariate continuous random variable X∼p(x)is given by\\nEX[g(x)] =∫\\nXg(x)p(x)dx. (6.28)\\nCorrespondingly, the expected value of a function gof a discrete random\\nvariableX∼p(x)is given by\\nEX[g(x)] =∑\\nx∈Xg(x)p(x), (6.29)\\nwhereXis the set of possible outcomes (the target space) of the random\\nvariableX.\\nIn this section, we consider discrete random variables to have numerical\\noutcomes. This can be seen by observing that the function gtakes real\\nnumbers as inputs. The expected value\\nof a function of a\\nrandom variable is\\nsometimes referred\\nto as the law of the\\nunconscious\\nstatistician (Casella\\nand Berger, 2002,\\nSection 2.2).Remark. We consider multivariate random variables Xas a ﬁnite vector\\nof univariate random variables [X1,...,XD]⊤. For multivariate random\\nvariables, we deﬁne the expected value element wise\\nEX[g(x)] =\\uf8ee\\n\\uf8ef\\uf8f0EX1[g(x1)]\\n...\\nEXD[g(xD)]\\uf8f9\\n\\uf8fa\\uf8fb∈RD, (6.30)\\nwhere the subscript EXdindicates that we are taking the expected value\\nwith respect to the dth element of the vector x. ♦\\nDeﬁnition 6.3 deﬁnes the meaning of the notation EXas the operator\\nindicating that we should take the integral with respect to the probabil-\\nity density (for continuous distributions) or the sum over all states (for\\ndiscrete distributions). The deﬁnition of the mean (Deﬁnition 6.4), is a\\nspecial case of the expected value, obtained by choosing gto be the iden-\\ntity function.\\nDeﬁnition 6.4 (Mean) .Themean of a random variable Xwith states mean\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='021d6a5e-7938-4772-9d91-24bc5ca45c33', embedding=None, metadata={'page_label': '188', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='188 Probability and Distributions\\nx∈RDis an average and is deﬁned as\\nEX[x] =\\uf8ee\\n\\uf8ef\\uf8f0EX1[x1]\\n...\\nEXD[xD]\\uf8f9\\n\\uf8fa\\uf8fb∈RD, (6.31)\\nwhere\\nEXd[xd] :=\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3∫\\nXxdp(xd)dxd ifXis a continuous random variable\\n∑\\nxi∈Xxip(xd=xi)ifXis a discrete random variable\\n(6.32)\\nford= 1,...,D , where the subscript dindicates the corresponding di-\\nmension ofx. The integral and sum are over the states Xof the target\\nspace of the random variable X.\\nIn one dimension, there are two other intuitive notions of “average”,\\nwhich are the median and the mode . The median is the “middle” value if median\\nwe sort the values, i.e., 50% of the values are greater than the median and\\n50% are smaller than the median. This idea can be generalized to contin-\\nuous values by considering the value where the cdf (Deﬁnition 6.2) is 0.5.\\nFor distributions, which are asymmetric or have long tails, the median\\nprovides an estimate of a typical value that is closer to human intuition\\nthan the mean value. Furthermore, the median is more robust to outliers\\nthan the mean. The generalization of the median to higher dimensions is\\nnon-trivial as there is no obvious way to “sort” in more than one dimen-\\nsion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the most mode\\nfrequently occurring value. For a discrete random variable, the mode is\\ndeﬁned as the value of xhaving the highest frequency of occurrence. For\\na continuous random variable, the mode is deﬁned as a peak in the density\\np(x). A particular density p(x)may have more than one mode, and fur-\\nthermore there may be a very large number of modes in high-dimensional\\ndistributions. Therefore, ﬁnding all the modes of a distribution can be\\ncomputationally challenging.\\nExample 6.4\\nConsider the two-dimensional distribution illustrated in Figure 6.4:\\np(x) = 0.4N(\\nx⏐⏐⏐⏐[10\\n2]\\n,[1 0\\n0 1])\\n+ 0.6N(\\nx⏐⏐⏐⏐[0\\n0]\\n,[8.4 2.0\\n2.0 1.7])\\n.\\n(6.33)\\nWe will deﬁne the Gaussian distribution N(µ, σ2)\\nin Section 6.5. Also\\nshown is its corresponding marginal distribution in each dimension. Ob-\\nserve that the distribution is bimodal (has two modes), but one of the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7e27b09-a807-47b8-982c-3e0b8e2d35e9', embedding=None, metadata={'page_label': '189', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Summary Statistics and Independence 189\\nmarginal distributions is unimodal (has one mode). The horizontal bi-\\nmodal univariate distribution illustrates that the mean and median can\\nbe different from each other. While it is tempting to deﬁne the two-\\ndimensional median to be the concatenation of the medians in each di-\\nmension, the fact that we cannot deﬁne an ordering of two-dimensional\\npoints makes it difﬁcult. When we say “cannot deﬁne an ordering”, we\\nmean that there is more than one way to deﬁne the relation <so that[3\\n0]\\n<[2\\n3]\\n.\\nFigure 6.4\\nIllustration of the\\nmean, mode, and\\nmedian for a\\ntwo-dimensional\\ndataset, as well as\\nits marginal\\ndensities.\\nMean\\nModes\\nMedian\\nRemark. The expected value (Deﬁnition 6.3) is a linear operator. For ex-\\nample, given a real-valued function f(x) =ag(x)+bh(x)wherea,b∈R\\nandx∈RD, we obtain\\nEX[f(x)] =∫\\nf(x)p(x)dx (6.34a)\\n=∫\\n[ag(x) +bh(x)]p(x)dx (6.34b)\\n=a∫\\ng(x)p(x)dx+b∫\\nh(x)p(x)dx (6.34c)\\n=aEX[g(x)] +bEX[h(x)]. (6.34d)\\n♦\\nFor two random variables, we may wish to characterize their correspon-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c4b2a02-39f8-46bf-b4ae-fba57b23621f', embedding=None, metadata={'page_label': '190', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='190 Probability and Distributions\\ndence to each other. The covariance intuitively represents the notion of\\nhow dependent random variables are to one another.\\nDeﬁnition 6.5 (Covariance (Univariate)) .Thecovariance between two covariance\\nunivariate random variables X,Y∈Ris given by the expected product\\nof their deviations from their respective means, i.e.,\\nCovX,Y[x,y] :=EX,Y[(x−EX[x])(y−EY[y])]. (6.35)\\nTerminology: The\\ncovariance of\\nmultivariate random\\nvariables Cov[x,y]\\nis sometimes\\nreferred to as\\ncross-covariance,\\nwith covariance\\nreferring to\\nCov[x,x].Remark. When the random variable associated with the expectation or\\ncovariance is clear by its arguments, the subscript is often suppressed (for\\nexample, EX[x]is often written as E[x]). ♦\\nBy using the linearity of expectations, the expression in Deﬁnition 6.5\\ncan be rewritten as the expected value of the product minus the product\\nof the expected values, i.e.,\\nCov[x,y] =E[xy]−E[x]E[y]. (6.36)\\nThe covariance of a variable with itself Cov[x,x]is called the variance and variance\\nis denoted by VX[x]. The square root of the variance is called the standard standard deviation\\ndeviation and is often denoted by σ(x). The notion of covariance can be\\ngeneralized to multivariate random variables.\\nDeﬁnition 6.6 (Covariance (Multivariate)) .If we consider two multivari-\\nate random variables XandYwith statesx∈RDandy∈RErespec-\\ntively, the covariance betweenXandYis deﬁned as covariance\\nCov[x,y] =E[xy⊤]−E[x]E[y]⊤= Cov[y,x]⊤∈RD×E. (6.37)\\nDeﬁnition 6.6 can be applied with the same multivariate random vari-\\nable in both arguments, which results in a useful concept that intuitively\\ncaptures the “spread” of a random variable. For a multivariate random\\nvariable, the variance describes the relation between individual dimen-\\nsions of the random variable.\\nDeﬁnition 6.7 (Variance) .The variance of a random variable Xwith variance\\nstatesx∈RDand a mean vector µ∈RDis deﬁned as\\nVX[x] = CovX[x,x] (6.38a)\\n=EX[(x−µ)(x−µ)⊤] =EX[xx⊤]−EX[x]EX[x]⊤(6.38b)\\n=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0Cov[x1,x1] Cov[x1,x2]... Cov[x1,xD]\\nCov[x2,x1] Cov[x2,x2]... Cov[x2,xD]\\n............\\nCov[xD,x1]... ... Cov[xD,xD]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (6.38c)\\nTheD×Dmatrix in (6.38c) is called the covariance matrix of the mul- covariance matrix\\ntivariate random variable X. The covariance matrix is symmetric and pos-\\nitive semideﬁnite and tells us something about the spread of the data. On\\nits diagonal, the covariance matrix contains the variances of the marginals marginal\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='78797d75-15c4-4358-9750-c301a0ccff89', embedding=None, metadata={'page_label': '191', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Summary Statistics and Independence 191\\nFigure 6.5\\nTwo-dimensional\\ndatasets with\\nidentical means and\\nvariances along\\neach axis (colored\\nlines) but with\\ndifferent\\ncovariances.\\n−5 0 5\\nx−20246y\\n(a)xandyare negatively correlated.\\n−5 0 5\\nx−20246y\\n (b)xandyare positively correlated.\\np(xi) =∫\\np(x1,...,xD)dx\\\\i, (6.39)\\nwhere “\\\\i” denotes “all variables but i”. The off-diagonal entries are the\\ncross-covariance terms Cov[xi,xj]fori,j= 1,...,D, i̸=j. cross-covariance\\nRemark. In this book, we generally assume that covariance matrices are\\npositive deﬁnite to enable better intuition. We therefore do not discuss\\ncorner cases that result in positive semideﬁnite (low-rank) covariance ma-\\ntrices. ♦\\nWhen we want to compare the covariances between different pairs of\\nrandom variables, it turns out that the variance of each random variable\\naffects the value of the covariance. The normalized version of covariance\\nis called the correlation.\\nDeﬁnition 6.8 (Correlation) .Thecorrelation between two random vari- correlation\\nablesX,Y is given by\\ncorr[x,y] =Cov[x,y]√\\nV[x]V[y]∈[−1,1]. (6.40)\\nThe correlation matrix is the covariance matrix of standardized random\\nvariables,x/σ(x). In other words, each random variable is divided by its\\nstandard deviation (the square root of the variance) in the correlation\\nmatrix.\\nThe covariance (and correlation) indicate how two random variables\\nare related; see Figure 6.5. Positive correlation corr [x,y]means that when\\nxgrows, then yis also expected to grow. Negative correlation means that\\nasxincreases, then ydecreases.\\n6.4.2 Empirical Means and Covariances\\nThe deﬁnitions in Section 6.4.1 are often also called the population mean population mean\\nand covariance and covariance , as it refers to the true statistics for the population. In ma-\\nchine learning, we need to learn from empirical observations of data. Con-\\nsider a random variable X. There are two conceptual steps to go from\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6e59c25-b88b-482b-a13f-550caf6ec78d', embedding=None, metadata={'page_label': '192', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='192 Probability and Distributions\\npopulation statistics to the realization of empirical statistics. First, we use\\nthe fact that we have a ﬁnite dataset (of size N) to construct an empirical\\nstatistic that is a function of a ﬁnite number of identical random variables,\\nX1,...,XN. Second, we observe the data, that is, we look at the realiza-\\ntionx1,...,xNof each of the random variables and apply the empirical\\nstatistic.\\nSpeciﬁcally, for the mean (Deﬁnition 6.4), given a particular dataset we\\ncan obtain an estimate of the mean, which is called the empirical mean or empirical mean\\nsample mean . The same holds for the empirical covariance. sample mean\\nDeﬁnition 6.9 (Empirical Mean and Covariance) .Theempirical mean vec- empirical mean\\ntor is the arithmetic average of the observations for each variable, and it\\nis deﬁned as\\n¯x:=1\\nNN∑\\nn=1xn, (6.41)\\nwherexn∈RD.\\nSimilar to the empirical mean, the empirical covariance matrix is aD×D empirical covariance\\nmatrix\\nΣ:=1\\nNN∑\\nn=1(xn−¯x)(xn−¯x)⊤. (6.42)\\nThroughout the\\nbook, we use the\\nempirical\\ncovariance, which is\\na biased estimate.\\nThe unbiased\\n(sometimes called\\ncorrected)\\ncovariance has the\\nfactorN−1in the\\ndenominator\\ninstead ofN.To compute the statistics for a particular dataset, we would use the\\nrealizations (observations) x1,...,xNand use (6.41) and (6.42). Em-\\npirical covariance matrices are symmetric, positive semideﬁnite (see Sec-\\ntion 3.2.3).\\n6.4.3 Three Expressions for the Variance\\nWe now focus on a single random variable Xand use the preceding em-\\npirical formulas to derive three possible expressions for the variance. The\\nThe derivations are\\nexercises at the end\\nof this chapter.following derivation is the same for the population variance, except that\\nwe need to take care of integrals. The standard deﬁnition of variance, cor-\\nresponding to the deﬁnition of covariance (Deﬁnition 6.5), is the expec-\\ntation of the squared deviation of a random variable Xfrom its expected\\nvalueµ, i.e.,\\nVX[x] :=EX[(x−µ)2]. (6.43)\\nThe expectation in (6.43) and the mean µ=EX(x)are computed us-\\ning (6.32), depending on whether Xis a discrete or continuous random\\nvariable. The variance as expressed in (6.43) is the mean of a new random\\nvariableZ:= (X−µ)2.\\nWhen estimating the variance in (6.43) empirically, we need to resort\\nto a two-pass algorithm: one pass through the data to calculate the mean\\nµusing (6.41), and then a second pass using this estimate ˆµcalculate the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90142447-8e49-485c-ab4e-475eb0c1cd51', embedding=None, metadata={'page_label': '193', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Summary Statistics and Independence 193\\nvariance. It turns out that we can avoid two passes by rearranging the\\nterms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula\\nfor variance formula for variance :\\nVX[x] =EX[x2]−(EX[x])2. (6.44)\\nThe expression in (6.44) can be remembered as “the mean of the square\\nminus the square of the mean”. It can be calculated empirically in one pass\\nthrough data since we can accumulate xi(to calculate the mean) and x2\\ni\\nsimultaneously, where xiis theith observation. Unfortunately, if imple- If the two terms\\nin (6.44) are huge\\nand approximately\\nequal, we may\\nsuffer from an\\nunnecessary loss of\\nnumerical precision\\nin ﬂoating-point\\narithmetic.mented in this way, it can be numerically unstable. The raw-score version\\nof the variance can be useful in machine learning, e.g., when deriving the\\nbias–variance decomposition (Bishop, 2006).\\nA third way to understand the variance is that it is a sum of pairwise dif-\\nferences between all pairs of observations. Consider a sample x1,...,xN\\nof realizations of random variable X, and we compute the squared differ-\\nence between pairs of xiandxj. By expanding the square, we can show\\nthat the sum of N2pairwise differences is the empirical variance of the\\nobservations:\\n1\\nN2N∑\\ni,j=1(xi−xj)2= 2\\uf8ee\\n\\uf8f01\\nNN∑\\ni=1x2\\ni−(\\n1\\nNN∑\\ni=1xi)2\\uf8f9\\n\\uf8fb. (6.45)\\nWe see that (6.45) is twice the raw-score expression (6.44). This means\\nthat we can express the sum of pairwise distances (of which there are N2\\nof them) as a sum of deviations from the mean (of which there are N). Ge-\\nometrically, this means that there is an equivalence between the pairwise\\ndistances and the distances from the center of the set of points. From a\\ncomputational perspective, this means that by computing the mean ( N\\nterms in the summation), and then computing the variance (again N\\nterms in the summation), we can obtain an expression (left-hand side\\nof (6.45)) that has N2terms.\\n6.4.4 Sums and Transformations of Random Variables\\nWe may want to model a phenomenon that cannot be well explained by\\ntextbook distributions (we introduce some in Sections 6.5 and 6.6), and\\nhence may perform simple manipulations of random variables (such as\\nadding two random variables).\\nConsider two random variables X,Y with statesx,y∈RD. Then:\\nE[x+y] =E[x] +E[y] (6.46)\\nE[x−y] =E[x]−E[y] (6.47)\\nV[x+y] =V[x] +V[y] + Cov[x,y] + Cov[y,x] (6.48)\\nV[x−y] =V[x] +V[y]−Cov[x,y]−Cov[y,x]. (6.49)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60af794c-156e-44a7-b7eb-b5a5717310a1', embedding=None, metadata={'page_label': '194', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='194 Probability and Distributions\\nMean and (co)variance exhibit some useful properties when it comes\\nto afﬁne transformation of random variables. Consider a random variable\\nXwith meanµand covariance matrix Σand a (deterministic) afﬁne\\ntransformation y=Ax+bofx. Thenyis itself a random variable\\nwhose mean vector and covariance matrix are given by\\nEY[y] =EX[Ax+b] =AEX[x] +b=Aµ+b, (6.50)\\nVY[y] =VX[Ax+b] =VX[Ax] =AVX[x]A⊤=AΣA⊤,(6.51)\\nrespectively. Furthermore, This can be shown\\ndirectly by using the\\ndeﬁnition of the\\nmean and\\ncovariance.Cov[x,y] =E[x(Ax+b)⊤]−E[x]E[Ax+b]⊤(6.52a)\\n=E[x]b⊤+E[xx⊤]A⊤−µb⊤−µµ⊤A⊤(6.52b)\\n=µb⊤−µb⊤+(\\nE[xx⊤]−µµ⊤)A⊤(6.52c)\\n(6.38b)=ΣA⊤, (6.52d)\\nwhere Σ=E[xx⊤]−µµ⊤is the covariance of X.\\n6.4.5 Statistical Independence\\nDeﬁnition 6.10 (Independence) .Two random variables X,Y arestatis- statistical\\nindependence tically independent if and only if\\np(x,y) =p(x)p(y). (6.53)\\nIntuitively, two random variables XandYare independent if the value\\nofy(once known) does not add any additional information about x(and\\nvice versa). If X,Y are (statistically) independent, then\\np(y|x) =p(y)\\np(x|y) =p(x)\\nVX,Y[x+y] =VX[x] +VY[y]\\nCovX,Y[x,y] =0\\nThe last point may not hold in converse, i.e., two random variables can\\nhave covariance zero but are not statistically independent. To understand\\nwhy, recall that covariance measures only linear dependence. Therefore,\\nrandom variables that are nonlinearly dependent could have covariance\\nzero.\\nExample 6.5\\nConsider a random variable Xwith zero mean ( EX[x] = 0 ) and also\\nEX[x3] = 0 . Lety=x2(hence,Yis dependent on X) and consider the\\ncovariance (6.36) between XandY. But this gives\\nCov[x,y] =E[xy]−E[x]E[y] =E[x3] = 0. (6.54)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bf514a5-982c-44d2-b6d8-a8d6e11e71c5', embedding=None, metadata={'page_label': '195', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Summary Statistics and Independence 195\\nIn machine learning, we often consider problems that can be mod-\\neled as independent and identically distributed (i.i.d.) random variables, independent and\\nidentically\\ndistributed\\ni.i.d.X1,...,XN. For more than two random variables, the word “indepen-\\ndent” (Deﬁnition 6.10) usually refers to mutually independent random\\nvariables, where all subsets are independent (see Pollard (2002, chap-\\nter 4) and Jacod and Protter (2004, chapter 3)). The phrase “identically\\ndistributed” means that all the random variables are from the same distri-\\nbution.\\nAnother concept that is important in machine learning is conditional\\nindependence.\\nDeﬁnition 6.11 (Conditional Independence) .Two random variables X\\nandYareconditionally independent givenZif and only if conditionally\\nindependent\\np(x,y|z) =p(x|z)p(y|z) for allz∈Z, (6.55)\\nwhereZis the set of states of random variable Z. We writeX⊥ ⊥Y|Zto\\ndenote that Xis conditionally independent of YgivenZ.\\nDeﬁnition 6.11 requires that the relation in (6.55) must hold true for\\nevery value of z. The interpretation of (6.55) can be understood as “given\\nknowledge about z, the distribution of xandyfactorizes”. Independence\\ncan be cast as a special case of conditional independence if we write X⊥ ⊥\\nY|∅. By using the product rule of probability (6.22), we can expand the\\nleft-hand side of (6.55) to obtain\\np(x,y|z) =p(x|y,z)p(y|z). (6.56)\\nBy comparing the right-hand side of (6.55) with (6.56), we see that p(y|z)\\nappears in both of them so that\\np(x|y,z) =p(x|z). (6.57)\\nEquation (6.57) provides an alternative deﬁnition of conditional indepen-\\ndence, i.e., X⊥ ⊥Y|Z. This alternative presentation provides the inter-\\npretation “given that we know z, knowledge about ydoes not change our\\nknowledge of x”.\\n6.4.6 Inner Products of Random Variables\\nRecall the deﬁnition of inner products from Section 3.2. We can deﬁne an Inner products\\nbetween\\nmultivariate random\\nvariables can be\\ntreated in a similar\\nfashioninner product between random variables, which we brieﬂy describe in this\\nsection. If we have two uncorrelated random variables X,Y , then\\nV[x+y] =V[x] +V[y]. (6.58)\\nSince variances are measured in squared units, this looks very much like\\nthe Pythagorean theorem for right triangles c2=a2+b2.\\nIn the following, we see whether we can ﬁnd a geometric interpreta-\\ntion of the variance relation of uncorrelated random variables in (6.58).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cfcfc2ab-441a-49ff-8e27-11a90bcec87a', embedding=None, metadata={'page_label': '196', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='196 Probability and Distributions\\nFigure 6.6\\nGeometry of\\nrandom variables. If\\nrandom variables X\\nandYare\\nuncorrelated, they\\nare orthogonal\\nvectors in a\\ncorresponding\\nvector space, and\\nthe Pythagorean\\ntheorem applies.\\n√\\nvar[y]\\n√\\nvar[x]\\n√\\nvar[x+y] =√\\nvar[x] + var[ y]\\na\\nc\\nb\\nRandom variables can be considered vectors in a vector space, and we\\ncan deﬁne inner products to obtain geometric properties of random vari-\\nables (Eaton, 2007). If we deﬁne\\n⟨X,Y⟩:= Cov[x,y] (6.59)\\nfor zero mean random variables XandY, we obtain an inner product. We\\nsee that the covariance is symmetric, positive deﬁnite, and linear in either Cov[x,x] = 0⇐⇒\\nx= 0 argument. The length of a random variable is\\nCov[αx+z,y] =\\nαCov[x,y] +\\nCov[z,y]forα∈R.∥X∥=√\\nCov[x,x] =√\\nV[x] =σ[x], (6.60)\\ni.e., its standard deviation. The “longer” the random variable, the more\\nuncertain it is; and a random variable with length 0is deterministic.\\nIf we look at the angle θbetween two random variables X,Y , we get\\ncosθ=⟨X,Y⟩\\n∥X∥∥Y∥=Cov[x,y]√\\nV[x]V[y], (6.61)\\nwhich is the correlation (Deﬁnition 6.8) between the two random vari-\\nables. This means that we can think of correlation as the cosine of the\\nangle between two random variables when we consider them geometri-\\ncally. We know from Deﬁnition 3.7 that X⊥Y⇐⇒ ⟨X,Y⟩= 0. In our\\ncase, this means that XandYare orthogonal if and only if Cov[x,y] = 0 ,\\ni.e., they are uncorrelated. Figure 6.6 illustrates this relationship.\\nRemark. While it is tempting to use the Euclidean distance (constructed\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='590806a2-d682-4480-8808-a3417e8e8bc8', embedding=None, metadata={'page_label': '197', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Gaussian Distribution 197\\nFigure 6.7\\nGaussian\\ndistribution of two\\nrandom variables x1\\nandx2.\\nx1−101x2\\n−5.0−2.50.02.55.07.5p(x1,x2)\\n0.000.050.100.150.20\\nfrom the preceding deﬁnition of inner products) to compare probability\\ndistributions, it is unfortunately not the best way to obtain distances be-\\ntween distributions. Recall that the probability mass (or density) is posi-\\ntive and needs to add up to 1. These constraints mean that distributions\\nlive on something called a statistical manifold. The study of this space of\\nprobability distributions is called information geometry. Computing dis-\\ntances between distributions are often done using Kullback-Leibler diver-\\ngence, which is a generalization of distances that account for properties of\\nthe statistical manifold. Just like the Euclidean distance is a special case of\\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\\ntwo more general classes of divergences called Bregman divergences and\\nf-divergences. The study of divergences is beyond the scope of this book,\\nand we refer for more details to the recent book by Amari (2016), one of\\nthe founders of the ﬁeld of information geometry. ♦\\n6.5 Gaussian Distribution\\nThe Gaussian distribution is the most well-studied probability distribution\\nfor continuous-valued random variables. It is also referred to as the normal normal distribution\\ndistribution . Its importance originates from the fact that it has many com- The Gaussian\\ndistribution arises\\nnaturally when we\\nconsider sums of\\nindependent and\\nidentically\\ndistributed random\\nvariables. This is\\nknown as the\\ncentral limit\\ntheorem (Grinstead\\nand Snell, 1997).putationally convenient properties, which we will be discussing in the fol-\\nlowing. In particular, we will use it to deﬁne the likelihood and prior for\\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\\ndensity estimation (Chapter 11).\\nThere are many other areas of machine learning that also beneﬁt from\\nusing a Gaussian distribution, for example Gaussian processes, variational\\ninference, and reinforcement learning. It is also widely used in other ap-\\nplication areas such as signal processing (e.g., Kalman ﬁlter), control (e.g.,\\nlinear quadratic regulator), and statistics (e.g., hypothesis testing).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9484b66-a5f7-4756-842a-db013d16ffde', embedding=None, metadata={'page_label': '198', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='198 Probability and Distributions\\nFigure 6.8\\nGaussian\\ndistributions\\noverlaid with 100\\nsamples. (a) One-\\ndimensional case;\\n(b) two-dimensional\\ncase.\\n−5.0−2.5 0.0 2.5 5.0 7.5\\nx0.000.050.100.150.20\\np(x)\\nMean\\nSample\\n2σ\\n(a) Univariate (one-dimensional) Gaussian;\\nThe red cross shows the mean and the red\\nline shows the extent of the variance.\\n−1 0 1\\nx1−4−202468x2Mean\\nSample(b) Multivariate (two-dimensional) Gaus-\\nsian, viewed from top. The red cross shows\\nthe mean and the colored lines show the con-\\ntour lines of the density.\\nFor a univariate random variable, the Gaussian distribution has a den-\\nsity that is given by\\np(x|µ,σ2) =1√\\n2πσ2exp(\\n−(x−µ)2\\n2σ2)\\n. (6.62)\\nThe multivariate Gaussian distribution is fully characterized by a mean multivariate\\nGaussian\\ndistribution\\nmean vectorvectorµand a covariance matrix Σand deﬁned as\\ncovariance matrixp(x|µ,Σ) = (2π)−D\\n2|Σ|−1\\n2exp(−1\\n2(x−µ)⊤Σ−1(x−µ)),(6.63)\\nwherex∈RD. We writep(x) =N(x|µ,Σ)\\norX∼N(µ,Σ)\\n. Fig- Also known as a\\nmultivariate normal\\ndistribution.ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\\ntour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\\nwith corresponding samples. The special case of the Gaussian with zero\\nmean and identity covariance, that is, µ=0andΣ=I, is referred to as\\nthestandard normal distribution . standard normal\\ndistribution Gaussians are widely used in statistical estimation and machine learn-\\ning as they have closed-form expressions for marginal and conditional dis-\\ntributions. In Chapter 9, we use these closed-form expressions extensively\\nfor linear regression. A major advantage of modeling with Gaussian ran-\\ndom variables is that variable transformations (Section 6.7) are often not\\nneeded. Since the Gaussian distribution is fully speciﬁed by its mean and\\ncovariance, we often can obtain the transformed distribution by applying\\nthe transformation to the mean and covariance of the random variable.\\n6.5.1 Marginals and Conditionals of Gaussians are Gaussians\\nIn the following, we present marginalization and conditioning in the gen-\\neral case of multivariate random variables. If this is confusing at ﬁrst read-\\ning, the reader is advised to consider two univariate random variables in-\\nstead. LetXandYbe two multivariate random variables, that may have\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c80cc48a-d1b3-4da6-94c6-008d098bd616', embedding=None, metadata={'page_label': '199', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Gaussian Distribution 199\\ndifferent dimensions. To consider the effect of applying the sum rule of\\nprobability and the effect of conditioning, we explicitly write the Gaus-\\nsian distribution in terms of the concatenated states [x⊤,y⊤],\\np(x,y) =N([µx\\nµy]\\n,[ΣxxΣxy\\nΣyxΣyy])\\n. (6.64)\\nwhere Σxx= Cov[x,x]andΣyy= Cov[y,y]are the marginal covari-\\nance matrices of xandy, respectively, and Σxy= Cov[x,y]is the cross-\\ncovariance matrix between xandy.\\nThe conditional distribution p(x|y)is also Gaussian (illustrated in Fig-\\nure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\\np(x|y) =N(µx|y,Σx|y)\\n(6.65)\\nµx|y=µx+ΣxyΣ−1\\nyy(y−µy) (6.66)\\nΣx|y=Σxx−ΣxyΣ−1\\nyyΣyx. (6.67)\\nNote that in the computation of the mean in (6.66), the y-value is an\\nobservation and no longer random.\\nRemark. The conditional Gaussian distribution shows up in many places,\\nwhere we are interested in posterior distributions:\\nThe Kalman ﬁlter (Kalman, 1960), one of the most central algorithms\\nfor state estimation in signal processing, does nothing but computing\\nGaussian conditionals of joint distributions (Deisenroth and Ohlsson,\\n2011; S ¨arkk¨a, 2013).\\nGaussian processes (Rasmussen and Williams, 2006), which are a prac-\\ntical implementation of a distribution over functions. In a Gaussian pro-\\ncess, we make assumptions of joint Gaussianity of random variables. By\\n(Gaussian) conditioning on observed data, we can determine a poste-\\nrior distribution over functions.\\nLatent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\\nphy, 2012), which include probabilistic principal component analysis\\n(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\\ntail in Section 10.7.\\n♦\\nThe marginal distribution p(x)of a joint Gaussian distribution p(x,y)\\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\\n(6.20) and given by\\np(x) =∫\\np(x,y)dy=N(x|µx,Σxx). (6.68)\\nThe corresponding result holds for p(y), which is obtained by marginaliz-\\ning with respect to x. Intuitively, looking at the joint distribution in (6.64),\\nwe ignore (i.e., integrate out) everything we are not interested in. This is\\nillustrated in Figure 6.9(b).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e1f939a-1ac7-46ef-bda6-8ff51a2eacbc', embedding=None, metadata={'page_label': '200', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='200 Probability and Distributions\\nExample 6.6\\nFigure 6.9\\n(a) Bivariate\\nGaussian;\\n(b) marginal of a\\njoint Gaussian\\ndistribution is\\nGaussian; (c) the\\nconditional\\ndistribution of a\\nGaussian is also\\nGaussian.\\n−1 0 1\\nx1−4−202468x2\\nx2=−1\\n(a) Bivariate Gaussian.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx10.00.20.40.6p(x1)\\nMean\\n2σ\\n(b) Marginal distribution.\\n−1.5−1.0−0.5 0.0 0.5 1.0 1.5\\nx10.00.20.40.60.81.01.2p(x1|x2=−1)\\nMean\\n2σ (c) Conditional distribution.\\nConsider the bivariate Gaussian distribution (illustrated in Figure 6.9):\\np(x1,x2) =N([0\\n2]\\n,[0.3−1\\n−1 5])\\n. (6.69)\\nWe can compute the parameters of the univariate Gaussian, conditioned\\nonx2=−1, by applying (6.66) and (6.67) to obtain the mean and vari-\\nance respectively. Numerically, this is\\nµx1|x2=−1= 0 + (−1)·0.2·(−1−2) = 0.6 (6.70)\\nand\\nσ2\\nx1|x2=−1= 0.3−(−1)·0.2·(−1) = 0.1. (6.71)\\nTherefore, the conditional Gaussian is given by\\np(x1|x2=−1) =N(0.6,0.1). (6.72)\\nThe marginal distribution p(x1), in contrast, can be obtained by apply-\\ning (6.68), which is essentially using the mean and variance of the random\\nvariablex1, giving us\\np(x1) =N(0,0.3). (6.73)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6335385b-ca7e-43e7-9678-c55440639e72', embedding=None, metadata={'page_label': '201', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Gaussian Distribution 201\\n6.5.2 Product of Gaussian Densities\\nFor linear regression (Chapter 9), we need to compute a Gaussian likeli-\\nhood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\\nWe apply Bayes’ Theorem to compute the posterior, which results in a mul-\\ntiplication of the likelihood and the prior, that is, the multiplication of two\\nGaussian densities. The product of two GaussiansN(x|a,A)N(x|b,B)\\nThe derivation is an\\nexercise at the end\\nof this chapter.is a Gaussian distribution scaled by a c∈R, given bycN(x|c,C)\\nwith\\nC= (A−1+B−1)−1(6.74)\\nc=C(A−1a+B−1b) (6.75)\\nc= (2π)−D\\n2|A+B|−1\\n2exp(−1\\n2(a−b)⊤(A+B)−1(a−b)).(6.76)\\nThe scaling constant citself can be written in the form of a Gaussian\\ndensity either in aor inbwith an “inﬂated” covariance matrix A+B,\\ni.e.,c=N(a|b,A+B)=N(b|a,A+B)\\n.\\nRemark. For notation convenience, we will sometimes use N(x|m,S)\\nto describe the functional form of a Gaussian density even if xis not a\\nrandom variable. We have just done this in the preceding demonstration\\nwhen we wrote\\nc=N(a|b,A+B)=N(b|a,A+B). (6.77)\\nHere, neither anorbare random variables. However, writing cin this way\\nis more compact than (6.76). ♦\\n6.5.3 Sums and Linear Transformations\\nIfX,Y are independent Gaussian random variables (i.e., the joint distri-\\nbution is given as p(x,y) =p(x)p(y)) withp(x) =N(x|µx,Σx)\\nand\\np(y) =N(y|µy,Σy)\\n, thenx+yis also Gaussian distributed and given\\nby\\np(x+y) =N(µx+µy,Σx+Σy). (6.78)\\nKnowing that p(x+y)is Gaussian, the mean and covariance matrix can\\nbe determined immediately using the results from (6.46) through (6.49).\\nThis property will be important when we consider i.i.d. Gaussian noise\\nacting on random variables, as is the case for linear regression (Chap-\\nter 9).\\nExample 6.7\\nSince expectations are linear operations, we can obtain the weighted sum\\nof independent Gaussian random variables\\np(ax+by) =N(aµx+bµy, a2Σx+b2Σy). (6.79)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0750f880-f23b-41c7-8a4f-d85535fe22b2', embedding=None, metadata={'page_label': '202', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='202 Probability and Distributions\\nRemark. A case that will be useful in Chapter 11 is the weighted sum of\\nGaussian densities. This is different from the weighted sum of Gaussian\\nrandom variables. ♦\\nIn Theorem 6.12, the random variable xis from a density that is a\\nmixture of two densities p1(x)andp2(x), weighted by α. The theorem can\\nbe generalized to the multivariate random variable case, since linearity of\\nexpectations holds also for multivariate random variables. However, the\\nidea of a squared random variable needs to be replaced by xx⊤.\\nTheorem 6.12. Consider a mixture of two univariate Gaussian densities\\np(x) =αp1(x) + (1−α)p2(x), (6.80)\\nwhere the scalar 0<α< 1is the mixture weight, and p1(x)andp2(x)are\\nunivariate Gaussian densities (Equation (6.62) ) with different parameters,\\ni.e.,(µ1,σ2\\n1)̸= (µ2,σ2\\n2).\\nThen the mean of the mixture density p(x)is given by the weighted sum\\nof the means of each random variable:\\nE[x] =αµ1+ (1−α)µ2. (6.81)\\nThe variance of the mixture density p(x)is given by\\nV[x] =[ασ2\\n1+ (1−α)σ2\\n2]+([αµ2\\n1+ (1−α)µ2\\n2]−[αµ1+ (1−α)µ2]2)\\n.\\n(6.82)\\nProof The mean of the mixture density p(x)is given by the weighted\\nsum of the means of each random variable. We apply the deﬁnition of the\\nmean (Deﬁnition 6.4), and plug in our mixture (6.80), which yields\\nE[x] =∫∞\\n−∞xp(x)dx (6.83a)\\n=∫∞\\n−∞(αxp 1(x) + (1−α)xp2(x)) dx (6.83b)\\n=α∫∞\\n−∞xp1(x)dx+ (1−α)∫∞\\n−∞xp2(x)dx (6.83c)\\n=αµ1+ (1−α)µ2. (6.83d)\\nTo compute the variance, we can use the raw-score version of the vari-\\nance from (6.44), which requires an expression of the expectation of the\\nsquared random variable. Here we use the deﬁnition of an expectation of\\na function (the square) of a random variable (Deﬁnition 6.3),\\nE[x2] =∫∞\\n−∞x2p(x)dx (6.84a)\\n=∫∞\\n−∞(αx2p1(x) + (1−α)x2p2(x))dx (6.84b)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5989b33a-c8f6-4ac8-a9f2-7089fa1431d0', embedding=None, metadata={'page_label': '203', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.5 Gaussian Distribution 203\\n=α∫∞\\n−∞x2p1(x)dx+ (1−α)∫∞\\n−∞x2p2(x)dx (6.84c)\\n=α(µ2\\n1+σ2\\n1) + (1−α)(µ2\\n2+σ2\\n2), (6.84d)\\nwhere in the last equality, we again used the raw-score version of the\\nvariance (6.44) giving σ2=E[x2]−µ2. This is rearranged such that the\\nexpectation of a squared random variable is the sum of the squared mean\\nand the variance.\\nTherefore, the variance is given by subtracting (6.83d) from (6.84d),\\nV[x] =E[x2]−(E[x])2(6.85a)\\n=α(µ2\\n1+σ2\\n1) + (1−α)(µ2\\n2+σ2\\n2)−(αµ1+ (1−α)µ2)2(6.85b)\\n=[ασ2\\n1+ (1−α)σ2\\n2]\\n+([αµ2\\n1+ (1−α)µ2\\n2]−[αµ1+ (1−α)µ2]2)\\n. (6.85c)\\nRemark. The preceding derivation holds for any density, but since the\\nGaussian is fully determined by the mean and variance, the mixture den-\\nsity can be determined in closed form. ♦\\nFor a mixture density, the individual components can be considered\\nto be conditional distributions (conditioned on the component identity).\\nEquation (6.85c) is an example of the conditional variance formula, also\\nknown as the law of total variance , which generally states that for two ran- law of total variance\\ndom variables XandYit holds that VX[x] =EY[VX[x|y]]+VY[EX[x|y]],\\ni.e., the (total) variance of Xis the expected conditional variance plus the\\nvariance of a conditional mean.\\nWe consider in Example 6.17 a bivariate standard Gaussian random\\nvariableXand performed a linear transformation Axon it. The outcome\\nis a Gaussian random variable with mean zero and covariance AA⊤. Ob-\\nserve that adding a constant vector will change the mean of the distribu-\\ntion, without affecting its variance, that is, the random variable x+µis\\nGaussian with mean µand identity covariance. Hence, any linear/afﬁne\\ntransformation of a Gaussian random variable is Gaussian distributed. Any linear/afﬁne\\ntransformation of a\\nGaussian random\\nvariable is also\\nGaussian\\ndistributed.Consider a Gaussian distributed random variable X∼N(µ,Σ)\\n. For\\na given matrix Aof appropriate shape, let Ybe a random variable such\\nthaty=Axis a transformed version of x. We can compute the mean of\\nyby exploiting that the expectation is a linear operator (6.50) as follows:\\nE[y] =E[Ax] =AE[x] =Aµ. (6.86)\\nSimilarly the variance of ycan be found by using (6.51):\\nV[y] =V[Ax] =AV[x]A⊤=AΣA⊤. (6.87)\\nThis means that the random variable yis distributed according to\\np(y) =N(y|Aµ,AΣA⊤). (6.88)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c55a3fc-d757-45a3-9522-c2431ca8599a', embedding=None, metadata={'page_label': '204', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='204 Probability and Distributions\\nLet us now consider the reverse transformation: when we know that a\\nrandom variable has a mean that is a linear transformation of another\\nrandom variable. For a given full rank matrix A∈RM×N, whereM⩾N,\\nlety∈RMbe a Gaussian random variable with mean Ax, i.e.,\\np(y) =N(y|Ax,Σ). (6.89)\\nWhat is the corresponding probability distribution p(x)? IfAis invert-\\nible, then we can write x=A−1yand apply the transformation in the\\nprevious paragraph. However, in general Ais not invertible, and we use\\nan approach similar to that of the pseudo-inverse (3.57). That is, we pre-\\nmultiply both sides with A⊤and then invert A⊤A, which is symmetric\\nand positive deﬁnite, giving us the relation\\ny=Ax⇐⇒ (A⊤A)−1A⊤y=x. (6.90)\\nHence,xis a linear transformation of y, and we obtain\\np(x) =N(x|(A⊤A)−1A⊤y,(A⊤A)−1A⊤ΣA(A⊤A)−1).(6.91)\\n6.5.4 Sampling from Multivariate Gaussian Distributions\\nWe will not explain the subtleties of random sampling on a computer, and\\nthe interested reader is referred to Gentle (2004). In the case of a mul-\\ntivariate Gaussian, this process consists of three stages: ﬁrst, we need a\\nsource of pseudo-random numbers that provide a uniform sample in the\\ninterval [0,1]; second, we use a non-linear transformation such as the\\nBox-M ¨uller transform (Devroye, 1986) to obtain a sample from a univari-\\nate Gaussian; and third, we collate a vector of these samples to obtain a\\nsample from a multivariate standard normal N(0,I)\\n.\\nFor a general multivariate Gaussian, that is, where the mean is non\\nzero and the covariance is not the identity matrix, we use the proper-\\nties of linear transformations of a Gaussian random variable. Assume we\\nare interested in generating samples xi,i= 1,...,n, from a multivariate\\nGaussian distribution with mean µand covariance matrix Σ. We would To compute the\\nCholesky\\nfactorization of a\\nmatrix, it is required\\nthat the matrix is\\nsymmetric and\\npositive deﬁnite\\n(Section 3.2.3).\\nCovariance matrices\\npossess this\\nproperty.like to construct the sample from a sampler that provides samples from\\nthe multivariate standard normal N(0,I)\\n.\\nTo obtain samples from a multivariate normal N(µ,Σ)\\n, we can use\\nthe properties of a linear transformation of a Gaussian random variable:\\nIfx∼N(0,I)\\n, theny=Ax+µ, whereAA⊤=Σis Gaussian dis-\\ntributed with mean µand covariance matrix Σ. One convenient choice of\\nAis to use the Cholesky decomposition (Section 4.3) of the covariance\\nmatrix Σ=AA⊤. The Cholesky decomposition has the beneﬁt that Ais\\ntriangular, leading to efﬁcient computation.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='269fe4c0-a5e0-459f-a935-91bbe31c72fc', embedding=None, metadata={'page_label': '205', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Conjugacy and the Exponential Family 205\\n6.6 Conjugacy and the Exponential Family\\nMany of the probability distributions “with names” that we ﬁnd in statis-\\ntics textbooks were discovered to model particular types of phenomena.\\nFor example, we have seen the Gaussian distribution in Section 6.5. The\\ndistributions are also related to each other in complex ways (Leemis and\\nMcQueston, 2008). For a beginner in the ﬁeld, it can be overwhelming to\\nﬁgure out which distribution to use. In addition, many of these distribu-\\ntions were discovered at a time that statistics and computation were done “Computers” used to\\nbe a job description. by pencil and paper. It is natural to ask what are meaningful concepts\\nin the computing age (Efron and Hastie, 2016). In the previous section,\\nwe saw that many of the operations required for inference can be conve-\\nniently calculated when the distribution is Gaussian. It is worth recalling\\nat this point the desiderata for manipulating probability distributions in\\nthe machine learning context:\\n1. There is some “closure property” when applying the rules of probability,\\ne.g., Bayes’ theorem. By closure, we mean that applying a particular\\noperation returns an object of the same type.\\n2. As we collect more data, we do not need more parameters to describe\\nthe distribution.\\n3. Since we are interested in learning from data, we want parameter es-\\ntimation to behave nicely.\\nIt turns out that the class of distributions called the exponential family exponential family\\nprovides the right balance of generality while retaining favorable compu-\\ntation and inference properties. Before we introduce the exponential fam-\\nily, let us see three more members of “named” probability distributions,\\nthe Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\\nple 6.10) distributions.\\nExample 6.8\\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\\ndistribution variableXwith statex∈{0,1}. It is governed by a single continuous pa-\\nrameterµ∈[0,1]that represents the probability of X= 1. The Bernoulli\\ndistribution Ber (µ)is deﬁned as\\np(x|µ) =µx(1−µ)1−x, x∈{0,1}, (6.92)\\nE[x] =µ, (6.93)\\nV[x] =µ(1−µ), (6.94)\\nwhere E[x]andV[x]are the mean and variance of the binary random\\nvariableX.\\nAn example where the Bernoulli distribution can be used is when we\\nare interested in modeling the probability of “heads” when ﬂipping a coin.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='421d30cf-d24f-489e-9411-49fd24663963', embedding=None, metadata={'page_label': '206', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='206 Probability and Distributions\\nFigure 6.10\\nExamples of the\\nBinomial\\ndistribution for\\nµ∈{0.1,0.4,0.75}\\nandN= 15 .\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\\nNumbermof observations x= 1 inN= 15 experiments0.00.10.20.3p(m)µ= 0.1\\nµ= 0.4\\nµ= 0.75\\nRemark. The rewriting above of the Bernoulli distribution, where we use\\nBoolean variables as numerical 0or1and express them in the exponents,\\nis a trick that is often used in machine learning textbooks. Another oc-\\ncurence of this is when expressing the Multinomial distribution. ♦\\nExample 6.9 (Binomial Distribution)\\nThe Binomial distribution is a generalization of the Bernoulli distribution Binomial\\ndistribution to a distribution over integers (illustrated in Figure 6.10). In particular,\\nthe Binomial can be used to describe the probability of observing moc-\\ncurrences of X= 1 in a set ofNsamples from a Bernoulli distribution\\nwherep(X= 1) =µ∈[0,1]. The Binomial distribution Bin (N,µ)is\\ndeﬁned as\\np(m|N,µ) =(\\nN\\nm)\\nµm(1−µ)N−m, (6.95)\\nE[m] =Nµ, (6.96)\\nV[m] =Nµ(1−µ), (6.97)\\nwhere E[m]andV[m]are the mean and variance of m, respectively.\\nAn example where the Binomial could be used is if we want to describe\\nthe probability of observing m“heads” inNcoin-ﬂip experiments if the\\nprobability for observing head in a single experiment is µ.\\nExample 6.10 (Beta Distribution)\\nWe may wish to model a continuous random variable on a ﬁnite interval.\\nThe Beta distribution is a distribution over a continuous random variable Beta distribution\\nµ∈[0,1], which is often used to represent the probability for some binary\\nevent (e.g., the parameter governing the Bernoulli distribution). The Beta\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73d323f9-6ebb-4e2d-b0e1-e95578ff7318', embedding=None, metadata={'page_label': '207', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Conjugacy and the Exponential Family 207\\ndistribution Beta (α,β)(illustrated in Figure 6.11) itself is governed by\\ntwo parameters α>0, β > 0and is deﬁned as\\np(µ|α,β) =Γ(α+β)\\nΓ(α)Γ(β)µα−1(1−µ)β−1(6.98)\\nE[µ] =α\\nα+β,V[µ] =αβ\\n(α+β)2(α+β+ 1)(6.99)\\nwhere Γ(·)is the Gamma function deﬁned as\\nΓ(t) :=∫∞\\n0xt−1exp(−x)dx, t> 0. (6.100)\\nΓ(t+ 1) =tΓ(t). (6.101)\\nNote that the fraction of Gamma functions in (6.98) normalizes the Beta\\ndistribution.\\nFigure 6.11\\nExamples of the\\nBeta distribution for\\ndifferent values of α\\nandβ.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nµ0246810p(µ|α,β)α= 0.5 =β\\nα= 1 =β\\nα= 2,β= 0.3\\nα= 4,β= 10\\nα= 5,β= 1\\nIntuitively,αmoves probability mass toward 1, whereasβmoves prob-\\nability mass toward 0. There are some special cases (Murphy, 2012):\\nForα= 1 =β, we obtain the uniform distribution U[0,1].\\nForα,β < 1, we get a bimodal distribution with spikes at 0and1.\\nForα,β > 1, the distribution is unimodal.\\nForα,β > 1andα=β, the distribution is unimodal, symmetric, and\\ncentered in the interval [0,1], i.e., the mode/mean is at1\\n2.\\nRemark. There is a whole zoo of distributions with names, and they are\\nrelated in different ways to each other (Leemis and McQueston, 2008).\\nIt is worth keeping in mind that each named distribution is created for a\\nparticular reason, but may have other applications. Knowing the reason\\nbehind the creation of a particular distribution often allows insight into\\nhow to best use it. We introduced preceding three distributions to be able\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c7e8972-c404-4cd6-90e6-b89165a2dee3', embedding=None, metadata={'page_label': '208', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='208 Probability and Distributions\\nto illustrate the concepts of conjugacy (Section 6.6.1) and exponential\\nfamilies (Section 6.6.3). ♦\\n6.6.1 Conjugacy\\nAccording to Bayes’ theorem (6.23), the posterior is proportional to the\\nproduct of the prior and the likelihood. The speciﬁcation of the prior can\\nbe tricky for two reasons: First, the prior should encapsulate our knowl-\\nedge about the problem before we see any data. This is often difﬁcult to\\ndescribe. Second, it is often not possible to compute the posterior distribu-\\ntion analytically. However, there are some priors that are computationally\\nconvenient: conjugate priors . conjugate prior\\nDeﬁnition 6.13 (Conjugate Prior) .A prior is conjugate for the likelihood conjugate\\nfunction if the posterior is of the same form/type as the prior.\\nConjugacy is particularly convenient because we can algebraically cal-\\nculate our posterior distribution by updating the parameters of the prior\\ndistribution.\\nRemark. When considering the geometry of probability distributions, con-\\njugate priors retain the same distance structure as the likelihood (Agarwal\\nand Daum ´e III, 2010). ♦\\nTo introduce a concrete example of conjugate priors, we describe in Ex-\\nample 6.11 the Binomial distribution (deﬁned on discrete random vari-\\nables) and the Beta distribution (deﬁned on continuous random vari-\\nables).\\nExample 6.11 (Beta-Binomial Conjugacy)\\nConsider a Binomial random variable x∼Bin(N,µ)where\\np(x|N,µ) =(\\nN\\nx)\\nµx(1−µ)N−x, x = 0,1,...,N, (6.102)\\nis the probability of ﬁnding xtimes the outcome “heads” in Ncoin ﬂips,\\nwhereµis the probability of a “head”. We place a Beta prior on the pa-\\nrameterµ, that is,µ∼Beta(α,β), where\\np(µ|α,β) =Γ(α+β)\\nΓ(α)Γ(β)µα−1(1−µ)β−1. (6.103)\\nIf we now observe some outcome x=h, that is, we see hheads inNcoin\\nﬂips, we compute the posterior distribution on µas\\np(µ|x=h,N,α,β )∝p(x|N,µ)p(µ|α,β) (6.104a)\\n∝µh(1−µ)(N−h)µα−1(1−µ)β−1(6.104b)\\n=µh+α−1(1−µ)(N−h)+β−1(6.104c)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e8dd501-6882-4dba-a976-951e1df3832a', embedding=None, metadata={'page_label': '209', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Conjugacy and the Exponential Family 209\\nTable 6.2 Examples\\nof conjugate priors\\nfor common\\nlikelihood functions.Likelihood Conjugate prior Posterior\\nBernoulli Beta Beta\\nBinomial Beta Beta\\nGaussian Gaussian/inverse Gamma Gaussian/inverse Gamma\\nGaussian Gaussian/inverse Wishart Gaussian/inverse Wishart\\nMultinomial Dirichlet Dirichlet\\n∝Beta(h+α,N−h+β), (6.104d)\\ni.e., the posterior distribution is a Beta distribution as the prior, i.e., the\\nBeta prior is conjugate for the parameter µin the Binomial likelihood\\nfunction.\\nIn the following example, we will derive a result that is similar to the\\nBeta-Binomial conjugacy result. Here we will show that the Beta distribu-\\ntion is a conjugate prior for the Bernoulli distribution.\\nExample 6.12 (Beta-Bernoulli Conjugacy)\\nLetx∈{0,1}be distributed according to the Bernoulli distribution with\\nparameterθ∈[0,1], that is,p(x= 1|θ) =θ. This can also be expressed\\nasp(x|θ) =θx(1−θ)1−x. Letθbe distributed according to a Beta distri-\\nbution with parameters α,β, that is,p(θ|α,β)∝θα−1(1−θ)β−1.\\nMultiplying the Beta and the Bernoulli distributions, we get\\np(θ|x,α,β ) =p(x|θ)p(θ|α,β) (6.105a)\\n∝θx(1−θ)1−xθα−1(1−θ)β−1(6.105b)\\n=θα+x−1(1−θ)β+(1−x)−1(6.105c)\\n∝p(θ|α+x,β+ (1−x)). (6.105d)\\nThe last line is the Beta distribution with parameters (α+x,β+ (1−x)).\\nTable 6.2 lists examples for conjugate priors for the parameters of some\\nstandard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is\\nconjugate for the\\nprecision (inverse\\nvariance) in the\\nunivariate Gaussian\\nlikelihood, and the\\nWishart prior is\\nconjugate for the\\nprecision matrix\\n(inverse covariance\\nmatrix) in the\\nmultivariate\\nGaussian likelihood.Multinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found\\nin any statistical text, and are described in Bishop (2006), for example.\\nThe Beta distribution is the conjugate prior for the parameter µin both\\nthe Binomial and the Bernoulli likelihood. For a Gaussian likelihood func-\\ntion, we can place a conjugate Gaussian prior on the mean. The reason\\nwhy the Gaussian likelihood appears twice in the table is that we need\\nto distinguish the univariate from the multivariate case. In the univariate\\n(scalar) case, the inverse Gamma is the conjugate prior for the variance.\\nIn the multivariate case, we use a conjugate inverse Wishart distribution\\nas a prior on the covariance matrix. The Dirichlet distribution is the conju-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73a2e8bf-6882-429d-87ed-95c5f16af300', embedding=None, metadata={'page_label': '210', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='210 Probability and Distributions\\ngate prior for the multinomial likelihood function. For further details, we\\nrefer to Bishop (2006).\\n6.6.2 Sufﬁcient Statistics\\nRecall that a statistic of a random variable is a deterministic function of\\nthat random variable. For example, if x= [x1,...,xN]⊤is a vector of\\nunivariate Gaussian random variables, that is, xn∼N(µ, σ2)\\n, then the\\nsample mean ˆµ=1\\nN(x1+···+xN)is a statistic. Sir Ronald Fisher dis-\\ncovered the notion of sufﬁcient statistics : the idea that there are statistics sufﬁcient statistics\\nthat will contain all available information that can be inferred from data\\ncorresponding to the distribution under consideration. In other words, suf-\\nﬁcient statistics carry all the information needed to make inference about\\nthe population, that is, they are the statistics that are sufﬁcient to repre-\\nsent the distribution.\\nFor a set of distributions parametrized by θ, letXbe a random variable\\nwith distribution p(x|θ0)given an unknown θ0. A vectorφ(x)of statistics\\nis called sufﬁcient statistics for θ0if they contain all possible informa-\\ntion aboutθ0. To be more formal about “contain all possible information”,\\nthis means that the probability of xgivenθcan be factored into a part\\nthat does not depend on θ, and a part that depends on θonly viaφ(x).\\nThe Fisher-Neyman factorization theorem formalizes this notion, which\\nwe state in Theorem 6.14 without proof.\\nTheorem 6.14 (Fisher-Neyman) .[Theorem 6.5 in Lehmann and Casella\\n(1998)] Let Xhave probability density function p(x|θ). Then the statistics Fisher-Neyman\\ntheorem φ(x)are sufﬁcient for θif and only if p(x|θ)can be written in the form\\np(x|θ) =h(x)gθ(φ(x)), (6.106)\\nwhereh(x)is a distribution independent of θandgθcaptures all the depen-\\ndence onθvia sufﬁcient statistics φ(x).\\nIfp(x|θ)does not depend on θ, thenφ(x)is trivially a sufﬁcient statistic\\nfor any function φ. The more interesting case is that p(x|θ)is dependent\\nonly onφ(x)and notxitself. In this case, φ(x)is a sufﬁcient statistic for\\nθ.\\nIn machine learning, we consider a ﬁnite number of samples from a\\ndistribution. One could imagine that for simple distributions (such as the\\nBernoulli in Example 6.8) we only need a small number of samples to\\nestimate the parameters of the distributions. We could also consider the\\nopposite problem: If we have a set of data (a sample from an unknown\\ndistribution), which distribution gives the best ﬁt? A natural question to\\nask is, as we observe more data, do we need more parameters θto de-\\nscribe the distribution? It turns out that the answer is yes in general, and\\nthis is studied in non-parametric statistics (Wasserman, 2007). A converse\\nquestion is to consider which class of distributions have ﬁnite-dimensional\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0aa84c14-bf25-467a-8d60-df65b5a21c93', embedding=None, metadata={'page_label': '211', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Conjugacy and the Exponential Family 211\\nsufﬁcient statistics, that is the number of parameters needed to describe\\nthem does not increase arbitrarily. The answer is exponential family dis-\\ntributions, described in the following section.\\n6.6.3 Exponential Family\\nThere are three possible levels of abstraction we can have when con-\\nsidering distributions (of discrete or continuous random variables). At\\nlevel one (the most concrete end of the spectrum), we have a particu-\\nlar named distribution with ﬁxed parameters, for example a univariate\\nGaussianN(0,1)\\nwith zero mean and unit variance. In machine learning,\\nwe often use the second level of abstraction, that is, we ﬁx the paramet-\\nric form (the univariate Gaussian) and infer the parameters from data. For\\nexample, we assume a univariate Gaussian N(µ, σ2)\\nwith unknown mean\\nµand unknown variance σ2, and use a maximum likelihood ﬁt to deter-\\nmine the best parameters (µ,σ2). We will see an example of this when\\nconsidering linear regression in Chapter 9. A third level of abstraction is\\nto consider families of distributions, and in this book, we consider the ex-\\nponential family. The univariate Gaussian is an example of a member of\\nthe exponential family. Many of the widely used statistical models, includ-\\ning all the “named” models in Table 6.2, are members of the exponential\\nfamily. They can all be uniﬁed into one concept (Brown, 1986).\\nRemark. A brief historical anecdote: Like many concepts in mathemat-\\nics and science, exponential families were independently discovered at\\nthe same time by different researchers. In the years 1935–1936, Edwin\\nPitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in\\nNew York independently showed that the exponential families are the only\\nfamilies that enjoy ﬁnite-dimensional sufﬁcient statistics under repeated\\nindependent sampling (Lehmann and Casella, 1998). ♦\\nAnexponential family is a family of probability distributions, parame- exponential family\\nterized byθ∈RD, of the form\\np(x|θ) =h(x) exp (⟨θ,φ(x)⟩−A(θ)), (6.107)\\nwhereφ(x)is the vector of sufﬁcient statistics. In general, any inner prod-\\nuct (Section 3.2) can be used in (6.107), and for concreteness we will use\\nthe standard dot product here ( ⟨θ,φ(x)⟩=θ⊤φ(x)). Note that the form\\nof the exponential family is essentially a particular expression of gθ(φ(x))\\nin the Fisher-Neyman theorem (Theorem 6.14).\\nThe factorh(x)can be absorbed into the dot product term by adding\\nanother entry ( logh(x)) to the vector of sufﬁcient statistics φ(x), and\\nconstraining the corresponding parameter θ0= 1. The termA(θ)is the\\nnormalization constant that ensures that the distribution sums up or inte-\\ngrates to one and is called the log-partition function . A good intuitive no- log-partition\\nfunction tion of exponential families can be obtained by ignoring these two terms\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3369b220-48e6-49f5-a919-f2e3c5543ebb', embedding=None, metadata={'page_label': '212', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='212 Probability and Distributions\\nand considering exponential families as distributions of the form\\np(x|θ)∝exp(θ⊤φ(x)). (6.108)\\nFor this form of parametrization, the parameters θare called the natural natural parameters\\nparameters . At ﬁrst glance, it seems that exponential families are a mun-\\ndane transformation by adding the exponential function to the result of a\\ndot product. However, there are many implications that allow for conve-\\nnient modeling and efﬁcient computation based on the fact that we can\\ncapture information about data in φ(x).\\nExample 6.13 (Gaussian as Exponential Family)\\nConsider the univariate Gaussian distribution N(µ, σ2)\\n. Letφ(x) =[x\\nx2]\\n.\\nThen by using the deﬁnition of the exponential family,\\np(x|θ)∝exp(θ1x+θ2x2). (6.109)\\nSetting\\nθ=[µ\\nσ2,−1\\n2σ2]⊤\\n(6.110)\\nand substituting into (6.109), we obtain\\np(x|θ)∝exp(µx\\nσ2−x2\\n2σ2)\\n∝exp(\\n−1\\n2σ2(x−µ)2)\\n. (6.111)\\nTherefore, the univariate Gaussian distribution is a member of the expo-\\nnential family with sufﬁcient statistic φ(x) =[x\\nx2]\\n, and natural parame-\\nters given byθin (6.110).\\nExample 6.14 (Bernoulli as Exponential Family)\\nRecall the Bernoulli distribution from Example 6.8\\np(x|µ) =µx(1−µ)1−x, x∈{0,1}. (6.112)\\nThis can be written in exponential family form\\np(x|µ) = exp[log(µx(1−µ)1−x)]\\n(6.113a)\\n= exp [xlogµ+ (1−x) log(1−µ)] (6.113b)\\n= exp [xlogµ−xlog(1−µ) + log(1−µ)] (6.113c)\\n= exp[\\nxlogµ\\n1−µ+ log(1−µ)]\\n. (6.113d)\\nThe last line (6.113d) can be identiﬁed as being in exponential family\\nform (6.107) by observing that\\nh(x) = 1 (6.114)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4e7fd89-00f1-47df-bdcb-2e9dd89e7a2a', embedding=None, metadata={'page_label': '213', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.6 Conjugacy and the Exponential Family 213\\nθ= logµ\\n1−µ(6.115)\\nφ(x) =x (6.116)\\nA(θ) =−log(1−µ) = log(1 + exp( θ)). (6.117)\\nThe relationship between θandµis invertible so that\\nµ=1\\n1 + exp(−θ). (6.118)\\nThe relation (6.118) is used to obtain the right equality of (6.117).\\nRemark. The relationship between the original Bernoulli parameter µand\\nthe natural parameter θis known as the sigmoid or logistic function. Ob- sigmoid\\nserve thatµ∈(0,1)butθ∈R, and therefore the sigmoid function\\nsqueezes a real value into the range (0,1). This property is useful in ma-\\nchine learning, for example it is used in logistic regression (Bishop, 2006,\\nsection 4.3.2), as well as as a nonlinear activation functions in neural net-\\nworks (Goodfellow et al., 2016, chapter 6). ♦\\nIt is often not obvious how to ﬁnd the parametric form of the conjugate\\ndistribution of a particular distribution (for example, those in Table 6.2).\\nExponential families provide a convenient way to ﬁnd conjugate pairs of\\ndistributions. Consider the random variable Xis a member of the expo-\\nnential family (6.107):\\np(x|θ) =h(x) exp (⟨θ,φ(x)⟩−A(θ)). (6.119)\\nEvery member of the exponential family has a conjugate prior (Brown,\\n1986)\\np(θ|γ) =hc(θ) exp(⟨[γ1\\nγ2]\\n,[θ\\n−A(θ)]⟩\\n−Ac(γ))\\n, (6.120)\\nwhereγ=[γ1\\nγ2]\\nhas dimension dim(θ) + 1 . The sufﬁcient statistics of\\nthe conjugate prior are[θ\\n−A(θ)]\\n. By using the knowledge of the general\\nform of conjugate priors for exponential families, we can derive functional\\nforms of conjugate priors corresponding to particular distributions.\\nExample 6.15\\nRecall the exponential family form of the Bernoulli distribution (6.113d)\\np(x|µ) = exp[\\nxlogµ\\n1−µ+ log(1−µ)]\\n. (6.121)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bed37aaf-3422-4e3b-baf8-87483dc1aa32', embedding=None, metadata={'page_label': '214', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='214 Probability and Distributions\\nThe canonical conjugate prior has the form\\np(µ|α,β) =µ\\n1−µexp[\\nαlogµ\\n1−µ+ (β+α) log(1−µ)−Ac(γ)]\\n,\\n(6.122)\\nwhere we deﬁned γ:= [α,β+α]⊤andhc(µ) :=µ/(1−µ). Equa-\\ntion (6.122) then simpliﬁes to\\np(µ|α,β) = exp [(α−1) logµ+ (β−1) log(1−µ)−Ac(α,β)].\\n(6.123)\\nPutting this in non-exponential family form yields\\np(µ|α,β)∝µα−1(1−µ)β−1, (6.124)\\nwhich we identify as the Beta distribution (6.98). In example 6.12, we\\nassumed that the Beta distribution is the conjugate prior of the Bernoulli\\ndistribution and showed that it was indeed the conjugate prior. In this\\nexample, we derived the form of the Beta distribution by looking at the\\ncanonical conjugate prior of the Bernoulli distribution in exponential fam-\\nily form.\\nAs mentioned in the previous section, the main motivation for expo-\\nnential families is that they have ﬁnite-dimensional sufﬁcient statistics.\\nAdditionally, conjugate distributions are easy to write down, and the con-\\njugate distributions also come from an exponential family. From an infer-\\nence perspective, maximum likelihood estimation behaves nicely because\\nempirical estimates of sufﬁcient statistics are optimal estimates of the pop-\\nulation values of sufﬁcient statistics (recall the mean and covariance of a\\nGaussian). From an optimization perspective, the log-likelihood function\\nis concave, allowing for efﬁcient optimization approaches to be applied\\n(Chapter 7).\\n6.7 Change of Variables/Inverse Transform\\nIt may seem that there are very many known distributions, but in reality\\nthe set of distributions for which we have names is quite limited. There-\\nfore, it is often useful to understand how transformed random variables\\nare distributed. For example, assuming that Xis a random variable dis-\\ntributed according to the univariate normal distribution N(0,1)\\n, what is\\nthe distribution of X2? Another example, which is quite common in ma-\\nchine learning, is, given that X1andX2are univariate standard normal,\\nwhat is the distribution of1\\n2(X1+X2)?\\nOne option to work out the distribution of1\\n2(X1+X2)is to calculate\\nthe mean and variance of X1andX2and then combine them. As we saw\\nin Section 6.4.4, we can calculate the mean and variance of resulting ran-\\ndom variables when we consider afﬁne transformations of random vari-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca81e807-b62d-4af6-94b3-78b5858122e2', embedding=None, metadata={'page_label': '215', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.7 Change of Variables/Inverse Transform 215\\nables. However, we may not be able to obtain the functional form of the\\ndistribution under transformations. Furthermore, we may be interested\\nin nonlinear transformations of random variables for which closed-form\\nexpressions are not readily available.\\nRemark (Notation) .In this section, we will be explicit about random vari-\\nables and the values they take. Hence, recall that we use capital letters\\nX,Y to denote random variables and small letters x,yto denote the val-\\nues in the target space Tthat the random variables take. We will explicitly\\nwrite pmfs of discrete random variables XasP(X=x). For continuous\\nrandom variables X(Section 6.2.2), the pdf is written as f(x)and the cdf\\nis written as FX(x). ♦\\nWe will look at two approaches for obtaining distributions of transfor-\\nmations of random variables: a direct approach using the deﬁnition of a\\ncumulative distribution function and a change-of-variable approach that\\nuses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating\\nfunctions can also\\nbe used to study\\ntransformations of\\nrandom\\nvariables (Casella\\nand Berger, 2002,\\nchapter 2).proach is widely used because it provides a “recipe” for attempting to\\ncompute the resulting distribution due to a transformation. We will ex-\\nplain the techniques for univariate random variables, and will only brieﬂy\\nprovide the results for the general case of multivariate random variables.\\nTransformations of discrete random variables can be understood di-\\nrectly. Suppose that there is a discrete random variable Xwith pmfP(X=\\nx)(Section 6.2.1), and an invertible function U(x). Consider the trans-\\nformed random variable Y:=U(X), with pmfP(Y=y). Then\\nP(Y=y) =P(U(X) =y) transformation of interest (6.125a)\\n=P(X=U−1(y)) inverse (6.125b)\\nwhere we can observe that x=U−1(y). Therefore, for discrete random\\nvariables, transformations directly change the individual events (with the\\nprobabilities appropriately transformed).\\n6.7.1 Distribution Function Technique\\nThe distribution function technique goes back to ﬁrst principles, and uses\\nthe deﬁnition of a cdf FX(x) =P(X⩽x)and the fact that its differential\\nis the pdff(x)(Wasserman, 2004, chapter 2). For a random variable X\\nand a function U, we ﬁnd the pdf of the random variable Y:=U(X)by\\n1. Finding the cdf:\\nFY(y) =P(Y⩽y) (6.126)\\n2. Differentiating the cdf FY(y)to get the pdf f(y).\\nf(y) =d\\ndyFY(y). (6.127)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='898aa300-9bce-4ce1-bfa4-cbc4a6ca1025', embedding=None, metadata={'page_label': '216', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='216 Probability and Distributions\\nWe also need to keep in mind that the domain of the random variable may\\nhave changed due to the transformation by U.\\nExample 6.16\\nLetXbe a continuous random variable with probability density function\\non0⩽x⩽1\\nf(x) = 3x2. (6.128)\\nWe are interested in ﬁnding the pdf of Y=X2.\\nThe function fis an increasing function of x, and therefore the resulting\\nvalue ofylies in the interval [0,1]. We obtain\\nFY(y) =P(Y⩽y) deﬁnition of cdf (6.129a)\\n=P(X2⩽y) transformation of interest (6.129b)\\n=P(X⩽y1\\n2) inverse (6.129c)\\n=FX(y1\\n2) deﬁnition of cdf (6.129d)\\n=∫y1\\n2\\n03t2dt cdf as a deﬁnite integral (6.129e)\\n=[t3]t=y1\\n2\\nt=0result of integration (6.129f)\\n=y3\\n2,0⩽y⩽1. (6.129g)\\nTherefore, the cdf of Yis\\nFY(y) =y3\\n2 (6.130)\\nfor0⩽y⩽1. To obtain the pdf, we differentiate the cdf\\nf(y) =d\\ndyFY(y) =3\\n2y1\\n2 (6.131)\\nfor0⩽y⩽1.\\nIn Example 6.16, we considered a strictly monotonically increasing func-\\ntionf(x) = 3x2. This means that we could compute an inverse function. Functions that have\\ninverses are called\\nbijective functions\\n(Section 2.7).In general, we require that the function of interest y=U(x)has an in-\\nversex=U−1(y). A useful result can be obtained by considering the cu-\\nmulative distribution function FX(x)of a random variable X, and using\\nit as the transformation U(x). This leads to the following theorem.\\nTheorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let Xbe a\\ncontinuous random variable with a strictly monotonic cumulative distribu-\\ntion function FX(x). Then the random variable Ydeﬁned as\\nY:=FX(X) (6.132)\\nhas a uniform distribution.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a2fc545-19e2-475d-b9e8-b14c239eaa4f', embedding=None, metadata={'page_label': '217', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.7 Change of Variables/Inverse Transform 217\\nTheorem 6.15 is known as the probability integral transform , and it is probability integral\\ntransform used to derive algorithms for sampling from distributions by transforming\\nthe result of sampling from a uniform random variable (Bishop, 2006).\\nThe algorithm works by ﬁrst generating a sample from a uniform distribu-\\ntion, then transforming it by the inverse cdf (assuming this is available)\\nto obtain a sample from the desired distribution. The probability integral\\ntransform is also used for hypothesis testing whether a sample comes from\\na particular distribution (Lehmann and Romano, 2005). The idea that the\\noutput of a cdf gives a uniform distribution also forms the basis of copu-\\nlas (Nelsen, 2006).\\n6.7.2 Change of Variables\\nThe distribution function technique in Section 6.7.1 is derived from ﬁrst\\nprinciples, based on the deﬁnitions of cdfs and using properties of in-\\nverses, differentiation, and integration. This argument from ﬁrst principles\\nrelies on two facts:\\n1. We can transform the cdf of Yinto an expression that is a cdf of X.\\n2. We can differentiate the cdf to obtain the pdf.\\nLet us break down the reasoning step by step, with the goal of understand-\\ning the more general change-of-variables approach in Theorem 6.16. Change of variables\\nin probability relies\\non the\\nchange-of-variables\\nmethod in\\ncalculus (Tandra,\\n2014).Remark. The name “change of variables” comes from the idea of chang-\\ning the variable of integration when faced with a difﬁcult integral. For\\nunivariate functions, we use the substitution rule of integration,\\n∫\\nf(g(x))g′(x)dx=∫\\nf(u)du, whereu=g(x). (6.133)\\nThe derivation of this rule is based on the chain rule of calculus (5.32) and\\nby applying twice the fundamental theorem of calculus. The fundamental\\ntheorem of calculus formalizes the fact that integration and differentiation\\nare somehow “inverses” of each other. An intuitive understanding of the\\nrule can be obtained by thinking (loosely) about small changes (differen-\\ntials) to the equation u=g(x), that is by considering ∆u=g′(x)∆xas a\\ndifferential of u=g(x). By subsituting u=g(x), the argument inside the\\nintegral on the right-hand side of (6.133) becomes f(g(x)). By pretending\\nthat the term ducan be approximated by du≈∆u=g′(x)∆x, and that\\ndx≈∆x, we obtain (6.133). ♦\\nConsider a univariate random variable X, and an invertible function\\nU, which gives us another random variable Y=U(X). We assume that\\nrandom variable Xhas statesx∈[a,b]. By the deﬁnition of the cdf, we\\nhave\\nFY(y) =P(Y⩽y). (6.134)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c22e683b-bc84-4256-8211-253a4d06bd4c', embedding=None, metadata={'page_label': '218', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='218 Probability and Distributions\\nWe are interested in a function Uof the random variable\\nP(Y⩽y) =P(U(X)⩽y), (6.135)\\nwhere we assume that the function Uis invertible. An invertible function\\non an interval is either strictly increasing or strictly decreasing. In the case\\nthatUis strictly increasing, then its inverse U−1is also strictly increasing.\\nBy applying the inverse U−1to the arguments of P(U(X)⩽y), we obtain\\nP(U(X)⩽y) =P(U−1(U(X))⩽U−1(y)) =P(X⩽U−1(y)).\\n(6.136)\\nThe right-most term in (6.136) is an expression of the cdf of X. Recall the\\ndeﬁnition of the cdf in terms of the pdf\\nP(X⩽U−1(y)) =∫U−1(y)\\naf(x)dx. (6.137)\\nNow we have an expression of the cdf of Yin terms ofx:\\nFY(y) =∫U−1(y)\\naf(x)dx. (6.138)\\nTo obtain the pdf, we differentiate (6.138) with respect to y:\\nf(y) =d\\ndyFy(y) =d\\ndy∫U−1(y)\\naf(x)dx. (6.139)\\nNote that the integral on the right-hand side is with respect to x, but we\\nneed an integral with respect to ybecause we are differentiating with\\nrespect toy. In particular, we use (6.133) to get the substitution\\n∫\\nf(U−1(y))U−1′(y)dy=∫\\nf(x)dxwherex=U−1(y).(6.140)\\nUsing (6.140) on the right-hand side of (6.139) gives us\\nf(y) =d\\ndy∫U−1(y)\\nafx(U−1(y))U−1′(y)dy. (6.141)\\nWe then recall that differentiation is a linear operator and we use the\\nsubscriptxto remind ourselves that fx(U−1(y))is a function of xand not\\ny. Invoking the fundamental theorem of calculus again gives us\\nf(y) =fx(U−1(y))·(d\\ndyU−1(y))\\n. (6.142)\\nRecall that we assumed that Uis a strictly increasing function. For decreas-\\ning functions, it turns out that we have a negative sign when we follow\\nthe same derivation. We introduce the absolute value of the differential to\\nhave the same expression for both increasing and decreasing U:\\nf(y) =fx(U−1(y))·⏐⏐⏐⏐d\\ndyU−1(y)⏐⏐⏐⏐. (6.143)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='45a26751-0e98-42a3-babd-9da026f73b61', embedding=None, metadata={'page_label': '219', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.7 Change of Variables/Inverse Transform 219\\nThis is called the change-of-variable technique . The term⏐⏐⏐d\\ndyU−1(y)⏐⏐⏐in change-of-variable\\ntechnique(6.143) measures how much a unit volume changes when applying U\\n(see also the deﬁnition of the Jacobian in Section 5.3).\\nRemark. In comparison to the discrete case in (6.125b), we have an addi-\\ntional factor⏐⏐⏐d\\ndyU−1(y)⏐⏐⏐. The continuous case requires more care because\\nP(Y=y) = 0 for ally. The probability density function f(y)does not\\nhave a description as a probability of an event involving y.♦\\nSo far in this section, we have been studying univariate change of vari-\\nables. The case for multivariate random variables is analogous, but com-\\nplicated by fact that the absolute value cannot be used for multivariate\\nfunctions. Instead, we use the determinant of the Jacobian matrix. Recall\\nfrom (5.58) that the Jacobian is a matrix of partial derivatives, and that\\nthe existence of a nonzero determinant shows that we can invert the Ja-\\ncobian. Recall the discussion in Section 4.1 that the determinant arises\\nbecause our differentials (cubes of volume) are transformed into paral-\\nlelepipeds by the Jacobian. Let us summarize preceding the discussion in\\nthe following theorem, which gives us a recipe for multivariate change of\\nvariables.\\nTheorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x)be the value\\nof the probability density of the multivariate continuous random variable X.\\nIf the vector-valued function y=U(x)is differentiable and invertible for\\nall values within the domain of x, then for corresponding values of y, the\\nprobability density of Y=U(X)is given by\\nf(y) =fx(U−1(y))·⏐⏐⏐⏐det(∂\\n∂yU−1(y))⏐⏐⏐⏐. (6.144)\\nThe theorem looks intimidating at ﬁrst glance, but the key point is that\\na change of variable of a multivariate random variable follows the pro-\\ncedure of the univariate change of variable. First we need to work out\\nthe inverse transform, and substitute that into the density of x. Then we\\ncalculate the determinant of the Jacobian and multiply the result. The\\nfollowing example illustrates the case of a bivariate random variable.\\nExample 6.17\\nConsider a bivariate random variable Xwith statesx=[x1\\nx2]\\nand proba-\\nbility density function\\nf([x1\\nx2])\\n=1\\n2πexp(\\n−1\\n2[x1\\nx2]⊤[x1\\nx2])\\n. (6.145)\\nWe use the change-of-variable technique from Theorem 6.16 to derive the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17c984b1-448e-4511-9579-b2731edd0bba', embedding=None, metadata={'page_label': '220', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='220 Probability and Distributions\\neffect of a linear transformation (Section 2.7) of the random variable.\\nConsider a matrix A∈R2×2deﬁned as\\nA=[a b\\nc d]\\n. (6.146)\\nWe are interested in ﬁnding the probability density function of the trans-\\nformed bivariate random variable Ywith statesy=Ax.\\nRecall that for change of variables we require the inverse transformation\\nofxas a function of y. Since we consider linear transformations, the\\ninverse transformation is given by the matrix inverse (see Section 2.2.2).\\nFor2×2matrices, we can explicitly write out the formula, given by\\n[x1\\nx2]\\n=A−1[y1\\ny2]\\n=1\\nad−bc[d−b\\n−c a][y1\\ny2]\\n. (6.147)\\nObserve that ad−bcis the determinant (Section 4.1) of A. The corre-\\nsponding probability density function is given by\\nf(x) =f(A−1y) =1\\n2πexp(\\n−1\\n2y⊤A−⊤A−1y)\\n. (6.148)\\nThe partial derivative of a matrix times a vector with respect to the vector\\nis the matrix itself (Section 5.5), and therefore\\n∂\\n∂yA−1y=A−1. (6.149)\\nRecall from Section 4.1 that the determinant of the inverse is the inverse\\nof the determinant so that the determinant of the Jacobian matrix is\\ndet(∂\\n∂yA−1y)\\n=1\\nad−bc. (6.150)\\nWe are now able to apply the change-of-variable formula from Theo-\\nrem 6.16 by multiplying (6.148) with (6.150), which yields\\nf(y) =f(x)⏐⏐⏐⏐det(∂\\n∂yA−1y)⏐⏐⏐⏐(6.151a)\\n=1\\n2πexp(\\n−1\\n2y⊤A−⊤A−1y)\\n|ad−bc|−1. (6.151b)\\nWhile Example 6.17 is based on a bivariate random variable, which al-\\nlows us to easily compute the matrix inverse, the preceding relation holds\\nfor higher dimensions.\\nRemark. We saw in Section 6.5 that the density f(x)in (6.148) is actually\\nthe standard Gaussian distribution, and the transformed density f(y)is a\\nbivariate Gaussian with covariance Σ=AA⊤. ♦\\nWe will use the ideas in this chapter to describe probabilistic modeling\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97d2f19b-bac8-4664-ad39-2460f6b2c65c', embedding=None, metadata={'page_label': '221', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.8 Further Reading 221\\nin Section 8.4, as well as introduce a graphical language in Section 8.5. We\\nwill see direct machine learning applications of these ideas in Chapters 9\\nand 11.\\n6.8 Further Reading\\nThis chapter is rather terse at times. Grinstead and Snell (1997) and\\nWalpole et al. (2011) provide more relaxed presentations that are suit-\\nable for self-study. Readers interested in more philosophical aspects of\\nprobability should consider Hacking (2001), whereas an approach that\\nis more related to software engineering is presented by Downey (2014).\\nAn overview of exponential families can be found in Barndorff-Nielsen\\n(2014). We will see more about how to use probability distributions to\\nmodel machine learning tasks in Chapter 8. Ironically, the recent surge\\nin interest in neural networks has resulted in a broader appreciation of\\nprobabilistic models. For example, the idea of normalizing ﬂows (Jimenez\\nRezende and Mohamed, 2015) relies on change of variables for transform-\\ning random variables. An overview of methods for variational inference as\\napplied to neural networks is described in chapters 16 to 20 of the book\\nby Goodfellow et al. (2016).\\nWe side stepped a large part of the difﬁculty in continuous random vari-\\nables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\\n2002), and by assuming without construction that we have real numbers,\\nand ways of deﬁning sets on real numbers as well as their appropriate fre-\\nquency of occurrence. These details do matter, for example, in the speciﬁ-\\ncation of conditional probability p(y|x)for continuous random variables\\nx,y(Proschan and Presnell, 1998). The lazy notation hides the fact that\\nwe want to specify that X=x(which is a set of measure zero). Fur-\\nthermore, we are interested in the probability density function of y. A\\nmore precise notation would have to say Ey[f(y)|σ(x)], where we take\\nthe expectation over yof a test function fconditioned on the σ-algebra of\\nx. A more technical audience interested in the details of probability the-\\nory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,\\n2004; Grimmett and Welsh, 2014), including some very technical discus-\\nsions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel\\nand Doksum, 2006; C ¸inlar, 2011). An alternative way to approach proba-\\nbility is to start with the concept of expectation, and “work backward” to\\nderive the necessary properties of a probability space (Whittle, 2000). As\\nmachine learning allows us to model more intricate distributions on ever\\nmore complex types of data, a developer of probabilistic machine learn-\\ning models would have to understand these more technical aspects. Ma-\\nchine learning texts with a probabilistic modeling focus include the books\\nby MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar-\\nber (2012); Murphy (2012).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f73d028-b5cf-496e-91e2-3be137ed53ad', embedding=None, metadata={'page_label': '222', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='222 Probability and Distributions\\nExercises\\n6.1 Consider the following bivariate distribution p(x,y)of two discrete random\\nvariablesXandY.\\nXx1x2x3x4x5Y\\ny3y2y10.01 0.02 0.03 0.1 0.1\\n0.05 0.1 0.05 0.07 0.2\\n0.1 0.05 0.03 0.05 0.04\\nCompute:\\na. The marginal distributions p(x)andp(y).\\nb. The conditional distributions p(x|Y=y1)andp(y|X=x3).\\n6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),\\n0.4N([\\n10\\n2]\\n,[\\n1 0\\n0 1])\\n+ 0.6N([\\n0\\n0]\\n,[\\n8.4 2.0\\n2.0 1.7])\\n.\\na. Compute the marginal distributions for each dimension.\\nb. Compute the mean, mode and median for each marginal distribution.\\nc. Compute the mean and mode for the two-dimensional distribution.\\n6.3 You have written a computer program that sometimes compiles and some-\\ntimes not (code does not change). You decide to model the apparent stochas-\\nticity (success vs. no success) xof the compiler using a Bernoulli distribution\\nwith parameter µ:\\np(x|µ) =µx(1−µ)1−x, x∈{0,1}.\\nChoose a conjugate prior for the Bernoulli likelihood and compute the pos-\\nterior distribution p(µ|x1,...,xN).\\n6.4 There are two bags. The ﬁrst bag contains four mangos and two apples; the\\nsecond bag contains four mangos and four apples.\\nWe also have a biased coin, which shows “heads” with probability 0.6 and\\n“tails” with probability 0.4. If the coin shows “heads”. we pick a fruit at\\nrandom from bag 1; otherwise we pick a fruit at random from bag 2.\\nYour friend ﬂips the coin (you cannot see the result), picks a fruit at random\\nfrom the corresponding bag, and presents you a mango.\\nWhat is the probability that the mango was picked from bag 2?\\nHint: Use Bayes’ theorem.\\n6.5 Consider the time-series model\\nxt+1=Axt+w,w∼N(\\n0,Q)\\nyt=Cxt+v,v∼N(\\n0,R)\\n,\\nwherew,vare i.i.d. Gaussian noise variables. Further, assume that p(x0) =\\nN(\\nµ0,Σ0).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='522e430a-bf25-477f-85f5-1551f6f276eb', embedding=None, metadata={'page_label': '223', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 223\\na. What is the form of p(x0,x1,...,xT)? Justify your answer (you do not\\nhave to explicitly compute the joint distribution).\\nb. Assume that p(xt|y1,...,yt) =N(\\nµt,Σt).\\n1. Compute p(xt+1|y1,...,yt).\\n2. Compute p(xt+1,yt+1|y1,...,yt).\\n3. At timet+1, we observe the value yt+1=ˆy. Compute the conditional\\ndistribution p(xt+1|y1,...,yt+1).\\n6.6 Prove the relationship in (6.44), which relates the standard deﬁnition of the\\nvariance to the raw-score expression for the variance.\\n6.7 Prove the relationship in (6.45), which relates the pairwise difference be-\\ntween examples in a dataset with the raw-score expression for the variance.\\n6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\\nponential family, see (6.107).\\n6.9 Express the Binomial distribution as an exponential family distribution. Also\\nexpress the Beta distribution is an exponential family distribution. Show that\\nthe product of the Beta and the Binomial distribution is also a member of\\nthe exponential family.\\n6.10 Derive the relationship in Section 6.5.2 in two ways:\\na. By completing the square\\nb. By expressing the Gaussian in its exponential family form\\nTheproduct of two Gaussians N(\\nx|a,A)\\nN(\\nx|b,B)is an unnormalized\\nGaussian distribution cN(\\nx|c,C)with\\nC= (A−1+B−1)−1\\nc=C(A−1a+B−1b)\\nc= (2π)−D\\n2|A+B|−1\\n2exp(\\n−1\\n2(a−b)⊤(A+B)−1(a−b))\\n.\\nNote that the normalizing constant citself can be considered a (normalized)\\nGaussian distribution either in aor inbwith an “inﬂated” covariance matrix\\nA+B, i.e.,c=N(\\na|b,A+B)\\n=N(\\nb|a,A+B).\\n6.11 Iterated Expectations.\\nConsider two random variables x,ywith joint distribution p(x,y). Show that\\nEX[x] =EY[EX[x|y]]\\n.\\nHere,EX[x|y]denotes the expected value of xunder the conditional distri-\\nbutionp(x|y).\\n6.12 Manipulation of Gaussian Random Variables.\\nConsider a Gaussian random variable x∼N(\\nx|µx,Σx), wherex∈RD.\\nFurthermore, we have\\ny=Ax+b+w,\\nwherey∈RE,A∈RE×D,b∈RE, andw∼N(\\nw|0,Q)is indepen-\\ndent Gaussian noise. “Independent” implies that xandware independent\\nrandom variables and that Qis diagonal.\\na. Write down the likelihood p(y|x).\\nb. The distribution p(y) =∫\\np(y|x)p(x)dxis Gaussian. Compute the mean\\nµyand the covariance Σy. Derive your result in detail.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33a98edf-f37e-45f0-8c3b-bba35a451308', embedding=None, metadata={'page_label': '224', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='224 Probability and Distributions\\nc. The random variable yis being transformed according to the measure-\\nment mapping\\nz=Cy+v,\\nwherez∈RF,C∈RF×E, andv∼N(\\nv|0,R)is independent Gaus-\\nsian (measurement) noise.\\nWrite down p(z|y).\\nComputep(z), i.e., the mean µzand the covariance Σz. Derive your\\nresult in detail.\\nd. Now, a value ˆyis measured. Compute the posterior distribution p(x|ˆy).\\nHint for solution: This posterior is also Gaussian, i.e., we need to de-\\ntermine only its mean and covariance matrix. Start by explicitly com-\\nputing the joint Gaussian p(x,y). This also requires us to compute the\\ncross-covariances Covx,y[x,y]and Covy,x[y,x]. Then apply the rules\\nfor Gaussian conditioning.\\n6.13 Probability Integral Transformation\\nGiven a continuous random variable X, with cdfFX(x), show that the ran-\\ndom variable Y:=FX(X)is uniformly distributed (Theorem 6.15).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bf5baf6-3c25-42c5-9e9b-6df16a110bc0', embedding=None, metadata={'page_label': '225', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7\\nContinuous Optimization\\nSince machine learning algorithms are implemented on a computer, the\\nmathematical formulations are expressed as numerical optimization meth-\\nods. This chapter describes the basic numerical methods for training ma-\\nchine learning models. Training a machine learning model often boils\\ndown to ﬁnding a good set of parameters. The notion of “good” is de-\\ntermined by the objective function or the probabilistic model, which we\\nwill see examples of in the second part of this book. Given an objective\\nfunction, ﬁnding the best value is done using optimization algorithms. Since we consider\\ndata and models in\\nRD, the\\noptimization\\nproblems we face\\narecontinuous\\noptimization\\nproblems, as\\nopposed to\\ncombinatorial\\noptimization\\nproblems for\\ndiscrete variables.This chapter covers two main branches of continuous optimization (Fig-\\nure 7.1): unconstrained and constrained optimization. We will assume in\\nthis chapter that our objective function is differentiable (see Chapter 5),\\nhence we have access to a gradient at each location in the space to help us\\nﬁnd the optimum value. By convention, most objective functions in ma-\\nchine learning are intended to be minimized, that is, the best value is the\\nminimum value. Intuitively ﬁnding the best value is like ﬁnding the val-\\nleys of the objective function, and the gradients point us uphill. The idea is\\nto move downhill (opposite to the gradient) and hope to ﬁnd the deepest\\npoint. For unconstrained optimization, this is the only concept we need,\\nbut there are several design choices, which we discuss in Section 7.1. For\\nconstrained optimization, we need to introduce other concepts to man-\\nage the constraints (Section 7.2). We will also introduce a special class\\nof problems (convex optimization problems in Section 7.3) where we can\\nmake statements about reaching the global optimum.\\nConsider the function in Figure 7.2. The function has a global minimum global minimum\\naroundx=−4.5, with a function value of approximately −47. Since\\nthe function is “smooth,” the gradients can be used to help ﬁnd the min-\\nimum by indicating whether we should take a step to the right or left.\\nThis assumes that we are in the correct bowl, as there exists another local local minimum\\nminimum aroundx= 0.7. Recall that we can solve for all the stationary\\npoints of a function by calculating its derivative and setting it to zero. For Stationary points\\nare the real roots of\\nthe derivative, that\\nis, points that have\\nzero gradient.ℓ(x) =x4+ 7x3+ 5x2−17x+ 3, (7.1)\\nwe obtain the corresponding gradient as\\ndℓ(x)\\ndx= 4x3+ 21x2+ 10x−17. (7.2)\\n225\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59c8d916-55af-485d-8692-648c54529528', embedding=None, metadata={'page_label': '226', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='226 Continuous Optimization\\nFigure 7.1 A mind\\nmap of the concepts\\nrelated to\\noptimization, as\\npresented in this\\nchapter. There are\\ntwo main ideas:\\ngradient descent\\nand convex\\noptimization.Continuous\\noptimization\\nUnconstrained\\noptimization\\nConstrained\\noptimizationGradient descentStepsize\\nMomentum\\nStochastic\\ngradient\\ndescent\\nLagrange\\nmultipliers\\nConvex optimization\\n& dualityConvex\\nConvex conjugateLinear\\nprogramming\\nQuadratic\\nprogrammingChapter 10\\nDimension reduc.\\nChapter 11\\nDensity estimation\\nChapter 12\\nClassiﬁcation\\nSince this is a cubic equation, it has in general three solutions when set to\\nzero. In the example, two of them are minimums and one is a maximum\\n(aroundx=−1.4). To check whether a stationary point is a minimum\\nor maximum, we need to take the derivative a second time and check\\nwhether the second derivative is positive or negative at the stationary\\npoint. In our case, the second derivative is\\nd2ℓ(x)\\ndx2= 12x2+ 42x+ 10. (7.3)\\nBy substituting our visually estimated values of x=−4.5,−1.4,0.7,we\\nwill observe that as expected the middle point is a maximum(\\nd2ℓ(x)\\ndx2<0)\\nand the other two stationary points are minimums.\\nNote that we have avoided analytically solving for values of xin the\\nprevious discussion, although for low-order polynomials such as the pre-\\nceding we could do so. In general, we are unable to ﬁnd analytic solu-\\ntions, and hence we need to start at some value, say x0=−6, and follow\\nthe negative gradient. The negative gradient indicates that we should go\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4276d35-b980-4dca-92e5-797cf381a291', embedding=None, metadata={'page_label': '227', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.1 Optimization Using Gradient Descent 227\\nFigure 7.2 Example\\nobjective function.\\nNegative gradients\\nare indicated by\\narrows, and the\\nglobal minimum is\\nindicated by the\\ndashed blue line.\\n−6−5−4−3−2−1 0 1 2\\nValue of parameter−60−40−200204060Objectivex4+ 7x3+ 5x2−17x+ 3\\nright, but not how far (this is called the step-size). Furthermore, if we According to the\\nAbel–Rufﬁni\\ntheorem, there is in\\ngeneral no algebraic\\nsolution for\\npolynomials of\\ndegree 5 or more\\n(Abel, 1826).had started at the right side (e.g., x0= 0) the negative gradient would\\nhave led us to the wrong minimum. Figure 7.2 illustrates the fact that for\\nx>−1, the negative gradient points toward the minimum on the right of\\nthe ﬁgure, which has a larger objective value.\\nIn Section 7.3, we will learn about a class of functions, called convex\\nfunctions, that do not exhibit this tricky dependency on the starting point\\nof the optimization algorithm. For convex functions, all local minimums\\nare global minimum. It turns out that many machine learning objective For convex functions\\nall local minima are\\nglobal minimum.functions are designed such that they are convex, and we will see an ex-\\nample in Chapter 12.\\nThe discussion in this chapter so far was about a one-dimensional func-\\ntion, where we are able to visualize the ideas of gradients, descent direc-\\ntions, and optimal values. In the rest of this chapter we develop the same\\nideas in high dimensions. Unfortunately, we can only visualize the con-\\ncepts in one dimension, but some concepts do not generalize directly to\\nhigher dimensions, therefore some care needs to be taken when reading.\\n7.1 Optimization Using Gradient Descent\\nWe now consider the problem of solving for the minimum of a real-valued\\nfunction\\nmin\\nxf(x), (7.4)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b8de034b-bf2e-4ad9-a136-10eeb0aa61dd', embedding=None, metadata={'page_label': '228', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='228 Continuous Optimization\\nwheref:Rd→Ris an objective function that captures the machine\\nlearning problem at hand. We assume that our function fis differentiable,\\nand we are unable to analytically ﬁnd a solution in closed form.\\nGradient descent is a ﬁrst-order optimization algorithm. To ﬁnd a local\\nminimum of a function using gradient descent, one takes steps propor-\\ntional to the negative of the gradient of the function at the current point.\\nRecall from Section 5.1 that the gradient points in the direction of the We use the\\nconvention of row\\nvectors for\\ngradients.steepest ascent. Another useful intuition is to consider the set of lines\\nwhere the function is at a certain value ( f(x) =cfor some value c∈R),\\nwhich are known as the contour lines. The gradient points in a direction\\nthat is orthogonal to the contour lines of the function we wish to optimize.\\nLet us consider multivariate functions. Imagine a surface (described by\\nthe function f(x)) with a ball starting at a particular location x0. When\\nthe ball is released, it will move downhill in the direction of steepest de-\\nscent. Gradient descent exploits the fact that f(x0)decreases fastest if one\\nmoves fromx0in the direction of the negative gradient −((∇f)(x0))⊤of\\nfatx0. We assume in this book that the functions are differentiable, and\\nrefer the reader to more general settings in Section 7.4. Then, if\\nx1=x0−γ((∇f)(x0))⊤(7.5)\\nfor a small step-sizeγ⩾0, thenf(x1)⩽f(x0). Note that we use the\\ntranspose for the gradient since otherwise the dimensions will not work\\nout.\\nThis observation allows us to deﬁne a simple gradient descent algo-\\nrithm: If we want to ﬁnd a local optimum f(x∗)of a function f:Rn→\\nR,x↦→f(x), we start with an initial guess x0of the parameters we wish\\nto optimize and then iterate according to\\nxi+1=xi−γi((∇f)(xi))⊤. (7.6)\\nFor suitable step-size γi, the sequence f(x0)⩾f(x1)⩾...converges to\\na local minimum.\\nExample 7.1\\nConsider a quadratic function in two dimensions\\nf([x1\\nx2])\\n=1\\n2[x1\\nx2]⊤[2 1\\n1 20][x1\\nx2]\\n−[5\\n3]⊤[x1\\nx2]\\n(7.7)\\nwith gradient\\n∇f([x1\\nx2])\\n=[x1\\nx2]⊤[2 1\\n1 20]\\n−[5\\n3]⊤\\n. (7.8)\\nStarting at the initial location x0= [−3,−1]⊤, we iteratively apply (7.6)\\nto obtain a sequence of estimates that converge to the minimum value\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0b531ac-cf7e-42a1-93e4-9bceb7c9b0d6', embedding=None, metadata={'page_label': '229', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.1 Optimization Using Gradient Descent 229\\nFigure 7.3 Gradient\\ndescent on a\\ntwo-dimensional\\nquadratic surface\\n(shown as a\\nheatmap). See\\nExample 7.1 for a\\ndescription.\\n−4−2 0 2 4\\nx1−2−1012x20.0\\n10.0\\n20.030.0\\n40.040.0\\n50.050.0\\n60.0 70.0\\n80.0−150153045607590\\n(illustrated in Figure 7.3). We can see (both from the ﬁgure and by plug-\\ngingx0into (7.8) with γ= 0.085) that the negative gradient at x0points\\nnorth and east, leading to x1= [−1.98,1.21]⊤. Repeating that argument\\ngives usx2= [−1.32,−0.42]⊤, and so on.\\nRemark. Gradient descent can be relatively slow close to the minimum:\\nIts asymptotic rate of convergence is inferior to many other methods. Us-\\ning the ball rolling down the hill analogy, when the surface is a long, thin\\nvalley, the problem is poorly conditioned (Trefethen and Bau III, 1997).\\nFor poorly conditioned convex problems, gradient descent increasingly\\n“zigzags” as the gradients point nearly orthogonally to the shortest di-\\nrection to a minimum point; see Figure 7.3. ♦\\n7.1.1 Step-size\\nAs mentioned earlier, choosing a good step-size is important in gradient\\ndescent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\\ncalled the learning\\nrate.step-size is chosen too large, gradient descent can overshoot, fail to con-\\nverge, or even diverge. We will discuss the use of momentum in the next\\nsection. It is a method that smoothes out erratic behavior of gradient up-\\ndates and dampens oscillations.\\nAdaptive gradient methods rescale the step-size at each iteration, de-\\npending on local properties of the function. There are two simple heuris-\\ntics (Toussaint, 2012):\\nWhen the function value increases after a gradient step, the step-size\\nwas too large. Undo the step and decrease the step-size.\\nWhen the function value decreases the step could have been larger. Try\\nto increase the step-size.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c820ef0d-feea-4468-8c00-c457f1078e3c', embedding=None, metadata={'page_label': '230', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='230 Continuous Optimization\\nAlthough the “undo” step seems to be a waste of resources, using this\\nheuristic guarantees monotonic convergence.\\nExample 7.2 (Solving a Linear Equation System)\\nWhen we solve linear equations of the form Ax=b, in practice we solve\\nAx−b=0approximately by ﬁnding x∗that minimizes the squared error\\n∥Ax−b∥2= (Ax−b)⊤(Ax−b) (7.9)\\nif we use the Euclidean norm. The gradient of (7.9) with respect to xis\\n∇x= 2(Ax−b)⊤A. (7.10)\\nWe can use this gradient directly in a gradient descent algorithm. How-\\never, for this particular special case, it turns out that there is an analytic\\nsolution, which can be found by setting the gradient to zero. We will see\\nmore on solving squared error problems in Chapter 9.\\nRemark. When applied to the solution of linear systems of equations Ax=\\nb, gradient descent may converge slowly. The speed of convergence of gra-\\ndient descent is dependent on the condition number κ=σ(A)max\\nσ(A)min, which condition number\\nis the ratio of the maximum to the minimum singular value (Section 4.5)\\nofA. The condition number essentially measures the ratio of the most\\ncurved direction versus the least curved direction, which corresponds to\\nour imagery that poorly conditioned problems are long, thin valleys: They\\nare very curved in one direction, but very ﬂat in the other. Instead of di-\\nrectly solving Ax=b, one could instead solve P−1(Ax−b) =0, where\\nPis called the preconditioner . The goal is to design P−1such thatP−1A preconditioner\\nhas a better condition number, but at the same time P−1is easy to com-\\npute. For further information on gradient descent, preconditioning, and\\nconvergence we refer to Boyd and Vandenberghe (2004, chapter 9). ♦\\n7.1.2 Gradient Descent With Momentum\\nAs illustrated in Figure 7.3, the convergence of gradient descent may be\\nvery slow if the curvature of the optimization surface is such that there\\nare regions that are poorly scaled. The curvature is such that the gradient\\ndescent steps hops between the walls of the valley and approaches the\\noptimum in small steps. The proposed tweak to improve convergence is\\nto give gradient descent some memory. Goh (2017) wrote\\nan intuitive blog\\npost on gradient\\ndescent with\\nmomentum.Gradient descent with momentum (Rumelhart et al., 1986) is a method\\nthat introduces an additional term to remember what happened in the\\nprevious iteration. This memory dampens oscillations and smoothes out\\nthe gradient updates. Continuing the ball analogy, the momentum term\\nemulates the phenomenon of a heavy ball that is reluctant to change di-\\nrections. The idea is to have a gradient update with memory to implement\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f8af58b-73d9-4795-bb45-50579622ae0b', embedding=None, metadata={'page_label': '231', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.1 Optimization Using Gradient Descent 231\\na moving average. The momentum-based method remembers the update\\n∆xiat each iteration iand determines the next update as a linear combi-\\nnation of the current and previous gradients\\nxi+1=xi−γi((∇f)(xi))⊤+α∆xi (7.11)\\n∆xi=xi−xi−1=α∆xi−1−γi−1((∇f)(xi−1))⊤, (7.12)\\nwhereα∈[0,1]. Sometimes we will only know the gradient approxi-\\nmately. In such cases, the momentum term is useful since it averages out\\ndifferent noisy estimates of the gradient. One particularly useful way to\\nobtain an approximate gradient is by using a stochastic approximation,\\nwhich we discuss next.\\n7.1.3 Stochastic Gradient Descent\\nComputing the gradient can be very time consuming. However, often it is\\npossible to ﬁnd a “cheap” approximation of the gradient. Approximating\\nthe gradient is still useful as long as it points in roughly the same direction\\nas the true gradient. stochastic gradient\\ndescent Stochastic gradient descent (often shortened as SGD) is a stochastic ap-\\nproximation of the gradient descent method for minimizing an objective\\nfunction that is written as a sum of differentiable functions. The word\\nstochastic here refers to the fact that we acknowledge that we do not\\nknow the gradient precisely, but instead only know a noisy approxima-\\ntion to it. By constraining the probability distribution of the approximate\\ngradients, we can still theoretically guarantee that SGD will converge.\\nIn machine learning, given n= 1,...,N data points, we often consider\\nobjective functions that are the sum of the losses Lnincurred by each\\nexamplen. In mathematical notation, we have the form\\nL(θ) =N∑\\nn=1Ln(θ), (7.13)\\nwhereθis the vector of parameters of interest, i.e., we want to ﬁnd θthat\\nminimizesL. An example from regression (Chapter 9) is the negative log-\\nlikelihood, which is expressed as a sum over log-likelihoods of individual\\nexamples so that\\nL(θ) =−N∑\\nn=1logp(yn|xn,θ), (7.14)\\nwherexn∈RDare the training inputs, ynare the training targets, and θ\\nare the parameters of the regression model.\\nStandard gradient descent, as introduced previously, is a “batch” opti-\\nmization method, i.e., optimization is performed using the full training set\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59fbadeb-6ddb-4a9c-85a3-f8441de1f36a', embedding=None, metadata={'page_label': '232', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='232 Continuous Optimization\\nby updating the vector of parameters according to\\nθi+1=θi−γi(∇L(θi))⊤=θi−γiN∑\\nn=1(∇Ln(θi))⊤(7.15)\\nfor a suitable step-size parameter γi. Evaluating the sum gradient may re-\\nquire expensive evaluations of the gradients from all individual functions\\nLn. When the training set is enormous and/or no simple formulas exist,\\nevaluating the sums of gradients becomes very expensive.\\nConsider the term∑N\\nn=1(∇Ln(θi))in (7.15), we can reduce the amount\\nof computation by taking a sum over a smaller set of Ln. In contrast to\\nbatch gradient descent, which uses all Lnforn= 1,...,N , we randomly\\nchoose a subset of Lnfor mini-batch gradient descent. In the extreme\\ncase, we randomly select only a single Lnto estimate the gradient. The\\nkey insight about why taking a subset of data is sensible is to realize that\\nfor gradient descent to converge, we only require that the gradient is an\\nunbiased estimate of the true gradient. In fact the term∑N\\nn=1(∇Ln(θi))\\nin (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\\nthe gradient. Therefore, any other unbiased empirical estimate of the ex-\\npected value, for example using any subsample of the data, would sufﬁce\\nfor convergence of gradient descent.\\nRemark. When the learning rate decreases at an appropriate rate, and sub-\\nject to relatively mild assumptions, stochastic gradient descent converges\\nalmost surely to local minimum (Bottou, 1998). ♦\\nWhy should one consider using an approximate gradient? A major rea-\\nson is practical implementation constraints, such as the size of central\\nprocessing unit (CPU)/graphics processing unit (GPU) memory or limits\\non computational time. We can think of the size of the subset used to esti-\\nmate the gradient in the same way that we thought of the size of a sample\\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\\nwill provide accurate estimates of the gradient, reducing the variance in\\nthe parameter update. Furthermore, large mini-batches take advantage of\\nhighly optimized matrix operations in vectorized implementations of the\\ncost and gradient. The reduction in variance leads to more stable conver-\\ngence, but each gradient calculation will be more expensive.\\nIn contrast, small mini-batches are quick to estimate. If we keep the\\nmini-batch size small, the noise in our gradient estimate will allow us to\\nget out of some bad local optima, which we may otherwise get stuck in.\\nIn machine learning, optimization methods are used for training by min-\\nimizing an objective function on the training data, but the overall goal\\nis to improve generalization performance (Chapter 8). Since the goal in\\nmachine learning does not necessarily need a precise estimate of the min-\\nimum of the objective function, approximate gradients using mini-batch\\napproaches have been widely used. Stochastic gradient descent is very\\neffective in large-scale machine learning problems (Bottou et al., 2018),\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='066176be-26ec-409e-a391-025739ee6f7a', embedding=None, metadata={'page_label': '233', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.2 Constrained Optimization and Lagrange Multipliers 233\\nFigure 7.4\\nIllustration of\\nconstrained\\noptimization. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side (indicated\\nby the circle). The\\nbox constraints\\n(−1⩽x⩽1and\\n−1⩽y⩽1) require\\nthat the optimal\\nsolution is within\\nthe box, resulting in\\nan optimal value\\nindicated by the\\nstar.\\n−3−2−1 0 1 2 3\\nx1−3−2−10123x2\\nsuch as training deep neural networks on millions of images (Dean et al.,\\n2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\\net al., 2015), or training of large-scale Gaussian process models (Hensman\\net al., 2013; Gal et al., 2014).\\n7.2 Constrained Optimization and Lagrange Multipliers\\nIn the previous section, we considered the problem of solving for the min-\\nimum of a function\\nmin\\nxf(x), (7.16)\\nwheref:RD→R.\\nIn this section, we have additional constraints. That is, for real-valued\\nfunctionsgi:RD→Rfori= 1,...,m , we consider the constrained\\noptimization problem (see Figure 7.4 for an illustration)\\nmin\\nxf(x) (7.17)\\nsubject to gi(x)⩽0for alli= 1,...,m.\\nIt is worth pointing out that the functions fandgicould be non-convex\\nin general, and we will consider the convex case in the next section.\\nOne obvious, but not very practical, way of converting the constrained\\nproblem (7.17) into an unconstrained one is to use an indicator function\\nJ(x) =f(x) +m∑\\ni=11(gi(x)), (7.18)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68d73dae-7efe-4b0f-9bcc-fde408d90f79', embedding=None, metadata={'page_label': '234', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='234 Continuous Optimization\\nwhere 1(z)is an inﬁnite step function\\n1(z) ={\\n0ifz⩽0\\n∞otherwise. (7.19)\\nThis gives inﬁnite penalty if the constraint is not satisﬁed, and hence\\nwould provide the same solution. However, this inﬁnite step function is\\nequally difﬁcult to optimize. We can overcome this difﬁculty by introduc-\\ningLagrange multipliers . The idea of Lagrange multipliers is to replace the Lagrange multiplier\\nstep function with a linear function.\\nWe associate to problem (7.17) the Lagrangian by introducing the La- Lagrangian\\ngrange multipliers λi⩾0corresponding to each inequality constraint re-\\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\\nL(x,λ) =f(x) +m∑\\ni=1λigi(x) (7.20a)\\n=f(x) +λ⊤g(x), (7.20b)\\nwhere in the last line we have concatenated all constraints gi(x)into a\\nvectorg(x), and all the Lagrange multipliers into a vector λ∈Rm.\\nWe now introduce the idea of Lagrangian duality. In general, duality\\nin optimization is the idea of converting an optimization problem in one\\nset of variables x(called the primal variables), into another optimization\\nproblem in a different set of variables λ(called the dual variables). We\\nintroduce two different approaches to duality: In this section, we discuss\\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality.\\nDeﬁnition 7.1. The problem in (7.17)\\nmin\\nxf(x) (7.21)\\nsubject to gi(x)⩽0for alli= 1,...,m\\nis known as the primal problem , corresponding to the primal variables x. primal problem\\nThe associated Lagrangian dual problem is given by Lagrangian dual\\nproblem\\nmax\\nλ∈RmD(λ)\\nsubject toλ⩾0,(7.22)\\nwhereλare the dual variables and D(λ) = minx∈RdL(x,λ).\\nRemark. In the discussion of Deﬁnition 7.1, we use two concepts that are\\nalso of independent interest (Boyd and Vandenberghe, 2004).\\nFirst is the minimax inequality , which says that for any function with minimax inequality\\ntwo arguments ϕ(x,y), the maximin is less than the minimax, i.e.,\\nmax\\nymin\\nxϕ(x,y)⩽min\\nxmax\\nyϕ(x,y). (7.23)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bf13d1fb-c34f-4d5a-9958-beaee9d7ef70', embedding=None, metadata={'page_label': '235', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.2 Constrained Optimization and Lagrange Multipliers 235\\nThis inequality can be proved by considering the inequality\\nFor allx,y min\\nxϕ(x,y)⩽max\\nyϕ(x,y). (7.24)\\nNote that taking the maximum over yof the left-hand side of (7.24) main-\\ntains the inequality since the inequality is true for all y. Similarly, we can\\ntake the minimum over xof the right-hand side of (7.24) to obtain (7.23).\\nThe second concept is weak duality , which uses (7.23) to show that weak duality\\nprimal values are always greater than or equal to dual values. This is de-\\nscribed in more detail in (7.27). ♦\\nRecall that the difference between J(x)in (7.18) and the Lagrangian\\nin (7.20b) is that we have relaxed the indicator function to a linear func-\\ntion. Therefore, when λ⩾0, the Lagrangian L(x,λ)is a lower bound of\\nJ(x). Hence, the maximum of L(x,λ)with respect to λis\\nJ(x) = max\\nλ⩾0L(x,λ). (7.25)\\nRecall that the original problem was minimizing J(x),\\nmin\\nx∈Rdmax\\nλ⩾0L(x,λ). (7.26)\\nBy the minimax inequality (7.23), it follows that swapping the order of\\nthe minimum and maximum results in a smaller value, i.e.,\\nmin\\nx∈Rdmax\\nλ⩾0L(x,λ)⩾max\\nλ⩾0min\\nx∈RdL(x,λ). (7.27)\\nThis is also known as weak duality . Note that the inner part of the right- weak duality\\nhand side is the dual objective function D(λ)and the deﬁnition follows.\\nIn contrast to the original optimization problem, which has constraints,\\nminx∈RdL(x,λ)is an unconstrained optimization problem for a given\\nvalue ofλ. If solving minx∈RdL(x,λ)is easy, then the overall problem is\\neasy to solve. We can see this by observing from (7.20b) that L(x,λ)is\\nafﬁne with respect to λ. Therefore minx∈RdL(x,λ)is a pointwise min-\\nimum of afﬁne functions of λ, and hence D(λ)is concave even though\\nf(·)andgi(·)may be nonconvex. The outer problem, maximization over\\nλ, is the maximum of a concave function and can be efﬁciently computed.\\nAssumingf(·)andgi(·)are differentiable, we ﬁnd the Lagrange dual\\nproblem by differentiating the Lagrangian with respect to x, setting the\\ndifferential to zero, and solving for the optimal value. We will discuss two\\nconcrete examples in Sections 7.3.1 and 7.3.2, where f(·)andgi(·)are\\nconvex.\\nRemark (Equality Constraints) .Consider (7.17) with additional equality\\nconstraints\\nmin\\nxf(x)\\nsubject to gi(x)⩽0for alli= 1,...,m\\nhj(x) = 0 for allj= 1,...,n.(7.28)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f372af6a-3795-47aa-b9d2-b27a6db52739', embedding=None, metadata={'page_label': '236', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='236 Continuous Optimization\\nWe can model equality constraints by replacing them with two inequality\\nconstraints. That is for each equality constraint hj(x) = 0 we equivalently\\nreplace it by two constraints hj(x)⩽0andhj(x)⩾0. It turns out that\\nthe resulting Lagrange multipliers are then unconstrained.\\nTherefore, we constrain the Lagrange multipliers corresponding to the\\ninequality constraints in (7.28) to be non-negative, and leave the La-\\ngrange multipliers corresponding to the equality constraints unconstrained.\\n♦\\n7.3 Convex Optimization\\nWe focus our attention of a particularly useful class of optimization prob-\\nlems, where we can guarantee global optimality. When f(·)is a convex\\nfunction, and when the constraints involving g(·)andh(·)are convex sets,\\nthis is called a convex optimization problem . In this setting, we have strong convex optimization\\nproblem\\nstrong dualityduality : The optimal solution of the dual problem is the same as the opti-\\nmal solution of the primal problem. The distinction between convex func-\\ntions and convex sets are often not strictly presented in machine learning\\nliterature, but one can often infer the implied meaning from context.\\nDeﬁnition 7.2. A setCis aconvex set if for anyx,y∈Cand for any scalar convex set\\nθwith 0⩽θ⩽1, we have\\nθx+ (1−θ)y∈C. (7.29)\\nFigure 7.5 Example\\nof a convex set.\\n Convex sets are sets such that a straight line connecting any two ele-\\nments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\\nand nonconvex sets, respectively.\\nFigure 7.6 Example\\nof a nonconvex set.\\nConvex functions are functions such that a straight line between any\\ntwo points of the function lie above the function. Figure 7.2 shows a non-\\nconvex function, and Figure 7.3 shows a convex function. Another convex\\nfunction is shown in Figure 7.7.\\nDeﬁnition 7.3. Let function f:RD→Rbe a function whose domain is a\\nconvex set. The function fis aconvex function if for allx,yin the domain\\nconvex functionoff, and for any scalar θwith 0⩽θ⩽1, we have\\nf(θx+ (1−θ)y)⩽θf(x) + (1−θ)f(y). (7.30)\\nRemark. Aconcave function is the negative of a convex function. ♦\\nconcave functionThe constraints involving g(·)andh(·)in (7.28) truncate functions at a\\nscalar value, resulting in sets. Another relation between convex functions\\nand convex sets is to consider the set obtained by “ﬁlling in” a convex\\nfunction. A convex function is a bowl-like object, and we imagine pouring\\nwater into it to ﬁll it up. This resulting ﬁlled-in set, called the epigraph of epigraph\\nthe convex function, is a convex set.\\nIf a function f:Rn→Ris differentiable, we can specify convexity in\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07f9871d-9f06-440e-ad98-34af93ace1cc', embedding=None, metadata={'page_label': '237', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Convex Optimization 237\\nFigure 7.7 Example\\nof a convex\\nfunction.\\n−3−2−1 0 1 2 3\\nx010203040yy= 3x2−5x+ 2\\nterms of its gradient ∇xf(x)(Section 5.2). A function f(x)is convex if\\nand only if for any two points x,yit holds that\\nf(y)⩾f(x) +∇xf(x)⊤(y−x). (7.31)\\nIf we further know that a function f(x)is twice differentiable, that is, the\\nHessian (5.147) exists for all values in the domain of x, then the function\\nf(x)is convex if and only if ∇2\\nxf(x)is positive semideﬁnite (Boyd and\\nVandenberghe, 2004).\\nExample 7.3\\nThe negative entropy f(x) =xlog2xis convex for x>0. A visualization\\nof the function is shown in Figure 7.8, and we can see that the function is\\nconvex. To illustrate the previous deﬁnitions of convexity, let us check the\\ncalculations for two points x= 2andx= 4. Note that to prove convexity\\noff(x)we would need to check for all points x∈R.\\nRecall Deﬁnition 7.3. Consider a point midway between the two points\\n(that isθ= 0.5); then the left-hand side is f(0.5·2 + 0.5·4) = 3 log23≈\\n4.75. The right-hand side is 0.5(2 log22) + 0.5(4 log24) = 1 + 4 = 5 . And\\ntherefore the deﬁnition is satisﬁed.\\nSincef(x)is differentiable, we can alternatively use (7.31). Calculating\\nthe derivative of f(x), we obtain\\n∇x(xlog2x) = 1·log2x+x·1\\nxloge2= log2x+1\\nloge2. (7.32)\\nUsing the same two test points x= 2 andx= 4, the left-hand side of\\n(7.31) is given by f(4) = 8 . The right-hand side is\\nf(x) +∇⊤\\nx(y−x) =f(2) +∇f(2)·(4−2) (7.33a)\\n= 2 + (1 +1\\nloge2)·2≈6.9. (7.33b)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='896a4806-4a0f-4b5b-b186-93bd7d527c8e', embedding=None, metadata={'page_label': '238', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='238 Continuous Optimization\\nFigure 7.8 The\\nnegative entropy\\nfunction (which is\\nconvex) and its\\ntangent atx= 2.\\n0 1 2 3 4 5\\nx0510f(x)xlog2x\\ntangent atx= 2\\nWe can check that a function or set is convex from ﬁrst principles by\\nrecalling the deﬁnitions. In practice, we often rely on operations that pre-\\nserve convexity to check that a particular function or set is convex. Al-\\nthough the details are vastly different, this is again the idea of closure\\nthat we introduced in Chapter 2 for vector spaces.\\nExample 7.4\\nA nonnegative weighted sum of convex functions is convex. Observe that\\niffis a convex function, and α⩾0is a nonnegative scalar, then the\\nfunctionαfis convex. We can see this by multiplying αto both sides of the\\nequation in Deﬁnition 7.3, and recalling that multiplying a nonnegative\\nnumber does not change the inequality.\\nIff1andf2are convex functions, then we have by the deﬁnition\\nf1(θx+ (1−θ)y)⩽θf1(x) + (1−θ)f1(y) (7.34)\\nf2(θx+ (1−θ)y)⩽θf2(x) + (1−θ)f2(y). (7.35)\\nSumming up both sides gives us\\nf1(θx+ (1−θ)y) +f2(θx+ (1−θ)y)\\n⩽θf1(x) + (1−θ)f1(y) +θf2(x) + (1−θ)f2(y), (7.36)\\nwhere the right-hand side can be rearranged to\\nθ(f1(x) +f2(x)) + (1−θ)(f1(y) +f2(y)), (7.37)\\ncompleting the proof that the sum of convex functions is convex.\\nCombining the preceding two facts, we see that αf1(x) +βf2(x)is\\nconvex forα,β⩾0. This closure property can be extended using a sim-\\nilar argument for nonnegative weighted sums of more than two convex\\nfunctions.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dee4c83e-2828-4537-b3e0-ef302ffe8f6c', embedding=None, metadata={'page_label': '239', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Convex Optimization 239\\nRemark. The inequality in (7.30) is sometimes called Jensen’s inequality .Jensen’s inequality\\nIn fact, a whole class of inequalities for taking nonnegative weighted sums\\nof convex functions are all called Jensen’s inequality. ♦\\nIn summary, a constrained optimization problem is called a convex opti- convex optimization\\nproblem mization problem if\\nmin\\nxf(x)\\nsubject togi(x)⩽0for alli= 1,...,m\\nhj(x) = 0 for allj= 1,...,n,(7.38)\\nwhere all functions f(x)andgi(x)are convex functions, and all hj(x) =\\n0are convex sets. In the following, we will describe two classes of convex\\noptimization problems that are widely used and well understood.\\n7.3.1 Linear Programming\\nConsider the special case when all the preceding functions are linear, i.e.,\\nmin\\nx∈Rdc⊤x (7.39)\\nsubject toAx⩽b,\\nwhereA∈Rm×dandb∈Rm. This is known as a linear program . It hasd linear program\\nLinear programs are\\none of the most\\nwidely used\\napproaches in\\nindustry.variables and mlinear constraints. The Lagrangian is given by\\nL(x,λ) =c⊤x+λ⊤(Ax−b), (7.40)\\nwhereλ∈Rmis the vector of non-negative Lagrange multipliers. Rear-\\nranging the terms corresponding to xyields\\nL(x,λ) = (c+A⊤λ)⊤x−λ⊤b. (7.41)\\nTaking the derivative of L(x,λ)with respect to xand setting it to zero\\ngives us\\nc+A⊤λ=0. (7.42)\\nTherefore, the dual Lagrangian is D(λ) =−λ⊤b. Recall we would like\\nto maximize D(λ). In addition to the constraint due to the derivative of\\nL(x,λ)being zero, we also have the fact that λ⩾0, resulting in the\\nfollowing dual optimization problem It is convention to\\nminimize the primal\\nand maximize the\\ndual.max\\nλ∈Rm−b⊤λ (7.43)\\nsubject toc+A⊤λ=0\\nλ⩾0.\\nThis is also a linear program, but with mvariables. We have the choice\\nof solving the primal (7.39) or the dual (7.43) program depending on\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8cb16a3-edce-49a6-8405-73e9bc115d87', embedding=None, metadata={'page_label': '240', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='240 Continuous Optimization\\nwhethermordis larger. Recall that dis the number of variables and mis\\nthe number of constraints in the primal linear program.\\nExample 7.5 (Linear Program)\\nConsider the linear program\\nmin\\nx∈R2−[5\\n3]⊤[x1\\nx2]\\nsubject to\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02 2\\n2−4\\n−2 1\\n0−1\\n0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb[x1\\nx2]\\n⩽\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f033\\n8\\n5\\n−1\\n8\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb(7.44)\\nwith two variables. This program is also shown in Figure 7.9. The objective\\nfunction is linear, resulting in linear contour lines. The constraint set in\\nstandard form is translated into the legend. The optimal value must lie in\\nthe shaded (feasible) region, and is indicated by the star.\\nFigure 7.9\\nIllustration of a\\nlinear program. The\\nunconstrained\\nproblem (indicated\\nby the contour\\nlines) has a\\nminimum on the\\nright side. The\\noptimal value given\\nthe constraints are\\nshown by the star.\\n0 2 4 6 8 10 12 14 16\\nx10246810x22x2≤33−2x1\\n4x2≥2x1−8\\nx2≤2x1−5\\nx2≥1\\nx2≤8\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ff06017-f1a1-4585-83dc-f0311b47f0e4', embedding=None, metadata={'page_label': '241', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Convex Optimization 241\\n7.3.2 Quadratic Programming\\nConsider the case of a convex quadratic objective function, where the con-\\nstraints are afﬁne, i.e.,\\nmin\\nx∈Rd1\\n2x⊤Qx+c⊤x (7.45)\\nsubject toAx⩽b,\\nwhereA∈Rm×d,b∈Rm, andc∈Rd. The square symmetric matrix Q∈\\nRd×dis positive deﬁnite, and therefore the objective function is convex.\\nThis is known as a quadratic program . Observe that it has dvariables and\\nmlinear constraints.\\nExample 7.6 (Quadratic Program)\\nConsider the quadratic program\\nmin\\nx∈R21\\n2[x1\\nx2]⊤[2 1\\n1 4][x1\\nx2]\\n+[5\\n3]⊤[x1\\nx2]\\n(7.46)\\nsubject to\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0\\n−1 0\\n0 1\\n0−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb[x1\\nx2]\\n⩽\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb(7.47)\\nof two variables. The program is also illustrated in Figure 7.4. The objec-\\ntive function is quadratic with a positive semideﬁnite matrix Q, resulting\\nin elliptical contour lines. The optimal value must lie in the shaded (feasi-\\nble) region, and is indicated by the star.\\nThe Lagrangian is given by\\nL(x,λ) =1\\n2x⊤Qx+c⊤x+λ⊤(Ax−b) (7.48a)\\n=1\\n2x⊤Qx+ (c+A⊤λ)⊤x−λ⊤b, (7.48b)\\nwhere again we have rearranged the terms. Taking the derivative of L(x,λ)\\nwith respect to xand setting it to zero gives\\nQx+ (c+A⊤λ) =0. (7.49)\\nAssuming that Qis invertible, we get\\nx=−Q−1(c+A⊤λ). (7.50)\\nSubstituting (7.50) into the primal Lagrangian L(x,λ), we get the dual\\nLagrangian\\nD(λ) =−1\\n2(c+A⊤λ)⊤Q−1(c+A⊤λ)−λ⊤b. (7.51)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3af85954-b787-4b3e-8144-4bd235321289', embedding=None, metadata={'page_label': '242', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='242 Continuous Optimization\\nTherefore, the dual optimization problem is given by\\nmax\\nλ∈Rm−1\\n2(c+A⊤λ)⊤Q−1(c+A⊤λ)−λ⊤b\\nsubject toλ⩾0.(7.52)\\nWe will see an application of quadratic programming in machine learning\\nin Chapter 12.\\n7.3.3 Legendre-Fenchel Transform and Convex Conjugate\\nLet us revisit the idea of duality from Section 7.2, without considering\\nconstraints. One useful fact about a convex set is that it can be equiva-\\nlently described by its supporting hyperplanes. A hyperplane is called a\\nsupporting hyperplane of a convex set if it intersects the convex set, and supporting\\nhyperplane the convex set is contained on just one side of it. Recall that we can ﬁll up\\na convex function to obtain the epigraph, which is a convex set. Therefore,\\nwe can also describe convex functions in terms of their supporting hyper-\\nplanes. Furthermore, observe that the supporting hyperplane just touches\\nthe convex function, and is in fact the tangent to the function at that\\npoint. And recall that the tangent of a function f(x)at a given point x0\\nis the evaluation of the gradient of that function at that pointdf(x)\\ndx⏐⏐⏐\\nx=x0.\\nIn summary, because convex sets can be equivalently described by its sup-\\nporting hyperplanes, convex functions can be equivalently described by a\\nfunction of their gradient. The Legendre transform formalizes this concept. Legendre transform\\nPhysics students are\\noften introduced to\\nthe Legendre\\ntransform as\\nrelating the\\nLagrangian and the\\nHamiltonian in\\nclassical mechanics.We begin with the most general deﬁnition, which unfortunately has a\\ncounter-intuitive form, and look at special cases to relate the deﬁnition to\\nthe intuition described in the preceding paragraph. The Legendre-Fenchel\\nLegendre-Fenchel\\ntransformtransform is a transformation (in the sense of a Fourier transform) from\\na convex differentiable function f(x)to a function that depends on the\\ntangentss(x) =∇xf(x). It is worth stressing that this is a transformation\\nof the function f(·)and not the variable xor the function evaluated at x.\\nThe Legendre-Fenchel transform is also known as the convex conjugate (for convex conjugate\\nreasons we will see soon) and is closely related to duality (Hiriart-Urruty\\nand Lemar ´echal, 2001, chapter 5).\\nDeﬁnition 7.4. The convex conjugate of a function f:RD→Ris a convex conjugate\\nfunctionf∗deﬁned by\\nf∗(s) = sup\\nx∈RD(⟨s,x⟩−f(x)). (7.53)\\nNote that the preceding convex conjugate deﬁnition does not need the\\nfunctionfto be convex nor differentiable. In Deﬁnition 7.4, we have used\\na general inner product (Section 3.2) but in the rest of this section we\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46c0eb97-476c-4cad-8ed0-6492767df807', embedding=None, metadata={'page_label': '243', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Convex Optimization 243\\nwill consider the standard dot product between ﬁnite-dimensional vectors\\n(⟨s,x⟩=s⊤x) to avoid too many technical details.\\nTo understand Deﬁnition 7.4 in a geometric fashion, consider a nice This derivation is\\neasiest to\\nunderstand by\\ndrawing the\\nreasoning as it\\nprogresses.simple one-dimensional convex and differentiable function, for example\\nf(x) =x2. Note that since we are looking at a one-dimensional problem,\\nhyperplanes reduce to a line. Consider a line y=sx+c. Recall that we are\\nable to describe convex functions by their supporting hyperplanes, so let\\nus try to describe this function f(x)by its supporting lines. Fix the gradi-\\nent of the line s∈Rand for each point (x0,f(x0))on the graph of f, ﬁnd\\nthe minimum value of csuch that the line still intersects (x0,f(x0)). Note\\nthat the minimum value of cis the place where a line with slope s“just\\ntouches” the function f(x) =x2. The line passing through (x0,f(x0))\\nwith gradient sis given by\\ny−f(x0) =s(x−x0). (7.54)\\nThey-intercept of this line is −sx0+f(x0). The minimum of cfor which\\ny=sx+cintersects with the graph of fis therefore\\ninf\\nx0−sx0+f(x0). (7.55)\\nThe preceding convex conjugate is by convention deﬁned to be the nega-\\ntive of this. The reasoning in this paragraph did not rely on the fact that\\nwe chose a one-dimensional convex and differentiable function, and holds\\nforf:RD→R, which are nonconvex and non-differentiable.The classical\\nLegendre transform\\nis deﬁned on convex\\ndifferentiable\\nfunctions in RD.Remark. Convex differentiable functions such as the example f(x) =x2is\\na nice special case, where there is no need for the supremum, and there is\\na one-to-one correspondence between a function and its Legendre trans-\\nform. Let us derive this from ﬁrst principles. For a convex differentiable\\nfunction, we know that at x0the tangent touches f(x0)so that\\nf(x0) =sx0+c. (7.56)\\nRecall that we want to describe the convex function f(x)in terms of its\\ngradient∇xf(x), and thats=∇xf(x0). We rearrange to get an expres-\\nsion for−cto obtain\\n−c=sx0−f(x0). (7.57)\\nNote that−cchanges with x0and therefore with s, which is why we can\\nthink of it as a function of s, which we call\\nf∗(s) :=sx0−f(x0). (7.58)\\nComparing (7.58) with Deﬁnition 7.4, we see that (7.58) is a special case\\n(without the supremum). ♦\\nThe conjugate function has nice properties; for example, for convex\\nfunctions, applying the Legendre transform again gets us back to the orig-\\ninal function. In the same way that the slope of f(x)iss, the slope of f∗(s)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ca3e87f-4550-4d01-8739-55081ecb01c5', embedding=None, metadata={'page_label': '244', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='244 Continuous Optimization\\nisx. The following two examples show common uses of convex conjugates\\nin machine learning.\\nExample 7.7 (Convex Conjugates)\\nTo illustrate the application of convex conjugates, consider the quadratic\\nfunction\\nf(y) =λ\\n2y⊤K−1y (7.59)\\nbased on a positive deﬁnite matrix K∈Rn×n. We denote the primal\\nvariable to be y∈Rnand the dual variable to be α∈Rn.\\nApplying Deﬁnition 7.4, we obtain the function\\nf∗(α) = sup\\ny∈Rn⟨y,α⟩−λ\\n2y⊤K−1y. (7.60)\\nSince the function is differentiable, we can ﬁnd the maximum by taking\\nthe derivative and with respect to ysetting it to zero.\\n∂[⟨y,α⟩−λ\\n2y⊤K−1y]\\n∂y= (α−λK−1y)⊤(7.61)\\nand hence when the gradient is zero we have y=1\\nλKα. Substituting\\ninto (7.60) yields\\nf∗(α) =1\\nλα⊤Kα−λ\\n2(1\\nλKα)⊤\\nK−1(1\\nλKα)\\n=1\\n2λα⊤Kα.\\n(7.62)\\nExample 7.8\\nIn machine learning, we often use sums of functions; for example, the ob-\\njective function of the training set includes a sum of the losses for each ex-\\nample in the training set. In the following, we derive the convex conjugate\\nof a sum of losses ℓ(t), whereℓ:R→R. This also illustrates the appli-\\ncation of the convex conjugate to the vector case. Let L(t) =∑n\\ni=1ℓi(ti).\\nThen,\\nL∗(z) = sup\\nt∈Rn⟨z,t⟩−n∑\\ni=1ℓi(ti) (7.63a)\\n= sup\\nt∈Rnn∑\\ni=1ziti−ℓi(ti) deﬁnition of dot product (7.63b)\\n=n∑\\ni=1sup\\nt∈Rnziti−ℓi(ti) (7.63c)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22b2a0ff-a9e2-4fb8-bc00-39cf08c3dcc6', embedding=None, metadata={'page_label': '245', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Convex Optimization 245\\n=n∑\\ni=1ℓ∗\\ni(zi). deﬁnition of conjugate (7.63d)\\nRecall that in Section 7.2 we derived a dual optimization problem using\\nLagrange multipliers. Furthermore, for convex optimization problems we\\nhave strong duality, that is the solutions of the primal and dual problem\\nmatch. The Legendre-Fenchel transform described here also can be used\\nto derive a dual optimization problem. Furthermore, when the function\\nis convex and differentiable, the supremum is unique. To further investi-\\ngate the relation between these two approaches, let us consider a linear\\nequality constrained convex optimization problem.\\nExample 7.9\\nLetf(y)andg(x)be convex functions, and Aa real matrix of appropriate\\ndimensions such that Ax=y. Then\\nmin\\nxf(Ax) +g(x) = min\\nAx=yf(y) +g(x). (7.64)\\nBy introducing the Lagrange multiplier ufor the constraints Ax=y,\\nmin\\nAx=yf(y) +g(x) = min\\nx,ymax\\nuf(y) +g(x) + (Ax−y)⊤u (7.65a)\\n= max\\numin\\nx,yf(y) +g(x) + (Ax−y)⊤u,(7.65b)\\nwhere the last step of swapping max and min is due to the fact that f(y)\\nandg(x)are convex functions. By splitting up the dot product term and\\ncollectingxandy,\\nmax\\numin\\nx,yf(y) +g(x) + (Ax−y)⊤u (7.66a)\\n= max\\nu[\\nmin\\ny−y⊤u+f(y)]\\n+[\\nmin\\nx(Ax)⊤u+g(x)]\\n(7.66b)\\n= max\\nu[\\nmin\\ny−y⊤u+f(y)]\\n+[\\nmin\\nxx⊤A⊤u+g(x)]\\n(7.66c)\\nRecall the convex conjugate (Deﬁnition 7.4) and the fact that dot prod- For general inner\\nproducts,A⊤is\\nreplaced by the\\nadjointA∗.ucts are symmetric,\\nmax\\nu[\\nmin\\ny−y⊤u+f(y)]\\n+[\\nmin\\nxx⊤A⊤u+g(x)]\\n(7.67a)\\n= max\\nu−f∗(u)−g∗(−A⊤u). (7.67b)\\nTherefore, we have shown that\\nmin\\nxf(Ax) +g(x) = max\\nu−f∗(u)−g∗(−A⊤u). (7.68)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='56c6dd58-2e6b-49fb-b95d-417439888b19', embedding=None, metadata={'page_label': '246', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='246 Continuous Optimization\\nThe Legendre-Fenchel conjugate turns out to be quite useful for ma-\\nchine learning problems that can be expressed as convex optimization\\nproblems. In particular, for convex loss functions that apply independently\\nto each example, the conjugate loss is a convenient way to derive a dual\\nproblem.\\n7.4 Further Reading\\nContinuous optimization is an active area of research, and we do not try\\nto provide a comprehensive account of recent advances.\\nFrom a gradient descent perspective, there are two major weaknesses\\nwhich each have their own set of literature. The ﬁrst challenge is the fact\\nthat gradient descent is a ﬁrst-order algorithm, and does not use infor-\\nmation about the curvature of the surface. When there are long valleys,\\nthe gradient points perpendicularly to the direction of interest. The idea\\nof momentum can be generalized to a general class of acceleration meth-\\nods (Nesterov, 2018). Conjugate gradient methods avoid the issues faced\\nby gradient descent by taking previous directions into account (Shewchuk,\\n1994). Second-order methods such as Newton methods use the Hessian to\\nprovide information about the curvature. Many of the choices for choos-\\ning step-sizes and ideas like momentum arise by considering the curvature\\nof the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\\nmethods such as L-BFGS try to use cheaper computational methods to ap-\\nproximate the Hessian (Nocedal and Wright, 2006). Recently there has\\nbeen interest in other metrics for computing descent directions, result-\\ning in approaches such as mirror descent (Beck and Teboulle, 2003) and\\nnatural gradient (Toussaint, 2012).\\nThe second challenge is to handle non-differentiable functions. Gradi-\\nent methods are not well deﬁned when there are kinks in the function.\\nIn these cases, subgradient methods can be used (Shor, 1985). For fur-\\nther information and algorithms for optimizing non-differentiable func-\\ntions, we refer to the book by Bertsekas (1999). There is a vast amount\\nof literature on different approaches for numerically solving continuous\\noptimization problems, including algorithms for constrained optimization\\nproblems. Good starting points to appreciate this literature are the books\\nby Luenberger (1969) and Bonnans et al. (2006). A recent survey of con-\\ntinuous optimization is provided by Bubeck (2015). Hugo Gonc ¸alves’\\nblog is also a good\\nresource for an\\neasier introduction\\nto Legendre–Fenchel\\ntransforms:\\nhttps://tinyurl.\\ncom/ydaal7hjModern applications of machine learning often mean that the size of\\ndatasets prohibit the use of batch gradient descent, and hence stochastic\\ngradient descent is the current workhorse of large-scale machine learning\\nmethods. Recent surveys of the literature include Hazan (2015) and Bot-\\ntou et al. (2018).\\nFor duality and convex optimization, the book by Boyd and Vanden-\\nberghe (2004) includes lectures and slides online. A more mathematical\\ntreatment is provided by Bertsekas (2009), and recent book by one of\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c24fc9b8-a98a-49fb-8d5c-2a3c07adb267', embedding=None, metadata={'page_label': '247', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Exercises 247\\nthe key researchers in the area of optimization is Nesterov (2018). Con-\\nvex optimization is based upon convex analysis, and the reader interested\\nin more foundational results about convex functions is referred to Rock-\\nafellar (1970), Hiriart-Urruty and Lemar ´echal (2001), and Borwein and\\nLewis (2006). Legendre–Fenchel transforms are also covered in the afore-\\nmentioned books on convex analysis, but a more beginner-friendly pre-\\nsentation is available at Zia et al. (2009). The role of Legendre–Fenchel\\ntransforms in the analysis of convex optimization algorithms is surveyed\\nin Polyak (2016).\\nExercises\\n7.1 Consider the univariate function\\nf(x) =x3+ 6x2−3x−5.\\nFind its stationary points and indicate whether they are maximum, mini-\\nmum, or saddle points.\\n7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).\\nWrite down the update when we use a mini-batch size of one.\\n7.3 Consider whether the following statements are true or false:\\na. The intersection of any two convex sets is convex.\\nb. The union of any two convex sets is convex.\\nc. The difference of a convex set Afrom another convex set Bis convex.\\n7.4 Consider whether the following statements are true or false:\\na. The sum of any two convex functions is convex.\\nb. The difference of any two convex functions is convex.\\nc. The product of any two convex functions is convex.\\nd. The maximum of any two convex functions is convex.\\n7.5 Express the following optimization problem as a standard linear program in\\nmatrix notation\\nmax\\nx∈R2,ξ∈Rp⊤x+ξ\\nsubject to the constraints that ξ⩾0,x0⩽0andx1⩽3.\\n7.6 Consider the linear program illustrated in Figure 7.9,\\nmin\\nx∈R2−[\\n5\\n3]⊤[\\nx1\\nx2]\\nsubject to\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02 2\\n2−4\\n−2 1\\n0−1\\n0 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb[\\nx1\\nx2]\\n⩽\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f033\\n8\\n5\\n−1\\n8\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nDerive the dual linear program using Lagrange duality.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d43a41e8-89eb-4d63-9c35-ca8316407dfc', embedding=None, metadata={'page_label': '248', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='248 Continuous Optimization\\n7.7 Consider the quadratic program illustrated in Figure 7.4,\\nmin\\nx∈R21\\n2[\\nx1\\nx2]⊤[\\n2 1\\n1 4][\\nx1\\nx2]\\n+[\\n5\\n3]⊤[\\nx1\\nx2]\\nsubject to\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01 0\\n−1 0\\n0 1\\n0−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb[\\nx1\\nx2]\\n⩽\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f01\\n1\\n1\\n1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nDerive the dual quadratic program using Lagrange duality.\\n7.8 Consider the following convex optimization problem\\nmin\\nw∈RD1\\n2w⊤w\\nsubject tow⊤x⩾1.\\nDerive the Lagrangian dual by introducing the Lagrange multiplier λ.\\n7.9 Consider the negative entropy of x∈RD,\\nf(x) =D∑\\nd=1xdlogxd.\\nDerive the convex conjugate function f∗(s), by assuming the standard dot\\nproduct.\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.10 Consider the function\\nf(x) =1\\n2x⊤Ax+b⊤x+c,\\nwhereAis strictly positive deﬁnite, which means that it is invertible. Derive\\nthe convex conjugate of f(x).\\nHint: Take the gradient of an appropriate function and set the gradient to zero.\\n7.11 The hinge loss (which is the loss used by the support vector machine) is\\ngiven by\\nL(α) = max{0,1−α},\\nIf we are interested in applying gradient methods such as L-BFGS, and do\\nnot want to resort to subgradient methods, we need to smooth the kink in\\nthe hinge loss. Compute the convex conjugate of the hinge loss L∗(β)where\\nβis the dual variable. Add a ℓ2proximal term, and compute the conjugate\\nof the resulting function\\nL∗(β) +γ\\n2β2,\\nwhereγis a given hyperparameter.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d7f4631-270c-4cf6-bfa5-70c74482224b', embedding=None, metadata={'page_label': '249', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Part II\\nCentral Machine Learning Problems\\n249\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e71a871f-2d74-46b0-b7f0-df9daa37622d', embedding=None, metadata={'page_label': '250', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d6e5c62-8e90-4aba-b2c7-c9eea4c2ad7b', embedding=None, metadata={'page_label': '251', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8\\nWhen Models Meet Data\\nIn the ﬁrst part of the book, we introduced the mathematics that form\\nthe foundations of many machine learning methods. The hope is that a\\nreader would be able to learn the rudimentary forms of the language of\\nmathematics from the ﬁrst part, which we will now use to describe and\\ndiscuss machine learning. The second part of the book introduces four\\npillars of machine learning:\\nRegression (Chapter 9)\\nDimensionality reduction (Chapter 10)\\nDensity estimation (Chapter 11)\\nClassiﬁcation (Chapter 12)\\nThe main aim of this part of the book is to illustrate how the mathematical\\nconcepts introduced in the ﬁrst part of the book can be used to design\\nmachine learning algorithms that can be used to solve tasks within the\\nremit of the four pillars. We do not intend to introduce advanced machine\\nlearning concepts, but instead to provide a set of practical methods that\\nallow the reader to apply the knowledge they gained from the ﬁrst part\\nof the book. It also provides a gateway to the wider machine learning\\nliterature for readers already familiar with the mathematics.\\n8.1 Data, Models, and Learning\\nIt is worth at this point, to pause and consider the problem that a ma-\\nchine learning algorithm is designed to solve. As discussed in Chapter 1,\\nthere are three major components of a machine learning system: data,\\nmodels, and learning. The main question of machine learning is “What do\\nwe mean by good models?”. The word model has many subtleties, and we model\\nwill revisit it multiple times in this chapter. It is also not entirely obvious\\nhow to objectively deﬁne the word “good”. One of the guiding principles\\nof machine learning is that good models should perform well on unseen\\ndata. This requires us to deﬁne some performance metrics, such as accu-\\nracy or distance from ground truth, as well as ﬁguring out ways to do well\\nunder these performance metrics. This chapter covers a few necessary bits\\nand pieces of mathematical and statistical language that are commonly\\n251\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5b171ce7-89b5-4bf1-be4e-0aef7e013b8c', embedding=None, metadata={'page_label': '252', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='252 When Models Meet Data\\nTable 8.1 Example\\ndata from a\\nﬁctitious human\\nresource database\\nthat is not in a\\nnumerical format.Name Gender Degree Postcode Age Annual salary\\nAditya M MSc W21BG 36 89563\\nBob M PhD EC1A1BA 47 123543\\nChlo´e F BEcon SW1A1BH 26 23989\\nDaisuke M BSc SE207AT 68 138769\\nElisabeth F MBA SE10AA 33 113888\\nused to talk about machine learning models. By doing so, we brieﬂy out-\\nline the current best practices for training a model such that the resulting\\npredictor does well on data that we have not yet seen.\\nAs mentioned in Chapter 1, there are two different senses in which we\\nuse the phrase “machine learning algorithm”: training and prediction. We\\nwill describe these ideas in this chapter, as well as the idea of selecting\\namong different models. We will introduce the framework of empirical\\nrisk minimization in Section 8.2, the principle of maximum likelihood in\\nSection 8.3, and the idea of probabilistic models in Section 8.4. We brieﬂy\\noutline a graphical language for specifying probabilistic models in Sec-\\ntion 8.5 and ﬁnally discuss model selection in Section 8.6. The rest of this\\nsection expands upon the three main components of machine learning:\\ndata, models and learning.\\n8.1.1 Data as Vectors\\nWe assume that our data can be read by a computer, and represented ade-\\nquately in a numerical format. Data is assumed to be tabular (Figure 8.1),\\nwhere we think of each row of the table as representing a particular in-\\nstance or example, and each column to be a particular feature. In recent Data is assumed to\\nbe in a tidy\\nformat (Wickham,\\n2014; Codd, 1990).years, machine learning has been applied to many types of data that do not\\nobviously come in the tabular numerical format, for example genomic se-\\nquences, text and image contents of a webpage, and social media graphs.\\nWe do not discuss the important and challenging aspects of identifying\\ngood features. Many of these aspects depend on domain expertise and re-\\nquire careful engineering, and, in recent years, they have been put under\\nthe umbrella of data science (Stray, 2016; Adhikari and DeNero, 2018).\\nEven when we have data in tabular format, there are still choices to be\\nmade to obtain a numerical representation. For example, in Table 8.1, the\\ngender column (a categorical variable) may be converted into numbers 0\\nrepresenting “Male” and 1representing “Female”. Alternatively, the gen-\\nder could be represented by numbers −1,+1, respectively (as shown in\\nTable 8.2). Furthermore, it is often important to use domain knowledge\\nwhen constructing the representation, such as knowing that university\\ndegrees progress from bachelor’s to master’s to PhD or realizing that the\\npostcode provided is not just a string of characters but actually encodes\\nan area in London. In Table 8.2, we converted the data from Table 8.1\\nto a numerical format, and each postcode is represented as two numbers,\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='28a47c78-161f-4e2e-ba78-2a5f5d52d52d', embedding=None, metadata={'page_label': '253', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.1 Data, Models, and Learning 253\\nTable 8.2 Example\\ndata from a\\nﬁctitious human\\nresource database\\n(see Table 8.1),\\nconverted to a\\nnumerical format.Gender ID Degree Latitude Longitude Age Annual Salary\\n(in degrees) (in degrees) (in thousands)\\n-1 2 51.5073 0.1290 36 89.563\\n-1 3 51.5074 0.1275 47 123.543\\n+1 1 51.5071 0.1278 26 23.989\\n-1 1 51.5075 0.1281 68 138.769\\n+1 2 51.5074 0.1278 33 113.888\\na latitude and longitude. Even numerical data that could potentially be\\ndirectly read into a machine learning algorithm should be carefully con-\\nsidered for units, scaling, and constraints. Without additional information,\\none should shift and scale all columns of the dataset such that they have\\nan empirical mean of 0and an empirical variance of 1. For the purposes\\nof this book, we assume that a domain expert already converted data ap-\\npropriately, i.e., each input xnis aD-dimensional vector of real numbers,\\nwhich are called features ,attributes , orcovariates . We consider a dataset to feature\\nattribute\\ncovariatebe of the form as illustrated by Table 8.2. Observe that we have dropped\\nthe Name column of Table 8.1 in the new numerical representation. There\\nare two main reasons why this is desirable: (1) we do not expect the iden-\\ntiﬁer (the Name) to be informative for a machine learning task; and (2)\\nwe may wish to anonymize the data to help protect the privacy of the\\nemployees.\\nIn this part of the book, we will use Nto denote the number of exam-\\nples in a dataset and index the examples with lowercase n= 1,...,N .\\nWe assume that we are given a set of numerical data, represented as an\\narray of vectors (Table 8.2). Each row is a particular individual xn, often\\nreferred to as an example ordata point in machine learning. The subscript example\\ndata point nrefers to the fact that this is the nth example out of a total of Nexam-\\nples in the dataset. Each column represents a particular feature of interest\\nabout the example, and we index the features as d= 1,...,D . Recall that\\ndata is represented as vectors, which means that each example (each data\\npoint) is aD-dimensional vector. The orientation of the table originates\\nfrom the database community, but for some machine learning algorithms\\n(e.g., in Chapter 10) it is more convenient to represent examples as col-\\numn vectors.\\nLet us consider the problem of predicting annual salary from age, based\\non the data in Table 8.2. This is called a supervised learning problem\\nwhere we have a labelyn(the salary) associated with each example xn label\\n(the age). The label ynhas various other names, including target, re-\\nsponse variable, and annotation. A dataset is written as a set of example-\\nlabel pairs{(x1,y1),..., (xn,yn),..., (xN,yN)}. The table of examples\\n{x1,...,xN}is often concatenated, and written as X∈RN×D. Fig-\\nure 8.1 illustrates the dataset consisting of the two rightmost columns\\nof Table 8.2, where x=age andy=salary.\\nWe use the concepts introduced in the ﬁrst part of the book to formalize\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eba7a4e8-596d-4b99-97fe-55ae07da08ec', embedding=None, metadata={'page_label': '254', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='254 When Models Meet Data\\nFigure 8.1 Toy data\\nfor linear regression.\\nTraining data in\\n(xn,yn)pairs from\\nthe rightmost two\\ncolumns of\\nTable 8.2. We are\\ninterested in the\\nsalary of a person\\naged sixty (x= 60 )\\nillustrated as a\\nvertical dashed red\\nline, which is not\\npart of the training\\ndata.\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\n?\\nthe machine learning problems such as that in the previous paragraph.\\nRepresenting data as vectors xnallows us to use concepts from linear al-\\ngebra (introduced in Chapter 2). In many machine learning algorithms,\\nwe need to additionally be able to compare two vectors. As we will see in\\nChapters 9 and 12, computing the similarity or distance between two ex-\\namples allows us to formalize the intuition that examples with similar fea-\\ntures should have similar labels. The comparison of two vectors requires\\nthat we construct a geometry (explained in Chapter 3) and allows us to\\noptimize the resulting learning problem using techniques from Chapter 7.\\nSince we have vector representations of data, we can manipulate data to\\nﬁnd potentially better representations of it. We will discuss ﬁnding good\\nrepresentations in two ways: ﬁnding lower-dimensional approximations\\nof the original feature vector, and using nonlinear higher-dimensional\\ncombinations of the original feature vector. In Chapter 10, we will see an\\nexample of ﬁnding a low-dimensional approximation of the original data\\nspace by ﬁnding the principal components. Finding principal components\\nis closely related to concepts of eigenvalue and singular value decomposi-\\ntion as introduced in Chapter 4. For the high-dimensional representation,\\nwe will see an explicit feature map φ(·)that allows us to represent in- feature map\\nputsxnusing a higher-dimensional representation φ(xn). The main mo-\\ntivation for higher-dimensional representations is that we can construct\\nnew features as non-linear combinations of the original features, which in\\nturn may make the learning problem easier. We will discuss the feature\\nmap in Section 9.2 and show how this feature map leads to a kernel in kernel\\nSection 12.4. In recent years, deep learning methods (Goodfellow et al.,\\n2016) have shown promise in using the data itself to learn new good fea-\\ntures and have been very successful in areas, such as computer vision,\\nspeech recognition, and natural language processing. We will not cover\\nneural networks in this part of the book, but the reader is referred to\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e80a915e-6abb-45f4-bea0-8cc4c08ecba3', embedding=None, metadata={'page_label': '255', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.1 Data, Models, and Learning 255\\nFigure 8.2 Example\\nfunction (black solid\\ndiagonal line) and\\nits prediction at\\nx= 60 , i.e.,\\nf(60) = 100 .\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nSection 5.6 for the mathematical description of backpropagation, a key\\nconcept for training neural networks.\\n8.1.2 Models as Functions\\nOnce we have data in an appropriate vector representation, we can get to\\nthe business of constructing a predictive function (known as a predictor ). predictor\\nIn Chapter 1, we did not yet have the language to be precise about models.\\nUsing the concepts from the ﬁrst part of the book, we can now introduce\\nwhat “model” means. We present two major approaches in this book: a\\npredictor as a function, and a predictor as a probabilistic model. We de-\\nscribe the former here and the latter in the next subsection.\\nA predictor is a function that, when given a particular input example\\n(in our case, a vector of features), produces an output. For now, consider\\nthe output to be a single number, i.e., a real-valued scalar output. This can\\nbe written as\\nf:RD→R, (8.1)\\nwhere the input vector xisD-dimensional (has Dfeatures), and the func-\\ntionfthen applied to it (written as f(x)) returns a real number. Fig-\\nure 8.2 illustrates a possible function that can be used to compute the\\nvalue of the prediction for input values x.\\nIn this book, we do not consider the general case of all functions, which\\nwould involve the need for functional analysis. Instead, we consider the\\nspecial case of linear functions\\nf(x) =θ⊤x+θ0 (8.2)\\nfor unknown θandθ0. This restriction means that the contents of Chap-\\nters 2 and 3 sufﬁce for precisely stating the notion of a predictor for\\nthe non-probabilistic (in contrast to the probabilistic view described next)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b5add36-0b57-4b38-947f-3f4ef710af75', embedding=None, metadata={'page_label': '256', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='256 When Models Meet Data\\nFigure 8.3 Example\\nfunction (black solid\\ndiagonal line) and\\nits predictive\\nuncertainty at\\nx= 60 (drawn as a\\nGaussian).\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nview of machine learning. Linear functions strike a good balance between\\nthe generality of the problems that can be solved and the amount of back-\\nground mathematics that is needed.\\n8.1.3 Models as Probability Distributions\\nWe often consider data to be noisy observations of some true underlying\\neffect, and hope that by applying machine learning we can identify the\\nsignal from the noise. This requires us to have a language for quantify-\\ning the effect of noise. We often would also like to have predictors that\\nexpress some sort of uncertainty, e.g., to quantify the conﬁdence we have\\nabout the value of the prediction for a particular test data point. As we\\nhave seen in Chapter 6, probability theory provides a language for quan-\\ntifying uncertainty. Figure 8.3 illustrates the predictive uncertainty of the\\nfunction as a Gaussian distribution.\\nInstead of considering a predictor as a single function, we could con-\\nsider predictors to be probabilistic models, i.e., models describing the dis-\\ntribution of possible functions. We limit ourselves in this book to the spe-\\ncial case of distributions with ﬁnite-dimensional parameters, which allows\\nus to describe probabilistic models without needing stochastic processes\\nand random measures. For this special case, we can think about prob-\\nabilistic models as multivariate probability distributions, which already\\nallow for a rich class of models.\\nWe will introduce how to use concepts from probability (Chapter 6) to\\ndeﬁne machine learning models in Section 8.4, and introduce a graphical\\nlanguage for describing probabilistic models in a compact way in Sec-\\ntion 8.5.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce8254a7-ed0d-435d-9233-075da0aeae15', embedding=None, metadata={'page_label': '257', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.1 Data, Models, and Learning 257\\n8.1.4 Learning is Finding Parameters\\nThe goal of learning is to ﬁnd a model and its corresponding parame-\\nters such that the resulting predictor will perform well on unseen data.\\nThere are conceptually three distinct algorithmic phases when discussing\\nmachine learning algorithms:\\n1. Prediction or inference\\n2. Training or parameter estimation\\n3. Hyperparameter tuning or model selection\\nThe prediction phase is when we use a trained predictor on previously un-\\nseen test data. In other words, the parameters and model choice is already\\nﬁxed and the predictor is applied to new vectors representing new input\\ndata points. As outlined in Chapter 1 and the previous subsection, we will\\nconsider two schools of machine learning in this book, corresponding to\\nwhether the predictor is a function or a probabilistic model. When we\\nhave a probabilistic model (discussed further in Section 8.4) the predic-\\ntion phase is called inference.\\nRemark. Unfortunately, there is no agreed upon naming for the different\\nalgorithmic phases. The word “inference” is sometimes also used to mean\\nparameter estimation of a probabilistic model, and less often may be also\\nused to mean prediction for non-probabilistic models. ♦\\nThe training or parameter estimation phase is when we adjust our pre-\\ndictive model based on training data. We would like to ﬁnd good predic-\\ntors given training data, and there are two main strategies for doing so:\\nﬁnding the best predictor based on some measure of quality (sometimes\\ncalled ﬁnding a point estimate), or using Bayesian inference. Finding a\\npoint estimate can be applied to both types of predictors, but Bayesian\\ninference requires probabilistic models.\\nFor the non-probabilistic model, we follow the principle of empirical risk empirical risk\\nminimization minimization , which we describe in Section 8.2. Empirical risk minimiza-\\ntion directly provides an optimization problem for ﬁnding good parame-\\nters. With a statistical model, the principle of maximum likelihood is used maximum likelihood\\nto ﬁnd a good set of parameters (Section 8.3). We can additionally model\\nthe uncertainty of parameters using a probabilistic model, which we will\\nlook at in more detail in Section 8.4.\\nWe use numerical methods to ﬁnd good parameters that “ﬁt” the data,\\nand most training methods can be thought of as hill-climbing approaches\\nto ﬁnd the maximum of an objective, for example the maximum of a likeli-\\nhood. To apply hill-climbing approaches we use the gradients described in The convention in\\noptimization is to\\nminimize objectives.\\nHence, there is often\\nan extra minus sign\\nin machine learning\\nobjectives.Chapter 5 and implement numerical optimization approaches from Chap-\\nter 7.\\nAs mentioned in Chapter 1, we are interested in learning a model based\\non data such that it performs well on future data. It is not enough for\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4fa260c-a168-44c5-9e63-8c1870ba94f0', embedding=None, metadata={'page_label': '258', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='258 When Models Meet Data\\nthe model to only ﬁt the training data well, the predictor needs to per-\\nform well on unseen data. We simulate the behavior of our predictor on\\nfuture unseen data using cross-validation (Section 8.2.4). As we will see cross-validation\\nin this chapter, to achieve the goal of performing well on unseen data,\\nwe will need to balance between ﬁtting well on training data and ﬁnding\\n“simple” explanations of the phenomenon. This trade-off is achieved us-\\ning regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In\\nphilosophy, this is considered to be neither induction nor deduction, but\\nis called abduction . According to the Stanford Encyclopedia of Philosophy , abduction\\nabduction is the process of inference to the best explanation (Douven,\\n2017). A good movie title is\\n“AI abduction”. We often need to make high-level modeling decisions about the struc-\\nture of the predictor, such as the number of components to use or the\\nclass of probability distributions to consider. The choice of the number of\\ncomponents is an example of a hyperparameter , and this choice can af- hyperparameter\\nfect the performance of the model signiﬁcantly. The problem of choosing\\namong different models is called model selection , which we describe in model selection\\nSection 8.6. For non-probabilistic models, model selection is often done\\nusing nested cross-validation , which is described in Section 8.6.1. We also nested\\ncross-validation use model selection to choose hyperparameters of our model.\\nRemark. The distinction between parameters and hyperparameters is some-\\nwhat arbitrary, and is mostly driven by the distinction between what can\\nbe numerically optimized versus what needs to use search techniques.\\nAnother way to consider the distinction is to consider parameters as the\\nexplicit parameters of a probabilistic model, and to consider hyperparam-\\neters (higher-level parameters) as parameters that control the distribution\\nof these explicit parameters. ♦\\nIn the following sections, we will look at three ﬂavors of machine learn-\\ning: empirical risk minimization (Section 8.2), the principle of maximum\\nlikelihood (Section 8.3), and probabilistic modeling (Section 8.4).\\n8.2 Empirical Risk Minimization\\nAfter having all the mathematics under our belt, we are now in a posi-\\ntion to introduce what it means to learn. The “learning” part of machine\\nlearning boils down to estimating parameters based on training data.\\nIn this section, we consider the case of a predictor that is a function,\\nand consider the case of probabilistic models in Section 8.3. We describe\\nthe idea of empirical risk minimization, which was originally popularized\\nby the proposal of the support vector machine (described in Chapter 12).\\nHowever, its general principles are widely applicable and allow us to ask\\nthe question of what is learning without explicitly constructing probabilis-\\ntic models. There are four main design choices, which we will cover in\\ndetail in the following subsections:\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa6a4f42-5eef-4a99-b26c-e6c3cf000b2f', embedding=None, metadata={'page_label': '259', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.2 Empirical Risk Minimization 259\\nSection 8.2.1 What is the set of functions we allow the predictor to take?\\nSection 8.2.2 How do we measure how well the predictor performs on\\nthe training data?\\nSection 8.2.3 How do we construct predictors from only training data\\nthat performs well on unseen test data?\\nSection 8.2.4 What is the procedure for searching over the space of mod-\\nels?\\n8.2.1 Hypothesis Class of Functions\\nAssume we are given Nexamplesxn∈RDand corresponding scalar la-\\nbelsyn∈R. We consider the supervised learning setting, where we obtain\\npairs (x1,y1),..., (xN,yN). Given this data, we would like to estimate a\\npredictorf(·,θ) :RD→R, parametrized by θ. We hope to be able to ﬁnd\\na good parameter θ∗such that we ﬁt the data well, that is,\\nf(xn,θ∗)≈ynfor alln= 1,...,N. (8.3)\\nIn this section, we use the notation ˆyn=f(xn,θ∗)to represent the output\\nof the predictor.\\nRemark. For ease of presentation, we will describe empirical risk mini-\\nmization in terms of supervised learning (where we have labels). This\\nsimpliﬁes the deﬁnition of the hypothesis class and the loss function. It\\nis also common in machine learning to choose a parametrized class of\\nfunctions, for example afﬁne functions. ♦\\nExample 8.1\\nWe introduce the problem of ordinary least-squares regression to illustrate\\nempirical risk minimization. A more comprehensive account of regression\\nis given in Chapter 9. When the label ynis real-valued, a popular choice\\nof function class for predictors is the set of afﬁne functions. We choose a Afﬁne functions are\\noften referred to as\\nlinear functions in\\nmachine learning.more compact notation for an afﬁne function by concatenating an addi-\\ntional unit feature x(0)= 1toxn, i.e.,xn= [1,x(1)\\nn,x(2)\\nn,...,x(D)\\nn]⊤. The\\nparameter vector is correspondingly θ= [θ0,θ1,θ2,...,θD]⊤, allowing us\\nto write the predictor as a linear function\\nf(xn,θ) =θ⊤xn. (8.4)\\nThis linear predictor is equivalent to the afﬁne model\\nf(xn,θ) =θ0+D∑\\nd=1θdx(d)\\nn. (8.5)\\nThe predictor takes the vector of features representing a single example\\nxnas input and produces a real-valued output, i.e., f:RD+1→R. The\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be4dddde-9ae8-4a18-9eb0-6909b03fe918', embedding=None, metadata={'page_label': '260', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='260 When Models Meet Data\\nprevious ﬁgures in this chapter had a straight line as a predictor, which\\nmeans that we have assumed an afﬁne function.\\nInstead of a linear function, we may wish to consider non-linear func-\\ntions as predictors. Recent advances in neural networks allow for efﬁcient\\ncomputation of more complex non-linear function classes.\\nGiven the class of functions, we want to search for a good predictor.\\nWe now move on to the second ingredient of empirical risk minimization:\\nhow to measure how well the predictor ﬁts the training data.\\n8.2.2 Loss Function for Training\\nConsider the label ynfor a particular example; and the corresponding pre-\\ndiction ˆynthat we make based on xn. To deﬁne what it means to ﬁt the\\ndata well, we need to specify a loss function ℓ(yn,ˆyn)that takes the ground loss function\\ntruth label and the prediction as input and produces a non-negative num-\\nber (referred to as the loss) representing how much error we have made\\non this particular prediction. Our goal for ﬁnding a good parameter vector The expression\\n“error” is often used\\nto mean loss.θ∗is to minimize the average loss on the set of Ntraining examples.\\nOne assumption that is commonly made in machine learning is that\\nthe set of examples (x1,y1),..., (xN,yN)isindependent and identically independent and\\nidentically\\ndistributeddistributed . The word independent (Section 6.4.5) means that two data\\npoints (xi,yi)and(xj,yj)do not statistically depend on each other, mean-\\ning that the empirical mean is a good estimate of the population mean\\n(Section 6.4.1). This implies that we can use the empirical mean of the\\nloss on the training data. For a given training set{(x1,y1),..., (xN,yN)}, training set\\nwe introduce the notation of an example matrix X:= [x1,...,xN]⊤∈\\nRN×Dand a label vector y:= [y1,...,yN]⊤∈RN. Using this matrix\\nnotation the average loss is given by\\nRemp(f,X,y) =1\\nNN∑\\nn=1ℓ(yn,ˆyn), (8.6)\\nwhere ˆyn=f(xn,θ). Equation (8.6) is called the empirical risk and de- empirical risk\\npends on three arguments, the predictor fand the dataX,y. This general\\nstrategy for learning is called empirical risk minimization . empirical risk\\nminimization\\nExample 8.2 (Least-Squares Loss)\\nContinuing the example of least-squares regression, we specify that we\\nmeasure the cost of making an error during training using the squared\\nlossℓ(yn,ˆyn) = (yn−ˆyn)2. We wish to minimize the empirical risk (8.6),\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db112fe1-ae92-4aa9-8f0d-376537bc8b71', embedding=None, metadata={'page_label': '261', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.2 Empirical Risk Minimization 261\\nwhich is the average of the losses over the data\\nmin\\nθ∈RD1\\nNN∑\\nn=1(yn−f(xn,θ))2, (8.7)\\nwhere we substituted the predictor ˆyn=f(xn,θ). By using our choice of\\na linear predictor f(xn,θ) =θ⊤xn, we obtain the optimization problem\\nmin\\nθ∈RD1\\nNN∑\\nn=1(yn−θ⊤xn)2. (8.8)\\nThis equation can be equivalently expressed in matrix form\\nmin\\nθ∈RD1\\nN∥y−Xθ∥2. (8.9)\\nThis is known as the least-squares problem . There exists a closed-form an- least-squares\\nproblemalytic solution for this by solving the normal equations, which we will\\ndiscuss in Section 9.2.\\nWe are not interested in a predictor that only performs well on the\\ntraining data. Instead, we seek a predictor that performs well (has low\\nrisk) on unseen test data. More formally, we are interested in ﬁnding a\\npredictorf(with parameters ﬁxed) that minimizes the expected risk expected risk\\nRtrue(f) =Ex,y[ℓ(y,f(x))], (8.10)\\nwhereyis the label and f(x)is the prediction based on the example x.\\nThe notation Rtrue(f)indicates that this is the true risk if we had access to\\nan inﬁnite amount of data. The expectation is over the (inﬁnite) set of all Another phrase\\ncommonly used for\\nexpected risk is\\n“population risk”.possible data and labels. There are two practical questions that arise from\\nour desire to minimize expected risk, which we address in the following\\ntwo subsections:\\nHow should we change our training procedure to generalize well?\\nHow do we estimate expected risk from (ﬁnite) data?\\nRemark. Many machine learning tasks are speciﬁed with an associated\\nperformance measure, e.g., accuracy of prediction or root mean squared\\nerror. The performance measure could be more complex, be cost sensitive,\\nand capture details about the particular application. In principle, the de-\\nsign of the loss function for empirical risk minimization should correspond\\ndirectly to the performance measure speciﬁed by the machine learning\\ntask. In practice, there is often a mismatch between the design of the loss\\nfunction and the performance measure. This could be due to issues such\\nas ease of implementation or efﬁciency of optimization. ♦\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12c29391-4eab-4f99-99f0-ddd29084ec0e', embedding=None, metadata={'page_label': '262', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='262 When Models Meet Data\\n8.2.3 Regularization to Reduce Overﬁtting\\nThis section describes an addition to empirical risk minimization that al-\\nlows it to generalize well (approximately minimizing expected risk). Re-\\ncall that the aim of training a machine learning predictor is so that we can\\nperform well on unseen data, i.e., the predictor generalizes well. We sim-\\nulate this unseen data by holding out a proportion of the whole dataset.\\nThis hold out set is referred to as the test set . Given a sufﬁciently rich class test set\\nEven knowing only\\nthe performance of\\nthe predictor on the\\ntest set leaks\\ninformation (Blum\\nand Hardt, 2015).of functions for the predictor f, we can essentially memorize the training\\ndata to obtain zero empirical risk. While this is great to minimize the loss\\n(and therefore the risk) on the training data, we would not expect the\\npredictor to generalize well to unseen data. In practice, we have only a\\nﬁnite set of data, and hence we split our data into a training and a test\\nset. The training set is used to ﬁt the model, and the test set (not seen\\nby the machine learning algorithm during training) is used to evaluate\\ngeneralization performance. It is important for the user to not cycle back\\nto a new round of training after having observed the test set. We use the\\nsubscripts trainand testto denote the training and test sets, respectively.\\nWe will revisit this idea of using a ﬁnite dataset to evaluate expected risk\\nin Section 8.2.4.\\nIt turns out that empirical risk minimization can lead to overﬁtting , i.e., overﬁtting\\nthe predictor ﬁts too closely to the training data and does not general-\\nize well to new data (Mitchell, 1997). This general phenomenon of hav-\\ning very small average loss on the training set but large average loss on\\nthe test set tends to occur when we have little data and a complex hy-\\npothesis class. For a particular predictor f(with parameters ﬁxed), the\\nphenomenon of overﬁtting occurs when the risk estimate from the train-\\ning data Remp(f,Xtrain,ytrain)underestimates the expected risk Rtrue(f).\\nSince we estimate the expected risk Rtrue(f)by using the empirical risk\\non the test set Remp(f,Xtest,ytest)if the test risk is much larger than\\nthe training risk, this is an indication of overﬁtting. We revisit the idea of\\noverﬁtting in Section 8.3.3.\\nTherefore, we need to somehow bias the search for the minimizer of\\nempirical risk by introducing a penalty term, which makes it harder for\\nthe optimizer to return an overly ﬂexible predictor. In machine learning,\\nthe penalty term is referred to as regularization . Regularization is a way regularization\\nto compromise between accurate solution of empirical risk minimization\\nand the size or complexity of the solution.\\nExample 8.3 (Regularized Least Squares)\\nRegularization is an approach that discourages complex or extreme solu-\\ntions to an optimization problem. The simplest regularization strategy is\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8efec489-862f-412b-aa8e-5d605138fc4f', embedding=None, metadata={'page_label': '263', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.2 Empirical Risk Minimization 263\\nto replace the least-squares problem\\nmin\\nθ1\\nN∥y−Xθ∥2. (8.11)\\nin the previous example with the “regularized” problem by adding a\\npenalty term involving only θ:\\nmin\\nθ1\\nN∥y−Xθ∥2+λ∥θ∥2. (8.12)\\nThe additional term ∥θ∥2is called the regularizer , and the parameter regularizer\\nλis the regularization parameter . The regularization parameter trades regularization\\nparameteroff minimizing the loss on the training set and the magnitude of the pa-\\nrametersθ. It often happens that the magnitude of the parameter values\\nbecomes relatively large if we run into overﬁtting (Bishop, 2006).\\nThe regularization term is sometimes called the penalty term , which bi- penalty term\\nases the vector θto be closer to the origin. The idea of regularization also\\nappears in probabilistic models as the prior probability of the parameters.\\nRecall from Section 6.6 that for the posterior distribution to be of the same\\nform as the prior distribution, the prior and the likelihood need to be con-\\njugate. We will revisit this idea in Section 8.3.2. We will see in Chapter 12\\nthat the idea of the regularizer is equivalent to the idea of a large margin.\\n8.2.4 Cross-Validation to Assess the Generalization Performance\\nWe mentioned in the previous section that we measure the generalization\\nerror by estimating it by applying the predictor on test data. This data is\\nalso sometimes referred to as the validation set . The validation set is a sub- validation set\\nset of the available training data that we keep aside. A practical issue with\\nthis approach is that the amount of data is limited, and ideally we would\\nuse as much of the data available to train the model. This would require\\nus to keep our validation set Vsmall, which then would lead to a noisy\\nestimate (with high variance) of the predictive performance. One solu-\\ntion to these contradictory objectives (large training set, large validation\\nset) is to use cross-validation .K-fold cross-validation effectively partitions cross-validation\\nthe data into Kchunks,K−1of which form the training set R, and\\nthe last chunk serves as the validation set V(similar to the idea outlined\\npreviously). Cross-validation iterates through (ideally) all combinations\\nof assignments of chunks to RandV; see Figure 8.4. This procedure is\\nrepeated for all Kchoices for the validation set, and the performance of\\nthe model from the Kruns is averaged.\\nWe partition our dataset into two sets D=R∪V , such that they do not\\noverlap (R∩V =∅), whereVis the validation set, and train our model\\nonR. After training, we assess the performance of the predictor fon the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6fa080d5-5b92-46b2-944e-7c43104cd1c8', embedding=None, metadata={'page_label': '264', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='264 When Models Meet Data\\nFigure 8.4K-fold\\ncross-validation.\\nThe dataset is\\ndivided into K= 5\\nchunks,K−1of\\nwhich serve as the\\ntraining set (blue)\\nand one as the\\nvalidation set\\n(orange hatch).Training\\nValidation\\nvalidation setV(e.g., by computing root mean square error (RMSE) of\\nthe trained model on the validation set). More precisely, for each partition\\nkthe training dataR(k)produces a predictor f(k), which is then applied\\nto validation setV(k)to compute the empirical risk R(f(k),V(k)). We cycle\\nthrough all possible partitionings of validation and training sets and com-\\npute the average generalization error of the predictor. Cross-validation\\napproximates the expected generalization error\\nEV[R(f,V)]≈1\\nKK∑\\nk=1R(f(k),V(k)), (8.13)\\nwhereR(f(k),V(k))is the risk (e.g., RMSE) on the validation set V(k)for\\npredictorf(k). The approximation has two sources: ﬁrst, due to the ﬁnite\\ntraining set, which results in not the best possible f(k); and second, due to\\nthe ﬁnite validation set, which results in an inaccurate estimation of the\\nriskR(f(k),V(k)). A potential disadvantage of K-fold cross-validation is\\nthe computational cost of training the model Ktimes, which can be bur-\\ndensome if the training cost is computationally expensive. In practice, it\\nis often not sufﬁcient to look at the direct parameters alone. For example,\\nwe need to explore multiple complexity parameters (e.g., multiple regu-\\nlarization parameters), which may not be direct parameters of the model.\\nEvaluating the quality of the model, depending on these hyperparameters,\\nmay result in a number of training runs that is exponential in the number\\nof model parameters. One can use nested cross-validation (Section 8.6.1)\\nto search for good hyperparameters.\\nHowever, cross-validation is an embarrassingly parallel problem, i.e., lit- embarrassingly\\nparallel tle effort is needed to separate the problem into a number of parallel\\ntasks. Given sufﬁcient computing resources (e.g., cloud computing, server\\nfarms), cross-validation does not require longer than a single performance\\nassessment.\\nIn this section, we saw that empirical risk minimization is based on the\\nfollowing concepts: the hypothesis class of functions, the loss function and\\nregularization. In Section 8.3, we will see the effect of using a probability\\ndistribution to replace the idea of loss functions and regularization.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1027155-e9b5-4e0d-be96-3ba2a54d4fd7', embedding=None, metadata={'page_label': '265', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Parameter Estimation 265\\n8.2.5 Further Reading\\nDue to the fact that the original development of empirical risk minimiza-\\ntion (Vapnik, 1998) was couched in heavily theoretical language, many\\nof the subsequent developments have been theoretical. The area of study\\nis called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statistical learning\\ntheory Hastie et al., 2001; von Luxburg and Sch ¨olkopf, 2011). A recent machine\\nlearning textbook that builds on the theoretical foundations and develops\\nefﬁcient learning algorithms is Shalev-Shwartz and Ben-David (2014).\\nThe concept of regularization has its roots in the solution of ill-posed in-\\nverse problems (Neumaier, 1998). The approach presented here is called\\nTikhonov regularization , and there is a closely related constrained version Tikhonov\\nregularization called Ivanov regularization. Tikhonov regularization has deep relation-\\nships to the bias-variance trade-off and feature selection (B ¨uhlmann and\\nVan De Geer, 2011). An alternative to cross-validation is bootstrap and\\njackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall,\\n1992).\\nThinking about empirical risk minimization (Section 8.2) as “probabil-\\nity free” is incorrect. There is an underlying unknown probability distri-\\nbutionp(x,y)that governs the data generation. However, the approach\\nof empirical risk minimization is agnostic to that choice of distribution.\\nThis is in contrast to standard statistical approaches that explicitly re-\\nquire the knowledge of p(x,y). Furthermore, since the distribution is a\\njoint distribution on both examples xand labelsy, the labels can be non-\\ndeterministic. In contrast to standard statistics we do not need to specify\\nthe noise distribution for the labels y.\\n8.3 Parameter Estimation\\nIn Section 8.2, we did not explicitly model our problem using probability\\ndistributions. In this section, we will see how to use probability distribu-\\ntions to model our uncertainty due to the observation process and our\\nuncertainty in the parameters of our predictors. In Section 8.3.1, we in-\\ntroduce the likelihood, which is analogous to the concept of loss functions\\n(Section 8.2.2) in empirical risk minimization. The concept of priors (Sec-\\ntion 8.3.2) is analogous to the concept of regularization (Section 8.2.3).\\n8.3.1 Maximum Likelihood Estimation\\nThe idea behind maximum likelihood estimation (MLE) is to deﬁne a func- maximum likelihood\\nestimation tion of the parameters that enables us to ﬁnd a model that ﬁts the data\\nwell. The estimation problem is focused on the likelihood function, or likelihood\\nmore precisely its negative logarithm. For data represented by a random\\nvariablexand for a family of probability densities p(x|θ)parametrized\\nbyθ, the negative log-likelihood is given by negative\\nlog-likelihood\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73b6129c-847a-48aa-a752-ea5e131fdf55', embedding=None, metadata={'page_label': '266', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='266 When Models Meet Data\\nLx(θ) =−logp(x|θ). (8.14)\\nThe notationLx(θ)emphasizes the fact that the parameter θis varying\\nand the dataxis ﬁxed. We very often drop the reference to xwhen writing\\nthe negative log-likelihood, as it is really a function of θ, and write it as\\nL(θ)when the random variable representing the uncertainty in the data\\nis clear from the context.\\nLet us interpret what the probability density p(x|θ)is modeling for a\\nﬁxed value of θ. It is a distribution that models the uncertainty of the data.\\nIn other words, once we have chosen the type of function we want as a\\npredictor, the likelihood provides the probability of observing data x.\\nIn a complementary view, if we consider the data to be ﬁxed (because\\nit has been observed), and we vary the parameters θ, what doesL(θ)tell\\nus? It tells us how likely a particular setting of θis for the observations x.\\nBased on this second view, the maximum likelihood estimator gives us the\\nmost likely parameter θfor the set of data.\\nWe consider the supervised learning setting, where we obtain pairs\\n(x1,y1),..., (xN,yN)withxn∈RDand labelsyn∈R. We are inter-\\nested in constructing a predictor that takes a feature vector xnas input\\nand produces a prediction yn(or something close to it), i.e., given a vec-\\ntorxnwe want the probability distribution of the label yn. In other words,\\nwe specify the conditional probability distribution of the labels given the\\nexamples for the particular parameter setting θ.\\nExample 8.4\\nThe ﬁrst example that is often used is to specify that the conditional\\nprobability of the labels given the examples is a Gaussian distribution. In\\nother words, we assume that we can explain our observation uncertainty\\nby independent Gaussian noise (refer to Section 6.5) with zero mean,\\nεn∼N(0, σ2)\\n. We further assume that the linear model x⊤\\nnθis used for\\nprediction. This means we specify a Gaussian likelihood for each example\\nlabel pair (xn,yn),\\np(yn|xn,θ) =N(yn|x⊤\\nnθ, σ2). (8.15)\\nAn illustration of a Gaussian likelihood for a given parameter θis shown\\nin Figure 8.3. We will see in Section 9.2 how to explicitly expand the\\npreceding expression out in terms of the Gaussian distribution.\\nWe assume that the set of examples (x1,y1),..., (xN,yN)areindependent independent and\\nidentically\\ndistributedand identically distributed (i.i.d.). The word “independent” (Section 6.4.5)\\nimplies that the likelihood of the whole dataset ( Y={y1,...,yN}and\\nX={x1,...,xN}factorizes into a product of the likelihoods of each\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ea40497-7353-43cc-b38d-41d8c7a23a9f', embedding=None, metadata={'page_label': '267', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Parameter Estimation 267\\nindividual example\\np(Y|X,θ) =N∏\\nn=1p(yn|xn,θ), (8.16)\\nwherep(yn|xn,θ)is a particular distribution (which was Gaussian in Ex-\\nample 8.4). The expression “identically distributed” means that each term\\nin the product (8.16) is of the same distribution, and all of them share\\nthe same parameters. It is often easier from an optimization viewpoint to\\ncompute functions that can be decomposed into sums of simpler functions.\\nHence, in machine learning we often consider the negative log-likelihood Recall log(ab) =\\nlog(a) + log(b)\\nL(θ) =−logp(Y|X,θ) =−N∑\\nn=1logp(yn|xn,θ). (8.17)\\nWhile it is temping to interpret the fact that θis on the right of the condi-\\ntioning inp(yn|xn,θ)(8.15), and hence should be interpreted as observed\\nand ﬁxed, this interpretation is incorrect. The negative log-likelihood L(θ)\\nis a function of θ. Therefore, to ﬁnd a good parameter vector θthat\\nexplains the data (x1,y1),..., (xN,yN)well, minimize the negative log-\\nlikelihoodL(θ)with respect to θ.\\nRemark. The negative sign in (8.17) is a historical artifact that is due\\nto the convention that we want to maximize likelihood, but numerical\\noptimization literature tends to study minimization of functions. ♦\\nExample 8.5\\nContinuing on our example of Gaussian likelihoods (8.15), the negative\\nlog-likelihood can be rewritten as\\nL(θ) =−N∑\\nn=1logp(yn|xn,θ) =−N∑\\nn=1logN(yn|x⊤\\nnθ, σ2)\\n(8.18a)\\n=−N∑\\nn=1log1√\\n2πσ2exp(\\n−(yn−x⊤\\nnθ)2\\n2σ2)\\n(8.18b)\\n=−N∑\\nn=1log exp(\\n−(yn−x⊤\\nnθ)2\\n2σ2)\\n−N∑\\nn=1log1√\\n2πσ2(8.18c)\\n=1\\n2σ2N∑\\nn=1(yn−x⊤\\nnθ)2−N∑\\nn=1log1√\\n2πσ2. (8.18d)\\nAsσis given, the second term in (8.18d) is constant, and minimizing L(θ)\\ncorresponds to solving the least-squares problem (compare with (8.8))\\nexpressed in the ﬁrst term.\\nIt turns out that for Gaussian likelihoods the resulting optimization\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be2cb1e8-486e-40d3-8aac-6f8d794a1a15', embedding=None, metadata={'page_label': '268', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='268 When Models Meet Data\\nFigure 8.5 For the\\ngiven data, the\\nmaximum likelihood\\nestimate of the\\nparameters results\\nin the black\\ndiagonal line. The\\norange square\\nshows the value of\\nthe maximum\\nlikelihood\\nprediction at\\nx= 60 .\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nFigure 8.6\\nComparing the\\npredictions with the\\nmaximum likelihood\\nestimate and the\\nMAP estimate at\\nx= 60 . The prior\\nbiases the slope to\\nbe less steep and the\\nintercept to be\\ncloser to zero. In\\nthis example, the\\nbias that moves the\\nintercept closer to\\nzero actually\\nincreases the slope.\\n0 10 20 30 40 50 60 70 80\\nx0255075100125150y\\nMLE\\nMAP\\nproblem corresponding to maximum likelihood estimation has a closed-\\nform solution. We will see more details on this in Chapter 9. Figure 8.5\\nshows a regression dataset and the function that is induced by the maxi-\\nmum-likelihood parameters. Maximum likelihood estimation may suffer\\nfrom overﬁtting (Section 8.3.3), analogous to unregularized empirical risk\\nminimization (Section 8.2.3). For other likelihood functions, i.e., if we\\nmodel our noise with non-Gaussian distributions, maximum likelihood es-\\ntimation may not have a closed-form analytic solution. In this case, we\\nresort to numerical optimization methods discussed in Chapter 7.\\n8.3.2 Maximum A Posteriori Estimation\\nIf we have prior knowledge about the distribution of the parameters θ, we\\ncan multiply an additional term to the likelihood. This additional term is\\na prior probability distribution on parameters p(θ). For a given prior, after\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55730f1d-85b9-480a-8254-08b28bde3339', embedding=None, metadata={'page_label': '269', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Parameter Estimation 269\\nobserving some data x, how should we update the distribution of θ? In\\nother words, how should we represent the fact that we have more speciﬁc\\nknowledge of θafter observing data x? Bayes’ theorem, as discussed in\\nSection 6.3, gives us a principled tool to update our probability distribu-\\ntions of random variables. It allows us to compute a posterior distribution posterior\\np(θ|x)(the more speciﬁc knowledge) on the parameters θfrom general\\nprior statements (prior distribution) p(θ)and the function p(x|θ)that prior\\nlinks the parameters θand the observed data x(called the likelihood ): likelihood\\np(θ|x) =p(x|θ)p(θ)\\np(x). (8.19)\\nRecall that we are interested in ﬁnding the parameter θthat maximizes\\nthe posterior. Since the distribution p(x)does not depend on θ, we can\\nignore the value of the denominator for the optimization and obtain\\np(θ|x)∝p(x|θ)p(θ). (8.20)\\nThe preceding proportion relation hides the density of the data p(x),\\nwhich may be difﬁcult to estimate. Instead of estimating the minimum\\nof the negative log-likelihood, we now estimate the minimum of the neg-\\native log-posterior, which is referred to as maximum a posteriori estima- maximum a\\nposteriori\\nestimationtion(MAP estimation ). An illustration of the effect of adding a zero-mean\\nMAP estimationGaussian prior is shown in Figure 8.6.\\nExample 8.6\\nIn addition to the assumption of Gaussian likelihood in the previous exam-\\nple, we assume that the parameter vector is distributed as a multivariate\\nGaussian with zero mean, i.e., p(θ) =N(0,Σ)\\n, where Σis the covari-\\nance matrix (Section 6.5). Note that the conjugate prior of a Gaussian\\nis also a Gaussian (Section 6.6.1), and therefore we expect the posterior\\ndistribution to also be a Gaussian. We will see the details of maximum a\\nposteriori estimation in Chapter 9.\\nThe idea of including prior knowledge about where good parameters\\nlie is widespread in machine learning. An alternative view, which we saw\\nin Section 8.2.3, is the idea of regularization, which introduces an addi-\\ntional term that biases the resulting parameters to be close to the origin.\\nMaximum a posteriori estimation can be considered to bridge the non-\\nprobabilistic and probabilistic worlds as it explicitly acknowledges the\\nneed for a prior distribution but it still only produces a point estimate\\nof the parameters.\\nRemark. The maximum likelihood estimate θMLpossesses the following\\nproperties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\\nAsymptotic consistency: The MLE converges to the true value in the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e05343a3-0ca3-4323-aac0-0b06a2f52a58', embedding=None, metadata={'page_label': '270', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='270 When Models Meet Data\\nFigure 8.7 Model\\nﬁtting. In a\\nparametrized class\\nMθof models, we\\noptimize the model\\nparametersθto\\nminimize the\\ndistance to the true\\n(unknown) model\\nM∗.Mθ\\nM∗Mθ∗\\nMθ0\\nlimit of inﬁnitely many observations, plus a random error that is ap-\\nproximately normal.\\nThe size of the samples necessary to achieve these properties can be\\nquite large.\\nThe error’s variance decays in 1/N, whereNis the number of data\\npoints.\\nEspecially, in the “small” data regime, maximum likelihood estimation\\ncan lead to overﬁtting .\\n♦\\nThe principle of maximum likelihood estimation (and maximum a pos-\\nteriori estimation) uses probabilistic modeling to reason about the uncer-\\ntainty in the data and model parameters. However, we have not yet taken\\nprobabilistic modeling to its full extent. In this section, the resulting train-\\ning procedure still produces a point estimate of the predictor, i.e., training\\nreturns one single set of parameter values that represent the best predic-\\ntor. In Section 8.4, we will take the view that the parameter values should\\nalso be treated as random variables, and instead of estimating “best” val-\\nues of that distribution, we will use the full parameter distribution when\\nmaking predictions.\\n8.3.3 Model Fitting\\nConsider the setting where we are given a dataset, and we are interested\\nin ﬁtting a parametrized model to the data. When we talk about “ﬁt-\\nting”, we typically mean optimizing/learning model parameters so that\\nthey minimize some loss function, e.g., the negative log-likelihood. With\\nmaximum likelihood (Section 8.3.1) and maximum a posteriori estima-\\ntion (Section 8.3.2), we already discussed two commonly used algorithms\\nfor model ﬁtting.\\nThe parametrization of the model deﬁnes a model class Mθwith which\\nwe can operate. For example, in a linear regression setting, we may deﬁne\\nthe relationship between inputs xand (noise-free) observations yto be\\ny=ax+b, whereθ:={a,b}are the model parameters. In this case, the\\nmodel parameters θdescribe the family of afﬁne functions, i.e., straight\\nlines with slope a, which are offset from 0byb. Assume the data comes\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f8b8f47-81d9-41a8-ba9d-6111ac1bb4bf', embedding=None, metadata={'page_label': '271', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Parameter Estimation 271\\nFigure 8.8 Fitting\\n(by maximum\\nlikelihood) of\\ndifferent model\\nclasses to a\\nregression dataset.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a) Overﬁtting\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b) Underﬁtting.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (c) Fitting well.\\nfrom a model M∗, which is unknown to us. For a given training dataset,\\nwe optimizeθso thatMθis as close as possible to M∗, where the “close-\\nness” is deﬁned by the objective function we optimize (e.g., squared loss\\non the training data). Figure 8.7 illustrates a setting where we have a small\\nmodel class (indicated by the circle Mθ), and the data generation model\\nM∗lies outside the set of considered models. We begin our parameter\\nsearch atMθ0. After the optimization, i.e., when we obtain the best pos-\\nsible parameters θ∗, we distinguish three different cases: (i) overﬁtting,\\n(ii) underﬁtting, and (iii) ﬁtting well. We will give a high-level intuition\\nof what these three concepts mean.\\nRoughly speaking, overﬁtting refers to the situation where the para- overﬁtting\\nmetrized model class is too rich to model the dataset generated by M∗,\\ni.e.,Mθcould model much more complicated datasets. For instance, if the\\ndataset was generated by a linear function, and we deﬁne Mθto be the\\nclass of seventh-order polynomials, we could model not only linear func-\\ntions, but also polynomials of degree two, three, etc. Models that over-\\nﬁt typically have a large number of parameters. An observation we often One way to detect\\noverﬁtting in\\npractice is to\\nobserve that the\\nmodel has low\\ntraining risk but\\nhigh test risk during\\ncross validation\\n(Section 8.2.4).make is that the overly ﬂexible model class Mθuses all its modeling power\\nto reduce the training error. If the training data is noisy, it will therefore\\nﬁnd some useful signal in the noise itself. This will cause enormous prob-\\nlems when we predict away from the training data. Figure 8.8(a) gives an\\nexample of overﬁtting in the context of regression where the model pa-\\nrameters are learned by means of maximum likelihood (see Section 8.3.1).\\nWe will discuss overﬁtting in regression more in Section 9.2.2.\\nWhen we run into underﬁtting , we encounter the opposite problem underﬁtting\\nwhere the model class Mθis not rich enough. For example, if our dataset\\nwas generated by a sinusoidal function, but θonly parametrizes straight\\nlines, the best optimization procedure will not get us close to the true\\nmodel. However, we still optimize the parameters and ﬁnd the best straight\\nline that models the dataset. Figure 8.8(b) shows an example of a model\\nthat underﬁts because it is insufﬁciently ﬂexible. Models that underﬁt typ-\\nically have few parameters.\\nThe third case is when the parametrized model class is about right.\\nThen, our model ﬁts well, i.e., it neither overﬁts nor underﬁts. This means\\nour model class is just rich enough to describe the dataset we are given.\\nFigure 8.8(c) shows a model that ﬁts the given dataset fairly well. Ideally,\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b2e4a06d-6d52-4618-ae92-e8e17ac78772', embedding=None, metadata={'page_label': '272', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='272 When Models Meet Data\\nthis is the model class we would want to work with since it has good\\ngeneralization properties.\\nIn practice, we often deﬁne very rich model classes Mθwith many pa-\\nrameters, such as deep neural networks. To mitigate the problem of over-\\nﬁtting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).\\nWe will discuss how to choose the model class in Section 8.6.\\n8.3.4 Further Reading\\nWhen considering probabilistic models, the principle of maximum likeli-\\nhood estimation generalizes the idea of least-squares regression for linear\\nmodels, which we will discuss in detail in Chapter 9. When restricting\\nthe predictor to have linear form with an additional nonlinear function ϕ\\napplied to the output, i.e.,\\np(yn|xn,θ) =ϕ(θ⊤xn), (8.21)\\nwe can consider other models for other prediction tasks, such as binary\\nclassiﬁcation or modeling count data (McCullagh and Nelder, 1989). An\\nalternative view of this is to consider likelihoods that are from the ex-\\nponential family (Section 6.6). The class of models, which have linear\\ndependence between parameters and data, and have potentially nonlin-\\near transformation ϕ(called a link function ), is referred to as generalized link function\\ngeneralized linear\\nmodellinear models (Agresti, 2002, chapter 4).\\nMaximum likelihood estimation has a rich history, and was originally\\nproposed by Sir Ronald Fisher in the 1930s. We will expand upon the idea\\nof a probabilistic model in Section 8.4. One debate among researchers\\nwho use probabilistic models, is the discussion between Bayesian and fre-\\nquentist statistics. As mentioned in Section 6.1.1, it boils down to the\\ndeﬁnition of probability. Recall from Section 6.1 that one can consider\\nprobability to be a generalization (by allowing uncertainty) of logical rea-\\nsoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\\nlihood estimation is frequentist in nature, and the interested reader is\\npointed to Efron and Hastie (2016) for a balanced view of both Bayesian\\nand frequentist statistics.\\nThere are some probabilistic models where maximum likelihood esti-\\nmation may not be possible. The reader is referred to more advanced sta-\\ntistical textbooks, e.g., Casella and Berger (2002), for approaches, such as\\nmethod of moments, M-estimation, and estimating equations.\\n8.4 Probabilistic Modeling and Inference\\nIn machine learning, we are frequently concerned with the interpretation\\nand analysis of data, e.g., for prediction of future events and decision\\nmaking. To make this task more tractable, we often build models that\\ndescribe the generative process that generates the observed data. generative process\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3d4355d-2bbd-4013-b5f5-4f198fa38bff', embedding=None, metadata={'page_label': '273', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.4 Probabilistic Modeling and Inference 273\\nFor example, we can describe the outcome of a coin-ﬂip experiment\\n(“heads” or “tails”) in two steps. First, we deﬁne a parameter µ, which\\ndescribes the probability of “heads” as the parameter of a Bernoulli distri-\\nbution (Chapter 6); second, we can sample an outcome x∈{head, tail}\\nfrom the Bernoulli distribution p(x|µ) =Ber(µ). The parameter µgives\\nrise to a speciﬁc dataset Xand depends on the coin used. Since µis un-\\nknown in advance and can never be observed directly, we need mecha-\\nnisms to learn something about µgiven observed outcomes of coin-ﬂip\\nexperiments. In the following, we will discuss how probabilistic modeling\\ncan be used for this purpose.\\n8.4.1 Probabilistic ModelsA probabilistic\\nmodel is speciﬁed\\nby the joint\\ndistribution of all\\nrandom variables.Probabilistic models represent the uncertain aspects of an experiment as\\nprobability distributions. The beneﬁt of using probabilistic models is that\\nthey offer a uniﬁed and consistent set of tools from probability theory\\n(Chapter 6) for modeling, inference, prediction, and model selection.\\nIn probabilistic modeling, the joint distribution p(x,θ)of the observed\\nvariablesxand the hidden parameters θis of central importance: It en-\\ncapsulates information from the following:\\nThe prior and the likelihood (product rule, Section 6.3).\\nThe marginal likelihood p(x), which will play an important role in\\nmodel selection (Section 8.6), can be computed by taking the joint dis-\\ntribution and integrating out the parameters (sum rule, Section 6.3).\\nThe posterior, which can be obtained by dividing the joint by the marginal\\nlikelihood.\\nOnly the joint distribution has this property. Therefore, a probabilistic\\nmodel is speciﬁed by the joint distribution of all its random variables.\\n8.4.2 Bayesian Inference\\nParameter\\nestimation can be\\nphrased as an\\noptimization\\nproblem.A key task in machine learning is to take a model and the data to uncover\\nthe values of the model’s hidden variables θgiven the observed variables\\nx. In Section 8.3.1, we already discussed two ways for estimating model\\nparametersθusing maximum likelihood or maximum a posteriori esti-\\nmation. In both cases, we obtain a single-best value for θso that the key\\nalgorithmic problem of parameter estimation is solving an optimization\\nproblem. Once these point estimates θ∗are known, we use them to make\\npredictions. More speciﬁcally, the predictive distribution will be p(x|θ∗),\\nwhere we use θ∗in the likelihood function.\\nAs discussed in Section 6.3, focusing solely on some statistic of the pos-\\nterior distribution (such as the parameter θ∗that maximizes the poste-\\nrior) leads to loss of information, which can be critical in a system that\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='612017eb-29a5-4991-b781-6743bb50636e', embedding=None, metadata={'page_label': '274', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='274 When Models Meet Data\\nuses the prediction p(x|θ∗)to make decisions. These decision-making\\nsystems typically have different objective functions than the likelihood, a Bayesian inference\\nis about learning the\\ndistribution of\\nrandom variables.squared-error loss or a mis-classiﬁcation error. Therefore, having the full\\nposterior distribution around can be extremely useful and leads to more\\nrobust decisions. Bayesian inference is about ﬁnding this posterior distri-Bayesian inference\\nbution (Gelman et al., 2004). For a dataset X, a parameter prior p(θ), and\\na likelihood function, the posterior\\np(θ|X) =p(X|θ)p(θ)\\np(X), p (X) =∫\\np(X|θ)p(θ)dθ, (8.22)\\nis obtained by applying Bayes’ theorem. The key idea is to exploit Bayes’ Bayesian inference\\ninverts the\\nrelationship\\nbetween parameters\\nand the data.theorem to invert the relationship between the parameters θand the data\\nX(given by the likelihood) to obtain the posterior distribution p(θ|X).\\nThe implication of having a posterior distribution on the parameters is\\nthat it can be used to propagate uncertainty from the parameters to the\\ndata. More speciﬁcally, with a distribution p(θ)on the parameters our\\npredictions will be\\np(x) =∫\\np(x|θ)p(θ)dθ=Eθ[p(x|θ)], (8.23)\\nand they no longer depend on the model parameters θ, which have been\\nmarginalized/integrated out. Equation (8.23) reveals that the prediction\\nis an average over all plausible parameter values θ, where the plausibility\\nis encapsulated by the parameter distribution p(θ).\\nHaving discussed parameter estimation in Section 8.3 and Bayesian in-\\nference here, let us compare these two approaches to learning. Parameter\\nestimation via maximum likelihood or MAP estimation yields a consistent\\npoint estimate θ∗of the parameters, and the key computational problem\\nto be solved is optimization. In contrast, Bayesian inference yields a (pos-\\nterior) distribution, and the key computational problem to be solved is\\nintegration. Predictions with point estimates are straightforward, whereas\\npredictions in the Bayesian framework require solving another integration\\nproblem; see (8.23). However, Bayesian inference gives us a principled\\nway to incorporate prior knowledge, account for side information, and\\nincorporate structural knowledge, all of which is not easily done in the\\ncontext of parameter estimation. Moreover, the propagation of parameter\\nuncertainty to the prediction can be valuable in decision-making systems\\nfor risk assessment and exploration in the context of data-efﬁcient learn-\\ning (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\\nWhile Bayesian inference is a mathematically principled framework for\\nlearning about parameters and making predictions, there are some prac-\\ntical challenges that come with it because of the integration problems we\\nneed to solve; see (8.22) and (8.23). More speciﬁcally, if we do not choose\\na conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22)\\nand (8.23) are not analytically tractable, and we cannot compute the pos-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='84e54ca6-30ae-4753-a920-82afc5537606', embedding=None, metadata={'page_label': '275', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.4 Probabilistic Modeling and Inference 275\\nterior, the predictions, or the marginal likelihood in closed form. In these\\ncases, we need to resort to approximations. Here, we can use stochas-\\ntic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks\\net al., 1996), or deterministic approximations, such as the Laplace ap-\\nproximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-\\nference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-\\ntion (Minka, 2001a).\\nDespite these challenges, Bayesian inference has been successfully ap-\\nplied to a variety of problems, including large-scale topic modeling (Hoff-\\nman et al., 2013), click-through-rate prediction (Graepel et al., 2010),\\ndata-efﬁcient reinforcement learning in control systems (Deisenroth et al.,\\n2015), online ranking systems (Herbrich et al., 2007), and large-scale rec-\\nommender systems. There are generic tools, such as Bayesian optimiza-\\ntion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that\\nare very useful ingredients for an efﬁcient search of meta parameters of\\nmodels or algorithms.\\nRemark. In the machine learning literature, there can be a somewhat ar-\\nbitrary separation between (random) “variables” and “parameters”. While\\nparameters are estimated (e.g., via maximum likelihood), variables are\\nusually marginalized out. In this book, we are not so strict with this sep-\\naration because, in principle, we can place a prior on any parameter and\\nintegrate it out, which would then turn the parameter into a random vari-\\nable according to the aforementioned separation. ♦\\n8.4.3 Latent-Variable Models\\nIn practice, it is sometimes useful to have additional latent variables z latent variable\\n(besides the model parameters θ) as part of the model (Moustaki et al.,\\n2015). These latent variables are different from the model parameters\\nθas they do not parametrize the model explicitly. Latent variables may\\ndescribe the data-generating process, thereby contributing to the inter-\\npretability of the model. They also often simplify the structure of the\\nmodel and allow us to deﬁne simpler and richer model structures. Sim-\\npliﬁcation of the model structure often goes hand in hand with a smaller\\nnumber of model parameters (Paquet, 2008; Murphy, 2012). Learning in\\nlatent-variable models (at least via maximum likelihood) can be done in a\\nprincipled way using the expectation maximization (EM) algorithm (Demp-\\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\\nare helpful, are principal component analysis for dimensionality reduc-\\ntion (Chapter 10), Gaussian mixture models for density estimation (Chap-\\nter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\\n(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,\\nand meta learning and task generalization (Hausman et al., 2018; Sæ-\\nmundsson et al., 2018). Although the introduction of these latent variables\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9429a062-bb67-4903-a4b5-04c31f3857a5', embedding=None, metadata={'page_label': '276', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='276 When Models Meet Data\\nmay make the model structure and the generative process easier, learning\\nin latent-variable models is generally hard, as we will see in Chapter 11.\\nSince latent-variable models also allow us to deﬁne the process that\\ngenerates data from parameters, let us have a look at this generative pro-\\ncess. Denoting data by x, the model parameters by θand the latent vari-\\nables byz, we obtain the conditional distribution\\np(x|z,θ) (8.24)\\nthat allows us to generate data for any model parameters and latent vari-\\nables. Given that zare latent variables, we place a prior p(z)on them.\\nAs the models we discussed previously, models with latent variables\\ncan be used for parameter learning and inference within the frameworks\\nwe discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by\\nmeans of maximum likelihood estimation or Bayesian inference), we fol-\\nlow a two-step procedure. First, we compute the likelihood p(x|θ)of the\\nmodel, which does not depend on the latent variables. Second, we use this\\nlikelihood for parameter estimation or Bayesian inference, where we use\\nexactly the same expressions as in Sections 8.3 and 8.4.2, respectively.\\nSince the likelihood function p(x|θ)is the predictive distribution of the\\ndata given the model parameters, we need to marginalize out the latent\\nvariables so that\\np(x|θ) =∫\\np(x|z,θ)p(z)dz, (8.25)\\nwherep(x|z,θ)is given in (8.24) and p(z)is the prior on the latent\\nvariables. Note that the likelihood must not depend on the latent variables The likelihood is a\\nfunction of the data\\nand the model\\nparameters, but is\\nindependent of the\\nlatent variables.z, but it is only a function of the data xand the model parameters θ.\\nThe likelihood in (8.25) directly allows for parameter estimation via\\nmaximum likelihood. MAP estimation is also straightforward with an ad-\\nditional prior on the model parameters θas discussed in Section 8.3.2.\\nMoreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\\nin a latent-variable model works in the usual way: We place a prior p(θ)\\non the model parameters and use Bayes’ theorem to obtain a posterior\\ndistribution\\np(θ|X) =p(X|θ)p(θ)\\np(X)(8.26)\\nover the model parameters given a dataset X. The posterior in (8.26) can\\nbe used for predictions within a Bayesian inference framework; see (8.23).\\nOne challenge we have in this latent-variable model is that the like-\\nlihoodp(X|θ)requires the marginalization of the latent variables ac-\\ncording to (8.25). Except when we choose a conjugate prior p(z)for\\np(x|z,θ), the marginalization in (8.25) is not analytically tractable, and\\nwe need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-\\nphy, 2012; Moustaki et al., 2015).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3cae67c0-49dd-4e69-b534-06165dd9403c', embedding=None, metadata={'page_label': '277', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.4 Probabilistic Modeling and Inference 277\\nSimilar to the parameter posterior (8.26) we can compute a posterior\\non the latent variables according to\\np(z|X) =p(X|z)p(z)\\np(X), p (X|z) =∫\\np(X|z,θ)p(θ)dθ,(8.27)\\nwherep(z)is the prior on the latent variables and p(X|z)requires us to\\nintegrate out the model parameters θ.\\nGiven the difﬁculty of solving integrals analytically, it is clear that mar-\\nginalizing out both the latent variables and the model parameters at the\\nsame time is not possible in general (Bishop, 2006; Murphy, 2012). A\\nquantity that is easier to compute is the posterior distribution on the latent\\nvariables, but conditioned on the model parameters, i.e.,\\np(z|X,θ) =p(X|z,θ)p(z)\\np(X|θ), (8.28)\\nwherep(z)is the prior on the latent variables and p(X|z,θ)is given\\nin (8.24).\\nIn Chapters 10 and 11, we derive the likelihood functions for PCA and\\nGaussian mixture models, respectively. Moreover, we compute the poste-\\nrior distributions (8.28) on the latent variables for both PCA and Gaussian\\nmixture models.\\nRemark. In the following chapters, we may not be drawing such a clear\\ndistinction between latent variables zand uncertain model parameters θ\\nand call the model parameters “latent” or “hidden” as well because they\\nare unobserved. In Chapters 10 and 11, where we use the latent variables\\nz, we will pay attention to the difference as we will have two different\\ntypes of hidden variables: model parameters θand latent variables z.♦\\nWe can exploit the fact that all the elements of a probabilistic model are\\nrandom variables to deﬁne a uniﬁed language for representing them. In\\nSection 8.5, we will see a concise graphical language for representing the\\nstructure of probabilistic models. We will use this graphical language to\\ndescribe the probabilistic models in the subsequent chapters.\\n8.4.4 Further Reading\\nProbabilistic models in machine learning (Bishop, 2006; Barber, 2012;\\nMurphy, 2012) provide a way for users to capture uncertainty about data\\nand predictive models in a principled fashion. Ghahramani (2015) presents\\na short review of probabilistic models in machine learning. Given a proba-\\nbilistic model, we may be lucky enough to be able to compute parameters\\nof interest analytically. However, in general, analytic solutions are rare,\\nand computational methods such as sampling (Gilks et al., 1996; Brooks\\net al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3aa16ca-457e-4fc2-b3ee-babf9904b16b', embedding=None, metadata={'page_label': '278', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='278 When Models Meet Data\\n2017) are used. Moustaki et al. (2015) and Paquet (2008) provide a good\\noverview of Bayesian inference in latent-variable models.\\nIn recent years, several programming languages have been proposed\\nthat aim to treat the variables deﬁned in software as random variables\\ncorresponding to probability distributions. The objective is to be able to\\nwrite complex functions of probability distributions, while under the hood\\nthe compiler automatically takes care of the rules of Bayesian inference.\\nThis rapidly changing ﬁeld is called probabilistic programming . probabilistic\\nprogramming\\n8.5 Directed Graphical Models\\nIn this section, we introduce a graphical language for specifying a prob-\\nabilistic model, called the directed graphical model . It provides a compact directed graphical\\nmodel and succinct way to specify probabilistic models, and allows the reader to\\nvisually parse dependencies between random variables. A graphical model\\nvisually captures the way in which the joint distribution over all random\\nvariables can be decomposed into a product of factors depending only on\\na subset of these variables. In Section 8.4, we identiﬁed the joint distri-\\nbution of a probabilistic model as the key quantity of interest because it\\ncomprises information about the prior, the likelihood, and the posterior.\\nHowever, the joint distribution by itself can be quite complicated, and Directed graphical\\nmodels are also\\nknown as Bayesian\\nnetworks.it does not tell us anything about structural properties of the probabilis-\\ntic model. For example, the joint distribution p(a,b,c )does not tell us\\nanything about independence relations. This is the point where graphical\\nmodels come into play. This section relies on the concepts of independence\\nand conditional independence, as described in Section 6.4.5.\\nIn agraphical model , nodes are random variables. In Figure 8.9(a), the graphical model\\nnodes represent the random variables a,b,c . Edges represent probabilistic\\nrelations between variables, e.g., conditional probabilities.\\nRemark. Not every distribution can be represented in a particular choice of\\ngraphical model. A discussion of this can be found in Bishop (2006). ♦\\nProbabilistic graphical models have some convenient properties:\\nThey are a simple way to visualize the structure of a probabilistic model.\\nThey can be used to design or motivate new kinds of statistical models.\\nInspection of the graph alone gives us insight into properties, e.g., con-\\nditional independence.\\nComplex computations for inference and learning in statistical models\\ncan be expressed in terms of graphical manipulations.\\n8.5.1 Graph Semantics\\nDirected graphical models /Bayesian networks are a method for representing directed graphical\\nmodel/Bayesian\\nnetworkconditional dependencies in a probabilistic model. They provide a visual\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd3f020c-78a1-42a7-a5cc-834b6e2c150e', embedding=None, metadata={'page_label': '279', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 Directed Graphical Models 279\\ndescription of the conditional probabilities, hence, providing a simple lan-\\nguage for describing complex interdependence. The modular description With additional\\nassumptions, the\\narrows can be used\\nto indicate causal\\nrelationships (Pearl,\\n2009).also entails computational simpliﬁcation. Directed links (arrows) between\\ntwo nodes (random variables) indicate conditional probabilities. For ex-\\nample, the arrow between aandbin Figure 8.9(a) gives the conditional\\nprobabilityp(b|a)ofbgivena.\\nFigure 8.9\\nExamples of\\ndirected graphical\\nmodels.a b\\nc\\n(a) Fully connected.x1 x2\\nx3 x4x5\\n(b) Not fully connected.\\nDirected graphical models can be derived from joint distributions if we\\nknow something about their factorization.\\nExample 8.7\\nConsider the joint distribution\\np(a,b,c ) =p(c|a,b)p(b|a)p(a) (8.29)\\nof three random variables a,b,c . The factorization of the joint distribution\\nin (8.29) tells us something about the relationship between the random\\nvariables:\\ncdepends directly on aandb.\\nbdepends directly on a.\\nadepends neither on bnor onc.\\nFor the factorization in (8.29), we obtain the directed graphical model in\\nFigure 8.9(a).\\nIn general, we can construct the corresponding directed graphical model\\nfrom a factorized joint distribution as follows:\\n1. Create a node for all random variables.\\n2. For each conditional distribution, we add a directed link (arrow) to\\nthe graph from the nodes corresponding to the variables on which the\\ndistribution is conditioned.\\nThe graph layout\\ndepends on the\\nfactorization of the\\njoint distribution.The graph layout depends on the choice of factorization of the joint dis-\\ntribution.\\nWe discussed how to get from a known factorization of the joint dis-\\ntribution to the corresponding directed graphical model. Now, we will do\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86d51e8c-6b47-4de7-aa8c-3820aa0717e4', embedding=None, metadata={'page_label': '280', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='280 When Models Meet Data\\nexactly the opposite and describe how to extract the joint distribution of\\na set of random variables from a given graphical model.\\nExample 8.8\\nLooking at the graphical model in Figure 8.9(b), we exploit two proper-\\nties:\\nThe joint distribution p(x1,...,x 5)we seek is the product of a set of\\nconditionals, one for each node in the graph. In this particular example,\\nwe will need ﬁve conditionals.\\nEach conditional depends only on the parents of the corresponding\\nnode in the graph. For example, x4will be conditioned on x2.\\nThese two properties yield the desired factorization of the joint distribu-\\ntion\\np(x1,x2,x3,x4,x5) =p(x1)p(x5)p(x2|x5)p(x3|x1,x2)p(x4|x2).(8.30)\\nIn general, the joint distribution p(x) =p(x1,...,xK)is given as\\np(x) =K∏\\nk=1p(xk|Pak), (8.31)\\nwhere Pa kmeans “the parent nodes of xk”. Parent nodes of xkare nodes\\nthat have arrows pointing to xk.\\nWe conclude this subsection with a concrete example of the coin-ﬂip\\nexperiment. Consider a Bernoulli experiment (Example 6.8) where the\\nprobability that the outcome xof this experiment is “heads” is\\np(x|µ) =Ber(µ). (8.32)\\nWe now repeat this experiment Ntimes and observe outcomes x1,...,xN\\nso that we obtain the joint distribution\\np(x1,...,xN|µ) =N∏\\nn=1p(xn|µ). (8.33)\\nThe expression on the right-hand side is a product of Bernoulli distribu-\\ntions on each individual outcome because the experiments are indepen-\\ndent. Recall from Section 6.4.5 that statistical independence means that\\nthe distribution factorizes. To write the graphical model down for this set-\\nting, we make the distinction between unobserved/latent variables and\\nobserved variables. Graphically, observed variables are denoted by shaded\\nnodes so that we obtain the graphical model in Figure 8.10(a). We see\\nthat the single parameter µis the same for all xn,n= 1,...,N as the\\noutcomesxnare identically distributed. A more compact, but equivalent,\\ngraphical model for this setting is given in Figure 8.10(b), where we use\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02f4b915-a462-4220-b5f8-957868c07962', embedding=None, metadata={'page_label': '281', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 Directed Graphical Models 281\\nFigure 8.10\\nGraphical models\\nfor a repeated\\nBernoulli\\nexperiment.µ\\nx1 xN\\n(a) Version with xnexplicit.µ\\nxn\\nn= 1,...,N\\n(b) Version with\\nplate notation.µ\\nxnβ α\\nn= 1,...,N\\n(c) Hyperparameters α\\nandβon the latent µ.\\ntheplate notation. The plate (box) repeats everything inside (in this case, plate\\nthe observations xn)Ntimes. Therefore, both graphical models are equiv-\\nalent, but the plate notation is more compact. Graphical models immedi-\\nately allow us to place a hyperprior on µ. Ahyperprior is a second layer hyperprior\\nof prior distributions on the parameters of the ﬁrst layer of priors. Fig-\\nure 8.10(c) places a Beta (α,β)prior on the latent variable µ. If we treat\\nαandβas deterministic parameters, i.e., not random variables, we omit\\nthe circle around it.\\n8.5.2 Conditional Independence and d-Separation\\nDirected graphical models allow us to ﬁnd conditional independence (Sec-\\ntion 6.4.5) relationship properties of the joint distribution only by looking\\nat the graph. A concept called d-separation (Pearl, 1988) is key to this. d-separation\\nConsider a general directed graph in which A,B,Care arbitrary nonin-\\ntersecting sets of nodes (whose union may be smaller than the complete\\nset of nodes in the graph). We wish to ascertain whether a particular con-\\nditional independence statement, “ Ais conditionally independent of B\\ngivenC”, denoted by\\nA⊥ ⊥B|C, (8.34)\\nis implied by a given directed acyclic graph. To do so, we consider all\\npossible trails (paths that ignore the direction of the arrows) from any\\nnode inAto any nodes inB. Any such path is said to be blocked if it\\nincludes any node such that either of the following are true:\\nThe arrows on the path meet either head to tail or tail to tail at the\\nnode, and the node is in the set C.\\nThe arrows meet head to head at the node, and neither the node nor\\nany of its descendants is in the set C.\\nIf all paths are blocked, then Ais said to be d-separated fromBbyC,\\nand the joint distribution over all of the variables in the graph will satisfy\\nA⊥ ⊥B|C .\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='19445315-6756-493b-a163-eae417bc1826', embedding=None, metadata={'page_label': '282', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='282 When Models Meet Data\\nFigure 8.12 Three\\ntypes of graphical\\nmodels: (a) Directed\\ngraphical models\\n(Bayesian\\nnetworks);\\n(b) Undirected\\ngraphical models\\n(Markov random\\nﬁelds); (c) Factor\\ngraphs.a b\\nc\\n(a) Directed graphical modela b\\nc\\n(b) Undirected graphical\\nmodela b\\nc\\n(c) Factor graph\\nExample 8.9 (Conditional Independence)\\nFigure 8.11\\nD-separation\\nexample.abc\\nd\\ne\\nConsider the graphical model in Figure 8.11. Visual inspection gives us\\nb⊥ ⊥d|a,c (8.35)\\na⊥ ⊥c|b (8.36)\\nb̸⊥ ⊥d|c (8.37)\\na̸⊥ ⊥c|b,e (8.38)\\nDirected graphical models allow a compact representation of proba-\\nbilistic models, and we will see examples of directed graphical models in\\nChapters 9, 10, and 11. The representation, along with the concept of con-\\nditional independence, allows us to factorize the respective probabilistic\\nmodels into expressions that are easier to optimize.\\nThe graphical representation of the probabilistic model allows us to\\nvisually see the impact of design choices we have made on the structure\\nof the model. We often need to make high-level assumptions about the\\nstructure of the model. These modeling assumptions (hyperparameters)\\naffect the prediction performance, but cannot be selected directly using\\nthe approaches we have seen so far. We will discuss different ways to\\nchoose the structure in Section 8.6.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d57ebb8-4fec-4e84-a8bb-b65f4965c4fd', embedding=None, metadata={'page_label': '283', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.6 Model Selection 283\\n8.5.3 Further Reading\\nAn introduction to probabilistic graphical models can be found in Bishop\\n(2006, chapter 8), and an extensive description of the different applica-\\ntions and corresponding algorithmic implications can be found in the book\\nby Koller and Friedman (2009). There are three main types of probabilistic\\ngraphical models:\\ndirected graphical\\nmodel Directed graphical models (Bayesian networks ); see Figure 8.12(a)\\nBayesian network\\nundirected graphical\\nmodelUndirected graphical models (Markov random ﬁelds ); see Figure 8.12(b)\\nMarkov random\\nﬁeld\\nfactor graphFactor graphs ; see Figure 8.12(c)\\nGraphical models allow for graph-based algorithms for inference and\\nlearning, e.g., via local message passing. Applications range from rank-\\ning in online games (Herbrich et al., 2007) and computer vision (e.g.,\\nimage segmentation, semantic labeling, image denoising, image restora-\\ntion (Kittler and F ¨oglein, 1984; Sucar and Gillies, 1994; Shotton et al.,\\n2006; Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solv-\\ning linear equation systems (Shental et al., 2008), and iterative Bayesian\\nstate estimation in signal processing (Bickson et al., 2007; Deisenroth and\\nMohamed, 2012).\\nOne topic that is particularly important in real applications that we do\\nnot discuss in this book is the idea of structured prediction (Bakir et al.,\\n2007; Nowozin et al., 2014), which allows machine learning models to\\ntackle predictions that are structured, for example sequences, trees, and\\ngraphs. The popularity of neural network models has allowed more ﬂex-\\nible probabilistic models to be used, resulting in many useful applica-\\ntions of structured models (Goodfellow et al., 2016, chapter 16). In recent\\nyears, there has been a renewed interest in graphical models due to their\\napplications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\\nPeters et al., 2017; Rosenbaum, 2017).\\n8.6 Model Selection\\nIn machine learning, we often need to make high-level modeling decisions\\nthat critically inﬂuence the performance of the model. The choices we\\nmake (e.g., the functional form of the likelihood) inﬂuence the number\\nand type of free parameters in the model and thereby also the ﬂexibility\\nand expressivity of the model. More complex models are more ﬂexible in A polynomial\\ny=a0+a1x+a2x2\\ncan also describe\\nlinear functions by\\nsettinga2= 0, i.e.,\\nit is strictly more\\nexpressive than a\\nﬁrst-order\\npolynomial.the sense that they can be used to describe more datasets. For instance, a\\npolynomial of degree 1(a liney=a0+a1x) can only be used to describe\\nlinear relations between inputs xand observations y. A polynomial of\\ndegree 2can additionally describe quadratic relationships between inputs\\nand observations.\\nOne would now think that very ﬂexible models are generally preferable\\nto simple models because they are more expressive. A general problem\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='54842dfc-e2a6-47e6-b08f-19b7760ebd20', embedding=None, metadata={'page_label': '284', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='284 When Models Meet Data\\nFigure 8.13 Nested\\ncross-validation. We\\nperform two levels\\nofK-fold\\ncross-validation.All labeled data\\nAll training data Test data\\nTo train model Validation\\nis that at training time we can only use the training set to evaluate the\\nperformance of the model and learn its parameters. However, the per-\\nformance on the training set is not really what we are interested in. In\\nSection 8.3, we have seen that maximum likelihood estimation can lead\\nto overﬁtting, especially when the training dataset is small. Ideally, our\\nmodel (also) works well on the test set (which is not available at training\\ntime). Therefore, we need some mechanisms for assessing how a model\\ngeneralizes to unseen test data. Model selection is concerned with exactly\\nthis problem.\\n8.6.1 Nested Cross-Validation\\nWe have already seen an approach (cross-validation in Section 8.2.4) that\\ncan be used for model selection. Recall that cross-validation provides an\\nestimate of the generalization error by repeatedly splitting the dataset into\\ntraining and validation sets. We can apply this idea one more time, i.e.,\\nfor each split, we can perform another round of cross-validation. This is\\nsometimes referred to as nested cross-validation ; see Figure 8.13. The inner nested\\ncross-validation level is used to estimate the performance of a particular choice of model\\nor hyperparameter on a internal validation set. The outer level is used to\\nestimate generalization performance for the best choice of model chosen\\nby the inner loop. We can test different model and hyperparameter choices\\nin the inner loop. To distinguish the two levels, the set used to estimate\\nthe generalization performance is often called the test set and the set used test set\\nfor choosing the best model is called the validation set . The inner loop validation set\\nestimates the expected value of the generalization error for a given model\\n(8.39), by approximating it using the empirical error on the validation set,\\ni.e., The standard error\\nis deﬁned asσ√\\nK,\\nwhereKis the\\nnumber of\\nexperiments and σ\\nis the standard\\ndeviation of the risk\\nof each experiment.EV[R(V|M)]≈1\\nKK∑\\nk=1R(V(k)|M), (8.39)\\nwhere R(V|M)is the empirical risk (e.g., root mean square error) on the\\nvalidation setVfor modelM. We repeat this procedure for all models and\\nchoose the model that performs best. Note that cross-validation not only\\ngives us the expected generalization error, but we can also obtain high-\\norder statistics, e.g., the standard error, an estimate of how uncertain the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c728b20-a767-4697-bc84-55e2c0066ad2', embedding=None, metadata={'page_label': '285', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.6 Model Selection 285\\nFigure 8.14\\nBayesian inference\\nembodies Occam’s\\nrazor. The\\nhorizontal axis\\ndescribes the space\\nof all possible\\ndatasetsD. The\\nevidence (vertical\\naxis) evaluates how\\nwell a model\\npredicts available\\ndata. Since\\np(D|Mi)needs to\\nintegrate to 1, we\\nshould choose the\\nmodel with the\\ngreatest evidence.\\nAdapted\\nfrom MacKay\\n(2003).Evidence\\nD\\nCp(D|M1)\\np(D|M2)\\nmean estimate is. Once the model is chosen, we can evaluate the ﬁnal\\nperformance on the test set.\\n8.6.2 Bayesian Model Selection\\nThere are many approaches to model selection, some of which are covered\\nin this section. Generally, they all attempt to trade off model complexity\\nand data ﬁt. We assume that simpler models are less prone to overﬁtting\\nthan complex models, and hence the objective of model selection is to ﬁnd\\nthe simplest model that explains the data reasonably well. This concept is\\nalso known as Occam’s razor . Occam’s razor\\nRemark. If we treat model selection as a hypothesis testing problem, we\\nare looking for the simplest hypothesis that is consistent with the data (Mur-\\nphy, 2012). ♦\\nOne may consider placing a prior on models that favors simpler models.\\nHowever, it is not necessary to do this: An “automatic Occam’s Razor” is\\nquantitatively embodied in the application of Bayesian probability (Smith\\nand Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\\nure 8.14, adapted from MacKay (2003), gives us the basic intuition why\\ncomplex and very expressive models may turn out to be a less probable\\nchoice for modeling a given dataset D. Let us think of the horizontal axis These predictions\\nare quantiﬁed by a\\nnormalized\\nprobability\\ndistribution onD,\\ni.e., it needs to\\nintegrate/sum to 1.representing the space of all possible datasets D. If we are interested in\\nthe posterior probability p(Mi|D)of modelMigiven the dataD, we can\\nemploy Bayes’ theorem. Assuming a uniform prior p(M)over all mod-\\nels, Bayes’ theorem rewards models in proportion to how much they pre-\\ndicted the data that occurred. This prediction of the data given model\\nMi,p(D|Mi), is called the evidence forMi. A simple model M1can only evidence\\npredict a small number of datasets, which is shown by p(D|M1); a more\\npowerful model M2that has, e.g., more free parameters than M1, is able\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98c91077-fbf4-4fa7-bc4b-5b3593b0e9e0', embedding=None, metadata={'page_label': '286', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='286 When Models Meet Data\\nto predict a greater variety of datasets. This means, however, that M2\\ndoes not predict the datasets in region Cas well asM1. Suppose that\\nequal prior probabilities have been assigned to the two models. Then, if\\nthe dataset falls into region C, the less powerful model M1is the more\\nprobable model.\\nEarlier in this chapter, we argued that models need to be able to explain\\nthe data, i.e., there should be a way to generate data from a given model.\\nFurthermore, if the model has been appropriately learned from the data,\\nthen we expect that the generated data should be similar to the empirical\\ndata. For this, it is helpful to phrase model selection as a hierarchical\\ninference problem, which allows us to compute the posterior distribution\\nover models.\\nLet us consider a ﬁnite number of models M={M1,...,MK}, where\\neach model Mkpossesses parameters θk. InBayesian model selection , we Bayesian model\\nselection place a prior p(M)on the set of models. The corresponding generative\\ngenerative processprocess that allows us to generate data from this model is\\nFigure 8.15\\nIllustration of the\\nhierarchical\\ngenerative process\\nin Bayesian model\\nselection. We place\\na priorp(M)on the\\nset of models. For\\neach model, there is\\na distribution\\np(θ|M)on the\\ncorresponding\\nmodel parameters,\\nwhich is used to\\ngenerate the dataD.\\nM\\nθ\\nDMk∼p(M) (8.40)\\nθk∼p(θ|Mk) (8.41)\\nD∼p(D|θk) (8.42)\\nand illustrated in Figure 8.15. Given a training set D, we apply Bayes’\\ntheorem and compute the posterior distribution over models as\\np(Mk|D)∝p(Mk)p(D|Mk). (8.43)\\nNote that this posterior no longer depends on the model parameters θk\\nbecause they have been integrated out in the Bayesian setting since\\np(D|Mk) =∫\\np(D|θk)p(θk|Mk)dθk, (8.44)\\nwherep(θk|Mk)is the prior distribution of the model parameters θkof\\nmodelMk. The term (8.44) is referred to as the model evidence ormarginal\\nmodel evidence\\nmarginal likelihoodlikelihood . From the posterior in (8.43), we determine the MAP estimate\\nM∗= arg max\\nMkp(Mk|D). (8.45)\\nWith a uniform prior p(Mk) =1\\nK, which gives every model equal (prior)\\nprobability, determining the MAP estimate over models amounts to pick-\\ning the model that maximizes the model evidence (8.44).\\nRemark (Likelihood and Marginal Likelihood) .There are some important\\ndifferences between a likelihood and a marginal likelihood (evidence):\\nWhile the likelihood is prone to overﬁtting, the marginal likelihood is typ-\\nically not as the model parameters have been marginalized out (i.e., we\\nno longer have to ﬁt the parameters). Furthermore, the marginal likeli-\\nhood automatically embodies a trade-off between model complexity and\\ndata ﬁt (Occam’s razor). ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b96fdba4-bbba-45e4-a233-ad6e2c5492c1', embedding=None, metadata={'page_label': '287', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.6 Model Selection 287\\n8.6.3 Bayes Factors for Model Comparison\\nConsider the problem of comparing two probabilistic models M1,M2,\\ngiven a datasetD. If we compute the posteriors p(M1|D)andp(M2|D),\\nwe can compute the ratio of the posteriors\\np(M1|D)\\np(M2|D)\\ued19\\ued18\\ued17\\ued1a\\nposterior odds=p(D|M1)p(M1)\\np(D)\\np(D|M2)p(M2)\\np(D)=p(M1)\\np(M2)\\ued19\\ued18\\ued17\\ued1a\\nprior oddsp(D|M1)\\np(D|M2)\\ued19\\ued18\\ued17\\ued1a\\nBayes factor. (8.46)\\nThe ratio of the posteriors is also called the posterior odds . The ﬁrst frac- posterior odds\\ntion on the right-hand side of (8.46), the prior odds , measures how much prior odds\\nour prior (initial) beliefs favor M1overM2. The ratio of the marginal like-\\nlihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\\nand measures how well the data Dis predicted by M1compared to M2.\\nRemark. TheJeffreys-Lindley paradox states that the “Bayes factor always Jeffreys-Lindley\\nparadox favors the simpler model since the probability of the data under a complex\\nmodel with a diffuse prior will be very small” (Murphy, 2012). Here, a\\ndiffuse prior refers to a prior that does not favor speciﬁc models, i.e.,\\nmany models are a priori plausible under this prior. ♦\\nIf we choose a uniform prior over models, the prior odds term in (8.46)\\nis1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\\nfactor)\\np(D|M1)\\np(D|M2). (8.47)\\nIf the Bayes factor is greater than 1, we choose model M1, otherwise\\nmodelM2. In a similar way to frequentist statistics, there are guidelines\\non the size of the ratio that one should consider before ”signiﬁcance” of\\nthe result (Jeffreys, 1961).\\nRemark (Computing the Marginal Likelihood) .The marginal likelihood\\nplays an important role in model selection: We need to compute Bayes\\nfactors (8.46) and posterior distributions over models (8.43).\\nUnfortunately, computing the marginal likelihood requires us to solve\\nan integral (8.44). This integration is generally analytically intractable,\\nand we will have to resort to approximation techniques, e.g., numerical\\nintegration (Stoer and Burlirsch, 2002), stochastic approximations using\\nMonte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O’Hagan,\\n1991; Rasmussen and Ghahramani, 2003).\\nHowever, there are special cases in which we can solve it. In Section 6.6.1,\\nwe discussed conjugate models. If we choose a conjugate parameter prior\\np(θ), we can compute the marginal likelihood in closed form. In Chap-\\nter 9, we will do exactly this in the context of linear regression. ♦\\nWe have seen a brief introduction to the basic concepts of machine\\nlearning in this chapter. For the rest of this part of the book we will see\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='31d09e9a-1751-4417-8bff-f25e306a7e12', embedding=None, metadata={'page_label': '288', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='288 When Models Meet Data\\nhow the three different ﬂavors of learning in Sections 8.2, 8.3, and 8.4 are\\napplied to the four pillars of machine learning (regression, dimensionality\\nreduction, density estimation, and classiﬁcation).\\n8.6.4 Further Reading\\nWe mentioned at the start of the section that there are high-level modeling\\nchoices that inﬂuence the performance of the model. Examples include the\\nfollowing:\\nThe degree of a polynomial in a regression setting\\nThe number of components in a mixture model\\nThe network architecture of a (deep) neural network\\nThe type of kernel in a support vector machine\\nThe dimensionality of the latent space in PCA\\nThe learning rate (schedule) in an optimization algorithm\\nIn parametric\\nmodels, the number\\nof parameters is\\noften related to the\\ncomplexity of the\\nmodel class.Rasmussen and Ghahramani (2001) showed that the automatic Occam’s\\nrazor does not necessarily penalize the number of parameters in a model,\\nbut it is active in terms of the complexity of functions. They also showed\\nthat the automatic Occam’s razor also holds for Bayesian nonparametric\\nmodels with many parameters, e.g., Gaussian processes.\\nIf we focus on the maximum likelihood estimate, there exist a number of\\nheuristics for model selection that discourage overﬁtting. They are called\\ninformation criteria, and we choose the model with the largest value. The\\nAkaike information criterion (AIC) (Akaike, 1974) Akaike information\\ncriterion\\nlogp(x|θ)−M (8.48)\\ncorrects for the bias of the maximum likelihood estimator by addition of\\na penalty term to compensate for the overﬁtting of more complex models\\nwith lots of parameters. Here, Mis the number of model parameters. The\\nAIC estimates the relative information lost by a given model.\\nTheBayesian information criterion (BIC) (Schwarz, 1978) Bayesian\\ninformation\\ncriterion logp(x) = log∫\\np(x|θ)p(θ)dθ≈logp(x|θ)−1\\n2MlogN (8.49)\\ncan be used for exponential family distributions. Here, Nis the number\\nof data points and Mis the number of parameters. BIC penalizes model\\ncomplexity more heavily than AIC.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='691f1143-a64d-4c08-befe-1567a72119c4', embedding=None, metadata={'page_label': '289', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9\\nLinear Regression\\nIn the following, we will apply the mathematical concepts from Chap-\\nters 2, 5, 6, and 7 to solve linear regression (curve ﬁtting) problems. In\\nregression , we aim to ﬁnd a function fthat maps inputs x∈RDto corre- regression\\nsponding function values f(x)∈R. We assume we are given a set of train-\\ning inputsxnand corresponding noisy observations yn=f(xn)+ϵ, where\\nϵis an i.i.d. random variable that describes measurement/observation\\nnoise and potentially unmodeled processes (which we will not consider\\nfurther in this chapter). Throughout this chapter, we assume zero-mean\\nGaussian noise. Our task is to ﬁnd a function that not only models the\\ntraining data, but generalizes well to predicting function values at input\\nlocations that are not part of the training data (see Chapter 8). An il-\\nlustration of such a regression problem is given in Figure 9.1. A typical\\nregression setting is given in Figure 9.1(a): For some input values xn, we\\nobserve (noisy) function values yn=f(xn) +ϵ. The task is to infer the\\nfunctionfthat generated the data and generalizes well to function values\\nat new input locations. A possible solution is given in Figure 9.1(b), where\\nwe also show three distributions centered at the function values f(x)that\\nrepresent the noise in the data.\\nRegression is a fundamental problem in machine learning, and regres-\\nsion problems appear in a diverse range of research areas and applica-\\nFigure 9.1\\n(a) Dataset;\\n(b) possible solution\\nto the regression\\nproblem.\\n−4−2 0 2 4\\nx−0.4−0.20.00.20.4y\\n(a) Regression problem: observed noisy func-\\ntion values from which we wish to infer the\\nunderlying function that generated the data.\\n−4−2 0 2 4\\nx−0.4−0.20.00.20.4y\\n(b) Regression solution: possible function\\nthat could have generated the data (blue)\\nwith indication of the measurement noise of\\nthe function value at the corresponding in-\\nputs (orange distributions).\\n289\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db04641b-7a9f-4c33-9c96-ad18c84d3c2a', embedding=None, metadata={'page_label': '290', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='290 Linear Regression\\ntions, including time-series analysis (e.g., system identiﬁcation), control\\nand robotics (e.g., reinforcement learning, forward/inverse model learn-\\ning), optimization (e.g., line searches, global optimization), and deep-\\nlearning applications (e.g., computer games, speech-to-text translation,\\nimage recognition, automatic video annotation). Regression is also a key\\ningredient of classiﬁcation algorithms. Finding a regression function re-\\nquires solving a variety of problems, including the following:\\nChoice of the model (type) and the parametrization of the regres-\\nsion function. Given a dataset, what function classes (e.g., polynomi- Normally, the type\\nof noise could also\\nbe a “model choice”,\\nbut we ﬁx the noise\\nto be Gaussian in\\nthis chapter.als) are good candidates for modeling the data, and what particular\\nparametrization (e.g., degree of the polynomial) should we choose?\\nModel selection, as discussed in Section 8.6, allows us to compare var-\\nious models to ﬁnd the simplest model that explains the training data\\nreasonably well.\\nFinding good parameters. Having chosen a model of the regression\\nfunction, how do we ﬁnd good model parameters? Here, we will need to\\nlook at different loss/objective functions (they determine what a “good”\\nﬁt is) and optimization algorithms that allow us to minimize this loss.\\nOverﬁtting and model selection. Overﬁtting is a problem when the\\nregression function ﬁts the training data “too well” but does not gen-\\neralize to unseen test data. Overﬁtting typically occurs if the underly-\\ning model (or its parametrization) is overly ﬂexible and expressive; see\\nSection 8.6. We will look at the underlying reasons and discuss ways to\\nmitigate the effect of overﬁtting in the context of linear regression.\\nRelationship between loss functions and parameter priors. Loss func-\\ntions (optimization objectives) are often motivated and induced by prob-\\nabilistic models. We will look at the connection between loss functions\\nand the underlying prior assumptions that induce these losses.\\nUncertainty modeling. In any practical setting, we have access to only\\na ﬁnite, potentially large, amount of (training) data for selecting the\\nmodel class and the corresponding parameters. Given that this ﬁnite\\namount of training data does not cover all possible scenarios, we may\\nwant to describe the remaining parameter uncertainty to obtain a mea-\\nsure of conﬁdence of the model’s prediction at test time; the smaller the\\ntraining set, the more important uncertainty modeling. Consistent mod-\\neling of uncertainty equips model predictions with conﬁdence bounds.\\nIn the following, we will be using the mathematical tools from Chap-\\nters 3, 5, 6 and 7 to solve linear regression problems. We will discuss\\nmaximum likelihood and maximum a posteriori (MAP) estimation to ﬁnd\\noptimal model parameters. Using these parameter estimates, we will have\\na brief look at generalization errors and overﬁtting. Toward the end of\\nthis chapter, we will discuss Bayesian linear regression, which allows us to\\nreason about model parameters at a higher level, thereby removing some\\nof the problems encountered in maximum likelihood and MAP estimation.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65a47b42-b1d3-4635-83ef-01588e0626a7', embedding=None, metadata={'page_label': '291', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 Problem Formulation 291\\n9.1 Problem Formulation\\nBecause of the presence of observation noise, we will adopt a probabilis-\\ntic approach and explicitly model the noise using a likelihood function.\\nMore speciﬁcally, throughout this chapter, we consider a regression prob-\\nlem with the likelihood function\\np(y|x) =N(y|f(x), σ2). (9.1)\\nHere,x∈RDare inputs and y∈Rare noisy function values (targets).\\nWith (9.1), the functional relationship between xandyis given as\\ny=f(x) +ϵ, (9.2)\\nwhereϵ∼N(0, σ2)\\nis independent, identically distributed (i.i.d.) Gaus-\\nsian measurement noise with mean 0and variance σ2. Our objective is\\nto ﬁnd a function that is close (similar) to the unknown function fthat\\ngenerated the data and that generalizes well.\\nIn this chapter, we focus on parametric models, i.e., we choose a para-\\nmetrized function and ﬁnd parameters θthat “work well” for modeling the\\ndata. For the time being, we assume that the noise variance σ2is known\\nand focus on learning the model parameters θ. In linear regression, we\\nconsider the special case that the parameters θappear linearly in our\\nmodel. An example of linear regression is given by\\np(y|x,θ) =N(y|x⊤θ, σ2)\\n(9.3)\\n⇐⇒y=x⊤θ+ϵ, ϵ∼N(0, σ2), (9.4)\\nwhereθ∈RDare the parameters we seek. The class of functions de-\\nscribed by (9.4) are straight lines that pass through the origin. In (9.4),\\nwe chose a parametrization f(x) =x⊤θ. A Dirac delta (delta\\nfunction) is zero\\neverywhere except\\nat a single point,\\nand its integral is 1.\\nIt can be considered\\na Gaussian in the\\nlimit ofσ2→0.Thelikelihood in (9.3) is the probability density function of yevalu-\\nlikelihoodated atx⊤θ. Note that the only source of uncertainty originates from the\\nobservation noise (as xandθare assumed known in (9.3)). Without ob-\\nservation noise, the relationship between xandywould be deterministic\\nand (9.3) would be a Dirac delta.\\nExample 9.1\\nForx,θ∈Rthe linear regression model in (9.4) describes straight lines\\n(linear functions), and the parameter θis the slope of the line. Fig-\\nure 9.2(a) shows some example functions for different values of θ.\\nLinear regression\\nrefers to models that\\nare linear in the\\nparameters.The linear regression model in (9.3)–(9.4) is not only linear in the pa-\\nrameters, but also linear in the inputs x. Figure 9.2(a) shows examples\\nof such functions. We will see later that y=φ⊤(x)θfor nonlinear trans-\\nformationsφis also a linear regression model because “linear regression”\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c2ed26a-5c50-40de-a3d7-d422586ed1c3', embedding=None, metadata={'page_label': '292', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='292 Linear Regression\\nFigure 9.2 Linear\\nregression example.\\n(a) Example\\nfunctions that fall\\ninto this category;\\n(b) training set;\\n(c) maximum\\nlikelihood estimate.\\n−10 0 10\\nx−20020y (a) Example functions (straight\\nlines) that can be described us-\\ning the linear model in (9.4).\\n−10−5 0 5 10\\nx−10010y\\n(b) Training set.\\n−10−5 0 5 10\\nx−10010y\\n (c) Maximum likelihood esti-\\nmate.\\nrefers to models that are “linear in the parameters”, i.e., models that de-\\nscribe a function by a linear combination of input features. Here, a “fea-\\nture” is a representation φ(x)of the inputsx.\\nIn the following, we will discuss in more detail how to ﬁnd good pa-\\nrametersθand how to evaluate whether a parameter set “works well”.\\nFor the time being, we assume that the noise variance σ2is known.\\n9.2 Parameter Estimation\\nConsider the linear regression setting (9.4) and assume we are given a\\ntraining setD:={(x1,y1),..., (xN,yN)}consisting of Ninputsxn∈ training set\\nRDand corresponding observations/targets yn∈R,n= 1,...,N . The Figure 9.3\\nProbabilistic\\ngraphical model for\\nlinear regression.\\nObserved random\\nvariables are\\nshaded,\\ndeterministic/\\nknown values are\\nwithout circles.\\nθ\\nynσ\\nxn\\nn= 1,...,Ncorresponding graphical model is given in Figure 9.3. Note that yiandyj\\nare conditionally independent given their respective inputs xi,xjso that\\nthe likelihood factorizes according to\\np(Y|X,θ) =p(y1,...,yN|x1,...,xN,θ) (9.5a)\\n=N∏\\nn=1p(yn|xn,θ) =N∏\\nn=1N(yn|x⊤\\nnθ, σ2), (9.5b)\\nwhere we deﬁned X:={x1,...,xN}andY:={y1,...,yN}as the sets\\nof training inputs and corresponding targets, respectively. The likelihood\\nand the factors p(yn|xn,θ)are Gaussian due to the noise distribution;\\nsee (9.3).\\nIn the following, we will discuss how to ﬁnd optimal parameters θ∗∈\\nRDfor the linear regression model (9.4). Once the parameters θ∗are\\nfound, we can predict function values by using this parameter estimate\\nin (9.4) so that at an arbitrary test input x∗the distribution of the corre-\\nsponding target y∗is\\np(y∗|x∗,θ∗) =N(y∗|x⊤\\n∗θ∗, σ2). (9.6)\\nIn the following, we will have a look at parameter estimation by maxi-\\nmizing the likelihood, a topic that we already covered to some degree in\\nSection 8.3.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3bb8b7bd-0e81-4932-8da7-15b0a77391fc', embedding=None, metadata={'page_label': '293', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Parameter Estimation 293\\n9.2.1 Maximum Likelihood Estimation\\nA widely used approach to ﬁnding the desired parameters θMLismaximum maximum likelihood\\nestimation likelihood estimation , where we ﬁnd parameters θMLthat maximize the\\nlikelihood (9.5b). Intuitively, maximizing the likelihood means maximiz- Maximizing the\\nlikelihood means\\nmaximizing the\\npredictive\\ndistribution of the\\n(training) data\\ngiven the\\nparameters.ing the predictive distribution of the training data given the model param-\\neters. We obtain the maximum likelihood parameters as\\nθML= arg max\\nθp(Y|X,θ). (9.7)\\nThe likelihood is not\\na probability\\ndistribution in the\\nparameters.Remark. The likelihood p(y|x,θ)is not a probability distribution in θ: It\\nis simply a function of the parameters θbut does not integrate to 1(i.e.,\\nit is unnormalized), and may not even be integrable with respect to θ.\\nHowever, the likelihood in (9.7) is a normalized probability distribution\\niny. ♦\\nTo ﬁnd the desired parameters θMLthat maximize the likelihood, we\\ntypically perform gradient ascent (or gradient descent on the negative\\nlikelihood). In the case of linear regression we consider here, however, Since the logarithm\\nis a (strictly)\\nmonotonically\\nincreasing function,\\nthe optimum of a\\nfunctionfis\\nidentical to the\\noptimum of logf.a closed-form solution exists, which makes iterative gradient descent un-\\nnecessary. In practice, instead of maximizing the likelihood directly, we\\napply the log-transformation to the likelihood function and minimize the\\nnegative log-likelihood.\\nRemark (Log-Transformation) .Since the likelihood (9.5b) is a product of\\nNGaussian distributions, the log-transformation is useful since (a) it does\\nnot suffer from numerical underﬂow, and (b) the differentiation rules will\\nturn out simpler. More speciﬁcally, numerical underﬂow will be a prob-\\nlem when we multiply Nprobabilities, where Nis the number of data\\npoints, since we cannot represent very small numbers, such as 10−256.\\nFurthermore, the log-transform will turn the product into a sum of log-\\nprobabilities such that the corresponding gradient is a sum of individual\\ngradients, instead of a repeated application of the product rule (5.46) to\\ncompute the gradient of a product of Nterms. ♦\\nTo ﬁnd the optimal parameters θMLof our linear regression problem,\\nwe minimize the negative log-likelihood\\n−logp(Y|X,θ) =−logN∏\\nn=1p(yn|xn,θ) =−N∑\\nn=1logp(yn|xn,θ),(9.8)\\nwhere we exploited that the likelihood (9.5b) factorizes over the number\\nof data points due to our independence assumption on the training set.\\nIn the linear regression model (9.4), the likelihood is Gaussian (due to\\nthe Gaussian additive noise term), such that we arrive at\\nlogp(yn|xn,θ) =−1\\n2σ2(yn−x⊤\\nnθ)2+const, (9.9)\\nwhere the constant includes all terms independent of θ. Using (9.9) in the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d829602-0dab-4687-8794-4bd5c88e1aca', embedding=None, metadata={'page_label': '294', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='294 Linear Regression\\nnegative log-likelihood (9.8), we obtain (ignoring the constant terms)\\nL(θ) :=1\\n2σ2N∑\\nn=1(yn−x⊤\\nnθ)2(9.10a)\\n=1\\n2σ2(y−Xθ)⊤(y−Xθ) =1\\n2σ2∥y−Xθ∥2, (9.10b)\\nwhere we deﬁne the design matrix X:= [x1,...,xN]⊤∈RN×Das the The negative\\nlog-likelihood\\nfunction is also\\ncalled error function .\\ndesign matrixcollection of training inputs and y:= [y1,...,yN]⊤∈RNas a vector that\\ncollects all training targets. Note that the nth row in the design matrix X\\ncorresponds to the training input xn. In (9.10b), we used the fact that the\\nThe squared error is\\noften used as a\\nmeasure of distance.sum of squared errors between the observations ynand the corresponding\\nmodel prediction x⊤\\nnθequals the squared distance between yandXθ.\\nRecall from\\nSection 3.1 that\\n∥x∥2=x⊤xif we\\nchoose the dot\\nproduct as the inner\\nproduct.With (9.10b), we have now a concrete form of the negative log-likelihood\\nfunction we need to optimize. We immediately see that (9.10b) is quadratic\\ninθ. This means that we can ﬁnd a unique global solution θMLfor mini-\\nmizing the negative log-likelihood L. We can ﬁnd the global optimum by\\ncomputing the gradient of L, setting it to 0and solving for θ.\\nUsing the results from Chapter 5, we compute the gradient of Lwith\\nrespect to the parameters as\\ndL\\ndθ=d\\ndθ(1\\n2σ2(y−Xθ)⊤(y−Xθ))\\n(9.11a)\\n=1\\n2σ2d\\ndθ(\\ny⊤y−2y⊤Xθ+θ⊤X⊤Xθ)\\n(9.11b)\\n=1\\nσ2(−y⊤X+θ⊤X⊤X)∈R1×D. (9.11c)\\nThe maximum likelihood estimator θMLsolvesdL\\ndθ=0⊤(necessary opti-\\nmality condition) and we obtain Ignoring the\\npossibility of\\nduplicate data\\npoints, rk(X) =D\\nifN⩾D, i.e., we\\ndo not have more\\nparameters than\\ndata points.dL\\ndθ=0⊤(9.11c)⇐⇒θ⊤\\nMLX⊤X=y⊤X (9.12a)\\n⇐⇒θ⊤\\nML=y⊤X(X⊤X)−1(9.12b)\\n⇐⇒θML= (X⊤X)−1X⊤y. (9.12c)\\nWe could right-multiply the ﬁrst equation by (X⊤X)−1becauseX⊤Xis\\npositive deﬁnite if rk(X) =D, where rk(X)denotes the rank of X.\\nRemark. Setting the gradient to 0⊤is a necessary and sufﬁcient condition,\\nand we obtain a global minimum since the Hessian ∇2\\nθL(θ) =X⊤X∈\\nRD×Dis positive deﬁnite. ♦\\nRemark. The maximum likelihood solution in (9.12c) requires us to solve\\na system of linear equations of the form Aθ=bwithA= (X⊤X)and\\nb=X⊤y. ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2eda46e-760d-4855-9396-1b2bbe44ca46', embedding=None, metadata={'page_label': '295', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Parameter Estimation 295\\nExample 9.2 (Fitting Lines)\\nLet us have a look at Figure 9.2, where we aim to ﬁt a straight line f(x) =\\nθx, whereθis an unknown slope, to a dataset using maximum likelihood\\nestimation. Examples of functions in this model class (straight lines) are\\nshown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we ﬁnd\\nthe maximum likelihood estimate of the slope parameter θusing (9.12c)\\nand obtain the maximum likelihood linear function in Figure 9.2(c).\\nMaximum Likelihood Estimation with Features\\nSo far, we considered the linear regression setting described in (9.4),\\nwhich allowed us to ﬁt straight lines to data using maximum likelihood\\nestimation. However, straight lines are not sufﬁciently expressive when it Linear regression\\nrefers to “linear-in-\\nthe-parameters”\\nregression models,\\nbut the inputs can\\nundergo any\\nnonlinear\\ntransformation.comes to ﬁtting more interesting data. Fortunately, linear regression offers\\nus a way to ﬁt nonlinear functions within the linear regression framework:\\nSince “linear regression” only refers to “linear in the parameters”, we can\\nperform an arbitrary nonlinear transformation φ(x)of the inputs xand\\nthen linearly combine the components of this transformation. The corre-\\nsponding linear regression model is\\np(y|x,θ) =N(y|φ⊤(x)θ, σ2)\\n⇐⇒y=φ⊤(x)θ+ϵ=K−1∑\\nk=0θkφk(x) +ϵ,(9.13)\\nwhereφ:RD→RKis a (nonlinear) transformation of the inputs xand\\nφk:RD→Ris thekth component of the feature vector φ. Note that the feature vector\\nmodel parameters θstill appear only linearly.\\nExample 9.3 (Polynomial Regression)\\nWe are concerned with a regression problem y=φ⊤(x)θ+ϵ, wherex∈R\\nandθ∈RK. A transformation that is often used in this context is\\nφ(x) =\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0φ0(x)\\nφ1(x)\\n...\\nφK−1(x)\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01\\nx\\nx2\\nx3\\n...\\nxK−1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈RK. (9.14)\\nThis means that we “lift” the original one-dimensional input space into\\naK-dimensional feature space consisting of all monomials xkfork=\\n0,...,K−1. With these features, we can model polynomials of degree\\n⩽K−1within the framework of linear regression: A polynomial of degree\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d97d343-a5e5-4c77-9c8e-0512ec74753e', embedding=None, metadata={'page_label': '296', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='296 Linear Regression\\nK−1is\\nf(x) =K−1∑\\nk=0θkxk=φ⊤(x)θ, (9.15)\\nwhereφis deﬁned in (9.14) and θ= [θ0,...,θK−1]⊤∈RKcontains the\\n(linear) parameters θk.\\nLet us now have a look at maximum likelihood estimation of the param-\\netersθin the linear regression model (9.13). We consider training inputs\\nxn∈RDand targets yn∈R,n= 1,...,N , and deﬁne the feature matrix feature matrix\\n(design matrix ) as design matrix\\nΦ:=\\uf8ee\\n\\uf8ef\\uf8f0φ⊤(x1)\\n...\\nφ⊤(xN)\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0φ0(x1)···φK−1(x1)\\nφ0(x2)···φK−1(x2)\\n......\\nφ0(xN)···φK−1(xN)\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈RN×K,(9.16)\\nwhere Φij=φj(xi)andφj:RD→R.\\nExample 9.4 (Feature Matrix for Second-order Polynomials)\\nFor a second-order polynomial and Ntraining points xn∈R,n=\\n1,...,N , the feature matrix is\\nΦ=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f01x1x2\\n1\\n1x2x2\\n2.........\\n1xNx2\\nN\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (9.17)\\nWith the feature matrix Φdeﬁned in (9.16), the negative log-likelihood\\nfor the linear regression model (9.13) can be written as\\n−logp(Y|X,θ) =1\\n2σ2(y−Φθ)⊤(y−Φθ) +const. (9.18)\\nComparing (9.18) with the negative log-likelihood in (9.10b) for the “fea-\\nture-free” model, we immediately see we just need to replace XwithΦ.\\nSince bothXandΦare independent of the parameters θthat we wish to\\noptimize, we arrive immediately at the maximum likelihood estimate maximum likelihood\\nestimate\\nθML= (Φ⊤Φ)−1Φ⊤y (9.19)\\nfor the linear regression problem with nonlinear features deﬁned in (9.13).\\nRemark. When we were working without features, we required X⊤Xto\\nbe invertible, which is the case when rk(X) =D, i.e., the columns of X\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1c6d5e4-4d47-42db-801c-b2d1cff11144', embedding=None, metadata={'page_label': '297', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Parameter Estimation 297\\nare linearly independent. In (9.19), we therefore require Φ⊤Φ∈RK×K\\nto be invertible. This is the case if and only if rk(Φ) =K.♦\\nExample 9.5 (Maximum Likelihood Polynomial Fit)\\nFigure 9.4\\nPolynomial\\nregression:\\n(a) dataset\\nconsisting of\\n(xn,yn)pairs,\\nn= 1,..., 10;\\n(b) maximum\\nlikelihood\\npolynomial of\\ndegree 4.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Regression dataset.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b) Polynomial of degree 4 determined by max-\\nimum likelihood estimation.\\nConsider the dataset in Figure 9.4(a). The dataset consists of N= 10\\npairs (xn,yn), wherexn∼U[−5,5]andyn=−sin(xn/5) + cos(xn) +ϵ,\\nwhereϵ∼N(0,0.22)\\n.\\nWe ﬁt a polynomial of degree 4using maximum likelihood estimation,\\ni.e., parameters θMLare given in (9.19). The maximum likelihood estimate\\nyields function values φ⊤(x∗)θMLat any test location x∗. The result is\\nshown in Figure 9.4(b).\\nEstimating the Noise Variance\\nThus far, we assumed that the noise variance σ2is known. However, we\\ncan also use the principle of maximum likelihood estimation to obtain the\\nmaximum likelihood estimator σ2\\nMLfor the noise variance. To do this, we\\nfollow the standard procedure: We write down the log-likelihood, com-\\npute its derivative with respect to σ2>0, set it to 0, and solve. The\\nlog-likelihood is given by\\nlogp(Y|X,θ,σ2) =N∑\\nn=1logN(yn|φ⊤(xn)θ, σ2)\\n(9.20a)\\n=N∑\\nn=1(\\n−1\\n2log(2π)−1\\n2logσ2−1\\n2σ2(yn−φ⊤(xn)θ)2)\\n(9.20b)\\n=−N\\n2logσ2−1\\n2σ2N∑\\nn=1(yn−φ⊤(xn)θ)2\\n\\ued19\\ued18\\ued17\\ued1a\\n=:s+const. (9.20c)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b367654-f74b-4702-ad4a-63b660a61258', embedding=None, metadata={'page_label': '298', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='298 Linear Regression\\nThe partial derivative of the log-likelihood with respect to σ2is then\\n∂logp(Y|X,θ,σ2)\\n∂σ2=−N\\n2σ2+1\\n2σ4s= 0 (9.21a)\\n⇐⇒N\\n2σ2=s\\n2σ4(9.21b)\\nso that we identify\\nσ2\\nML=s\\nN=1\\nNN∑\\nn=1(yn−φ⊤(xn)θ)2. (9.22)\\nTherefore, the maximum likelihood estimate of the noise variance is the\\nempirical mean of the squared distances between the noise-free function\\nvaluesφ⊤(xn)θand the corresponding noisy observations ynat input lo-\\ncationsxn.\\n9.2.2 Overﬁtting in Linear Regression\\nWe just discussed how to use maximum likelihood estimation to ﬁt lin-\\near models (e.g., polynomials) to data. We can evaluate the quality of\\nthe model by computing the error/loss incurred. One way of doing this\\nis to compute the negative log-likelihood (9.10b), which we minimized\\nto determine the maximum likelihood estimator. Alternatively, given that\\nthe noise parameter σ2is not a free model parameter, we can ignore the\\nscaling by 1/σ2, so that we end up with a squared-error-loss function\\n∥y−Φθ∥2. Instead of using this squared loss, we often use the root mean root mean square\\nerror square error (RMSE )\\nRMSE√\\n1\\nN∥y−Φθ∥2=\\ued6a\\ued6b\\ued6b√1\\nNN∑\\nn=1(yn−φ⊤(xn)θ)2, (9.23)\\nwhich (a) allows us to compare errors of datasets with different sizes\\nand (b) has the same scale and the same units as the observed func- The RMSE is\\nnormalized. tion values yn. For example, if we ﬁt a model that maps post-codes ( x\\nis given in latitude, longitude) to house prices ( y-values are EUR) then\\nthe RMSE is also measured in EUR, whereas the squared error is given\\nin EUR2. If we choose to include the factor σ2from the original negative The negative\\nlog-likelihood is\\nunitless.log-likelihood (9.10b), then we end up with a unitless objective, i.e., in\\nthe preceding example, our objective would no longer be in EUR or EUR2.\\nFor model selection (see Section 8.6), we can use the RMSE (or the\\nnegative log-likelihood) to determine the best degree of the polynomial by\\nﬁnding the polynomial degree Mthat minimizes the objective. Given that\\nthe polynomial degree is a natural number, we can perform a brute-force\\nsearch and enumerate all (reasonable) values of M. For a training set of\\nsizeNit is sufﬁcient to test 0⩽M⩽N−1. ForM <N , the maximum\\nlikelihood estimator is unique. For M⩾N, we have more parameters\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d817ae57-7ee8-4164-b1ae-0e296fa3fb70', embedding=None, metadata={'page_label': '299', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Parameter Estimation 299\\nFigure 9.5\\nMaximum\\nlikelihood ﬁts for\\ndifferent polynomial\\ndegreesM.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(a)M= 0\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (b)M= 1\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (c)M= 3\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\n(d)M= 4\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (e)M= 6\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE (f)M= 9\\nthan data points, and would need to solve an underdetermined system of\\nlinear equations ( Φ⊤Φin (9.19) would also no longer be invertible) so\\nthat there are inﬁnitely many possible maximum likelihood estimators.\\nFigure 9.5 shows a number of polynomial ﬁts determined by maximum\\nlikelihood for the dataset from Figure 9.4(a) with N= 10 observations.\\nWe notice that polynomials of low degree (e.g., constants ( M= 0) or\\nlinear (M= 1)) ﬁt the data poorly and, hence, are poor representations\\nof the true underlying function. For degrees M= 3,..., 6, the ﬁts look\\nplausible and smoothly interpolate the data. When we go to higher-degree The case of\\nM=N−1is\\nextreme in the sense\\nthat otherwise the\\nnull space of the\\ncorresponding\\nsystem of linear\\nequations would be\\nnon-trivial, and we\\nwould have\\ninﬁnitely many\\noptimal solutions to\\nthe linear regression\\nproblem.polynomials, we notice that they ﬁt the data better and better. In the ex-\\ntreme case of M=N−1 = 9 , the function will pass through every single\\ndata point. However, these high-degree polynomials oscillate wildly and\\nare a poor representation of the underlying function that generated the\\ndata, such that we suffer from overﬁtting .\\noverﬁtting\\nNote that the noise\\nvarianceσ2>0.Remember that the goal is to achieve good generalization by making\\naccurate predictions for new (unseen) data. We obtain some quantita-\\ntive insight into the dependence of the generalization performance on the\\npolynomial of degree Mby considering a separate test set comprising 200\\ndata points generated using exactly the same procedure used to generate\\nthe training set. As test inputs, we chose a linear grid of 200points in the\\ninterval of [−5,5]. For each choice of M, we evaluate the RMSE (9.23) for\\nboth the training data and the test data.\\nLooking now at the test error, which is a qualitive measure of the gen-\\neralization properties of the corresponding polynomial, we notice that ini-\\ntially the test error decreases; see Figure 9.6 (orange). For fourth-order\\npolynomials, the test error is relatively low and stays relatively constant up\\nto degree 5. However, from degree 6onward the test error increases signif-\\nicantly, and high-order polynomials have very bad generalization proper-\\nties. In this particular example, this also is evident from the corresponding\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bdda2b5b-c42b-4cc2-b3c3-daef20a04226', embedding=None, metadata={'page_label': '300', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='300 Linear Regression\\nFigure 9.6 Training\\nand test error.\\n0 2 4 6 8 10\\nDegree of polynomial0246810RMSETraining error\\nTest error\\nmaximum likelihood ﬁts in Figure 9.5. Note that the training error (blue training error\\ncurve in Figure 9.6) never increases when the degree of the polynomial in-\\ncreases. In our example, the best generalization (the point of the smallest\\ntest error ) is obtained for a polynomial of degree M= 4. test error\\n9.2.3 Maximum A Posteriori Estimation\\nWe just saw that maximum likelihood estimation is prone to overﬁtting.\\nWe often observe that the magnitude of the parameter values becomes\\nrelatively large if we run into overﬁtting (Bishop, 2006).\\nTo mitigate the effect of huge parameter values, we can place a prior\\ndistribution p(θ)on the parameters. The prior distribution explicitly en-\\ncodes what parameter values are plausible (before having seen any data).\\nFor example, a Gaussian prior p(θ) =N(0,1)\\non a single parameter\\nθencodes that parameter values are expected lie in the interval [−2,2]\\n(two standard deviations around the mean value). Once a dataset X,Y\\nis available, instead of maximizing the likelihood we seek parameters that\\nmaximize the posterior distribution p(θ|X,Y). This procedure is called\\nmaximum a posteriori (MAP) estimation. maximum a\\nposteriori\\nMAPThe posterior over the parameters θ, given the training data X,Y, is\\nobtained by applying Bayes’ theorem (Section 6.3) as\\np(θ|X,Y) =p(Y|X,θ)p(θ)\\np(Y|X ). (9.24)\\nSince the posterior explicitly depends on the parameter prior p(θ), the\\nprior will have an effect on the parameter vector we ﬁnd as the maximizer\\nof the posterior. We will see this more explicitly in the following. The\\nparameter vector θMAPthat maximizes the posterior (9.24) is the MAP\\nestimate.\\nTo ﬁnd the MAP estimate, we follow steps that are similar in ﬂavor\\nto maximum likelihood estimation. We start with the log-transform and\\ncompute the log-posterior as\\nlogp(θ|X,Y) = logp(Y|X,θ) + logp(θ) +const, (9.25)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='021cb361-d3a6-40a7-a411-3789bed7581e', embedding=None, metadata={'page_label': '301', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Parameter Estimation 301\\nwhere the constant comprises the terms that are independent of θ. We see\\nthat the log-posterior in (9.25) is the sum of the log-likelihood p(Y|X,θ)\\nand the log-prior logp(θ)so that the MAP estimate will be a “compromise”\\nbetween the prior (our suggestion for plausible parameter values before\\nobserving data) and the data-dependent likelihood.\\nTo ﬁnd the MAP estimate θMAP, we minimize the negative log-posterior\\ndistribution with respect to θ, i.e., we solve\\nθMAP∈arg min\\nθ{−logp(Y|X,θ)−logp(θ)}. (9.26)\\nThe gradient of the negative log-posterior with respect to θis\\n−d logp(θ|X,Y)\\ndθ=−d logp(Y|X,θ)\\ndθ−d logp(θ)\\ndθ, (9.27)\\nwhere we identify the ﬁrst term on the right-hand side as the gradient of\\nthe negative log-likelihood from (9.11c).\\nWith a (conjugate) Gaussian prior p(θ) =N(0, b2I)\\non the parameters\\nθ, the negative log-posterior for the linear regression setting (9.13), we\\nobtain the negative log posterior\\n−logp(θ|X,Y) =1\\n2σ2(y−Φθ)⊤(y−Φθ) +1\\n2b2θ⊤θ+const.(9.28)\\nHere, the ﬁrst term corresponds to the contribution from the log-likelihood,\\nand the second term originates from the log-prior. The gradient of the log-\\nposterior with respect to the parameters θis then\\n−d logp(θ|X,Y)\\ndθ=1\\nσ2(θ⊤Φ⊤Φ−y⊤Φ) +1\\nb2θ⊤. (9.29)\\nWe will ﬁnd the MAP estimate θMAPby setting this gradient to 0⊤and\\nsolving forθMAP. We obtain\\n1\\nσ2(θ⊤Φ⊤Φ−y⊤Φ) +1\\nb2θ⊤=0⊤(9.30a)\\n⇐⇒θ⊤(1\\nσ2Φ⊤Φ+1\\nb2I)\\n−1\\nσ2y⊤Φ=0⊤(9.30b)\\n⇐⇒θ⊤(\\nΦ⊤Φ+σ2\\nb2I)\\n=y⊤Φ (9.30c)\\n⇐⇒θ⊤=y⊤Φ(\\nΦ⊤Φ+σ2\\nb2I)−1\\n(9.30d)\\nso that the MAP estimate is (by transposing both sides of the last equality) Φ⊤Φis symmetric,\\npositive semi\\ndeﬁnite. The\\nadditional term\\nin (9.31) is strictly\\npositive deﬁnite so\\nthat the inverse\\nexists.θMAP=(\\nΦ⊤Φ+σ2\\nb2I)−1\\nΦ⊤y. (9.31)\\nComparing the MAP estimate in (9.31) with the maximum likelihood es-\\ntimate in (9.19), we see that the only difference between both solutions\\nis the additional termσ2\\nb2Iin the inverse matrix. This term ensures that\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f14e1e9f-ff25-4061-af8c-0f5b108693f6', embedding=None, metadata={'page_label': '302', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='302 Linear Regression\\nΦ⊤Φ+σ2\\nb2Iis symmetric and strictly positive deﬁnite (i.e., its inverse\\nexists and the MAP estimate is the unique solution of a system of linear\\nequations). Moreover, it reﬂects the impact of the regularizer.\\nExample 9.6 (MAP Estimation for Polynomial Regression)\\nIn the polynomial regression example from Section 9.2.1, we place a Gaus-\\nsian priorp(θ) =N(0,I)\\non the parameters θand determine the MAP\\nestimates according to (9.31). In Figure 9.7, we show both the maximum\\nlikelihood and the MAP estimates for polynomials of degree 6(left) and\\ndegree 8(right). The prior (regularizer) does not play a signiﬁcant role\\nfor the low-degree polynomial, but keeps the function relatively smooth\\nfor higher-degree polynomials. Although the MAP estimate can push the\\nboundaries of overﬁtting, it is not a general solution to this problem, so\\nwe need a more principled approach to tackle overﬁtting.\\nFigure 9.7\\nPolynomial\\nregression:\\nmaximum likelihood\\nand MAP estimates.\\n(a) Polynomials of\\ndegree 6;\\n(b) polynomials of\\ndegree 8.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\n(a) Polynomials of degree 6.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP (b) Polynomials of degree 8.\\n9.2.4 MAP Estimation as Regularization\\nInstead of placing a prior distribution on the parameters θ, it is also pos-\\nsible to mitigate the effect of overﬁtting by penalizing the amplitude of\\nthe parameter by means of regularization . Inregularized least squares , we regularization\\nregularized least\\nsquaresconsider the loss function\\n∥y−Φθ∥2+λ∥θ∥2\\n2, (9.32)\\nwhich we minimize with respect to θ(see Section 8.2.3). Here, the ﬁrst\\nterm is a data-ﬁt term (also called misﬁt term ), which is proportional to data-ﬁt term\\nmisﬁt term the negative log-likelihood; see (9.10b). The second term is called the\\nregularizer , and the regularization parameter λ⩾0controls the “strict- regularizer\\nregularization\\nparameterness” of the regularization.\\nRemark. Instead of the Euclidean norm ∥·∥2, we can choose any p-norm\\n∥·∥pin (9.32). In practice, smaller values for plead to sparser solutions.\\nHere, “sparse” means that many parameter values θd= 0, which is also\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3ab5912-d27e-4689-a394-2984917728bc', embedding=None, metadata={'page_label': '303', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 Bayesian Linear Regression 303\\nuseful for variable selection. For p= 1, the regularizer is called LASSO LASSO\\n(least absolute shrinkage and selection operator) and was proposed by Tib-\\nshirani (1996). ♦\\nThe regularizer λ∥θ∥2\\n2in (9.32) can be interpreted as a negative log-\\nGaussian prior, which we use in MAP estimation; see (9.26). More specif-\\nically, with a Gaussian prior p(θ) =N(0, b2I)\\n, we obtain the negative\\nlog-Gaussian prior\\n−logp(θ) =1\\n2b2∥θ∥2\\n2+const (9.33)\\nso that forλ=1\\n2b2the regularization term and the negative log-Gaussian\\nprior are identical.\\nGiven that the regularized least-squares loss function in (9.32) consists\\nof terms that are closely related to the negative log-likelihood plus a neg-\\native log-prior, it is not surprising that, when we minimize this loss, we\\nobtain a solution that closely resembles the MAP estimate in (9.31). More\\nspeciﬁcally, minimizing the regularized least-squares loss function yields\\nθRLS= (Φ⊤Φ+λI)−1Φ⊤y, (9.34)\\nwhich is identical to the MAP estimate in (9.31) for λ=σ2\\nb2, whereσ2is\\nthe noise variance and b2the variance of the (isotropic) Gaussian prior\\np(θ) =N(0, b2I)\\n. A point estimate is a\\nsingle speciﬁc\\nparameter value,\\nunlike a distribution\\nover plausible\\nparameter settings.So far, we have covered parameter estimation using maximum likeli-\\nhood and MAP estimation where we found point estimates θ∗that op-\\ntimize an objective function (likelihood or posterior). We saw that both\\nmaximum likelihood and MAP estimation can lead to overﬁtting. In the\\nnext section, we will discuss Bayesian linear regression, where we use\\nBayesian inference (Section 8.4) to ﬁnd a posterior distribution over the\\nunknown parameters, which we subsequently use to make predictions.\\nMore speciﬁcally, for predictions we will average over all plausible sets of\\nparameters instead of focusing on a point estimate.\\n9.3 Bayesian Linear Regression\\nPreviously, we looked at linear regression models where we estimated the\\nmodel parameters θ, e.g., by means of maximum likelihood or MAP esti-\\nmation. We discovered that MLE can lead to severe overﬁtting, in particu-\\nlar, in the small-data regime. MAP addresses this issue by placing a prior\\non the parameters that plays the role of a regularizer. Bayesian linear\\nregression Bayesian linear regression pushes the idea of the parameter prior a step\\nfurther and does not even attempt to compute a point estimate of the\\nparameters, but instead the full posterior distribution over the parameters\\nis taken into account when making predictions. This means we do not ﬁt\\nany parameters, but we compute a mean over all plausible parameters\\nsettings (according to the posterior).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9142a50c-d290-4217-ad87-0258b1ebfd08', embedding=None, metadata={'page_label': '304', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='304 Linear Regression\\n9.3.1 Model\\nIn Bayesian linear regression, we consider the model\\nprior p(θ) =N(m0,S0),\\nlikelihood p(y|x,θ) =N(y|φ⊤(x)θ, σ2),(9.35)\\nwhere we now explicitly place a Gaussian prior p(θ) =N(m0,S0)\\nonθ, Figure 9.8\\nGraphical model for\\nBayesian linear\\nregression.\\nθ\\nyσ\\nxm0S0which turns the parameter vector into a random variable. This allows us\\nto write down the corresponding graphical model in Figure 9.8, where we\\nmade the parameters of the Gaussian prior on θexplicit. The full proba-\\nbilistic model, i.e., the joint distribution of observed and unobserved ran-\\ndom variables, yandθ, respectively, is\\np(y,θ|x) =p(y|x,θ)p(θ). (9.36)\\n9.3.2 Prior Predictions\\nIn practice, we are usually not so much interested in the parameter values\\nθthemselves. Instead, our focus often lies in the predictions we make\\nwith those parameter values. In a Bayesian setting, we take the parameter\\ndistribution and average over all plausible parameter settings when we\\nmake predictions. More speciﬁcally, to make predictions at an input x∗,\\nwe integrate out θand obtain\\np(y∗|x∗) =∫\\np(y∗|x∗,θ)p(θ)dθ=Eθ[p(y∗|x∗,θ)], (9.37)\\nwhich we can interpret as the average prediction of y∗|x∗,θfor all plau-\\nsible parameters θaccording to the prior distribution p(θ). Note that pre-\\ndictions using the prior distribution only require us to specify the input\\nx∗, but no training data.\\nIn our model (9.35), we chose a conjugate (Gaussian) prior on θso\\nthat the predictive distribution is Gaussian as well (and can be computed\\nin closed form): With the prior distribution p(θ) =N(m0,S0)\\n, we obtain\\nthe predictive distribution as\\np(y∗|x∗) =N(φ⊤(x∗)m0,φ⊤(x∗)S0φ(x∗) +σ2), (9.38)\\nwhere we exploited that (i) the prediction is Gaussian due to conjugacy\\n(see Section 6.6) and the marginalization property of Gaussians (see Sec-\\ntion 6.5), (ii) the Gaussian noise is independent so that\\nV[y∗] =Vθ[φ⊤(x∗)θ] +Vϵ[ϵ], (9.39)\\nand (iii)y∗is a linear transformation of θso that we can apply the rules\\nfor computing the mean and covariance of the prediction analytically by\\nusing (6.50) and (6.51), respectively. In (9.38), the term φ⊤(x∗)S0φ(x∗)\\nin the predictive variance explicitly accounts for the uncertainty associated\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c433de8c-e75c-4816-86f8-fce23fc9ae25', embedding=None, metadata={'page_label': '305', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 Bayesian Linear Regression 305\\nwith the parameters θ, whereasσ2is the uncertainty contribution due to\\nthe measurement noise.\\nIf we are interested in predicting noise-free function values f(x∗) =\\nφ⊤(x∗)θinstead of the noise-corrupted targets y∗we obtain\\np(f(x∗)) =N(φ⊤(x∗)m0,φ⊤(x∗)S0φ(x∗)), (9.40)\\nwhich only differs from (9.38) in the omission of the noise variance σ2in\\nthe predictive variance.\\nRemark (Distribution over Functions) .Since we can represent the distri- The parameter\\ndistribution p(θ)\\ninduces a\\ndistribution over\\nfunctions.butionp(θ)using a set of samples θiand every sample θigives rise to a\\nfunctionfi(·) =θ⊤\\niφ(·), it follows that the parameter distribution p(θ)\\ninduces a distribution p(f(·))over functions. Here we use the notation (·)\\nto explicitly denote a functional relationship. ♦\\nExample 9.7 (Prior over Functions)\\nFigure 9.9 Prior\\nover functions.\\n(a) Distribution over\\nfunctions\\nrepresented by the\\nmean function\\n(black line) and the\\nmarginal\\nuncertainties\\n(shaded),\\nrepresenting the\\n67% and 95%\\nconﬁdence bounds,\\nrespectively;\\n(b) samples from\\nthe prior over\\nfunctions, which are\\ninduced by the\\nsamples from the\\nparameter prior.\\n−4−2 0 2 4\\nx−4−2024y(a) Prior distribution over functions.\\n−4−2 0 2 4\\nx−4−2024y (b) Samples from the prior distribution over\\nfunctions.\\nLet us consider a Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(θ) =N(0,1\\n4I)\\n. Figure 9.9\\nvisualizes the induced prior distribution over functions (shaded area: dark\\ngray: 67% conﬁdence bound; light gray: 95% conﬁdence bound) induced\\nby this parameter prior, including some function samples from this prior.\\nA function sample is obtained by ﬁrst sampling a parameter vector\\nθi∼p(θ)and then computing fi(·) =θ⊤\\niφ(·). We used 200input lo-\\ncationsx∗∈[−5,5]to which we apply the feature function φ(·). The\\nuncertainty (represented by the shaded area) in Figure 9.9 is solely due to\\nthe parameter uncertainty because we considered the noise-free predictive\\ndistribution (9.40).\\nSo far, we looked at computing predictions using the parameter prior\\np(θ). However, when we have a parameter posterior (given some train-\\ning dataX,Y), the same principles for prediction and inference hold\\nas in (9.37) – we just need to replace the prior p(θ)with the posterior\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b0104ca-5ae9-482a-b3e1-b5a0f8bd5d11', embedding=None, metadata={'page_label': '306', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='306 Linear Regression\\np(θ|X,Y). In the following, we will derive the posterior distribution in\\ndetail before using it to make predictions.\\n9.3.3 Posterior Distribution\\nGiven a training set of inputs xn∈RDand corresponding observations\\nyn∈R,n= 1,...,N , we compute the posterior over the parameters\\nusing Bayes’ theorem as\\np(θ|X,Y) =p(Y|X,θ)p(θ)\\np(Y|X ), (9.41)\\nwhereXis the set of training inputs and Ythe collection of correspond-\\ning training targets. Furthermore, p(Y|X,θ)is the likelihood, p(θ)the\\nparameter prior, and\\np(Y|X ) =∫\\np(Y|X,θ)p(θ)dθ=Eθ[p(Y|X,θ)] (9.42)\\nthemarginal likelihood /evidence , which is independent of the parameters marginal likelihood\\nevidence θand ensures that the posterior is normalized, i.e., it integrates to 1. We\\nThe marginal\\nlikelihood is the\\nexpected likelihood\\nunder the parameter\\nprior.can think of the marginal likelihood as the likelihood averaged over all\\npossible parameter settings (with respect to the prior distribution p(θ)).\\nTheorem 9.1 (Parameter Posterior) .In our model (9.35) , the parameter\\nposterior (9.41) can be computed in closed form as\\np(θ|X,Y) =N(θ|mN,SN), (9.43a)\\nSN= (S−1\\n0+σ−2Φ⊤Φ)−1, (9.43b)\\nmN=SN(S−1\\n0m0+σ−2Φ⊤y), (9.43c)\\nwhere the subscript Nindicates the size of the training set.\\nProof Bayes’ theorem tells us that the posterior p(θ|X,Y)is propor-\\ntional to the product of the likelihood p(Y|X,θ)and the prior p(θ):\\nPosterior p(θ|X,Y) =p(Y|X,θ)p(θ)\\np(Y|X )(9.44a)\\nLikelihood p(Y|X,θ) =N(y|Φθ, σ2I)\\n(9.44b)\\nPrior p(θ) =N(θ|m0,S0). (9.44c)\\nInstead of looking at the product of the prior and the likelihood, we\\ncan transform the problem into log-space and solve for the mean and\\ncovariance of the posterior by completing the squares.\\nThe sum of the log-prior and the log-likelihood is\\nlogN(y|Φθ, σ2I)+ logN(θ|m0,S0)\\n(9.45a)\\n=−1\\n2(σ−2(y−Φθ)⊤(y−Φθ) + (θ−m0)⊤S−1\\n0(θ−m0))+const\\n(9.45b)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db751dd0-8b8f-4484-a82a-11687028c307', embedding=None, metadata={'page_label': '307', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 Bayesian Linear Regression 307\\nwhere the constant contains terms independent of θ. We will ignore the\\nconstant in the following. We now factorize (9.45b), which yields\\n−1\\n2(σ−2y⊤y−2σ−2y⊤Φθ+θ⊤σ−2Φ⊤Φθ+θ⊤S−1\\n0θ\\n−2m⊤\\n0S−1\\n0θ+m⊤\\n0S−1\\n0m0)(9.46a)\\n=−1\\n2(θ⊤(σ−2Φ⊤Φ+S−1\\n0)θ−2(σ−2Φ⊤y+S−1\\n0m0)⊤θ)+const,\\n(9.46b)\\nwhere the constant contains the black terms in (9.46a), which are inde-\\npendent ofθ. The orange terms are terms that are linear in θ, and the\\nblue terms are the ones that are quadratic in θ. Inspecting (9.46b), we\\nﬁnd that this equation is quadratic in θ. The fact that the unnormalized\\nlog-posterior distribution is a (negative) quadratic form implies that the\\nposterior is Gaussian, i.e.,\\np(θ|X,Y) = exp(log p(θ|X,Y))∝exp(logp(Y|X,θ) + logp(θ))\\n(9.47a)\\n∝exp(\\n−1\\n2(θ⊤(σ−2Φ⊤Φ+S−1\\n0)θ−2(σ−2Φ⊤y+S−1\\n0m0)⊤θ))\\n,\\n(9.47b)\\nwhere we used (9.46b) in the last expression.\\nThe remaining task is it to bring this (unnormalized) Gaussian into the\\nform that is proportional to N(θ|mN,SN)\\n, i.e., we need to identify the\\nmeanmNand the covariance matrix SN. To do this, we use the concept\\nofcompleting the squares . The desired log-posterior is completing the\\nsquares\\nlogN(θ|mN,SN)=−1\\n2(θ−mN)⊤S−1\\nN(θ−mN) +const (9.48a)\\n=−1\\n2(θ⊤S−1\\nNθ−2m⊤\\nNS−1\\nNθ+m⊤\\nNS−1\\nNmN). (9.48b)\\nHere, we factorized the quadratic form (θ−mN)⊤S−1\\nN(θ−mN)into a Sincep(θ|X,Y) =\\nN(\\nmN,SN)\\n, it\\nholds that\\nθMAP=mN.term that is quadratic in θalone (blue), a term that is linear in θ(orange),\\nand a constant term (black). This allows us now to ﬁnd SNandmNby\\nmatching the colored expressions in (9.46b) and (9.48b), which yields\\nS−1\\nN=Φ⊤σ−2IΦ+S−1\\n0 (9.49a)\\n⇐⇒SN= (σ−2Φ⊤Φ+S−1\\n0)−1(9.49b)\\nand\\nm⊤\\nNS−1\\nN= (σ−2Φ⊤y+S−1\\n0m0)⊤(9.50a)\\n⇐⇒mN=SN(σ−2Φ⊤y+S−1\\n0m0). (9.50b)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4af07fc-f9ca-4d9b-8acd-3df61098f1c4', embedding=None, metadata={'page_label': '308', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='308 Linear Regression\\nRemark (General Approach to Completing the Squares) .If we are given\\nan equation\\nx⊤Ax−2a⊤x+const 1, (9.51)\\nwhereAis symmetric and positive deﬁnite, which we wish to bring into\\nthe form\\n(x−µ)⊤Σ(x−µ) +const 2, (9.52)\\nwe can do this by setting\\nΣ:=A, (9.53)\\nµ:=Σ−1a (9.54)\\nand const 2=const 1−µ⊤Σµ. ♦\\nWe can see that the terms inside the exponential in (9.47b) are of the\\nform (9.51) with\\nA:=σ−2Φ⊤Φ+S−1\\n0, (9.55)\\na:=σ−2Φ⊤y+S−1\\n0m0. (9.56)\\nSinceA,acan be difﬁcult to identify in equations like (9.46a), it is of-\\nten helpful to bring these equations into the form (9.51) that decouples\\nquadratic term, linear terms, and constants, which simpliﬁes ﬁnding the\\ndesired solution.\\n9.3.4 Posterior Predictions\\nIn (9.37), we computed the predictive distribution of y∗at a test input\\nx∗using the parameter prior p(θ). In principle, predicting with the pa-\\nrameter posterior p(θ|X,Y)is not fundamentally different given that\\nin our conjugate model the prior and posterior are both Gaussian (with\\ndifferent parameters). Therefore, by following the same reasoning as in\\nSection 9.3.2, we obtain the (posterior) predictive distribution\\np(y∗|X,Y,x∗) =∫\\np(y∗|x∗,θ)p(θ|X,Y)dθ (9.57a)\\n=∫\\nN(y∗|φ⊤(x∗)θ, σ2)N(θ|mN,SN)dθ(9.57b)\\n=N(y∗|φ⊤(x∗)mN,φ⊤(x∗)SNφ(x∗) +σ2).(9.57c)\\nThe termφ⊤(x∗)SNφ(x∗)reﬂects the posterior uncertainty associated E[y∗|X,Y,x∗] =\\nφ⊤(x∗)mN=\\nφ⊤(x∗)θMAP.with the parameters θ. Note thatSNdepends on the training inputs\\nthrough Φ; see (9.43b). The predictive mean φ⊤(x∗)mNcoincides with\\nthe predictions made with the MAP estimate θMAP.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2559ed64-7f98-45f1-a22e-c370455352dd', embedding=None, metadata={'page_label': '309', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 Bayesian Linear Regression 309\\nRemark (Marginal Likelihood and Posterior Predictive Distribution) .By\\nreplacing the integral in (9.57a), the predictive distribution can be equiv-\\nalently written as the expectation Eθ|X,Y[p(y∗|x∗,θ)], where the expec-\\ntation is taken with respect to the parameter posterior p(θ|X,Y).\\nWriting the posterior predictive distribution in this way highlights a\\nclose resemblance to the marginal likelihood (9.42). The key difference\\nbetween the marginal likelihood and the posterior predictive distribution\\nare (i) the marginal likelihood can be thought of predicting the training\\ntargetsyand not the test targets y∗, and (ii) the marginal likelihood av-\\nerages with respect to the parameter prior and not the parameter poste-\\nrior. ♦\\nRemark (Mean and Variance of Noise-Free Function Values) .In many\\ncases, we are not interested in the predictive distribution p(y∗|X,Y,x∗)\\nof a (noisy) observation y∗. Instead, we would like to obtain the distribu-\\ntion of the (noise-free) function values f(x∗) =φ⊤(x∗)θ. We determine\\nthe corresponding moments by exploiting the properties of means and\\nvariances, which yields\\nE[f(x∗)|X,Y] =Eθ[φ⊤(x∗)θ|X,Y] =φ⊤(x∗)Eθ[θ|X,Y]\\n=φ⊤(x∗)mN=m⊤\\nNφ(x∗),(9.58)\\nVθ[f(x∗)|X,Y] =Vθ[φ⊤(x∗)θ|X,Y]\\n=φ⊤(x∗)Vθ[θ|X,Y]φ(x∗)\\n=φ⊤(x∗)SNφ(x∗).(9.59)\\nWe see that the predictive mean is the same as the predictive mean for\\nnoisy observations as the noise has mean 0, and the predictive variance\\nonly differs by σ2, which is the variance of the measurement noise: When\\nwe predict noisy function values, we need to include σ2as a source of\\nuncertainty, but this term is not needed for noise-free predictions. Here,\\nthe only remaining uncertainty stems from the parameter posterior. ♦Integrating out\\nparameters induces\\na distribution over\\nfunctions.Remark (Distribution over Functions) .The fact that we integrate out the\\nparametersθinduces a distribution over functions: If we sample θi∼\\np(θ|X,Y)from the parameter posterior, we obtain a single function re-\\nalizationθ⊤\\niφ(·). The mean function , i.e., the set of all expected function mean function\\nvalues Eθ[f(·)|θ,X,Y], of this distribution over functions is m⊤\\nNφ(·).\\nThe (marginal) variance, i.e., the variance of the function f(·), is given by\\nφ⊤(·)SNφ(·). ♦\\nExample 9.8 (Posterior over Functions)\\nLet us revisit the Bayesian linear regression problem with polynomials\\nof degree 5. We choose a parameter prior p(θ) =N(0,1\\n4I)\\n. Figure 9.9\\nvisualizes the prior over functions induced by the parameter prior and\\nsample functions from this prior.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a4fc829-1491-423c-9480-d20fe3564232', embedding=None, metadata={'page_label': '310', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='310 Linear Regression\\nFigure 9.10 shows the posterior over functions that we obtain via\\nBayesian linear regression. The training dataset is shown in panel (a);\\npanel (b) shows the posterior distribution over functions, including the\\nfunctions we would obtain via maximum likelihood and MAP estimation.\\nThe function we obtain using the MAP estimate also corresponds to the\\nposterior mean function in the Bayesian linear regression setting. Panel (c)\\nshows some plausible realizations (samples) of functions under that pos-\\nterior over functions.\\nFigure 9.10\\nBayesian linear\\nregression and\\nposterior over\\nfunctions.\\n(a) training data;\\n(b) posterior\\ndistribution over\\nfunctions;\\n(c) Samples from\\nthe posterior over\\nfunctions.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Training data.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR (b) Posterior over functions rep-\\nresented by the marginal uncer-\\ntainties (shaded) showing the\\n67% and 95% predictive con-\\nﬁdence bounds, the maximum\\nlikelihood estimate (MLE) and\\nthe MAP estimate (MAP), the\\nlatter of which is identical to\\nthe posterior mean function.\\n−4−2 0 2 4\\nx−4−2024y\\n(c) Samples from the posterior\\nover functions, which are in-\\nduced by the samples from the\\nparameter posterior.\\nFigure 9.11 shows some posterior distributions over functions induced\\nby the parameter posterior. For different polynomial degrees M, the left\\npanels show the maximum likelihood function θ⊤\\nMLφ(·), the MAP func-\\ntionθ⊤\\nMAPφ(·)(which is identical to the posterior mean function), and the\\n67% and 95% predictive conﬁdence bounds obtained by Bayesian linear\\nregression, represented by the shaded areas.\\nThe right panels show samples from the posterior over functions: Here,\\nwe sampled parameters θifrom the parameter posterior and computed\\nthe functionφ⊤(x∗)θi, which is a single realization of a function under\\nthe posterior distribution over functions. For low-order polynomials, the\\nparameter posterior does not allow the parameters to vary much: The\\nsampled functions are nearly identical. When we make the model more\\nﬂexible by adding more parameters (i.e., we end up with a higher-order\\npolynomial), these parameters are not sufﬁciently constrained by the pos-\\nterior, and the sampled functions can be easily visually separated. We also\\nsee in the corresponding panels on the left how the uncertainty increases,\\nespecially at the boundaries.\\nAlthough for a seventh-order polynomial the MAP estimate yields a rea-\\nsonable ﬁt, the Bayesian linear regression model additionally tells us that\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b60af335-bb65-491f-8a18-1ec5df5e891e', embedding=None, metadata={'page_label': '311', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 Bayesian Linear Regression 311\\nFigure 9.11\\nBayesian linear\\nregression. Left\\npanels: Shaded\\nareas indicate the\\n67% (dark gray)\\nand 95% (light\\ngray) predictive\\nconﬁdence bounds.\\nThe mean of the\\nBayesian linear\\nregression model\\ncoincides with the\\nMAP estimate. The\\npredictive\\nuncertainty is the\\nsum of the noise\\nterm and the\\nposterior parameter\\nuncertainty, which\\ndepends on the\\nlocation of the test\\ninput. Right panels:\\nsampled functions\\nfrom the posterior\\ndistribution.\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Posterior distribution for polynomials of degree M= 3(left) and samples from the pos-\\nterior over functions (right).\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(b) Posterior distribution for polynomials of degree M= 5 (left) and samples from the\\nposterior over functions (right).\\n−4−2 0 2 4\\nx−4−2024y\\nTraining data\\nMLE\\nMAP\\nBLR\\n−4−2 0 2 4\\nx−4−2024y\\n(c) Posterior distribution for polynomials of degree M= 7(left) and samples from the pos-\\nterior over functions (right).\\nthe posterior uncertainty is huge. This information can be critical when\\nwe use these predictions in a decision-making system, where bad deci-\\nsions can have signiﬁcant consequences (e.g., in reinforcement learning\\nor robotics).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f328f1b-7108-4378-ad4c-a001b1963a8e', embedding=None, metadata={'page_label': '312', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='312 Linear Regression\\n9.3.5 Computing the Marginal Likelihood\\nIn Section 8.6.2, we highlighted the importance of the marginal likelihood\\nfor Bayesian model selection. In the following, we compute the marginal\\nlikelihood for Bayesian linear regression with a conjugate Gaussian prior\\non the parameters, i.e., exactly the setting we have been discussing in this\\nchapter.\\nJust to recap, we consider the following generative process:\\nθ∼N(m0,S0)\\n(9.60a)\\nyn|xn,θ∼N(x⊤\\nnθ, σ2), (9.60b)\\nn= 1,...,N . The marginal likelihood is given by The marginal\\nlikelihood can be\\ninterpreted as the\\nexpected likelihood\\nunder the prior, i.e.,\\nEθ[p(Y|X,θ)].p(Y|X ) =∫\\np(Y|X,θ)p(θ)dθ (9.61a)\\n=∫\\nN(y|Xθ, σ2I)N(θ|m0,S0)dθ, (9.61b)\\nwhere we integrate out the model parameters θ. We compute the marginal\\nlikelihood in two steps: First, we show that the marginal likelihood is\\nGaussian (as a distribution in y); second, we compute the mean and co-\\nvariance of this Gaussian.\\n1. The marginal likelihood is Gaussian: From Section 6.5.2, we know that\\n(i) the product of two Gaussian random variables is an (unnormalized)\\nGaussian distribution, and (ii) a linear transformation of a Gaussian\\nrandom variable is Gaussian distributed. In (9.61b), we require a linear\\ntransformation to bring N(y|Xθ, σ2I)\\ninto the formN(θ|µ,Σ)\\nfor\\nsomeµ,Σ. Once this is done, the integral can be solved in closed form.\\nThe result is the normalizing constant of the product of the two Gaus-\\nsians. The normalizing constant itself has Gaussian shape; see (6.76).\\n2. Mean and covariance. We compute the mean and covariance matrix\\nof the marginal likelihood by exploiting the standard results for means\\nand covariances of afﬁne transformations of random variables; see Sec-\\ntion 6.4.4. The mean of the marginal likelihood is computed as\\nE[Y|X ] =Eθ,ϵ[Xθ+ϵ] =XEθ[θ] =Xm 0. (9.62)\\nNote thatϵ∼N(0, σ2I)\\nis a vector of i.i.d. random variables. The\\ncovariance matrix is given as\\nCov[Y|X] = Covθ,ϵ[Xθ+ϵ] = Covθ[Xθ] +σ2I (9.63a)\\n=XCovθ[θ]X⊤+σ2I=XS 0X⊤+σ2I.(9.63b)\\nHence, the marginal likelihood is\\np(Y|X ) = (2π)−N\\n2det(XS 0X⊤+σ2I)−1\\n2 (9.64a)\\n·exp(−1\\n2(y−Xm 0)⊤(XS 0X⊤+σ2I)−1(y−Xm 0))\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ea7459f-c537-4bc2-9bf6-df869dc1d7c5', embedding=None, metadata={'page_label': '313', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 Maximum Likelihood as Orthogonal Projection 313\\nFigure 9.12\\nGeometric\\ninterpretation of\\nleast squares.\\n(a) Dataset;\\n(b) maximum\\nlikelihood solution\\ninterpreted as a\\nprojection.\\n−4−2 0 2 4\\nx−4−2024y\\n(a) Regression dataset consisting of noisy ob-\\nservationsyn(blue) of function values f(xn)\\nat input locations xn.\\n−4−2 0 2 4\\nx−4−2024y\\nProjection\\nObservations\\nMaximum likelihood estimate(b) The orange dots are the projections of\\nthe noisy observations (blue dots) onto the\\nlineθMLx. The maximum likelihood solution to\\na linear regression problem ﬁnds a subspace\\n(line) onto which the overall projection er-\\nror (orange lines) of the observations is mini-\\nmized.\\n=N(y|Xm 0,XS 0X⊤+σ2I). (9.64b)\\nGiven the close connection with the posterior predictive distribution (see\\nRemark on Marginal Likelihood and Posterior Predictive Distribution ear-\\nlier in this section), the functional form of the marginal likelihood should\\nnot be too surprising.\\n9.4 Maximum Likelihood as Orthogonal Projection\\nHaving crunched through much algebra to derive maximum likelihood\\nand MAP estimates, we will now provide a geometric interpretation of\\nmaximum likelihood estimation. Let us consider a simple linear regression\\nsetting\\ny=xθ+ϵ, ϵ∼N(0, σ2), (9.65)\\nin which we consider linear functions f:R→Rthat go through the\\norigin (we omit features here for clarity). The parameter θdetermines the\\nslope of the line. Figure 9.12(a) shows a one-dimensional dataset.\\nWith a training data set {(x1,y1),..., (xN,yN)}we recall the results\\nfrom Section 9.2.1 and obtain the maximum likelihood estimator for the\\nslope parameter as\\nθML= (X⊤X)−1X⊤y=X⊤y\\nX⊤X∈R, (9.66)\\nwhereX= [x1,...,xN]⊤∈RN,y= [y1,...,yN]⊤∈RN.\\nThis means for the training inputs Xwe obtain the optimal (maximum\\nlikelihood) reconstruction of the training targets as\\nXθML=XX⊤y\\nX⊤X=XX⊤\\nX⊤Xy, (9.67)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0609f2f4-b70f-489a-9367-a7d3cd296655', embedding=None, metadata={'page_label': '314', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='314 Linear Regression\\ni.e., we obtain the approximation with the minimum least-squares error\\nbetweenyandXθ.\\nAs we are looking for a solution of y=Xθ, we can think of linear\\nregression as a problem for solving systems of linear equations. There- Linear regression\\ncan be thought of as\\na method for solving\\nsystems of linear\\nequations.fore, we can relate to concepts from linear algebra and analytic geometry\\nthat we discussed in Chapters 2 and 3. In particular, looking carefully\\nat (9.67) we see that the maximum likelihood estimator θMLin our ex-\\nample from (9.65) effectively does an orthogonal projection of yonto\\nthe one-dimensional subspace spanned by X. Recalling the results on or- Maximum\\nlikelihood linear\\nregression performs\\nan orthogonal\\nprojection.thogonal projections from Section 3.8, we identifyXX⊤\\nX⊤Xas the projection\\nmatrix,θMLas the coordinates of the projection onto the one-dimensional\\nsubspace of RNspanned byXandXθMLas the orthogonal projection of\\nyonto this subspace.\\nTherefore, the maximum likelihood solution provides also a geometri-\\ncally optimal solution by ﬁnding the vectors in the subspace spanned by\\nXthat are “closest” to the corresponding observations y, where “clos-\\nest” means the smallest (squared) distance of the function values ynto\\nxnθ. This is achieved by orthogonal projections. Figure 9.12(b) shows the\\nprojection of the noisy observations onto the subspace that minimizes the\\nsquared distance between the original dataset and its projection (note that\\nthex-coordinate is ﬁxed), which corresponds to the maximum likelihood\\nsolution.\\nIn the general linear regression case where\\ny=φ⊤(x)θ+ϵ, ϵ∼N(0, σ2)\\n(9.68)\\nwith vector-valued features φ(x)∈RK, we again can interpret the maxi-\\nmum likelihood result\\ny≈ΦθML, (9.69)\\nθML= (Φ⊤Φ)−1Φ⊤y (9.70)\\nas a projection onto a K-dimensional subspace of RN, which is spanned\\nby the columns of the feature matrix Φ; see Section 3.8.2.\\nIf the feature functions φkthat we use to construct the feature ma-\\ntrixΦare orthonormal (see Section 3.7), we obtain a special case where\\nthe columns of Φform an orthonormal basis (see Section 3.5), such that\\nΦ⊤Φ=I. This will then lead to the projection\\nΦ(Φ⊤Φ)−1Φ⊤y=ΦΦ⊤y=(K∑\\nk=1φkφ⊤\\nk)\\ny (9.71)\\nso that the maximum likelihood projection is simply the sum of projections\\nofyonto the individual basis vectors φk, i.e., the columns of Φ. Further-\\nmore, the coupling between different features has disappeared due to the\\northogonality of the basis. Many popular basis functions in signal process-\\ning, such as wavelets and Fourier bases, are orthogonal basis functions.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='041fe5b9-01b1-470c-aaad-c51e101e1b5a', embedding=None, metadata={'page_label': '315', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.5 Further Reading 315\\nWhen the basis is not orthogonal, one can convert a set of linearly inde-\\npendent basis functions to an orthogonal basis by using the Gram-Schmidt\\nprocess; see Section 3.8.3 and (Strang, 2003).\\n9.5 Further Reading\\nIn this chapter, we discussed linear regression for Gaussian likelihoods\\nand conjugate Gaussian priors on the parameters of the model. This al-\\nlowed for closed-form Bayesian inference. However, in some applications\\nwe may want to choose a different likelihood function. For example, in\\na binary classiﬁcation setting, we observe only two possible (categorical) classiﬁcation\\noutcomes, and a Gaussian likelihood is inappropriate in this setting. In-\\nstead, we can choose a Bernoulli likelihood that will return a probability of\\nthe predicted label to be 1(or0). We refer to the books by Barber (2012),\\nBishop (2006), and Murphy (2012) for an in-depth introduction to classiﬁ-\\ncation problems. A different example where non-Gaussian likelihoods are\\nimportant is count data. Counts are non-negative integers, and in this case\\na Binomial or Poisson likelihood would be a better choice than a Gaussian.\\nAll these examples fall into the category of generalized linear models , a ﬂex- generalized linear\\nmodel ible generalization of linear regression that allows for response variables\\nthat have error distributions other than a Gaussian distribution. The GLM Generalized linear\\nmodels are the\\nbuilding blocks of\\ndeep neural\\nnetworks.generalizes linear regression by allowing the linear model to be related\\nto the observed values via a smooth and invertible function σ(·)that may\\nbe nonlinear so that y=σ(f(x)), wheref(x) =θ⊤φ(x)is the linear\\nregression model from (9.13). We can therefore think of a generalized\\nlinear model in terms of function composition y=σ◦f, wherefis a\\nlinear regression model and σthe activation function. Note that although\\nwe are talking about “generalized linear models”, the outputs yare no\\nlonger linear in the parameters θ. Inlogistic regression , we choose the logistic regression\\nlogistic sigmoid σ(f) =1\\n1+exp(−f)∈[0,1], which can be interpreted as the logistic sigmoid\\nprobability of observing y= 1of a Bernoulli random variable y∈{0,1}.\\nThe function σ(·)is called transfer function oractivation function , and its transfer function\\nactivation function inverse is called the canonical link function . From this perspective, it is\\ncanonical link\\nfunction\\nFor ordinary linear\\nregression the\\nactivation function\\nwould simply be the\\nidentity.also clear that generalized linear models are the building blocks of (deep)\\nfeedforward neural networks: If we consider a generalized linear model\\ny=σ(Ax+b), whereAis a weight matrix and ba bias vector, we iden-\\ntify this generalized linear model as a single-layer neural network with\\nactivation function σ(·). We can now recursively compose these functions\\nvia\\nA great post on the\\nrelation between\\nGLMs and deep\\nnetworks is\\navailable at\\nhttps://tinyurl.\\ncom/glm-dnn .xk+1=fk(xk)\\nfk(xk) =σk(Akxk+bk)(9.72)\\nfork= 0,...,K−1, wherex0are the input features and xK=yare\\nthe observed outputs, such that fK−1◦···◦f0is aK-layer deep neural\\nnetwork. Therefore, the building blocks of this deep neural network are\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fdbb830-1709-48ae-95ce-4c64d6c44c1c', embedding=None, metadata={'page_label': '316', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='316 Linear Regression\\nthe generalized linear models deﬁned in (9.72). Neural networks (Bishop,\\n1995; Goodfellow et al., 2016) are signiﬁcantly more expressive and ﬂexi-\\nble than linear regression models. However, maximum likelihood parame-\\nter estimation is a non-convex optimization problem, and marginalization\\nof the parameters in a fully Bayesian setting is analytically intractable.\\nWe brieﬂy hinted at the fact that a distribution over parameters in-\\nduces a distribution over regression functions. Gaussian processes (Ras- Gaussian process\\nmussen and Williams, 2006) are regression models where the concept of\\na distribution over function is central. Instead of placing a distribution\\nover parameters, a Gaussian process places a distribution directly on the\\nspace of functions without the “detour” via the parameters. To do so, the\\nGaussian process exploits the kernel trick (Sch¨olkopf and Smola, 2002), kernel trick\\nwhich allows us to compute inner products between two function values\\nf(xi),f(xj)only by looking at the corresponding input xi,xj. A Gaus-\\nsian process is closely related to both Bayesian linear regression and sup-\\nport vector regression but can also be interpreted as a Bayesian neural\\nnetwork with a single hidden layer where the number of units tends to\\ninﬁnity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\\nprocesses can be found in MacKay (1998) and Rasmussen and Williams\\n(2006).\\nWe focused on Gaussian parameter priors in the discussions in this chap-\\nter, because they allow for closed-form inference in linear regression mod-\\nels. However, even in a regression setting with Gaussian likelihoods, we\\nmay choose a non-Gaussian prior. Consider a setting, where the inputs are\\nx∈RDand our training set is small and of size N≪D. This means that\\nthe regression problem is underdetermined. In this case, we can choose\\na parameter prior that enforces sparsity, i.e., a prior that tries to set as\\nmany parameters to 0as possible ( variable selection ). This prior provides variable selection\\na stronger regularizer than the Gaussian prior, which often leads to an in-\\ncreased prediction accuracy and interpretability of the model. The Laplace\\nprior is one example that is frequently used for this purpose. A linear re-\\ngression model with the Laplace prior on the parameters is equivalent to\\nlinear regression with L1 regularization ( LASSO ) (Tibshirani, 1996). The LASSO\\nLaplace distribution is sharply peaked at zero (its ﬁrst derivative is discon-\\ntinuous) and it concentrates its probability mass closer to zero than the\\nGaussian distribution, which encourages parameters to be 0. Therefore,\\nthe nonzero parameters are relevant for the regression problem, which is\\nthe reason why we also speak of “variable selection”.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='05706201-423b-4c82-b9fa-361c1fd97570', embedding=None, metadata={'page_label': '317', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10\\nDimensionality Reduction with Principal\\nComponent Analysis\\nWorking directly with high-dimensional data, such as images, comes with A640×480pixel\\ncolor image is a data\\npoint in a\\nmillion-dimensional\\nspace, where every\\npixel responds to\\nthree dimensions,\\none for each color\\nchannel (red, green,\\nblue).some difﬁculties: It is hard to analyze, interpretation is difﬁcult, visualiza-\\ntion is nearly impossible, and (from a practical point of view) storage of\\nthe data vectors can be expensive. However, high-dimensional data often\\nhas properties that we can exploit. For example, high-dimensional data is\\noften overcomplete, i.e., many dimensions are redundant and can be ex-\\nplained by a combination of other dimensions. Furthermore, dimensions\\nin high-dimensional data are often correlated so that the data possesses an\\nintrinsic lower-dimensional structure. Dimensionality reduction exploits\\nstructure and correlation and allows us to work with a more compact rep-\\nresentation of the data, ideally without losing information. We can think\\nof dimensionality reduction as a compression technique, similar to jpeg or\\nmp3, which are compression algorithms for images and music.\\nIn this chapter, we will discuss principal component analysis (PCA), an principal component\\nanalysis\\nPCAalgorithm for linear dimensionality reduction . PCA, proposed by Pearson\\ndimensionality\\nreduction(1901) and Hotelling (1933), has been around for more than 100years\\nand is still one of the most commonly used techniques for data compres-\\nsion and data visualization. It is also used for the identiﬁcation of simple\\npatterns, latent factors, and structures of high-dimensional data. In the\\nFigure 10.1\\nIllustration:\\ndimensionality\\nreduction. (a) The\\noriginal dataset\\ndoes not vary much\\nalong thex2\\ndirection. (b) The\\ndata from (a) can be\\nrepresented using\\nthex1-coordinate\\nalone with nearly no\\nloss.\\n−5.0−2.5 0.0 2.5 5.0\\nx1−4−2024x2\\n(a) Dataset with x1andx2coordinates.\\n−5.0−2.5 0.0 2.5 5.0\\nx1−4−2024x2\\n (b) Compressed dataset where only the x1coor-\\ndinate is relevant.\\n317\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='722a09f0-ef70-42d2-87cd-11f617b9232e', embedding=None, metadata={'page_label': '318', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='318 Dimensionality Reduction with Principal Component Analysis\\nsignal processing community, PCA is also known as the Karhunen-Lo `eve Karhunen-Lo `eve\\ntransform transform . In this chapter, we derive PCA from ﬁrst principles, drawing on\\nour understanding of basis and basis change (Sections 2.6.1 and 2.7.2),\\nprojections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-\\ntions (Section 6.5), and constrained optimization (Section 7.2).\\nDimensionality reduction generally exploits a property of high-dimen-\\nsional data (e.g., images) that it often lies on a low-dimensional subspace.\\nFigure 10.1 gives an illustrative example in two dimensions. Although\\nthe data in Figure 10.1(a) does not quite lie on a line, the data does not\\nvary much in the x2-direction, so that we can express it as if it were on\\na line – with nearly no loss; see Figure 10.1(b). To describe the data in\\nFigure 10.1(b), only the x1-coordinate is required, and the data lies in a\\none-dimensional subspace of R2.\\n10.1 Problem Setting\\nIn PCA, we are interested in ﬁnding projections ˜xnof data points xnthat\\nare as similar to the original data points as possible, but which have a sig-\\nniﬁcantly lower intrinsic dimensionality. Figure 10.1 gives an illustration\\nof what this could look like.\\nMore concretely, we consider an i.i.d. dataset X={x1,...,xN},xn∈\\nRD, with mean 0that possesses the data covariance matrix (6.42) data covariance\\nmatrix\\nS=1\\nNN∑\\nn=1xnx⊤\\nn. (10.1)\\nFurthermore, we assume there exists a low-dimensional compressed rep-\\nresentation (code)\\nzn=B⊤xn∈RM(10.2)\\nofxn, where we deﬁne the projection matrix\\nB:= [b1,...,bM]∈RD×M. (10.3)\\nWe assume that the columns of Bare orthonormal (Deﬁnition 3.7) so that\\nb⊤\\nibj= 0if and only if i̸=jandb⊤\\nibi= 1. We seek an M-dimensional The columns\\nb1,...,bMofB\\nform a basis of the\\nM-dimensional\\nsubspace in which\\nthe projected data\\n˜x=BB⊤x∈RD\\nlive.subspaceU⊆RD,dim(U) =M <D onto which we project the data. We\\ndenote the projected data by ˜xn∈U, and their coordinates (with respect\\nto the basis vectors b1,...,bMofU) byzn. Our aim is to ﬁnd projections\\n˜xn∈RD(or equivalently the codes znand the basis vectors b1,...,bM)\\nso that they are as similar to the original data xnand minimize the loss\\ndue to compression.\\nExample 10.1 (Coordinate Representation/Code)\\nConsider R2with the canonical basis e1= [1,0]⊤,e2= [0,1]⊤. From\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c856aa8-76ed-4922-8f88-c687e6325f11', embedding=None, metadata={'page_label': '319', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Problem Setting 319\\nFigure 10.2\\nGraphical\\nillustration of PCA.\\nIn PCA, we ﬁnd a\\ncompressed version\\nzof original data x.\\nThe compressed\\ndata can be\\nreconstructed into\\n˜x, which lives in the\\noriginal data space,\\nbut has an intrinsic\\nlower-dimensional\\nrepresentation than\\nx.x ˜xzOriginal\\nCompressedReconstructed\\nRDRD\\nRM\\nChapter 2, we know that x∈R2can be represented as a linear combina-\\ntion of these basis vectors, e.g.,\\n[5\\n3]\\n= 5e1+ 3e2. (10.4)\\nHowever, when we consider vectors of the form\\n˜x=[0\\nz]\\n∈R2, z∈R, (10.5)\\nthey can always be written as 0e1+ze2. To represent these vectors it is\\nsufﬁcient to remember/store the coordinate/code zof˜xwith respect to\\nthee2vector. The dimension of a\\nvector space\\ncorresponds to the\\nnumber of its basis\\nvectors (see\\nSection 2.6.1).More precisely, the set of ˜xvectors (with the standard vector addition\\nand scalar multiplication) forms a vector subspace U(see Section 2.4)\\nwith dim(U) = 1 becauseU= span[e2].\\nIn Section 10.2, we will ﬁnd low-dimensional representations that re-\\ntain as much information as possible and minimize the compression loss.\\nAn alternative derivation of PCA is given in Section 10.3, where we will\\nbe looking at minimizing the squared reconstruction error ∥xn−˜xn∥2be-\\ntween the original data xnand its projection ˜xn.\\nFigure 10.2 illustrates the setting we consider in PCA, where zrepre-\\nsents the lower-dimensional representation of the compressed data ˜xand\\nplays the role of a bottleneck, which controls how much information can\\nﬂow between xand˜x. In PCA, we consider a linear relationship between\\nthe original data xand its low-dimensional code zso thatz=B⊤xand\\n˜x=Bzfor a suitable matrix B. Based on the motivation of thinking\\nof PCA as a data compression technique, we can interpret the arrows in\\nFigure 10.2 as a pair of operations representing encoders and decoders.\\nThe linear mapping represented by Bcan be thought of as a decoder,\\nwhich maps the low-dimensional code z∈RMback into the original data\\nspaceRD. Similarly,B⊤can be thought of an encoder, which encodes the\\noriginal dataxas a low-dimensional (compressed) code z.\\nThroughout this chapter, we will use the MNIST digits dataset as a re-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b454486b-3cd3-4d93-b8f6-68f7258292ab', embedding=None, metadata={'page_label': '320', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='320 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.3\\nExamples of\\nhandwritten digits\\nfrom the MNIST\\ndataset. http:\\n//yann.lecun.\\ncom/exdb/mnist/ .\\noccurring example, which contains 60,000examples of handwritten digits\\n0through 9. Each digit is a grayscale image of size 28×28, i.e., it contains\\n784pixels so that we can interpret every image in this dataset as a vector\\nx∈R784. Examples of these digits are shown in Figure 10.3.\\n10.2 Maximum Variance Perspective\\nFigure 10.1 gave an example of how a two-dimensional dataset can be\\nrepresented using a single coordinate. In Figure 10.1(b), we chose to ig-\\nnore thex2-coordinate of the data because it did not add too much in-\\nformation so that the compressed data is similar to the original data in\\nFigure 10.1(a). We could have chosen to ignore the x1-coordinate, but\\nthen the compressed data had been very dissimilar from the original data,\\nand much information in the data would have been lost.\\nIf we interpret information content in the data as how “space ﬁlling”\\nthe dataset is, then we can describe the information contained in the data\\nby looking at the spread of the data. From Section 6.4.1, we know that the\\nvariance is an indicator of the spread of the data, and we can derive PCA as\\na dimensionality reduction algorithm that maximizes the variance in the\\nlow-dimensional representation of the data to retain as much information\\nas possible. Figure 10.4 illustrates this.\\nConsidering the setting discussed in Section 10.1, our aim is to ﬁnd\\na matrixB(see (10.3)) that retains as much information as possible\\nwhen compressing data by projecting it onto the subspace spanned by\\nthe columnsb1,...,bMofB. Retaining most information after data com-\\npression is equivalent to capturing the largest amount of variance in the\\nlow-dimensional code (Hotelling, 1933).\\nRemark. (Centered Data) For the data covariance matrix in (10.1), we\\nassumed centered data. We can make this assumption without loss of gen-\\nerality: Let us assume that µis the mean of the data. Using the properties\\nof the variance, which we discussed in Section 6.4.4, we obtain\\nVz[z] =Vx[B⊤(x−µ)] =Vx[B⊤x−B⊤µ] =Vx[B⊤x],(10.6)\\ni.e., the variance of the low-dimensional code does not depend on the\\nmean of the data. Therefore, we assume without loss of generality that the\\ndata has mean 0for the remainder of this section. With this assumption\\nthe mean of the low-dimensional code is also 0sinceEz[z] =Ex[B⊤x] =\\nB⊤Ex[x] =0. ♦\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81be9f38-b9c3-4fa8-bd29-8b552061bdf7', embedding=None, metadata={'page_label': '321', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.2 Maximum Variance Perspective 321\\nFigure 10.4 PCA\\nﬁnds a\\nlower-dimensional\\nsubspace (line) that\\nmaintains as much\\nvariance (spread of\\nthe data) as possible\\nwhen the data\\n(blue) is projected\\nonto this subspace\\n(orange).\\n10.2.1 Direction with Maximal Variance\\nWe maximize the variance of the low-dimensional code using a sequential\\napproach. We start by seeking a single vector b1∈RDthat maximizes the The vectorb1will\\nbe the ﬁrst column\\nof the matrixBand\\ntherefore the ﬁrst of\\nMorthonormal\\nbasis vectors that\\nspan the\\nlower-dimensional\\nsubspace.variance of the projected data, i.e., we aim to maximize the variance of\\nthe ﬁrst coordinate z1ofz∈RMso that\\nV1:=V[z1] =1\\nNN∑\\nn=1z2\\n1n (10.7)\\nis maximized, where we exploited the i.i.d. assumption of the data and\\ndeﬁnedz1nas the ﬁrst coordinate of the low-dimensional representation\\nzn∈RMofxn∈RD. Note that ﬁrst component of znis given by\\nz1n=b⊤\\n1xn, (10.8)\\ni.e., it is the coordinate of the orthogonal projection of xnonto the one-\\ndimensional subspace spanned by b1(Section 3.8). We substitute (10.8)\\ninto (10.7), which yields\\nV1=1\\nNN∑\\nn=1(b⊤\\n1xn)2=1\\nNN∑\\nn=1b⊤\\n1xnx⊤\\nnb1 (10.9a)\\n=b⊤\\n1(\\n1\\nNN∑\\nn=1xnx⊤\\nn)\\nb1=b⊤\\n1Sb1, (10.9b)\\nwhereSis the data covariance matrix deﬁned in (10.1). In (10.9a), we\\nhave used the fact that the dot product of two vectors is symmetric with\\nrespect to its arguments, that is, b⊤\\n1xn=x⊤\\nnb1.\\nNotice that arbitrarily increasing the magnitude of the vector b1in-\\ncreasesV1, that is, a vector b1that is two times longer can result in V1\\nthat is potentially four times larger. Therefore, we restrict all solutions to ∥b1∥2= 1\\n⇐⇒ ∥b1∥= 1. ∥b1∥2= 1, which results in a constrained optimization problem in which\\nwe seek the direction along which the data varies most.\\nWith the restriction of the solution space to unit vectors the vector b1\\nthat points in the direction of maximum variance can be found by the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef55c4f5-7662-4ece-b4bc-3fa2d13289de', embedding=None, metadata={'page_label': '322', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='322 Dimensionality Reduction with Principal Component Analysis\\nconstrained optimization problem\\nmax\\nb1b⊤\\n1Sb1\\nsubject to∥b1∥2= 1.(10.10)\\nFollowing Section 7.2, we obtain the Lagrangian\\nL(b1,λ) =b⊤\\n1Sb1+λ1(1−b⊤\\n1b1) (10.11)\\nto solve this constrained optimization problem. The partial derivatives of\\nLwith respect to b1andλ1are\\n∂L\\n∂b1= 2b⊤\\n1S−2λ1b⊤\\n1,∂L\\n∂λ1= 1−b⊤\\n1b1, (10.12)\\nrespectively. Setting these partial derivatives to 0gives us the relations\\nSb1=λ1b1, (10.13)\\nb⊤\\n1b1= 1. (10.14)\\nBy comparing this with the deﬁnition of an eigenvalue decomposition\\n(Section 4.4), we see that b1is an eigenvector of the data covariance\\nmatrixS, and the Lagrange multiplier λ1plays the role of the correspond-\\ning eigenvalue. This eigenvector property (10.13) allows us to rewrite our The quantity√λ1is\\nalso called the\\nloading of the unit\\nvectorb1and\\nrepresents the\\nstandard deviation\\nof the data\\naccounted for by the\\nprincipal subspace\\nspan[b1].variance objective (10.10) as\\nV1=b⊤\\n1Sb1=λ1b⊤\\n1b1=λ1, (10.15)\\ni.e., the variance of the data projected onto a one-dimensional subspace\\nequals the eigenvalue that is associated with the basis vector b1that spans\\nthis subspace. Therefore, to maximize the variance of the low-dimensional\\ncode, we choose the basis vector associated with the largest eigenvalue\\nof the data covariance matrix. This eigenvector is called the ﬁrst principalprincipal component\\ncomponent . We can determine the effect/contribution of the principal com-\\nponentb1in the original data space by mapping the coordinate z1nback\\ninto data space, which gives us the projected data point\\n˜xn=b1z1n=b1b⊤\\n1xn∈RD(10.16)\\nin the original data space.\\nRemark. Although ˜xnis aD-dimensional vector, it only requires a single\\ncoordinatez1nto represent it with respect to the basis vector b1∈RD.♦\\n10.2.2M-dimensional Subspace with Maximal Variance\\nAssume we have found the ﬁrst m−1principal components as the m−1\\neigenvectors of Sthat are associated with the largest m−1eigenvalues.\\nSinceSis symmetric, the spectral theorem (Theorem 4.15) states that we\\ncan use these eigenvectors to construct an orthonormal eigenbasis of an\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d04a2482-bcb8-414e-a87c-1c11e105d1d1', embedding=None, metadata={'page_label': '323', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.2 Maximum Variance Perspective 323\\n(m−1)-dimensional subspace of RD. Generally, the mth principal com-\\nponent can be found by subtracting the effect of the ﬁrst m−1principal\\ncomponentsb1,...,bm−1from the data, thereby trying to ﬁnd principal\\ncomponents that compress the remaining information. We then arrive at\\nthe new data matrix\\nˆX:=X−m−1∑\\ni=1bib⊤\\niX=X−Bm−1X, (10.17)\\nwhereX= [x1,...,xN]∈RD×Ncontains the data points as column The matrix ˆX:=\\n[ˆx1,..., ˆxN]∈\\nRD×Nin (10.17)\\ncontains the\\ninformation in the\\ndata that has not yet\\nbeen compressed.vectors andBm−1:=∑m−1\\ni=1bib⊤\\niis a projection matrix that projects onto\\nthe subspace spanned by b1,...,bm−1.\\nRemark (Notation) .Throughout this chapter, we do not follow the con-\\nvention of collecting data x1,...,xNas the rows of the data matrix, but\\nwe deﬁne them to be the columns of X. This means that our data ma-\\ntrixXis aD×Nmatrix instead of the conventional N×Dmatrix. The\\nreason for our choice is that the algebra operations work out smoothly\\nwithout the need to either transpose the matrix or to redeﬁne vectors as\\nrow vectors that are left-multiplied onto matrices. ♦\\nTo ﬁnd themth principal component, we maximize the variance\\nVm=V[zm] =1\\nNN∑\\nn=1z2\\nmn=1\\nNN∑\\nn=1(b⊤\\nmˆxn)2=b⊤\\nmˆSbm, (10.18)\\nsubject to∥bm∥2= 1, where we followed the same steps as in (10.9b)\\nand deﬁned ˆSas the data covariance matrix of the transformed dataset\\nˆX:={ˆx1,..., ˆxN}. As previously, when we looked at the ﬁrst principal\\ncomponent alone, we solve a constrained optimization problem and dis-\\ncover that the optimal solution bmis the eigenvector of ˆSthat is associated\\nwith the largest eigenvalue of ˆS.\\nIt turns out that bmis also an eigenvector of S. More generally, the sets\\nof eigenvectors of SandˆSare identical. Since both SandˆSare sym-\\nmetric, we can ﬁnd an ONB of eigenvectors (spectral theorem 4.15), i.e.,\\nthere existDdistinct eigenvectors for both SandˆS. Next, we show that\\nevery eigenvector of Sis an eigenvector of ˆS. Assume we have already\\nfound eigenvectors b1,...,bm−1ofˆS. Consider an eigenvector biofS,\\ni.e.,Sbi=λibi. In general,\\nˆSbi=1\\nNˆXˆX⊤bi=1\\nN(X−Bm−1X)(X−Bm−1X)⊤bi(10.19a)\\n= (S−SBm−1−Bm−1S+Bm−1SBm−1)bi. (10.19b)\\nWe distinguish between two cases. If i⩾m, i.e.,biis an eigenvector\\nthat is not among the ﬁrst m−1principal components, then biis orthogo-\\nnal to the ﬁrst m−1principal components and Bm−1bi=0. Ifi<m , i.e.,\\nbiis among the ﬁrst m−1principal components, then biis a basis vector\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b163bd14-6366-41db-a0be-10aa6712a66b', embedding=None, metadata={'page_label': '324', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='324 Dimensionality Reduction with Principal Component Analysis\\nof the principal subspace onto which Bm−1projects. Since b1,...,bm−1\\nare an ONB of this principal subspace, we obtain Bm−1bi=bi. The two\\ncases can be summarized as follows:\\nBm−1bi=biifi<m,Bm−1bi=0ifi⩾m. (10.20)\\nIn the case i⩾m, by using (10.20) in (10.19b), we obtain ˆSbi= (S−\\nBm−1S)bi=Sbi=λibi,i.e.,biis also an eigenvector of ˆSwith eigen-\\nvalueλi. Speciﬁcally,\\nˆSbm=Sbm=λmbm. (10.21)\\nEquation (10.21) reveals that bmis not only an eigenvector of Sbut also\\nofˆS. Speciﬁcally, λmis the largest eigenvalue of ˆSandλmis themth\\nlargest eigenvalue of S, and both have the associated eigenvector bm.\\nIn the casei<m , by using (10.20) in (10.19b), we obtain\\nˆSbi= (S−SBm−1−Bm−1S+Bm−1SBm−1)bi=0= 0bi(10.22)\\nThis means that b1,...,bm−1are also eigenvectors of ˆS, but they are as-\\nsociated with eigenvalue 0so thatb1,...,bm−1span the null space of ˆS.\\nOverall, every eigenvector of Sis also an eigenvector of ˆS. However,\\nif the eigenvectors of Sare part of the (m−1)dimensional principal\\nsubspace, then the associated eigenvalue of ˆSis0. This derivation\\nshows that there is\\nan intimate\\nconnection between\\ntheM-dimensional\\nsubspace with\\nmaximal variance\\nand the eigenvalue\\ndecomposition. We\\nwill revisit this\\nconnection in\\nSection 10.4.With the relation (10.21) and b⊤\\nmbm= 1, the variance of the data pro-\\njected onto the mth principal component is\\nVm=b⊤\\nmSbm(10.21)=λmb⊤\\nmbm=λm. (10.23)\\nThis means that the variance of the data, when projected onto an M-\\ndimensional subspace, equals the sum of the eigenvalues that are associ-\\nated with the corresponding eigenvectors of the data covariance matrix.\\nExample 10.2 (Eigenvalues of MNIST “8”)\\nFigure 10.5\\nProperties of the\\ntraining data of\\nMNIST “8”. (a)\\nEigenvalues sorted\\nin descending order;\\n(b) Variance\\ncaptured by the\\nprincipal\\ncomponents\\nassociated with the\\nlargest eigenvalues.\\n0 50 100 150 200\\nIndex01020304050Eigenvalue\\n(a) Eigenvalues (sorted in descending order) of\\nthe data covariance matrix of all digits “8” in\\nthe MNIST training set.\\n0 50 100 150 200\\nNumber of principal components100200300400500Captured variance(b) Variance captured by the principal compo-\\nnents.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='146727b7-8065-46ca-b55b-f008af04c667', embedding=None, metadata={'page_label': '325', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.3 Projection Perspective 325\\nFigure 10.6\\nIllustration of the\\nprojection\\napproach: Find a\\nsubspace (line) that\\nminimizes the\\nlength of the\\ndifference vector\\nbetween projected\\n(orange) and\\noriginal (blue) data.\\nTaking all digits “8” in the MNIST training data, we compute the eigen-\\nvalues of the data covariance matrix. Figure 10.5(a) shows the 200largest\\neigenvalues of the data covariance matrix. We see that only a few of\\nthem have a value that differs signiﬁcantly from 0. Therefore, most of\\nthe variance, when projecting data onto the subspace spanned by the cor-\\nresponding eigenvectors, is captured by only a few principal components,\\nas shown in Figure 10.5(b).\\nOverall, to ﬁnd an M-dimensional subspace of RDthat retains as much\\ninformation as possible, PCA tells us to choose the columns of the matrix\\nBin (10.3) as the Meigenvectors of the data covariance matrix Sthat\\nare associated with the Mlargest eigenvalues. The maximum amount of\\nvariance PCA can capture with the ﬁrst Mprincipal components is\\nVM=M∑\\nm=1λm, (10.24)\\nwhere theλmare theMlargest eigenvalues of the data covariance matrix\\nS. Consequently, the variance lost by data compression via PCA is\\nJM:=D∑\\nj=M+1λj=VD−VM. (10.25)\\nInstead of these absolute quantities, we can deﬁne the relative variance\\ncaptured asVM\\nVD, and the relative variance lost by compression as 1−VM\\nVD.\\n10.3 Projection Perspective\\nIn the following, we will derive PCA as an algorithm that directly mini-\\nmizes the average reconstruction error. This perspective allows us to in-\\nterpret PCA as implementing an optimal linear auto-encoder. We will draw\\nheavily from Chapters 2 and 3.\\nIn the previous section, we derived PCA by maximizing the variance\\nin the projected space to retain as much information as possible. In the\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7991b0f2-a931-42fd-b04b-0c6e87cc25ad', embedding=None, metadata={'page_label': '326', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='326 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.7\\nSimpliﬁed\\nprojection setting.\\n(a) A vectorx∈R2\\n(red cross) shall be\\nprojected onto a\\none-dimensional\\nsubspaceU⊆R2\\nspanned byb. (b)\\nshows the difference\\nvectors between x\\nand some\\ncandidates ˜x.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU\\n(a) Setting.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU (b) Differences x−˜xifor50different ˜xiare\\nshown by the red lines.\\nfollowing, we will look at the difference vectors between the original data\\nxnand their reconstruction ˜xnand minimize this distance so that xnand\\n˜xnare as close as possible. Figure 10.6 illustrates this setting.\\n10.3.1 Setting and Objective\\nAssume an (ordered) orthonormal basis (ONB) B= (b1,...,bD)ofRD,\\ni.e.,b⊤\\nibj= 1if and only if i=jand0otherwise.\\nFrom Section 2.5 we know that for a basis (b1,...,bD)ofRDanyx∈\\nRDcan be written as a linear combination of the basis vectors of RD, i.e.,\\nVectors ˜x∈Ucould\\nbe vectors on a\\nplane in R3. The\\ndimensionality of\\nthe plane is 2, but\\nthe vectors still have\\nthree coordinates\\nwith respect to the\\nstandard basis of\\nR3.x=D∑\\nd=1ζdbd=M∑\\nm=1ζmbm+D∑\\nj=M+1ζjbj (10.26)\\nfor suitable coordinates ζd∈R.\\nWe are interested in ﬁnding vectors ˜x∈RD, which live in lower-\\ndimensional subspace U⊆RD,dim(U) =M, so that\\n˜x=M∑\\nm=1zmbm∈U⊆RD(10.27)\\nis as similar to xas possible. Note that at this point we need to assume\\nthat the coordinates zmof˜xandζmofxare not identical.\\nIn the following, we use exactly this kind of representation of ˜xto ﬁnd\\noptimal coordinates zand basis vectors b1,...,bMsuch that ˜xis as sim-\\nilar to the original data point xas possible, i.e., we aim to minimize the\\n(Euclidean) distance ∥x−˜x∥. Figure 10.7 illustrates this setting.\\nWithout loss of generality, we assume that the dataset X={x1,...,xN},\\nxn∈RD, is centered at 0, i.e.,E[X] =0. Without the zero-mean assump-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8d9c1c0-d570-4271-9076-4710b1465fe5', embedding=None, metadata={'page_label': '327', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.3 Projection Perspective 327\\ntion, we would arrive at exactly the same solution, but the notation would\\nbe substantially more cluttered.\\nWe are interested in ﬁnding the best linear projection of Xonto a lower-\\ndimensional subspace UofRDwith dim(U) =Mand orthonormal basis\\nvectorsb1,...,bM. We will call this subspace Utheprincipal subspace .principal subspace\\nThe projections of the data points are denoted by\\n˜xn:=M∑\\nm=1zmnbm=Bzn∈RD, (10.28)\\nwherezn:= [z1n,...,zMn]⊤∈RMis the coordinate vector of ˜xnwith\\nrespect to the basis (b1,...,bM). More speciﬁcally, we are interested in\\nhaving the ˜xnas similar toxnas possible.\\nThe similarity measure we use in the following is the squared distance\\n(Euclidean norm) ∥x−˜x∥2betweenxand˜x. We therefore deﬁne our ob-\\njective as minimizing the average squared Euclidean distance ( reconstruction reconstruction error\\nerror ) (Pearson, 1901)\\nJM:=1\\nNN∑\\nn=1∥xn−˜xn∥2, (10.29)\\nwhere we make it explicit that the dimension of the subspace onto which\\nwe project the data is M. In order to ﬁnd this optimal linear projection,\\nwe need to ﬁnd the orthonormal basis of the principal subspace and the\\ncoordinateszn∈RMof the projections with respect to this basis.\\nTo ﬁnd the coordinates znand the ONB of the principal subspace, we\\nfollow a two-step approach. First, we optimize the coordinates znfor a\\ngiven ONB (b1,...,bM); second, we ﬁnd the optimal ONB.\\n10.3.2 Finding Optimal Coordinates\\nLet us start by ﬁnding the optimal coordinates z1n,...,zMnof the projec-\\ntions ˜xnforn= 1,...,N . Consider Figure 10.7(b), where the principal\\nsubspace is spanned by a single vector b. Geometrically speaking, ﬁnding\\nthe optimal coordinates zcorresponds to ﬁnding the representation of the\\nlinear projection ˜xwith respect to bthat minimizes the distance between\\n˜x−x. From Figure 10.7(b), it is clear that this will be the orthogonal\\nprojection, and in the following we will show exactly this.\\nWe assume an ONB (b1,...,bM)ofU⊆RD. To ﬁnd the optimal co-\\nordinateszmwith respect to this basis, we require the partial derivatives\\n∂JM\\n∂zin=∂JM\\n∂˜xn∂˜xn\\n∂zin, (10.30a)\\n∂JM\\n∂˜xn=−2\\nN(xn−˜xn)⊤∈R1×D, (10.30b)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16a560e1-1c4b-47e4-8aa5-78ccf460196a', embedding=None, metadata={'page_label': '328', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='328 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.8\\nOptimal projection\\nof a vectorx∈R2\\nonto a\\none-dimensional\\nsubspace\\n(continuation from\\nFigure 10.7).\\n(a) Distances\\n∥x−˜x∥for some\\n˜x∈U.\\n(b) Orthogonal\\nprojection and\\noptimal coordinates.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx11.251.501.752.002.252.502.753.003.25∥x−˜x∥\\n(a) Distances∥x−˜x∥for some ˜x=z1b∈\\nU= span[b]; see panel (b) for the setting.\\n−1.0−0.5 0.0 0.5 1.0 1.5 2.0\\nx1−0.50.00.51.01.52.02.5x2\\nbU\\n˜x(b) The vector ˜xthat minimizes the distance\\nin panel (a) is its orthogonal projection onto\\nU. The coordinate of the projection ˜xwith\\nrespect to the basis vector bthat spansU\\nis the factor we need to scale bin order to\\n“reach” ˜x.\\n∂˜xn\\n∂zin(10.28)=∂\\n∂zin(M∑\\nm=1zmnbm)\\n=bi (10.30c)\\nfori= 1,...,M , such that we obtain\\n∂JM\\n∂zin(10.30b)\\n(10.30c)=−2\\nN(xn−˜xn)⊤bi(10.28)=−2\\nN(\\nxn−M∑\\nm=1zmnbm)⊤\\nbi\\n(10.31a)\\nONB=−2\\nN(x⊤\\nnbi−zinb⊤\\nibi) =−2\\nN(x⊤\\nnbi−zin). (10.31b)\\nsinceb⊤\\nibi= 1. Setting this partial derivative to 0yields immediately the The coordinates of\\nthe optimal\\nprojection ofxn\\nwith respect to the\\nbasis vectors\\nb1,...,bMare the\\ncoordinates of the\\northogonal\\nprojection ofxn\\nonto the principal\\nsubspace.optimal coordinates\\nzin=x⊤\\nnbi=b⊤\\nixn (10.32)\\nfori= 1,...,M andn= 1,...,N . This means that the optimal co-\\nordinateszinof the projection ˜xnare the coordinates of the orthogonal\\nprojection (see Section 3.8) of the original data point xnonto the one-\\ndimensional subspace that is spanned by bi. Consequently:\\nThe optimal linear projection ˜xnofxnis an orthogonal projection.\\nThe coordinates of ˜xnwith respect to the basis (b1,...,bM)are the\\ncoordinates of the orthogonal projection of xnonto the principal sub-\\nspace.\\nAn orthogonal projection is the best linear mapping given the objec-\\ntive (10.29).\\nThe coordinates ζmofxin (10.26) and the coordinates zmof˜xin (10.27)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5938cf3c-1b0d-4033-9a10-165c739bdb49', embedding=None, metadata={'page_label': '329', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.3 Projection Perspective 329\\nmust be identical for m= 1,...,M sinceU⊥= span[bM+1,...,bD]is\\nthe orthogonal complement (see Section 3.6) of U= span[b1,...,bM].\\nRemark (Orthogonal Projections with Orthonormal Basis Vectors) .Let us\\nbrieﬂy recap orthogonal projections from Section 3.8. If (b1,...,bD)is an\\northonormal basis of RDthen b⊤\\njxis the\\ncoordinate of the\\northogonal\\nprojection ofxonto\\nthe subspace\\nspanned bybj.˜x=bj(b⊤\\njbj)−1b⊤\\njx=bjb⊤\\njx∈RD(10.33)\\nis the orthogonal projection of xonto the subspace spanned by the jth ba-\\nsis vector, and zj=b⊤\\njxis the coordinate of this projection with respect to\\nthe basis vector bjthat spans that subspace since zjbj=˜x. Figure 10.8(b)\\nillustrates this setting.\\nMore generally, if we aim to project onto an M-dimensional subspace\\nofRD, we obtain the orthogonal projection of xonto theM-dimensional\\nsubspace with orthonormal basis vectors b1,...,bMas\\n˜x=B(B⊤B\\ued19\\ued18\\ued17\\ued1a\\n=I)−1B⊤x=BB⊤x, (10.34)\\nwhere we deﬁned B:= [b1,...,bM]∈RD×M. The coordinates of this\\nprojection with respect to the ordered basis (b1,...,bM)arez:=B⊤x\\nas discussed in Section 3.8.\\nWe can think of the coordinates as a representation of the projected\\nvector in a new coordinate system deﬁned by (b1,...,bM). Note that al-\\nthough ˜x∈RD, we only need Mcoordinates z1,...,zMto represent\\nthis vector; the other D−Mcoordinates with respect to the basis vectors\\n(bM+1,...,bD)are always 0. ♦\\nSo far we have shown that for a given ONB we can ﬁnd the optimal\\ncoordinates of ˜xby an orthogonal projection onto the principal subspace.\\nIn the following, we will determine what the best basis is.\\n10.3.3 Finding the Basis of the Principal Subspace\\nTo determine the basis vectors b1,...,bMof the principal subspace, we\\nrephrase the loss function (10.29) using the results we have so far. This\\nwill make it easier to ﬁnd the basis vectors. To reformulate the loss func-\\ntion, we exploit our results from before and obtain\\n˜xn=M∑\\nm=1zmnbm(10.32)=M∑\\nm=1(x⊤\\nnbm)bm. (10.35)\\nWe now exploit the symmetry of the dot product, which yields\\n˜xn=(M∑\\nm=1bmb⊤\\nm)\\nxn. (10.36)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ffb01400-f7ef-4328-92d0-0a314fe64b73', embedding=None, metadata={'page_label': '330', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='330 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.9\\nOrthogonal\\nprojection and\\ndisplacement\\nvectors. When\\nprojecting data\\npointsxn(blue)\\nonto subspace U1,\\nwe obtain ˜xn\\n(orange). The\\ndisplacement vector\\n˜xn−xnlies\\ncompletely in the\\northogonal\\ncomplement U2of\\nU1.\\n−5 0 5\\nx1−6−4−20246x2\\nUU⊥\\nSince we can generally write the original data point xnas a linear combi-\\nnation of all basis vectors, it holds that\\nxn=D∑\\nd=1zdnbd(10.32)=D∑\\nd=1(x⊤\\nnbd)bd=(D∑\\nd=1bdb⊤\\nd)\\nxn (10.37a)\\n=(M∑\\nm=1bmb⊤\\nm)\\nxn+(D∑\\nj=M+1bjb⊤\\nj)\\nxn, (10.37b)\\nwhere we split the sum with Dterms into a sum over Mand a sum\\noverD−Mterms. With this result, we ﬁnd that the displacement vector\\nxn−˜xn, i.e., the difference vector between the original data point and its\\nprojection, is\\nxn−˜xn=(D∑\\nj=M+1bjb⊤\\nj)\\nxn (10.38a)\\n=D∑\\nj=M+1(x⊤\\nnbj)bj. (10.38b)\\nThis means the difference is exactly the projection of the data point onto\\nthe orthogonal complement of the principal subspace: We identify the ma-\\ntrix∑D\\nj=M+1bjb⊤\\njin (10.38a) as the projection matrix that performs this\\nprojection. Hence the displacement vector xn−˜xnlies in the subspace\\nthat is orthogonal to the principal subspace as illustrated in Figure 10.9.\\nRemark (Low-Rank Approximation) .In (10.38a), we saw that the projec-\\ntion matrix, which projects xonto ˜x, is given by\\nM∑\\nm=1bmb⊤\\nm=BB⊤. (10.39)\\nBy construction as a sum of rank-one matrices bmb⊤\\nmwe see thatBB⊤is\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2984e65f-0302-46ee-bca2-1294ff9fcf09', embedding=None, metadata={'page_label': '331', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.3 Projection Perspective 331\\nsymmetric and has rank M. Therefore, the average squared reconstruction\\nerror can also be written as\\n1\\nNN∑\\nn=1∥xn−˜xn∥2=1\\nNN∑\\nn=1\\ued79\\ued79\\ued79xn−BB⊤xn\\ued79\\ued79\\ued792\\n(10.40a)\\n=1\\nNN∑\\nn=1\\ued79\\ued79\\ued79(I−BB⊤)xn\\ued79\\ued79\\ued792\\n. (10.40b)\\nFinding orthonormal basis vectors b1,...,bM, which minimize the differ- PCA ﬁnds the best\\nrank-M\\napproximation of\\nthe identity matrix.ence between the original data xnand their projections ˜xn, is equivalent\\nto ﬁnding the best rank- Mapproximation BB⊤of the identity matrix I\\n(see Section 4.6). ♦\\nNow we have all the tools to reformulate the loss function (10.29).\\nJM=1\\nNN∑\\nn=1∥xn−˜xn∥2(10.38b)=1\\nNN∑\\nn=1\\ued79\\ued79\\ued79\\ued79\\ued79D∑\\nj=M+1(b⊤\\njxn)bj\\ued79\\ued79\\ued79\\ued79\\ued792\\n.(10.41)\\nWe now explicitly compute the squared norm and exploit the fact that the\\nbjform an ONB, which yields\\nJM=1\\nNN∑\\nn=1D∑\\nj=M+1(b⊤\\njxn)2=1\\nNN∑\\nn=1D∑\\nj=M+1b⊤\\njxnb⊤\\njxn (10.42a)\\n=1\\nNN∑\\nn=1D∑\\nj=M+1b⊤\\njxnx⊤\\nnbj, (10.42b)\\nwhere we exploited the symmetry of the dot product in the last step to\\nwriteb⊤\\njxn=x⊤\\nnbj. We now swap the sums and obtain\\nJM=D∑\\nj=M+1b⊤\\nj(\\n1\\nNN∑\\nn=1xnx⊤\\nn)\\n\\ued19\\ued18\\ued17\\ued1a\\n=:Sbj=D∑\\nj=M+1b⊤\\njSbj (10.43a)\\n=D∑\\nj=M+1tr(b⊤\\njSbj) =D∑\\nj=M+1tr(Sbjb⊤\\nj) =tr((D∑\\nj=M+1bjb⊤\\nj)\\n\\ued19\\ued18\\ued17\\ued1a\\nprojection matrixS)\\n,\\n(10.43b)\\nwhere we exploited the property that the trace operator tr (·)(see (4.18))\\nis linear and invariant to cyclic permutations of its arguments. Since we\\nassumed that our dataset is centered, i.e., E[X] =0, we identifySas the\\ndata covariance matrix. Since the projection matrix in (10.43b) is con-\\nstructed as a sum of rank-one matrices bjb⊤\\njit itself is of rank D−M.\\nEquation (10.43a) implies that we can formulate the average squared\\nreconstruction error equivalently as the covariance matrix of the data,\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5de00141-397e-4542-aa27-4d1e155e218b', embedding=None, metadata={'page_label': '332', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='332 Dimensionality Reduction with Principal Component Analysis\\nprojected onto the orthogonal complement of the principal subspace. Min-\\nimizing the average squared reconstruction error is therefore equivalent to Minimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nminimizing the\\nprojection of the\\ndata covariance\\nmatrix onto the\\northogonal\\ncomplement of the\\nprincipal subspace.minimizing the variance of the data when projected onto the subspace we\\nignore, i.e., the orthogonal complement of the principal subspace. Equiva-\\nlently, we maximize the variance of the projection that we retain in the\\nprincipal subspace, which links the projection loss immediately to the\\nmaximum-variance formulation of PCA discussed in Section 10.2. But this\\nthen also means that we will obtain the same solution that we obtained\\nMinimizing the\\naverage squared\\nreconstruction error\\nis equivalent to\\nmaximizing the\\nvariance of the\\nprojected data.for the maximum-variance perspective. Therefore, we omit a derivation\\nthat is identical to the one presented in Section 10.2 and summarize the\\nresults from earlier in the light of the projection perspective.\\nThe average squared reconstruction error, when projecting onto the M-\\ndimensional principal subspace, is\\nJM=D∑\\nj=M+1λj, (10.44)\\nwhereλjare the eigenvalues of the data covariance matrix. Therefore,\\nto minimize (10.44) we need to select the smallest D−Meigenvalues,\\nwhich then implies that their corresponding eigenvectors are the basis of\\nthe orthogonal complement of the principal subspace. Consequently, this\\nmeans that the basis of the principal subspace comprises the eigenvectors\\nb1,...,bMthat are associated with the largest Meigenvalues of the data\\ncovariance matrix.\\nExample 10.3 (MNIST Digits Embedding)\\nFigure 10.10\\nEmbedding of\\nMNIST digits 0\\n(blue) and 1\\n(orange) in a\\ntwo-dimensional\\nprincipal subspace\\nusing PCA. Four\\nembeddings of the\\ndigits “0” and “1” in\\nthe principal\\nsubspace are\\nhighlighted in red\\nwith their\\ncorresponding\\noriginal digit.\\nFigure 10.10 visualizes the training data of the MMIST digits “0” and “1”\\nembedded in the vector subspace spanned by the ﬁrst two principal com-\\nponents. We observe a relatively clear separation between “0”s (blue dots)\\nand “1”s (orange dots), and we see the variation within each individual\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bcf19af-659a-4651-9208-cc8a0bc60836', embedding=None, metadata={'page_label': '333', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.4 Eigenvector Computation and Low-Rank Approximations 333\\ncluster. Four embeddings of the digits “0” and “1” in the principal subspace\\nare highlighted in red with their corresponding original digit. The ﬁgure\\nreveals that the variation within the set of “0” is signiﬁcantly greater than\\nthe variation within the set of “1”.\\n10.4 Eigenvector Computation and Low-Rank Approximations\\nIn the previous sections, we obtained the basis of the principal subspace\\nas the eigenvectors that are associated with the largest eigenvalues of the\\ndata covariance matrix\\nS=1\\nNN∑\\nn=1xnx⊤\\nn=1\\nNXX⊤, (10.45)\\nX= [x1,...,xN]∈RD×N. (10.46)\\nNote thatXis aD×Nmatrix, i.e., it is the transpose of the “typical”\\ndata matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\\nthe corresponding eigenvectors) of S, we can follow two approaches: Use\\neigendecomposition\\nor SVD to compute\\neigenvectors.We perform an eigendecomposition (see Section 4.2) and compute the\\neigenvalues and eigenvectors of Sdirectly.\\nWe use a singular value decomposition (see Section 4.5). Since Sis\\nsymmetric and factorizes into XX⊤(ignoring the factor1\\nN), the eigen-\\nvalues ofSare the squared singular values of X.\\nMore speciﬁcally, the SVD of Xis given by\\nX\\ued19\\ued18\\ued17\\ued1a\\nD×N=U\\ued19\\ued18\\ued17\\ued1a\\nD×DΣ\\ued19\\ued18\\ued17\\ued1a\\nD×NV⊤\\n\\ued19\\ued18\\ued17\\ued1a\\nN×N, (10.47)\\nwhereU∈RD×DandV⊤∈RN×Nare orthogonal matrices and Σ∈\\nRD×Nis a matrix whose only nonzero entries are the singular values σii⩾\\n0. It then follows that\\nS=1\\nNXX⊤=1\\nNUΣV⊤V\\ued19\\ued18\\ued17\\ued1a\\n=INΣ⊤U⊤=1\\nNUΣΣ⊤U⊤. (10.48)\\nWith the results from Section 4.5, we get that the columns of Uare the The columns of U\\nare the eigenvectors\\nofS.eigenvectors of XX⊤(and therefore S). Furthermore, the eigenvalues\\nλdofSare related to the singular values of Xvia\\nλd=σ2\\nd\\nN. (10.49)\\nThis relationship between the eigenvalues of Sand the singular values\\nofXprovides the connection between the maximum variance view (Sec-\\ntion 10.2) and the singular value decomposition.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be78acdc-345c-4592-9337-9cfb667d9f18', embedding=None, metadata={'page_label': '334', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='334 Dimensionality Reduction with Principal Component Analysis\\n10.4.1 PCA Using Low-Rank Matrix Approximations\\nTo maximize the variance of the projected data (or minimize the average\\nsquared reconstruction error), PCA chooses the columns of Uin (10.48)\\nto be the eigenvectors that are associated with the Mlargest eigenvalues\\nof the data covariance matrix Sso that we identify Uas the projection ma-\\ntrixBin (10.3), which projects the original data onto a lower-dimensional\\nsubspace of dimension M. The Eckart-Young theorem (Theorem 4.25 in Eckart-Young\\ntheorem Section 4.6) offers a direct way to estimate the low-dimensional represen-\\ntation. Consider the best rank- Mapproximation\\n˜XM:= argminrk(A)⩽M∥X−A∥2∈RD×N(10.50)\\nofX, where∥·∥2is the spectral norm deﬁned in (4.93). The Eckart-Young\\ntheorem states that ˜XMis given by truncating the SVD at the top- M\\nsingular value. In other words, we obtain\\n˜XM=UM\\ued19\\ued18\\ued17\\ued1a\\nD×MΣM\\ued19\\ued18\\ued17\\ued1a\\nM×MV⊤\\nM\\ued19\\ued18\\ued17\\ued1a\\nM×N∈RD×N(10.51)\\nwith orthogonal matrices UM:= [u1,...,uM]∈RD×MandVM:=\\n[v1,...,vM]∈RN×Mand a diagonal matrix ΣM∈RM×Mwhose diago-\\nnal entries are the Mlargest singular values of X.\\n10.4.2 Practical Aspects\\nFinding eigenvalues and eigenvectors is also important in other funda-\\nmental machine learning methods that require matrix decompositions. In\\ntheory, as we discussed in Section 4.2, we can solve for the eigenvalues as\\nroots of the characteristic polynomial. However, for matrices larger than\\n4×4this is not possible because we would need to ﬁnd the roots of a poly-\\nnomial of degree 5or higher. However, the Abel-Rufﬁni theorem (Rufﬁni, Abel-Rufﬁni\\ntheorem 1799; Abel, 1826) states that there exists no algebraic solution to this\\nproblem for polynomials of degree 5or more. Therefore, in practice, wenp.linalg.eigh\\nor\\nnp.linalg.svdsolve for eigenvalues or singular values using iterative methods, which are\\nimplemented in all modern packages for linear algebra.\\nIn many applications (such as PCA presented in this chapter), we only\\nrequire a few eigenvectors. It would be wasteful to compute the full de-\\ncomposition, and then discard all eigenvectors with eigenvalues that are\\nbeyond the ﬁrst few. It turns out that if we are interested in only the ﬁrst\\nfew eigenvectors (with the largest eigenvalues), then iterative processes,\\nwhich directly optimize these eigenvectors, are computationally more efﬁ-\\ncient than a full eigendecomposition (or SVD). In the extreme case of only\\nneeding the ﬁrst eigenvector, a simple method called the power iteration power iteration\\nis very efﬁcient. Power iteration chooses a random vector x0that is not in\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='40707d83-7e78-4aac-b0f2-59d016ddf849', embedding=None, metadata={'page_label': '335', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 PCA in High Dimensions 335\\nthe null space of Sand follows the iteration\\nxk+1=Sxk\\n∥Sxk∥, k = 0,1,... . (10.52)\\nThis means the vector xkis multiplied by Sin every iteration and then IfSis invertible, it\\nis sufﬁcient to\\nensure thatx0̸=0.normalized, i.e., we always have ∥xk∥= 1. This sequence of vectors con-\\nverges to the eigenvector associated with the largest eigenvalue of S. The\\noriginal Google PageRank algorithm (Page et al., 1999) uses such an al-\\ngorithm for ranking web pages based on their hyperlinks.\\n10.5 PCA in High Dimensions\\nIn order to do PCA, we need to compute the data covariance matrix. In D\\ndimensions, the data covariance matrix is a D×Dmatrix. Computing the\\neigenvalues and eigenvectors of this matrix is computationally expensive\\nas it scales cubically in D. Therefore, PCA, as we discussed earlier, will be\\ninfeasible in very high dimensions. For example, if our xnare images with\\n10,000pixels (e.g., 100×100pixel images), we would need to compute\\nthe eigendecomposition of a 10,000×10,000covariance matrix. In the\\nfollowing, we provide a solution to this problem for the case that we have\\nsubstantially fewer data points than dimensions, i.e., N≪D.\\nAssume we have a centered dataset x1,...,xN,xn∈RD. Then the\\ndata covariance matrix is given as\\nS=1\\nNXX⊤∈RD×D, (10.53)\\nwhereX= [x1,...,xN]is aD×Nmatrix whose columns are the data\\npoints.\\nWe now assume that N≪D, i.e., the number of data points is smaller\\nthan the dimensionality of the data. If there are no duplicate data points,\\nthe rank of the covariance matrix SisN, so it hasD−N+1many eigen-\\nvalues that are 0. Intuitively, this means that there are some redundancies.\\nIn the following, we will exploit this and turn the D×Dcovariance matrix\\ninto anN×Ncovariance matrix whose eigenvalues are all positive.\\nIn PCA, we ended up with the eigenvector equation\\nSbm=λmbm, m = 1,...,M, (10.54)\\nwherebmis a basis vector of the principal subspace. Let us rewrite this\\nequation a bit: With Sdeﬁned in (10.53), we obtain\\nSbm=1\\nNXX⊤bm=λmbm. (10.55)\\nWe now multiply X⊤∈RN×Dfrom the left-hand side, which yields\\n1\\nNX⊤X\\ued19\\ued18\\ued17\\ued1a\\nN×NX⊤bm\\ued19\\ued18\\ued17\\ued1a\\n=:cm=λmX⊤bm⇐⇒1\\nNX⊤Xcm=λmcm,(10.56)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='96104e31-0e5d-4c83-9298-a09c5690bd96', embedding=None, metadata={'page_label': '336', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='336 Dimensionality Reduction with Principal Component Analysis\\nand we get a new eigenvector/eigenvalue equation: λmremains eigen-\\nvalue, which conﬁrms our results from Section 4.5.3 that the nonzero\\neigenvalues of XX⊤equal the nonzero eigenvalues of X⊤X. We obtain\\nthe eigenvector of the matrix1\\nNX⊤X∈RN×Nassociated with λmas\\ncm:=X⊤bm. Assuming we have no duplicate data points, this matrix\\nhas rankNand is invertible. This also implies that1\\nNX⊤Xhas the same\\n(nonzero) eigenvalues as the data covariance matrix S. But this is now an\\nN×Nmatrix, so that we can compute the eigenvalues and eigenvectors\\nmuch more efﬁciently than for the original D×Ddata covariance matrix.\\nNow that we have the eigenvectors of1\\nNX⊤X, we are going to re-\\ncover the original eigenvectors, which we still need for PCA. Currently,\\nwe know the eigenvectors of1\\nNX⊤X. If we left-multiply our eigenvalue/\\neigenvector equation with X, we get\\n1\\nNXX⊤\\n\\ued19\\ued18\\ued17\\ued1a\\nSXcm=λmXcm (10.57)\\nand we recover the data covariance matrix again. This now also means\\nthat we recover Xcmas an eigenvector of S.\\nRemark. If we want to apply the PCA algorithm that we discussed in Sec-\\ntion 10.6, we need to normalize the eigenvectors XcmofSso that they\\nhave norm 1. ♦\\n10.6 Key Steps of PCA in Practice\\nIn the following, we will go through the individual steps of PCA using a\\nrunning example, which is summarized in Figure 10.11. We are given a\\ntwo-dimensional dataset (Figure 10.11(a)), and we want to use PCA to\\nproject it onto a one-dimensional subspace.\\n1.Mean subtraction We start by centering the data by computing the\\nmeanµof the dataset and subtracting it from every single data point.\\nThis ensures that the dataset has mean 0(Figure 10.11(b)). Mean sub-\\ntraction is not strictly necessary but reduces the risk of numerical prob-\\nlems.\\n2.Standardization Divide the data points by the standard deviation σd\\nof the dataset for every dimension d= 1,...,D . Now the data is unit\\nfree, and it has variance 1along each axis, which is indicated by the\\ntwo arrows in Figure 10.11(c). This step completes the standardization standardization\\nof the data.\\n3.Eigendecomposition of the covariance matrix Compute the data\\ncovariance matrix and its eigenvalues and corresponding eigenvectors.\\nSince the covariance matrix is symmetric, the spectral theorem (The-\\norem 4.15) states that we can ﬁnd an ONB of eigenvectors. In Fig-\\nure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07b45e3c-c0aa-446d-842d-b291ad5dfa1e', embedding=None, metadata={'page_label': '337', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.6 Key Steps of PCA in Practice 337\\nFigure 10.11 Steps\\nof PCA. (a) Original\\ndataset;\\n(b) centering;\\n(c) divide by\\nstandard deviation;\\n(d) eigendecomposi-\\ntion; (e) projection;\\n(f) mapping back to\\noriginal data space.\\n0 5\\nx1−2.50.02.55.0x2\\n(a) Original dataset.\\n0 5\\nx1−2.50.02.55.0x2\\n (b) Step 1: Centering by sub-\\ntracting the mean from each\\ndata point.\\n0 5\\nx1−2.50.02.55.0x2\\n(c) Step 2: Dividing by the\\nstandard deviation to make\\nthe data unit free. Data has\\nvariance 1along each axis.\\n0 5\\nx1−2.50.02.55.0x2\\n(d) Step 3: Compute eigenval-\\nues and eigenvectors (arrows)\\nof the data covariance matrix\\n(ellipse).\\n0 5\\nx1−2.50.02.55.0x2\\n(e) Step 4: Project data onto\\nthe principal subspace.\\n0 5\\nx1−2.50.02.55.0x2\\n(f) Undo the standardization\\nand move projected data back\\ninto the original data space\\nfrom (a).\\nresponding eigenvalue. The longer vector spans the principal subspace,\\nwhich we denote by U. The data covariance matrix is represented by\\nthe ellipse.\\n4.Projection We can project any data point x∗∈RDonto the principal\\nsubspace: To get this right, we need to standardize x∗using the mean\\nµdand standard deviation σdof the training data in the dth dimension,\\nrespectively, so that\\nx(d)\\n∗←x(d)\\n∗−µd\\nσd, d = 1,...,D, (10.58)\\nwherex(d)\\n∗is thedth component of x∗. We obtain the projection as\\n˜x∗=BB⊤x∗ (10.59)\\nwith coordinates\\nz∗=B⊤x∗ (10.60)\\nwith respect to the basis of the principal subspace. Here, Bis the ma-\\ntrix that contains the eigenvectors that are associated with the largest\\neigenvalues of the data covariance matrix as columns. PCA returns the\\ncoordinates (10.60), not the projections x∗.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bb4ceb4-a4c6-41b1-bfa2-07c0ef064525', embedding=None, metadata={'page_label': '338', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='338 Dimensionality Reduction with Principal Component Analysis\\nHaving standardized our dataset, (10.59) only yields the projections in\\nthe context of the standardized dataset. To obtain our projection in the\\noriginal data space (i.e., before standardization), we need to undo the\\nstandardization (10.58) and multiply by the standard deviation before\\nadding the mean so that we obtain\\n˜x(d)\\n∗←˜x(d)\\n∗σd+µd, d = 1,...,D. (10.61)\\nFigure 10.11(f) illustrates the projection in the original data space.\\nExample 10.4 (MNIST Digits: Reconstruction)\\nIn the following, we will apply PCA to the MNIST digits dataset, which\\ncontains 60,000examples of handwritten digits 0through 9. Each digit is\\nan image of size 28×28, i.e., it contains 784pixels so that we can interpret\\nevery image in this dataset as a vector x∈R784. Examples of these digits\\nare shown in Figure 10.3.\\nFigure 10.12 Effect\\nof increasing the\\nnumber of principal\\ncomponents on\\nreconstruction.\\nOriginal\\nPCs: 1\\nPCs: 10\\nPCs: 100\\nPCs: 500\\nFor illustration purposes, we apply PCA to a subset of the MNIST digits,\\nand we focus on the digit “8”. We used 5,389 training images of the digit\\n“8” and determined the principal subspace as detailed in this chapter. We\\nthen used the learned projection matrix to reconstruct a set of test im-\\nages, which is illustrated in Figure 10.12. The ﬁrst row of Figure 10.12\\nshows a set of four original digits from the test set. The following rows\\nshow reconstructions of exactly these digits when using a principal sub-\\nspace of dimensions 1,10,100, and 500, respectively. We see that even\\nwith a single-dimensional principal subspace we get a halfway decent re-\\nconstruction of the original digits, which, however, is blurry and generic.\\nWith an increasing number of principal components (PCs), the reconstruc-\\ntions become sharper and more details are accounted for. With 500prin-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b08e0dd5-5df6-4d7f-9507-9adab72a268b', embedding=None, metadata={'page_label': '339', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.7 Latent Variable Perspective 339\\ncipal components, we effectively obtain a near-perfect reconstruction. If\\nwe were to choose 784PCs, we would recover the exact digit without any\\ncompression loss.\\nFigure 10.13 shows the average squared reconstruction error, which is\\n1\\nNN∑\\nn=1∥xn−˜xn∥2=D∑\\ni=M+1λi, (10.62)\\nas a function of the number Mof principal components. We can see that\\nthe importance of the principal components drops off rapidly, and only\\nmarginal gains can be achieved by adding more PCs. This matches exactly\\nour observation in Figure 10.5, where we discovered that the most of the\\nvariance of the projected data is captured by only a few principal compo-\\nnents. With about 550PCs, we can essentially fully reconstruct the training\\ndata that contains the digit “8” (some pixels around the boundaries show\\nno variation across the dataset as they are always black).\\nFigure 10.13\\nAverage squared\\nreconstruction error\\nas a function of the\\nnumber of principal\\ncomponents. The\\naverage squared\\nreconstruction error\\nis the sum of the\\neigenvalues in the\\northogonal\\ncomplement of the\\nprincipal subspace.\\n0 200 400 600 800\\nNumber of PCs0100200300400500Average squared reconstruction error\\n10.7 Latent Variable Perspective\\nIn the previous sections, we derived PCA without any notion of a prob-\\nabilistic model using the maximum-variance and the projection perspec-\\ntives. On the one hand, this approach may be appealing as it allows us to\\nsidestep all the mathematical difﬁculties that come with probability the-\\nory, but on the other hand, a probabilistic model would offer us more ﬂex-\\nibility and useful insights. More speciﬁcally, a probabilistic model would\\nCome with a likelihood function, and we can explicitly deal with noisy\\nobservations (which we did not even discuss earlier)\\nAllow us to do Bayesian model comparison via the marginal likelihood\\nas discussed in Section 8.6\\nView PCA as a generative model, which allows us to simulate new data\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d325c07-e3ac-41cb-b9b3-bd7015ee8e6c', embedding=None, metadata={'page_label': '340', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='340 Dimensionality Reduction with Principal Component Analysis\\nAllow us to make straightforward connections to related algorithms\\nDeal with data dimensions that are missing at random by applying\\nBayes’ theorem\\nGive us a notion of the novelty of a new data point\\nGive us a principled way to extend the model, e.g., to a mixture of PCA\\nmodels\\nHave the PCA we derived in earlier sections as a special case\\nAllow for a fully Bayesian treatment by marginalizing out the model\\nparameters\\nBy introducing a continuous-valued latent variable z∈RMit is possible\\nto phrase PCA as a probabilistic latent-variable model. Tipping and Bishop\\n(1999) proposed this latent-variable model as probabilistic PCA (PPCA ). probabilistic PCA\\nPPCA PPCA addresses most of the aforementioned issues, and the PCA solution\\nthat we obtained by maximizing the variance in the projected space or\\nby minimizing the reconstruction error is obtained as the special case of\\nmaximum likelihood estimation in a noise-free setting.\\n10.7.1 Generative Process and Probabilistic Model\\nIn PPCA, we explicitly write down the probabilistic model for linear di-\\nmensionality reduction. For this we assume a continuous latent variable\\nz∈RMwith a standard-normal prior p(z) =N(0,I)\\nand a linear rela-\\ntionship between the latent variables and the observed xdata where\\nx=Bz+µ+ϵ∈RD, (10.63)\\nwhereϵ∼ N(0, σ2I)\\nis Gaussian observation noise and B∈RD×M\\nandµ∈RDdescribe the linear/afﬁne mapping from latent to observed\\nvariables. Therefore, PPCA links latent and observed variables via\\np(x|z,B,µ,σ2) =N(x|Bz+µ, σ2I). (10.64)\\nOverall, PPCA induces the following generative process:\\nzn∼N(z|0,I)\\n(10.65)\\nxn|zn∼N(x|Bzn+µ, σ2I)\\n(10.66)\\nTo generate a data point that is typical given the model parameters, we\\nfollow an ancestral sampling scheme: We ﬁrst sample a latent variable zn ancestral sampling\\nfromp(z). Then we use znin (10.64) to sample a data point conditioned\\non the sampled zn, i.e.,xn∼p(x|zn,B,µ,σ2).\\nThis generative process allows us to write down the probabilistic model\\n(i.e., the joint distribution of all random variables; see Section 8.4) as\\np(x,z|B,µ,σ2) =p(x|z,B,µ,σ2)p(z), (10.67)\\nwhich immediately gives rise to the graphical model in Figure 10.14 using\\nthe results from Section 8.5.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='beca53ad-9240-4c5f-867d-43117eeb19a2', embedding=None, metadata={'page_label': '341', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.7 Latent Variable Perspective 341\\nFigure 10.14\\nGraphical model for\\nprobabilistic PCA.\\nThe observations xn\\nexplicitly depend on\\ncorresponding\\nlatent variables\\nzn∼N(\\n0,I)\\n. The\\nmodel parameters\\nB,µand the\\nlikelihood\\nparameterσare\\nshared across the\\ndataset.xnBzn\\nσµ\\nn= 1,...,N\\nRemark. Note the direction of the arrow that connects the latent variables\\nzand the observed data x: The arrow points from ztox, which means\\nthat the PPCA model assumes a lower-dimensional latent cause zfor high-\\ndimensional observations x. In the end, we are obviously interested in\\nﬁnding something out about zgiven some observations. To get there we\\nwill apply Bayesian inference to “invert” the arrow implicitly and go from\\nobservations to latent variables. ♦\\nExample 10.5 (Generating New Data Using Latent Variables)\\nFigure 10.15\\nGenerating new\\nMNIST digits. The\\nlatent variables z\\ncan be used to\\ngenerate new data\\n˜x=Bz. The closer\\nwe stay to the\\ntraining data, the\\nmore realistic the\\ngenerated data.\\nFigure 10.15 shows the latent coordinates of the MNIST digits “8” found\\nby PCA when using a two-dimensional principal subspace (blue dots).\\nWe can query any vector z∗in this latent space and generate an image\\n˜x∗=Bz∗that resembles the digit “8”. We show eight of such generated\\nimages with their corresponding latent space representation. Depending\\non where we query the latent space, the generated images look different\\n(shape, rotation, size, etc.). If we query away from the training data, we\\nsee more an more artifacts, e.g., the top-left and top-right digits. Note that\\nthe intrinsic dimensionality of these generated images is only two.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97a2cf27-19c6-442e-9ca5-0076195cee47', embedding=None, metadata={'page_label': '342', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='342 Dimensionality Reduction with Principal Component Analysis\\n10.7.2 Likelihood and Joint DistributionThe likelihood does\\nnot depend on the\\nlatent variables z.Using the results from Chapter 6, we obtain the likelihood of this proba-\\nbilistic model by integrating out the latent variable z(see Section 8.4.3)\\nso that\\np(x|B,µ,σ2) =∫\\np(x|z,B,µ,σ2)p(z)dz (10.68a)\\n=∫\\nN(x|Bz+µ, σ2I)N(z|0,I)dz.(10.68b)\\nFrom Section 6.5, we know that the solution to this integral is a Gaussian\\ndistribution with mean\\nEx[x] =Ez[Bz+µ] +Eϵ[ϵ] =µ (10.69)\\nand with covariance matrix\\nV[x] =Vz[Bz+µ] +Vϵ[ϵ] =Vz[Bz] +σ2I (10.70a)\\n=BVz[z]B⊤+σ2I=BB⊤+σ2I. (10.70b)\\nThe likelihood in (10.68b) can be used for maximum likelihood or MAP\\nestimation of the model parameters.\\nRemark. We cannot use the conditional distribution in (10.64) for maxi-\\nmum likelihood estimation as it still depends on the latent variables. The\\nlikelihood function we require for maximum likelihood (or MAP) estima-\\ntion should only be a function of the data xand the model parameters,\\nbut must not depend on the latent variables. ♦\\nFrom Section 6.5, we know that a Gaussian random variable zand\\na linear/afﬁne transformation x=Bzof it are jointly Gaussian dis-\\ntributed. We already know the marginals p(z) =N(z|0,I)\\nandp(x) =\\nN(x|µ,BB⊤+σ2I)\\n. The missing cross-covariance is given as\\nCov[x,z] = Covz[Bz+µ] =BCovz[z,z] =B. (10.71)\\nTherefore, the probabilistic model of PPCA, i.e., the joint distribution of\\nlatent and observed random variables is explicitly given by\\np(x,z|B,µ,σ2) =N([x\\nz]⏐⏐⏐⏐[µ\\n0]\\n,[BB⊤+σ2I B\\nB⊤I])\\n,(10.72)\\nwith a mean vector of length D+Mand a covariance matrix of size\\n(D+M)×(D+M).\\n10.7.3 Posterior Distribution\\nThe joint Gaussian distribution p(x,z|B,µ,σ2)in (10.72) allows us to\\ndetermine the posterior distribution p(z|x)immediately by applying the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b86c560-077f-4692-82a4-a431e14e74fb', embedding=None, metadata={'page_label': '343', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.8 Further Reading 343\\nrules of Gaussian conditioning from Section 6.5.1. The posterior distribu-\\ntion of the latent variable given an observation xis then\\np(z|x) =N(z|m,C), (10.73)\\nm=B⊤(BB⊤+σ2I)−1(x−µ), (10.74)\\nC=I−B⊤(BB⊤+σ2I)−1B. (10.75)\\nNote that the posterior covariance does not depend on the observed data\\nx. For a new observation x∗in data space, we use (10.73) to determine\\nthe posterior distribution of the corresponding latent variable z∗. The co-\\nvariance matrix Callows us to assess how conﬁdent the embedding is. A\\ncovariance matrix Cwith a small determinant (which measures volumes)\\ntells us that the latent embedding z∗is fairly certain. If we obtain a pos-\\nterior distribution p(z∗|x∗)with much variance, we may be faced with\\nan outlier. However, we can explore this posterior distribution to under-\\nstand what other data points xare plausible under this posterior. To do\\nthis, we exploit the generative process underlying PPCA, which allows us\\nto explore the posterior distribution on the latent variables by generating\\nnew data that is plausible under this posterior:\\n1. Sample a latent variable z∗∼p(z|x∗)from the posterior distribution\\nover the latent variables (10.73).\\n2. Sample a reconstructed vector ˜x∗∼p(x|z∗,B,µ,σ2) from (10.64).\\nIf we repeat this process many times, we can explore the posterior dis-\\ntribution (10.73) on the latent variables z∗and its implications on the\\nobserved data. The sampling process effectively hypothesizes data, which\\nis plausible under the posterior distribution.\\n10.8 Further Reading\\nWe derived PCA from two perspectives: (a) maximizing the variance in the\\nprojected space; (b) minimizing the average reconstruction error. How-\\never, PCA can also be interpreted from different perspectives. Let us recap\\nwhat we have done: We took high-dimensional data x∈RDand used\\na matrixB⊤to ﬁnd a lower-dimensional representation z∈RM. The\\ncolumns ofBare the eigenvectors of the data covariance matrix Sthat are\\nassociated with the largest eigenvalues. Once we have a low-dimensional\\nrepresentation z, we can get a high-dimensional version of it (in the orig-\\ninal data space) as x≈˜x=Bz=BB⊤x∈RD, whereBB⊤is a\\nprojection matrix.\\nWe can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder\\nure 10.16. An auto-encoder encodes the data xn∈RDto acodezn∈RMcode\\nand decodes it to a ˜xnsimilar toxn. The mapping from the data to the\\ncode is called the encoder , and the mapping from the code back to the orig- encoder\\ninal data space is called the decoder . If we consider linear mappings where decoder\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a9556dc3-cd33-47d2-a3eb-e8b0b4776339', embedding=None, metadata={'page_label': '344', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='344 Dimensionality Reduction with Principal Component Analysis\\nFigure 10.16 PCA\\ncan be viewed as a\\nlinear auto-encoder.\\nIt encodes the\\nhigh-dimensional\\ndataxinto a\\nlower-dimensional\\nrepresentation\\n(code)z∈RMand\\ndecodeszusing a\\ndecoder. The\\ndecoded vector ˜xis\\nthe orthogonal\\nprojection of the\\noriginal dataxonto\\ntheM-dimensional\\nprincipal subspace.B⊤\\nx ˜xzB\\nEncoder DecoderOriginal\\nCodeRDRD\\nRM\\nthe code is given by zn=B⊤xn∈RMand we are interested in minimiz-\\ning the average squared error between the data xnand its reconstruction\\n˜xn=Bzn,n= 1,...,N , we obtain\\n1\\nNN∑\\nn=1∥xn−˜xn∥2=1\\nNN∑\\nn=1\\ued79\\ued79\\ued79xn−BB⊤xn\\ued79\\ued79\\ued792\\n. (10.76)\\nThis means we end up with the same objective function as in (10.29) that\\nwe discussed in Section 10.3 so that we obtain the PCA solution when we\\nminimize the squared auto-encoding loss. If we replace the linear map-\\nping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.\\nA prominent example of this is a deep auto-encoder where the linear func-\\ntions are replaced with deep neural networks. In this context, the encoder\\nis also known as a recognition network orinference network , whereas the recognition network\\ninference network decoder is also called a generator .\\ngeneratorAnother interpretation of PCA is related to information theory. We can\\nthink of the code as a smaller or compressed version of the original data\\npoint. When we reconstruct our original data using the code, we do not\\nget the exact data point back, but a slightly distorted or noisy version\\nof it. This means that our compression is “lossy”. Intuitively, we want The code is a\\ncompressed version\\nof the original data.to maximize the correlation between the original data and the lower-\\ndimensional code. More formally, this is related to the mutual information.\\nWe would then get the same solution to PCA we discussed in Section 10.3\\nby maximizing the mutual information, a core concept in information the-\\nory (MacKay, 2003).\\nIn our discussion on PPCA, we assumed that the parameters of the\\nmodel, i.e.,B,µ, and the likelihood parameter σ2, are known. Tipping\\nand Bishop (1999) describe how to derive maximum likelihood estimates\\nfor these parameters in the PPCA setting (note that we use a different\\nnotation in this chapter). The maximum likelihood parameters, when pro-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2419193a-00bc-4491-b6db-f4fe887a080b', embedding=None, metadata={'page_label': '345', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.8 Further Reading 345\\njectingD-dimensional data onto an M-dimensional subspace, are\\nµML=1\\nNN∑\\nn=1xn, (10.77)\\nBML=T(Λ−σ2I)1\\n2R, (10.78)\\nσ2\\nML=1\\nD−MD∑\\nj=M+1λj, (10.79)\\nwhereT∈RD×McontainsMeigenvectors of the data covariance matrix, The matrix Λ−σ2I\\nin (10.78) is\\nguaranteed to be\\npositive semideﬁnite\\nas the smallest\\neigenvalue of the\\ndata covariance\\nmatrix is bounded\\nfrom below by the\\nnoise variance σ2.Λ= diag(λ1,...,λM)∈RM×Mis a diagonal matrix with the eigenvalues\\nassociated with the principal axes on its diagonal, and R∈RM×Mis\\nan arbitrary orthogonal matrix. The maximum likelihood solution BMLis\\nunique up to an arbitrary orthogonal transformation, e.g., we can right-\\nmultiplyBMLwith any rotation matrix Rso that (10.78) essentially is a\\nsingular value decomposition (see Section 4.5). An outline of the proof is\\ngiven by Tipping and Bishop (1999).\\nThe maximum likelihood estimate for µgiven in (10.77) is the sample\\nmean of the data. The maximum likelihood estimator for the observation\\nnoise variance σ2given in (10.79) is the average variance in the orthog-\\nonal complement of the principal subspace, i.e., the average leftover vari-\\nance that we cannot capture with the ﬁrst Mprincipal components is\\ntreated as observation noise.\\nIn the noise-free limit where σ→0, PPCA and PCA provide identical\\nsolutions: Since the data covariance matrix Sis symmetric, it can be di-\\nagonalized (see Section 4.4), i.e., there exists a matrix Tof eigenvectors\\nofSso that\\nS=TΛT−1. (10.80)\\nIn the PPCA model, the data covariance matrix is the covariance matrix of\\nthe Gaussian likelihood p(x|B,µ,σ2), which isBB⊤+σ2I, see (10.70b).\\nForσ→0, we obtainBB⊤so that this data covariance must equal the\\nPCA data covariance (and its factorization given in (10.80)) so that\\nCov[X] =TΛT−1=BB⊤⇐⇒B=TΛ1\\n2R, (10.81)\\ni.e., we obtain the maximum likelihood estimate in (10.78) for σ= 0.\\nFrom (10.78) and (10.80), it becomes clear that (P)PCA performs a de-\\ncomposition of the data covariance matrix.\\nIn a streaming setting, where data arrives sequentially, it is recom-\\nmended to use the iterative expectation maximization (EM) algorithm for\\nmaximum likelihood estimation (Roweis, 1998).\\nTo determine the dimensionality of the latent variables (the length of\\nthe code, the dimensionality of the lower-dimensional subspace onto which\\nwe project the data), Gavish and Donoho (2014) suggest the heuristic\\nthat, if we can estimate the noise variance σ2of the data, we should\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97fe8725-e548-42ec-a4cc-9731c4268c5b', embedding=None, metadata={'page_label': '346', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='346 Dimensionality Reduction with Principal Component Analysis\\ndiscard all singular values smaller than4σ√\\nD√\\n3. Alternatively, we can use\\n(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-\\nteria (discussed in Section 8.6.2) to determine a good estimate of the\\nintrinsic dimensionality of the data (Minka, 2001b).\\nSimilar to our discussion on linear regression in Chapter 9, we can place\\na prior distribution on the parameters of the model and integrate them\\nout. By doing so, we (a) avoid point estimates of the parameters and the\\nissues that come with these point estimates (see Section 8.6) and (b) al-\\nlow for an automatic selection of the appropriate dimensionality Mof the\\nlatent space. In this Bayesian PCA , which was proposed by Bishop (1999), Bayesian PCA\\na priorp(µ,B,σ2)is placed on the model parameters. The generative\\nprocess allows us to integrate the model parameters out instead of condi-\\ntioning on them, which addresses overﬁtting issues. Since this integration\\nis analytically intractable, Bishop (1999) proposes to use approximate in-\\nference methods, such as MCMC or variational inference. We refer to the\\nwork by Gilks et al. (1996) and Blei et al. (2017) for more details on these\\napproximate inference techniques.\\nIn PPCA, we considered the linear model p(xn|zn) =N(xn|Bzn+\\nµ, σ2I)\\nwith priorp(zn) =N(0,I)\\n, where all observation dimensions\\nare affected by the same amount of noise. If we allow each observation\\ndimensiondto have a different variance σ2\\nd, we obtain factor analysis factor analysis\\n(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA\\ngives the likelihood some more ﬂexibility than PPCA, but still forces the\\ndata to be explained by the model parameters B,µ.However, FA no An overly ﬂexible\\nlikelihood would be\\nable to explain more\\nthan just the noise.longer allows for a closed-form maximum likelihood solution so that we\\nneed to use an iterative scheme, such as the expectation maximization\\nalgorithm, to estimate the model parameters. While in PPCA all station-\\nary points are global optima, this no longer holds for FA. Compared to\\nPPCA, FA does not change if we scale the data, but it does return different\\nsolutions if we rotate the data.\\nAn algorithm that is also closely related to PCA is independent com- independent\\ncomponent analysis ponent analysis (ICA(Hyvarinen et al., 2001)). Starting again with the\\nICAlatent-variable perspective p(xn|zn) =N(xn|Bzn+µ, σ2I)\\nwe now\\nchange the prior on znto non-Gaussian distributions. ICA can be used\\nforblind-source separation . Imagine you are in a busy train station with blind-source\\nseparation many people talking. Your ears play the role of microphones, and they\\nlinearly mix different speech signals in the train station. The goal of blind-\\nsource separation is to identify the constituent parts of the mixed signals.\\nAs discussed previously in the context of maximum likelihood estimation\\nfor PPCA, the original PCA solution is invariant to any rotation. Therefore,\\nPCA can identify the best lower-dimensional subspace in which the sig-\\nnals live, but not the signals themselves (Murphy, 2012). ICA addresses\\nthis issue by modifying the prior distribution p(z)on the latent sources\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e049785-d182-4ca4-b7ff-0e50df5e252e', embedding=None, metadata={'page_label': '347', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.8 Further Reading 347\\nto require non-Gaussian priors p(z). We refer to the books by Hyvarinen\\net al. (2001) and Murphy (2012) for more details on ICA.\\nPCA, factor analysis, and ICA are three examples for dimensionality re-\\nduction with linear models. Cunningham and Ghahramani (2015) provide\\na broader survey of linear dimensionality reduction.\\nThe (P)PCA model we discussed here allows for several important ex-\\ntensions. In Section 10.5, we explained how to do PCA when the in-\\nput dimensionality Dis signiﬁcantly greater than the number Nof data\\npoints. By exploiting the insight that PCA can be performed by computing\\n(many) inner products, this idea can be pushed to the extreme by consid-\\nering inﬁnite-dimensional features. The kernel trick is the basis of kernel kernel trick\\nkernel PCA PCAand allows us to implicitly compute inner products between inﬁnite-\\ndimensional features (Sch ¨olkopf et al., 1998; Sch ¨olkopf and Smola, 2002).\\nThere are nonlinear dimensionality reduction techniques that are de-\\nrived from PCA (Burges (2010) provides a good overview). The auto-\\nencoder perspective of PCA that we discussed previously in this section\\ncan be used to render PCA as a special case of a deep auto-encoder . In the deep auto-encoder\\ndeep auto-encoder, both the encoder and the decoder are represented by\\nmultilayer feedforward neural networks, which themselves are nonlinear\\nmappings. If we set the activation functions in these neural networks to be\\nthe identity, the model becomes equivalent to PCA. A different approach to\\nnonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process\\nlatent-variable\\nmodelmodel (GP-LVM ) proposed by Lawrence (2005). The GP-LVM starts off with\\nGP-LVMthe latent-variable perspective that we used to derive PPCA and replaces\\nthe linear relationship between the latent variables zand the observations\\nxwith a Gaussian process (GP). Instead of estimating the parameters of\\nthe mapping (as we do in PPCA), the GP-LVM marginalizes out the model\\nparameters and makes point estimates of the latent variables z. Similar\\nto Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM\\n(2010) maintains a distribution on the latent variables zand uses approx-\\nimate inference to integrate them out as well.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca18b7c2-320e-403e-b321-b1ab3efd3cd9', embedding=None, metadata={'page_label': '348', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11\\nDensity Estimation with Gaussian Mixture\\nModels\\nIn earlier chapters, we covered already two fundamental problems in\\nmachine learning: regression (Chapter 9) and dimensionality reduction\\n(Chapter 10). In this chapter, we will have a look at a third pillar of ma-\\nchine learning: density estimation. On our journey, we introduce impor-\\ntant concepts, such as the expectation maximization (EM) algorithm and\\na latent variable perspective of density estimation with mixture models.\\nWhen we apply machine learning to data we often aim to represent\\ndata in some way. A straightforward way is to take the data points them-\\nselves as the representation of the data; see Figure 11.1 for an example.\\nHowever, this approach may be unhelpful if the dataset is huge or if we\\nare interested in representing characteristics of the data. In density esti-\\nmation, we represent the data compactly using a density from a paramet-\\nric family, e.g., a Gaussian or Beta distribution. For example, we may be\\nlooking for the mean and variance of a dataset in order to represent the\\ndata compactly using a Gaussian distribution. The mean and variance can\\nbe found using tools we discussed in Section 8.3: maximum likelihood or\\nmaximum a posteriori estimation. We can then use the mean and variance\\nof this Gaussian to represent the distribution underlying the data, i.e., we\\nthink of the dataset to be a typical realization from this distribution if we\\nwere to sample from it.\\nFigure 11.1\\nTwo-dimensional\\ndataset that cannot\\nbe meaningfully\\nrepresented by a\\nGaussian.\\n−5 0 5\\nx1−4−2024x2\\n348\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7fb20372-6080-4532-9e64-64ae35d36a8e', embedding=None, metadata={'page_label': '349', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.1 Gaussian Mixture Model 349\\nIn practice, the Gaussian (or similarly all other distributions we encoun-\\ntered so far) have limited modeling capabilities. For example, a Gaussian\\napproximation of the density that generated the data in Figure 11.1 would\\nbe a poor approximation. In the following, we will look at a more ex-\\npressive family of distributions, which we can use for density estimation:\\nmixture models . mixture model\\nMixture models can be used to describe a distribution p(x)by a convex\\ncombination of Ksimple (base) distributions\\np(x) =K∑\\nk=1πkpk(x) (11.1)\\n0⩽πk⩽1,K∑\\nk=1πk= 1, (11.2)\\nwhere the components pkare members of a family of basic distributions,\\ne.g., Gaussians, Bernoullis, or Gammas, and the πkaremixture weights .mixture weight\\nMixture models are more expressive than the corresponding base distri-\\nbutions because they allow for multimodal data representations, i.e., they\\ncan describe datasets with multiple “clusters”, such as the example in Fig-\\nure 11.1.\\nWe will focus on Gaussian mixture models (GMMs), where the basic\\ndistributions are Gaussians. For a given dataset, we aim to maximize the\\nlikelihood of the model parameters to train the GMM. For this purpose,\\nwe will use results from Chapter 5, Chapter 6, and Section 7.2. However,\\nunlike other applications we discussed earlier (linear regression or PCA),\\nwe will not ﬁnd a closed-form maximum likelihood solution. Instead, we\\nwill arrive at a set of dependent simultaneous equations, which we can\\nonly solve iteratively.\\n11.1 Gaussian Mixture Model\\nAGaussian mixture model is a density model where we combine a ﬁnite Gaussian mixture\\nmodel number ofKGaussian distributions N(x|µk,Σk)\\nso that\\np(x|θ) =K∑\\nk=1πkN(x|µk,Σk)\\n(11.3)\\n0⩽πk⩽1,K∑\\nk=1πk= 1, (11.4)\\nwhere we deﬁned θ:={µk,Σk,πk:k= 1,...,K}as the collection of\\nall parameters of the model. This convex combination of Gaussian distri-\\nbution gives us signiﬁcantly more ﬂexibility for modeling complex densi-\\nties than a simple Gaussian distribution (which we recover from (11.3) for\\nK= 1). An illustration is given in Figure 11.2, displaying the weighted\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7eee03d3-23a1-4d10-9f65-d016805326a0', embedding=None, metadata={'page_label': '350', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='350 Density Estimation with Gaussian Mixture Models\\nFigure 11.2\\nGaussian mixture\\nmodel. The\\nGaussian mixture\\ndistribution (black)\\nis composed of a\\nconvex combination\\nof Gaussian\\ndistributions and is\\nmore expressive\\nthan any individual\\ncomponent. Dashed\\nlines represent the\\nweighted Gaussian\\ncomponents.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)Component 1\\nComponent 2\\nComponent 3\\nGMM density\\ncomponents and the mixture density, which is given as\\np(x|θ) = 0.5N(x|−2,1\\n2)+ 0.2N(x|1,2)+ 0.3N(x|4,1).(11.5)\\n11.2 Parameter Learning via Maximum Likelihood\\nAssume we are given a dataset X={x1,...,xN}, wherexn, n =\\n1,...,N , are drawn i.i.d. from an unknown distribution p(x). Our ob-\\njective is to ﬁnd a good approximation/representation of this unknown\\ndistribution p(x)by means of a GMM with Kmixture components. The\\nparameters of the GMM are the Kmeansµk, the covariances Σk, and\\nmixture weights πk. We summarize all these free parameters in θ:=\\n{πk,µk,Σk:k= 1,...,K}.\\nExample 11.1 (Initial Setting)\\nFigure 11.3 Initial\\nsetting: GMM\\n(black) with\\nmixture three\\nmixture components\\n(dashed) and seven\\ndata points (discs).\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\nThroughout this chapter, we will have a simple running example that\\nhelps us illustrate and visualize important concepts.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='acab8fff-24ba-4d1b-9626-4b175c7ef153', embedding=None, metadata={'page_label': '351', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Parameter Learning via Maximum Likelihood 351\\nWe consider a one-dimensional dataset X={−3,−2.5,−1,0,2,4,5}\\nconsisting of seven data points and wish to ﬁnd a GMM with K= 3\\ncomponents that models the density of the data. We initialize the mixture\\ncomponents as\\np1(x) =N(x|−4,1)\\n(11.6)\\np2(x) =N(x|0,0.2)\\n(11.7)\\np3(x) =N(x|8,3)\\n(11.8)\\nand assign them equal weights π1=π2=π3=1\\n3. The corresponding\\nmodel (and the data points) are shown in Figure 11.3.\\nIn the following, we detail how to obtain a maximum likelihood esti-\\nmateθMLof the model parameters θ. We start by writing down the like-\\nlihood, i.e., the predictive distribution of the training data given the pa-\\nrameters. We exploit our i.i.d. assumption, which leads to the factorized\\nlikelihood\\np(X|θ) =N∏\\nn=1p(xn|θ), p(xn|θ) =K∑\\nk=1πkN(xn|µk,Σk),(11.9)\\nwhere every individual likelihood term p(xn|θ)is a Gaussian mixture\\ndensity. Then we obtain the log-likelihood as\\nlogp(X|θ) =N∑\\nn=1logp(xn|θ) =N∑\\nn=1logK∑\\nk=1πkN(xn|µk,Σk)\\n\\ued19\\ued18\\ued17 \\ued1a\\n=:L.(11.10)\\nWe aim to ﬁnd parameters θ∗\\nMLthat maximize the log-likelihood Ldeﬁned\\nin (11.10). Our “normal” procedure would be to compute the gradient\\ndL/dθof the log-likelihood with respect to the model parameters θ, set\\nit to0, and solve for θ. However, unlike our previous examples for max-\\nimum likelihood estimation (e.g., when we discussed linear regression in\\nSection 9.2), we cannot obtain a closed-form solution. However, we can\\nexploit an iterative scheme to ﬁnd good model parameters θML, which will\\nturn out to be the EM algorithm for GMMs. The key idea is to update one\\nmodel parameter at a time while keeping the others ﬁxed.\\nRemark. If we were to consider a single Gaussian as the desired density,\\nthe sum over kin (11.10) vanishes, and the logcan be applied directly to\\nthe Gaussian component, such that we get\\nlogN(x|µ,Σ)=−D\\n2log(2π)−1\\n2log det( Σ)−1\\n2(x−µ)⊤Σ−1(x−µ).\\n(11.11)\\nThis simple form allows us to ﬁnd closed-form maximum likelihood esti-\\nmates ofµandΣ, as discussed in Chapter 8. In (11.10), we cannot move\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1539ef0-e537-4c97-a66a-e599ae7c4f4e', embedding=None, metadata={'page_label': '352', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='352 Density Estimation with Gaussian Mixture Models\\ntheloginto the sum over kso that we cannot obtain a simple closed-form\\nmaximum likelihood solution. ♦\\nAny local optimum of a function exhibits the property that its gradi-\\nent with respect to the parameters must vanish (necessary condition); see\\nChapter 7. In our case, we obtain the following necessary conditions when\\nwe optimize the log-likelihood in (11.10) with respect to the GMM param-\\netersµk,Σk,πk:\\n∂L\\n∂µk=0⊤⇐⇒N∑\\nn=1∂logp(xn|θ)\\n∂µk=0⊤, (11.12)\\n∂L\\n∂Σk=0⇐⇒N∑\\nn=1∂logp(xn|θ)\\n∂Σk=0, (11.13)\\n∂L\\n∂πk= 0⇐⇒N∑\\nn=1∂logp(xn|θ)\\n∂πk= 0. (11.14)\\nFor all three necessary conditions, by applying the chain rule (see Sec-\\ntion 5.2.2), we require partial derivatives of the form\\n∂logp(xn|θ)\\n∂θ=1\\np(xn|θ)∂p(xn|θ)\\n∂θ, (11.15)\\nwhereθ={µk,Σk,πk,k= 1,...,K}are the model parameters and\\n1\\np(xn|θ)=1\\n∑K\\nj=1πjN(xn|µj,Σj). (11.16)\\nIn the following, we will compute the partial derivatives (11.12) through\\n(11.14). But before we do this, we introduce a quantity that will play a\\ncentral role in the remainder of this chapter: responsibilities.\\n11.2.1 Responsibilities\\nWe deﬁne the quantity\\nrnk:=πkN(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj) (11.17)\\nas the responsibility of thekth mixture component for the nth data point. responsibility\\nThe responsibility rnkof thekth mixture component for data point xnis\\nproportional to the likelihood\\np(xn|πk,µk,Σk) =πkN(xn|µk,Σk)\\n(11.18)\\nof the mixture component given the data point. Therefore, mixture com- rnfollows a\\nBoltzmann/Gibbs\\ndistribution.ponents have a high responsibility for a data point when the data point\\ncould be a plausible sample from that mixture component. Note that\\nrn:= [rn1,...,rnK]⊤∈RKis a (normalized) probability vector, i.e.,\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1bf7f252-db5f-4b04-bff5-244cd06e8d96', embedding=None, metadata={'page_label': '353', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Parameter Learning via Maximum Likelihood 353\\n∑\\nkrnk= 1 withrnk⩾0. This probability vector distributes probabil-\\nity mass among the Kmixture components, and we can think of rnas a\\n“soft assignment” of xnto theKmixture components. Therefore, the re- The responsibility\\nrnkis the\\nprobability that the\\nkth mixture\\ncomponent\\ngenerated the nth\\ndata point.sponsibility rnkfrom (11.17) represents the probability that xnhas been\\ngenerated by the kth mixture component.\\nExample 11.2 (Responsibilities)\\nFor our example from Figure 11.3, we compute the responsibilities rnk\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f01.0 0.0 0.0\\n1.0 0.0 0.0\\n0.057 0.943 0.0\\n0.001 0.999 0.0\\n0.0 0.066 0.934\\n0.0 0.0 1.0\\n0.0 0.0 1.0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈RN×K. (11.19)\\nHere thenth row tells us the responsibilities of all mixture components\\nforxn. The sum of all Kresponsibilities for a data point (sum of every\\nrow) is 1. Thekth column gives us an overview of the responsibility of\\nthekth mixture component. We can see that the third mixture component\\n(third column) is not responsible for any of the ﬁrst four data points, but\\ntakes much responsibility of the remaining data points. The sum of all\\nentries of a column gives us the values Nk, i.e., the total responsibility of\\nthekth mixture component. In our example, we get N1= 2.058, N 2=\\n2.008, N3= 2.934.\\nIn the following, we determine the updates of the model parameters\\nµk,Σk,πkfor given responsibilities. We will see that the update equa-\\ntions all depend on the responsibilities, which makes a closed-form solu-\\ntion to the maximum likelihood estimation problem impossible. However,\\nfor given responsibilities we will be updating one model parameter at a\\ntime, while keeping the others ﬁxed. After this, we will recompute the\\nresponsibilities. Iterating these two steps will eventually converge to a lo-\\ncal optimum and is a speciﬁc instantiation of the EM algorithm. We will\\ndiscuss this in some more detail in Section 11.3.\\n11.2.2 Updating the Means\\nTheorem 11.1 (Update of the GMM Means) .The update of the mean pa-\\nrametersµk,k= 1,...,K , of the GMM is given by\\nµnew\\nk=∑N\\nn=1rnkxn∑N\\nn=1rnk, (11.20)\\nwhere the responsibilities rnkare deﬁned in (11.17) .\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d62b50ce-afab-46f9-978a-dbc5ffa2dbbf', embedding=None, metadata={'page_label': '354', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='354 Density Estimation with Gaussian Mixture Models\\nRemark. The update of the means µkof the individual mixture compo-\\nnents in (11.20) depends on all means, covariance matrices Σk, and mix-\\nture weights πkviarnkgiven in (11.17). Therefore, we cannot obtain a\\nclosed-form solution for all µkat once. ♦\\nProof From (11.15), we see that the gradient of the log-likelihood with\\nrespect to the mean parameters µk,k= 1,...,K , requires us to compute\\nthe partial derivative\\n∂p(xn|θ)\\n∂µk=K∑\\nj=1πj∂N(xn|µj,Σj)\\n∂µk=πk∂N(xn|µk,Σk)\\n∂µk(11.21a)\\n=πk(xn−µk)⊤Σ−1\\nkN(xn|µk,Σk), (11.21b)\\nwhere we exploited that only the kth mixture component depends on µk.\\nWe use our result from (11.21b) in (11.15) and put everything together\\nso that the desired partial derivative of Lwith respect to µkis given as\\n∂L\\n∂µk=N∑\\nn=1∂logp(xn|θ)\\n∂µk=N∑\\nn=11\\np(xn|θ)∂p(xn|θ)\\n∂µk(11.22a)\\n=N∑\\nn=1(xn−µk)⊤Σ−1\\nkπkN(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj)\\n\\ued19\\ued18\\ued17\\ued1a\\n=rnk(11.22b)\\n=N∑\\nn=1rnk(xn−µk)⊤Σ−1\\nk. (11.22c)\\nHere we used the identity from (11.16) and the result of the partial deriva-\\ntive in (11.21b) to get to (11.22b). The values rnkare the responsibilities\\nwe deﬁned in (11.17).\\nWe now solve (11.22c) for µnew\\nkso that∂L(µnew\\nk)\\n∂µk=0⊤and obtain\\nN∑\\nn=1rnkxn=N∑\\nn=1rnkµnew\\nk⇐⇒µnew\\nk=∑N\\nn=1rnkxn\\n∑N\\nn=1rnk=1\\nNkN∑\\nn=1rnkxn,\\n(11.23)\\nwhere we deﬁned\\nNk:=N∑\\nn=1rnk (11.24)\\nas the total responsibility of the kth mixture component for the entire\\ndataset. This concludes the proof of Theorem 11.1.\\nIntuitively, (11.20) can be interpreted as an importance-weighted Monte\\nCarlo estimate of the mean, where the importance weights of data point\\nxnare the responsibilities rnkof thekth cluster for xn,k= 1,...,K .\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='606c3b6c-677e-4506-ac62-a085223ec2cc', embedding=None, metadata={'page_label': '355', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Parameter Learning via Maximum Likelihood 355\\nTherefore, the mean µkis pulled toward a data point xnwith strength Figure 11.4 Update\\nof the mean\\nparameter of\\nmixture component\\nin a GMM. The\\nmeanµis being\\npulled toward\\nindividual data\\npoints with the\\nweights given by the\\ncorresponding\\nresponsibilities.\\nr1r2\\nr3x1x2x3\\nµgiven byrnk. The means are pulled stronger toward data points for which\\nthe corresponding mixture component has a high responsibility, i.e., a high\\nlikelihood. Figure 11.4 illustrates this. We can also interpret the mean up-\\ndate in (11.20) as the expected value of all data points under the distri-\\nbution given by\\nrk:= [r1k,...,rNk]⊤/Nk, (11.25)\\nwhich is a normalized probability vector, i.e.,\\nµk←Erk[X]. (11.26)\\nExample 11.3 (Mean Updates)\\nFigure 11.5 Effect\\nof updating the\\nmean values in a\\nGMM. (a) GMM\\nbefore updating the\\nmean values;\\n(b) GMM after\\nupdating the mean\\nvaluesµkwhile\\nretaining the\\nvariances and\\nmixture weights.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density (a) GMM density and individual components\\nprior to updating the mean values.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the mean values.\\nIn our example from Figure 11.3, the mean values are updated as fol-\\nlows:\\nµ1:−4→−2.7 (11.27)\\nµ2: 0→−0.4 (11.28)\\nµ3: 8→3.7 (11.29)\\nHere we see that the means of the ﬁrst and third mixture component\\nmove toward the regime of the data, whereas the mean of the second\\ncomponent does not change so dramatically. Figure 11.5 illustrates this\\nchange, where Figure 11.5(a) shows the GMM density prior to updating\\nthe means and Figure 11.5(b) shows the GMM density after updating the\\nmean values µk.\\nThe update of the mean parameters in (11.20) look fairly straight-\\nforward. However, note that the responsibilities rnkare a function of\\nπj,µj,Σjfor allj= 1,...,K , such that the updates in (11.20) depend\\non all parameters of the GMM, and a closed-form solution, which we ob-\\ntained for linear regression in Section 9.2 or PCA in Chapter 10, cannot\\nbe obtained.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1110e715-b648-4672-a27b-f6017c087680', embedding=None, metadata={'page_label': '356', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='356 Density Estimation with Gaussian Mixture Models\\n11.2.3 Updating the Covariances\\nTheorem 11.2 (Updates of the GMM Covariances) .The update of the co-\\nvariance parameters Σk,k= 1,...,K of the GMM is given by\\nΣnew\\nk=1\\nNkN∑\\nn=1rnk(xn−µk)(xn−µk)⊤, (11.30)\\nwherernkandNkare deﬁned in (11.17) and(11.24) , respectively.\\nProof To prove Theorem 11.2, our approach is to compute the partial\\nderivatives of the log-likelihood Lwith respect to the covariances Σk, set\\nthem to 0, and solve for Σk. We start with our general approach\\n∂L\\n∂Σk=N∑\\nn=1∂logp(xn|θ)\\n∂Σk=N∑\\nn=11\\np(xn|θ)∂p(xn|θ)\\n∂Σk. (11.31)\\nWe already know 1/p(xn|θ)from (11.16). To obtain the remaining par-\\ntial derivative ∂p(xn|θ)/∂Σk, we write down the deﬁnition of the Gaus-\\nsian distribution p(xn|θ)(see (11.9)) and drop all terms but the kth. We\\nthen obtain\\n∂p(xn|θ)\\n∂Σk(11.32a)\\n=∂\\n∂Σk(\\nπk(2π)−D\\n2det(Σk)−1\\n2exp(−1\\n2(xn−µk)⊤Σ−1\\nk(xn−µk)))\\n(11.32b)\\n=πk(2π)−D\\n2[∂\\n∂Σkdet(Σk)−1\\n2exp(−1\\n2(xn−µk)⊤Σ−1\\nk(xn−µk))\\n+ det( Σk)−1\\n2∂\\n∂Σkexp(−1\\n2(xn−µk)⊤Σ−1\\nk(xn−µk))]\\n.(11.32c)\\nWe now use the identities\\n∂\\n∂Σkdet(Σk)−1\\n2(5.101)=−1\\n2det(Σk)−1\\n2Σ−1\\nk, (11.33)\\n∂\\n∂Σk(xn−µk)⊤Σ−1\\nk(xn−µk)(5.103)=−Σ−1\\nk(xn−µk)(xn−µk)⊤Σ−1\\nk\\n(11.34)\\nand obtain (after some rearranging) the desired partial derivative required\\nin (11.31) as\\n∂p(xn|θ)\\n∂Σk=πkN(xn|µk,Σk)\\n·[−1\\n2(Σ−1\\nk−Σ−1\\nk(xn−µk)(xn−µk)⊤Σ−1\\nk)].(11.35)\\nPutting everything together, the partial derivative of the log-likelihood\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='797a7ba0-07ad-4e8f-87b2-095e428602aa', embedding=None, metadata={'page_label': '357', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Parameter Learning via Maximum Likelihood 357\\nwith respect to Σkis given by\\n∂L\\n∂Σk=N∑\\nn=1∂logp(xn|θ)\\n∂Σk=N∑\\nn=11\\np(xn|θ)∂p(xn|θ)\\n∂Σk(11.36a)\\n=N∑\\nn=1πkN(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj)\\n\\ued19\\ued18\\ued17\\ued1a\\n=rnk\\n·[−1\\n2(Σ−1\\nk−Σ−1\\nk(xn−µk)(xn−µk)⊤Σ−1\\nk)]\\n(11.36b)\\n=−1\\n2N∑\\nn=1rnk(Σ−1\\nk−Σ−1\\nk(xn−µk)(xn−µk)⊤Σ−1\\nk) (11.36c)\\n=−1\\n2Σ−1\\nkN∑\\nn=1rnk\\n\\ued19\\ued18\\ued17\\ued1a\\n=Nk+1\\n2Σ−1\\nk(N∑\\nn=1rnk(xn−µk)(xn−µk)⊤)\\nΣ−1\\nk.\\n(11.36d)\\nWe see that the responsibilities rnkalso appear in this partial derivative.\\nSetting this partial derivative to 0, we obtain the necessary optimality\\ncondition\\nNkΣ−1\\nk=Σ−1\\nk(N∑\\nn=1rnk(xn−µk)(xn−µk)⊤)\\nΣ−1\\nk (11.37a)\\n⇐⇒NkI=(N∑\\nn=1rnk(xn−µk)(xn−µk)⊤)\\nΣ−1\\nk. (11.37b)\\nBy solving for Σk, we obtain\\nΣnew\\nk=1\\nNkN∑\\nn=1rnk(xn−µk)(xn−µk)⊤, (11.38)\\nwhererkis the probability vector deﬁned in (11.25). This gives us a sim-\\nple update rule for Σkfork= 1,...,K and proves Theorem 11.2.\\nSimilar to the update of µkin (11.20), we can interpret the update of\\nthe covariance in (11.30) as an importance-weighted expected value of\\nthe square of the centered data ˜Xk:={x1−µk,...,xN−µk}.\\nExample 11.4 (Variance Updates)\\nIn our example from Figure 11.3, the variances are updated as follows:\\nσ2\\n1: 1→0.14 (11.39)\\nσ2\\n2: 0.2→0.44 (11.40)\\nσ2\\n3: 3→1.53 (11.41)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8030daa8-7508-4c2c-ad68-ca5a8e1cf5c5', embedding=None, metadata={'page_label': '358', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='358 Density Estimation with Gaussian Mixture Models\\nHere we see that the variances of the ﬁrst and third component shrink\\nsigniﬁcantly, whereas the variance of the second component increases\\nslightly.\\nFigure 11.6 illustrates this setting. Figure 11.6(a) is identical (but\\nzoomed in) to Figure 11.5(b) and shows the GMM density and its indi-\\nvidual components prior to updating the variances. Figure 11.6(b) shows\\nthe GMM density after updating the variances.\\nFigure 11.6 Effect\\nof updating the\\nvariances in a GMM.\\n(a) GMM before\\nupdating the\\nvariances; (b) GMM\\nafter updating the\\nvariances while\\nretaining the means\\nand mixture\\nweights.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\n(a) GMM density and individual components\\nprior to updating the variances.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the variances.\\nSimilar to the update of the mean parameters, we can interpret (11.30)\\nas a Monte Carlo estimate of the weighted covariance of data points xn\\nassociated with the kth mixture component, where the weights are the\\nresponsibilities rnk. As with the updates of the mean parameters, this up-\\ndate depends on all πj,µj,Σj, j= 1,...,K , through the responsibilities\\nrnk, which prohibits a closed-form solution.\\n11.2.4 Updating the Mixture Weights\\nTheorem 11.3 (Update of the GMM Mixture Weights) .The mixture weights\\nof the GMM are updated as\\nπnew\\nk=Nk\\nN, k = 1,...,K, (11.42)\\nwhereNis the number of data points and Nkis deﬁned in (11.24) .\\nProof To ﬁnd the partial derivative of the log-likelihood with respect\\nto the weight parameters πk,k= 1,...,K , we account for the con-\\nstraint∑\\nkπk= 1 by using Lagrange multipliers (see Section 7.2). The\\nLagrangian is\\nL=L+λ(K∑\\nk=1πk−1)\\n(11.43a)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3edab242-1214-4d2b-a888-7bb25f17d624', embedding=None, metadata={'page_label': '359', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.2 Parameter Learning via Maximum Likelihood 359\\n=N∑\\nn=1logK∑\\nk=1πkN(xn|µk,Σk)+λ(K∑\\nk=1πk−1)\\n, (11.43b)\\nwhereLis the log-likelihood from (11.10) and the second term encodes\\nfor the equality constraint that all the mixture weights need to sum up to\\n1. We obtain the partial derivative with respect to πkas\\n∂L\\n∂πk=N∑\\nn=1N(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj)+λ (11.44a)\\n=1\\nπkN∑\\nn=1πkN(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj)\\n\\ued19\\ued18\\ued17 \\ued1a\\n=Nk+λ=Nk\\nπk+λ, (11.44b)\\nand the partial derivative with respect to the Lagrange multiplier λas\\n∂L\\n∂λ=K∑\\nk=1πk−1. (11.45)\\nSetting both partial derivatives to 0(necessary condition for optimum)\\nyields the system of equations\\nπk=−Nk\\nλ, (11.46)\\n1 =K∑\\nk=1πk. (11.47)\\nUsing (11.46) in (11.47) and solving for πk, we obtain\\nK∑\\nk=1πk= 1⇐⇒ −K∑\\nk=1Nk\\nλ= 1⇐⇒ −N\\nλ= 1⇐⇒λ=−N.\\n(11.48)\\nThis allows us to substitute −Nforλin (11.46) to obtain\\nπnew\\nk=Nk\\nN, (11.49)\\nwhich gives us the update for the weight parameters πkand proves Theo-\\nrem 11.3.\\nWe can identify the mixture weight in (11.42) as the ratio of the to-\\ntal responsibility of the kth cluster and the number of data points. Since\\nN=∑\\nkNk, the number of data points can also be interpreted as the\\ntotal responsibility of all mixture components together, such that πkis the\\nrelative importance of the kth mixture component for the dataset.\\nRemark. SinceNk=∑N\\ni=1rnk, the update equation (11.42) for the mix-\\nture weights πkalso depends on all πj,µj,Σj,j= 1,...,K via the re-\\nsponsibilities rnk. ♦\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='76e5ca7f-6f70-4f05-b05e-c4ff9899ffee', embedding=None, metadata={'page_label': '360', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='360 Density Estimation with Gaussian Mixture Models\\nExample 11.5 (Weight Parameter Updates)\\nFigure 11.7 Effect\\nof updating the\\nmixture weights in a\\nGMM. (a) GMM\\nbefore updating the\\nmixture weights;\\n(b) GMM after\\nupdating the\\nmixture weights\\nwhile retaining the\\nmeans and\\nvariances. Note the\\ndifferent scales of\\nthe vertical axes.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.300.35p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(a) GMM density and individual components\\nprior to updating the mixture weights.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density(b) GMM density and individual components\\nafter updating the mixture weights.\\nIn our running example from Figure 11.3, the mixture weights are up-\\ndated as follows:\\nπ1:1\\n3→0.29 (11.50)\\nπ2:1\\n3→0.29 (11.51)\\nπ3:1\\n3→0.42 (11.52)\\nHere we see that the third component gets more weight/importance,\\nwhile the other components become slightly less important. Figure 11.7\\nillustrates the effect of updating the mixture weights. Figure 11.7(a) is\\nidentical to Figure 11.6(b) and shows the GMM density and its individual\\ncomponents prior to updating the mixture weights. Figure 11.7(b) shows\\nthe GMM density after updating the mixture weights.\\nOverall, having updated the means, the variances, and the weights\\nonce, we obtain the GMM shown in Figure 11.7(b). Compared with the\\ninitialization shown in Figure 11.3, we can see that the parameter updates\\ncaused the GMM density to shift some of its mass toward the data points.\\nAfter updating the means, variances, and weights once, the GMM ﬁt\\nin Figure 11.7(b) is already remarkably better than its initialization from\\nFigure 11.3. This is also evidenced by the log-likelihood values, which in-\\ncreased from 28.3(initialization) to 14.4after one complete update cycle.\\n11.3 EM Algorithm\\nUnfortunately, the updates in (11.20), (11.30), and (11.42) do not consti-\\ntute a closed-form solution for the updates of the parameters µk,Σk,πk\\nof the mixture model because the responsibilities rnkdepend on those pa-\\nrameters in a complex way. However, the results suggest a simple iterative\\nscheme for ﬁnding a solution to the parameters estimation problem via\\nmaximum likelihood. The expectation maximization algorithm ( EM algo- EM algorithm\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51db2118-717d-43cf-bf02-42caa166343c', embedding=None, metadata={'page_label': '361', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.3 EM Algorithm 361\\nrithm ) was proposed by Dempster et al. (1977) and is a general iterative\\nscheme for learning parameters (maximum likelihood or MAP) in mixture\\nmodels and, more generally, latent-variable models.\\nIn our example of the Gaussian mixture model, we choose initial values\\nforµk,Σk,πkand alternate until convergence between\\nE-step: Evaluate the responsibilities rnk(posterior probability of data\\npointnbelonging to mixture component k).\\nM-step: Use the updated responsibilities to reestimate the parameters\\nµk,Σk,πk.\\nEvery step in the EM algorithm increases the log-likelihood function (Neal\\nand Hinton, 1999). For convergence, we can check the log-likelihood or\\nthe parameters directly. A concrete instantiation of the EM algorithm for\\nestimating the parameters of a GMM is as follows:\\n1. Initializeµk,Σk,πk.\\n2.E-step: Evaluate responsibilities rnkfor every data point xnusing cur-\\nrent parameters πk,µk,Σk:\\nrnk=πkN(xn|µk,Σk)\\n∑\\njπjN(xn|µj,Σj). (11.53)\\n3.M-step: Reestimate parameters πk,µk,Σkusing the current responsi-\\nbilitiesrnk(from E-step): Having updated the\\nmeansµk\\nin (11.54), they are\\nsubsequently used\\nin (11.55) to update\\nthe corresponding\\ncovariances.µk=1\\nNkN∑\\nn=1rnkxn, (11.54)\\nΣk=1\\nNkN∑\\nn=1rnk(xn−µk)(xn−µk)⊤, (11.55)\\nπk=Nk\\nN. (11.56)\\nExample 11.6 (GMM Fit)\\nFigure 11.8 EM\\nalgorithm applied to\\nthe GMM from\\nFigure 11.2. (a)\\nFinal GMM ﬁt;\\n(b) negative\\nlog-likelihood as a\\nfunction of the EM\\niteration.\\n−5 0 5 10 15\\nx0.000.050.100.150.200.250.30p(x)π1N(x|µ1,σ2\\n1)\\nπ2N(x|µ2,σ2\\n2)\\nπ3N(x|µ3,σ2\\n3)\\nGMM density\\n(a) Final GMM ﬁt. After ﬁve iterations, the EM\\nalgorithm converges and returns this GMM.\\n0 1 2 3 4 5\\nIteration1416182022242628Negative log-likelihood\\n(b) Negative log-likelihood as a function of the\\nEM iterations.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f4ae4b5-9822-429f-8f3e-453135a95bef', embedding=None, metadata={'page_label': '362', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='362 Density Estimation with Gaussian Mixture Models\\nFigure 11.9\\nIllustration of the\\nEM algorithm for\\nﬁtting a Gaussian\\nmixture model with\\nthree components to\\na two-dimensional\\ndataset. (a) Dataset;\\n(b) negative\\nlog-likelihood\\n(lower is better) as\\na function of the EM\\niterations. The red\\ndots indicate the\\niterations for which\\nthe mixture\\ncomponents of the\\ncorresponding GMM\\nﬁts are shown in (c)\\nthrough (f). The\\nyellow discs indicate\\nthe means of the\\nGaussian mixture\\ncomponents.\\nFigure 11.10(a)\\nshows the ﬁnal\\nGMM ﬁt.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(a) Dataset.\\n0 20 40 60\\nEM iteration104\\n4×1036×103Negative log-likelihood (b) Negative log-likelihood.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(c) EM initialization.\\n−10−5 0 5 10\\nx1−10−50510x2\\n (d) EM after one iteration.\\n−10−5 0 5 10\\nx1−10−50510x2\\n(e) EM after 10iterations.\\n−10−5 0 5 10\\nx1−10−50510x2\\n (f) EM after 62iterations.\\nWhen we run EM on our example from Figure 11.3, we obtain the ﬁnal\\nresult shown in Figure 11.8(a) after ﬁve iterations, and Figure 11.8(b)\\nshows how the negative log-likelihood evolves as a function of the EM\\niterations. The ﬁnal GMM is given as\\np(x) = 0.29N(x|−2.75,0.06)+ 0.28N(x|−0.50,0.25)\\n+ 0.43N(x|3.64,1.63).(11.57)\\nWe applied the EM algorithm to the two-dimensional dataset shown\\nin Figure 11.1 with K= 3 mixture components. Figure 11.9 illustrates\\nsome steps of the EM algorithm and shows the negative log-likelihood as\\na function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af94a7c3-691e-47e4-9301-fdd04963ba1c', embedding=None, metadata={'page_label': '363', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4 Latent-Variable Perspective 363\\nFigure 11.10 GMM\\nﬁt and\\nresponsibilities\\nwhen EM converges.\\n(a) GMM ﬁt when\\nEM converges;\\n(b) each data point\\nis colored according\\nto the\\nresponsibilities of\\nthe mixture\\ncomponents.\\n−5 0 5\\nx1−6−4−20246x2\\n(a) GMM ﬁt after 62iterations.\\n−5 0 5\\nx1−6−4−20246x2 (b) Dataset colored according to the respon-\\nsibilities of the mixture components.\\nthe corresponding ﬁnal GMM ﬁt. Figure 11.10(b) visualizes the ﬁnal re-\\nsponsibilities of the mixture components for the data points. The dataset is\\ncolored according to the responsibilities of the mixture components when\\nEM converges. While a single mixture component is clearly responsible\\nfor the data on the left, the overlap of the two data clusters on the right\\ncould have been generated by two mixture components. It becomes clear\\nthat there are data points that cannot be uniquely assigned to a single\\ncomponent (either blue or yellow), such that the responsibilities of these\\ntwo clusters for those points are around 0.5.\\n11.4 Latent-Variable Perspective\\nWe can look at the GMM from the perspective of a discrete latent-variable\\nmodel, i.e., where the latent variable zcan attain only a ﬁnite set of val-\\nues. This is in contrast to PCA, where the latent variables were continuous-\\nvalued numbers in RM.\\nThe advantages of the probabilistic perspective are that (i) it will jus-\\ntify some ad hoc decisions we made in the previous sections, (ii) it allows\\nfor a concrete interpretation of the responsibilities as posterior probabil-\\nities, and (iii) the iterative algorithm for updating the model parameters\\ncan be derived in a principled manner as the EM algorithm for maximum\\nlikelihood parameter estimation in latent-variable models.\\n11.4.1 Generative Process and Probabilistic Model\\nTo derive the probabilistic model for GMMs, it is useful to think about the\\ngenerative process, i.e., the process that allows us to generate data, using\\na probabilistic model.\\nWe assume a mixture model with Kcomponents and that a data point\\nxcan be generated by exactly one mixture component. We introduce a\\nbinary indicator variable zk∈{0,1}with two states (see Section 6.2) that\\nindicates whether the kth mixture component generated that data point\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dd7c47b8-dfcc-4385-bb65-c9fdf21ef890', embedding=None, metadata={'page_label': '364', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='364 Density Estimation with Gaussian Mixture Models\\nso that\\np(x|zk= 1) =N(x|µk,Σk). (11.58)\\nWe deﬁnez:= [z1,...,zK]⊤∈RKas a probability vector consisting of\\nK−1many 0s and exactly one 1. For example, for K= 3, a validzwould\\nbez= [z1,z2,z3]⊤= [0,1,0]⊤, which would select the second mixture\\ncomponent since z2= 1.\\nRemark. Sometimes this kind of probability distribution is called “multi-\\nnoulli”, a generalization of the Bernoulli distribution to more than two\\nvalues (Murphy, 2012). ♦\\nThe properties of zimply that∑K\\nk=1zk= 1. Therefore,zis aone-hot one-hot encoding\\nencoding (also: 1-of-Krepresentation ). 1-of-K\\nrepresentation Thus far, we assumed that the indicator variables zkare known. How-\\never, in practice, this is not the case, and we place a prior distribution\\np(z) =π= [π1,...,πK]⊤,K∑\\nk=1πk= 1, (11.59)\\non the latent variable z. Then thekth entry\\nπk=p(zk= 1) (11.60)\\nof this probability vector describes the probability that the kth mixture\\ncomponent generated data point x. Figure 11.11\\nGraphical model for\\na GMM with a single\\ndata point.\\nπ\\nz\\nx Σkµk\\nk= 1,...,KRemark (Sampling from a GMM) .The construction of this latent-variable\\nmodel (see the corresponding graphical model in Figure 11.11) lends it-\\nself to a very simple sampling procedure (generative process) to generate\\ndata:\\n1. Samplez(i)∼p(z).\\n2. Samplex(i)∼p(x|z(i)= 1).\\nIn the ﬁrst step, we select a mixture component i(via the one-hot encod-\\ningz) at random according to p(z) =π; in the second step we draw a\\nsample from the corresponding mixture component. When we discard the\\nsamples of the latent variable so that we are left with the x(i), we have\\nvalid samples from the GMM. This kind of sampling, where samples of\\nrandom variables depend on samples from the variable’s parents in the\\ngraphical model, is called ancestral sampling . ♦ ancestral sampling\\nGenerally, a probabilistic model is deﬁned by the joint distribution of\\nthe data and the latent variables (see Section 8.4). With the prior p(z)\\ndeﬁned in (11.59) and (11.60) and the conditional p(x|z)from (11.58),\\nwe obtain all Kcomponents of this joint distribution via\\np(x,zk= 1) =p(x|zk= 1)p(zk= 1) =πkN(x|µk,Σk)\\n(11.61)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d4d29bd-4291-4da4-92d7-0054f62e5a94', embedding=None, metadata={'page_label': '365', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4 Latent-Variable Perspective 365\\nfork= 1,...,K , so that\\np(x,z) =\\uf8ee\\n\\uf8ef\\uf8f0p(x,z1= 1)\\n...\\np(x,zK= 1)\\uf8f9\\n\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8f0π1N(x|µ1,Σ1)\\n...\\nπKN(x|µK,ΣK)\\uf8f9\\n\\uf8fa\\uf8fb, (11.62)\\nwhich fully speciﬁes the probabilistic model.\\n11.4.2 Likelihood\\nTo obtain the likelihood p(x|θ)in a latent-variable model, we need to\\nmarginalize out the latent variables (see Section 8.4.3). In our case, this\\ncan be done by summing out all latent variables from the joint p(x,z)\\nin (11.62) so that\\np(x|θ) =∑\\nzp(x|θ,z)p(z|θ),θ:={µk,Σk,πk:k= 1,...,K}.\\n(11.63)\\nWe now explicitly condition on the parameters θof the probabilistic model,\\nwhich we previously omitted. In (11.63), we sum over all Kpossible one-\\nhot encodings of z, which is denoted by∑\\nz. Since there is only a single\\nnonzero single entry in each zthere are only Kpossible conﬁgurations/\\nsettings ofz. For example, if K= 3, thenzcan have the conﬁgurations\\n\\uf8ee\\n\\uf8f01\\n0\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n1\\n0\\uf8f9\\n\\uf8fb,\\uf8ee\\n\\uf8f00\\n0\\n1\\uf8f9\\n\\uf8fb. (11.64)\\nSumming over all possible conﬁgurations of zin (11.63) is equivalent to\\nlooking at the nonzero entry of the z-vector and writing\\np(x|θ) =∑\\nzp(x|θ,z)p(z|θ) (11.65a)\\n=K∑\\nk=1p(x|θ,zk= 1)p(zk= 1|θ) (11.65b)\\nso that the desired marginal distribution is given as\\np(x|θ)(11.65b)=K∑\\nk=1p(x|θ,zk= 1)p(zk= 1|θ) (11.66a)\\n=K∑\\nk=1πkN(x|µk,Σk), (11.66b)\\nwhich we identify as the GMM model from (11.3). Given a dataset X, we\\nimmediately obtain the likelihood\\np(X|θ) =N∏\\nn=1p(xn|θ)(11.66b)=N∏\\nn=1K∑\\nk=1πkN(xn|µk,Σk),(11.67)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26a7cf30-ca97-4e8c-b078-bb07dbef74c4', embedding=None, metadata={'page_label': '366', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='366 Density Estimation with Gaussian Mixture Models\\nFigure 11.12\\nGraphical model for\\na GMM with Ndata\\npoints.π\\nzn\\nxn Σkµk\\nn= 1,...,Nk= 1,...,K\\nwhich is exactly the GMM likelihood from (11.9). Therefore, the latent-\\nvariable model with latent indicators zkis an equivalent way of thinking\\nabout a Gaussian mixture model.\\n11.4.3 Posterior Distribution\\nLet us have a brief look at the posterior distribution on the latent variable\\nz. According to Bayes’ theorem, the posterior of the kth component having\\ngenerated data point x\\np(zk= 1|x) =p(zk= 1)p(x|zk= 1)\\np(x), (11.68)\\nwhere the marginal p(x)is given in (11.66b). This yields the posterior\\ndistribution for the kth indicator variable zk\\np(zk= 1|x) =p(zk= 1)p(x|zk= 1)\\n∑K\\nj=1p(zj= 1)p(x|zj= 1)=πkN(x|µk,Σk)\\n∑K\\nj=1πjN(x|µj,Σj),\\n(11.69)\\nwhich we identify as the responsibility of the kth mixture component for\\ndata pointx. Note that we omitted the explicit conditioning on the GMM\\nparameters πk,µk,Σkwherek= 1,...,K .\\n11.4.4 Extension to a Full Dataset\\nThus far, we have only discussed the case where the dataset consists only\\nof a single data point x. However, the concepts of the prior and posterior\\ncan be directly extended to the case of Ndata pointsX:={x1,...,xN}.\\nIn the probabilistic interpretation of the GMM, every data point xnpos-\\nsesses its own latent variable\\nzn= [zn1,...,znK]⊤∈RK. (11.70)\\nPreviously (when we only considered a single data point x), we omitted\\nthe indexn, but now this becomes important.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3a7eb52-7fb7-40f9-917d-a60cac33c814', embedding=None, metadata={'page_label': '367', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4 Latent-Variable Perspective 367\\nWe share the same prior distribution πacross all latent variables zn.\\nThe corresponding graphical model is shown in Figure 11.12, where we\\nuse the plate notation.\\nThe conditional distribution p(x1,...,xN|z1,...,zN)factorizes over\\nthe data points and is given as\\np(x1,...,xN|z1,...,zN) =N∏\\nn=1p(xn|zn). (11.71)\\nTo obtain the posterior distribution p(znk= 1|xn), we follow the same\\nreasoning as in Section 11.4.3 and apply Bayes’ theorem to obtain\\np(znk= 1|xn) =p(xn|znk= 1)p(znk= 1)\\n∑K\\nj=1p(xn|znj= 1)p(znj= 1)(11.72a)\\n=πkN(xn|µk,Σk)\\n∑K\\nj=1πjN(xn|µj,Σj)=rnk. (11.72b)\\nThis means that p(zk= 1|xn)is the (posterior) probability that the kth\\nmixture component generated data point xnand corresponds to the re-\\nsponsibility rnkwe introduced in (11.17). Now the responsibilities also\\nhave not only an intuitive but also a mathematically justiﬁed interpreta-\\ntion as posterior probabilities.\\n11.4.5 EM Algorithm Revisited\\nThe EM algorithm that we introduced as an iterative scheme for maximum\\nlikelihood estimation can be derived in a principled way from the latent-\\nvariable perspective. Given a current setting θ(t)of model parameters, the\\nE-step calculates the expected log-likelihood\\nQ(θ|θ(t)) =Ez|x,θ(t)[logp(x,z|θ)] (11.73a)\\n=∫\\nlogp(x,z|θ)p(z|x,θ(t))dz, (11.73b)\\nwhere the expectation of logp(x,z|θ)is taken with respect to the poste-\\nriorp(z|x,θ(t))of the latent variables. The M-step selects an updated set\\nof model parameters θ(t+1)by maximizing (11.73b).\\nAlthough an EM iteration does increase the log-likelihood, there are\\nno guarantees that EM converges to the maximum likelihood solution.\\nIt is possible that the EM algorithm converges to a local maximum of\\nthe log-likelihood. Different initializations of the parameters θcould be\\nused in multiple EM runs to reduce the risk of ending up in a bad local\\noptimum. We do not go into further details here, but refer to the excellent\\nexpositions by Rogers and Girolami (2016) and Bishop (2006).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff2757e2-84b4-42f8-8790-01d32bdd94a5', embedding=None, metadata={'page_label': '368', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='368 Density Estimation with Gaussian Mixture Models\\n11.5 Further Reading\\nThe GMM can be considered a generative model in the sense that it is\\nstraightforward to generate new data using ancestral sampling (Bishop,\\n2006). For given GMM parameters πk,µk,Σk,k= 1,...,K , we sample\\nan indexkfrom the probability vector [π1,...,πK]⊤and then sample a\\ndata pointx∼N(µk,Σk)\\n. If we repeat this Ntimes, we obtain a dataset\\nthat has been generated by a GMM. Figure 11.1 was generated using this\\nprocedure.\\nThroughout this chapter, we assumed that the number of components\\nKis known. In practice, this is often not the case. However, we could use\\nnested cross-validation, as discussed in Section 8.6.1, to ﬁnd good models.\\nGaussian mixture models are closely related to the K-means clustering\\nalgorithm.K-means also uses the EM algorithm to assign data points to\\nclusters. If we treat the means in the GMM as cluster centers and ignore\\nthe covariances (or set them to I), we arrive at K-means. As also nicely\\ndescribed by MacKay (2003), K-means makes a “hard” assignment of data\\npoints to cluster centers µk, whereas a GMM makes a “soft” assignment\\nvia the responsibilities.\\nWe only touched upon the latent-variable perspective of GMMs and the\\nEM algorithm. Note that EM can be used for parameter learning in general\\nlatent-variable models, e.g., nonlinear state-space models (Ghahramani\\nand Roweis, 1999; Roweis and Ghahramani, 1999) and for reinforcement\\nlearning as discussed by Barber (2012). Therefore, the latent-variable per-\\nspective of a GMM is useful to derive the corresponding EM algorithm in\\na principled way (Bishop, 2006; Barber, 2012; Murphy, 2012).\\nWe only discussed maximum likelihood estimation (via the EM algo-\\nrithm) for ﬁnding GMM parameters. The standard criticisms of maximum\\nlikelihood also apply here:\\nAs in linear regression, maximum likelihood can suffer from severe\\noverﬁtting. In the GMM case, this happens when the mean of a mix-\\nture component is identical to a data point and the covariance tends to\\n0. Then, the likelihood approaches inﬁnity. Bishop (2006) and Barber\\n(2012) discuss this issue in detail.\\nWe only obtain a point estimate of the parameters πk,µk,Σkfork=\\n1,...,K , which does not give any indication of uncertainty in the pa-\\nrameter values. A Bayesian approach would place a prior on the param-\\neters, which can be used to obtain a posterior distribution on the param-\\neters. This posterior allows us to compute the model evidence (marginal\\nlikelihood), which can be used for model comparison, which gives us a\\nprincipled way to determine the number of mixture components. Un-\\nfortunately, closed-form inference is not possible in this setting because\\nthere is no conjugate prior for this model. However, approximations,\\nsuch as variational inference, can be used to obtain an approximate\\nposterior (Bishop, 2006).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='24ea33db-e80e-4185-9bc3-99cdd9341ac5', embedding=None, metadata={'page_label': '369', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.5 Further Reading 369\\nFigure 11.13\\nHistogram (orange\\nbars) and kernel\\ndensity estimation\\n(blue line). The\\nkernel density\\nestimator produces\\na smooth estimate\\nof the underlying\\ndensity, whereas the\\nhistogram is an\\nunsmoothed count\\nmeasure of how\\nmany data points\\n(black) fall into a\\nsingle bin.\\n−4−2 0 2 4 6 8\\nx0.000.050.100.150.200.250.30p(x)\\nData\\nKDE\\nHistogram In this chapter, we discussed mixture models for density estimation.\\nThere is a plethora of density estimation techniques available. In practice,\\nwe often use histograms and kernel density estimation. histogram\\nHistograms provide a nonparametric way to represent continuous den-\\nsities and have been proposed by Pearson (1895). A histogram is con-\\nstructed by “binning” the data space and count, how many data points fall\\ninto each bin. Then a bar is drawn at the center of each bin, and the height\\nof the bar is proportional to the number of data points within that bin. The\\nbin size is a critical hyperparameter, and a bad choice can lead to overﬁt-\\nting and underﬁtting. Cross-validation, as discussed in Section 8.2.4, can\\nbe used to determine a good bin size. kernel density\\nestimation Kernel density estimation , independently proposed by Rosenblatt (1956)\\nand Parzen (1962), is a nonparametric way for density estimation. Given\\nNi.i.d. samples, the kernel density estimator represents the underlying\\ndistribution as\\np(x) =1\\nNhN∑\\nn=1k(x−xn\\nh)\\n, (11.74)\\nwherekis a kernel function, i.e., a nonnegative function that integrates to\\n1andh >0is a smoothing/bandwidth parameter, which plays a similar\\nrole as the bin size in histograms. Note that we place a kernel on every\\nsingle data point xnin the dataset. Commonly used kernel functions are\\nthe uniform distribution and the Gaussian distribution. Kernel density esti-\\nmates are closely related to histograms, but by choosing a suitable kernel,\\nwe can guarantee smoothness of the density estimate. Figure 11.13 illus-\\ntrates the difference between a histogram and a kernel density estimator\\n(with a Gaussian-shaped kernel) for a given dataset of 250data points.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ac95dc0-cb76-435d-8688-7f7d32236932', embedding=None, metadata={'page_label': '370', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12\\nClassiﬁcation with Support Vector Machines\\nIn many situations, we want our machine learning algorithm to predict\\none of a number of (discrete) outcomes. For example, an email client sorts\\nmail into personal mail and junk mail, which has two outcomes. Another\\nexample is a telescope that identiﬁes whether an object in the night sky\\nis a galaxy, star, or planet. There are usually a small number of outcomes,\\nand more importantly there is usually no additional structure on these\\noutcomes. In this chapter, we consider predictors that output binary val- An example of\\nstructure is if the\\noutcomes were\\nordered, like in the\\ncase of small,\\nmedium, and large\\nt-shirts.ues, i.e., there are only two possible outcomes. This machine learning task\\nis called binary classiﬁcation . This is in contrast to Chapter 9, where we\\nbinary classiﬁcationconsidered a prediction problem with continuous-valued outputs.\\nFor binary classiﬁcation, the set of possible values that the label/output\\ncan attain is binary, and for this chapter we denote them by {+1,−1}. In\\nother words, we consider predictors of the form\\nf:RD→{+1,−1}. (12.1)\\nRecall from Chapter 8 that we represent each example (data point) xn\\nas a feature vector of Dreal numbers. The labels are often referred to as Input example xn\\nmay also be referred\\nto as inputs, data\\npoints, features, or\\ninstances.the positive and negative classes , respectively. One should be careful not\\nclassto infer intuitive attributes of positiveness of the +1class. For example,\\nin a cancer detection task, a patient with cancer is often labeled +1. In\\nprinciple, any two distinct values can be used, e.g., {True,False},{0,1}\\nor{red,blue}. The problem of binary classiﬁcation is well studied, and For probabilistic\\nmodels, it is\\nmathematically\\nconvenient to use\\n{0,1}as a binary\\nrepresentation; see\\nthe remark after\\nExample 6.12.we defer a survey of other approaches to Section 12.6.\\nWe present an approach known as the support vector machine (SVM),\\nwhich solves the binary classiﬁcation task. As in regression, we have a su-\\npervised learning task, where we have a set of examples xn∈RDalong\\nwith their corresponding (binary) labels yn∈{+1,−1}. Given a train-\\ning data set consisting of example–label pairs {(x1,y1),..., (xN,yN)}, we\\nwould like to estimate parameters of the model that will give the smallest\\nclassiﬁcation error. Similar to Chapter 9, we consider a linear model, and\\nhide away the nonlinearity in a transformation φof the examples (9.13).\\nWe will revisit φin Section 12.4.\\nThe SVM provides state-of-the-art results in many applications, with\\nsound theoretical guarantees (Steinwart and Christmann, 2008). There\\nare two main reasons why we chose to illustrate binary classiﬁcation using\\n370\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='225923f6-f057-460c-94c0-5385141d3a8e', embedding=None, metadata={'page_label': '371', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Classiﬁcation with Support Vector Machines 371\\nFigure 12.1\\nExample 2D data,\\nillustrating the\\nintuition of data\\nwhere we can ﬁnd a\\nlinear classiﬁer that\\nseparates orange\\ncrosses from blue\\ndiscs.\\nx(1)x(2)\\nSVMs. First, the SVM allows for a geometric way to think about supervised\\nmachine learning. While in Chapter 9 we considered the machine learning\\nproblem in terms of probabilistic models and attacked it using maximum\\nlikelihood estimation and Bayesian inference, here we will consider an\\nalternative approach where we reason geometrically about the machine\\nlearning task. It relies heavily on concepts, such as inner products and\\nprojections, which we discussed in Chapter 3. The second reason why we\\nﬁnd SVMs instructive is that in contrast to Chapter 9, the optimization\\nproblem for SVM does not admit an analytic solution so that we need to\\nresort to a variety of optimization tools introduced in Chapter 7.\\nThe SVM view of machine learning is subtly different from the max-\\nimum likelihood view of Chapter 9. The maximum likelihood view pro-\\nposes a model based on a probabilistic view of the data distribution, from\\nwhich an optimization problem is derived. In contrast, the SVM view starts\\nby designing a particular function that is to be optimized during training,\\nbased on geometric intuitions. We have seen something similar already\\nin Chapter 10, where we derived PCA from geometric principles. In the\\nSVM case, we start by designing a loss function that is to be minimized\\non training data, following the principles of empirical risk minimization\\n(Section 8.2).\\nLet us derive the optimization problem corresponding to training an\\nSVM on example–label pairs. Intuitively, we imagine binary classiﬁcation\\ndata, which can be separated by a hyperplane as illustrated in Figure 12.1.\\nHere, every example xn(a vector of dimension 2) is a two-dimensional\\nlocation (x(1)\\nnandx(2)\\nn), and the corresponding binary label ynis one of\\ntwo different symbols (orange cross or blue disc). “Hyperplane” is a word\\nthat is commonly used in machine learning, and we encountered hyper-\\nplanes already in Section 2.8. A hyperplane is an afﬁne subspace of di-\\nmensionD−1(if the corresponding vector space is of dimension D).\\nThe examples consist of two classes (there are two possible labels) that\\nhave features (the components of the vector representing the example)\\narranged in such a way as to allow us to separate/classify them by draw-\\ning a straight line.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee91bf5d-38d3-4a42-8534-96900052168a', embedding=None, metadata={'page_label': '372', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='372 Classiﬁcation with Support Vector Machines\\nIn the following, we formalize the idea of ﬁnding a linear separator\\nof the two classes. We introduce the idea of the margin and then extend\\nlinear separators to allow for examples to fall on the “wrong” side, incur-\\nring a classiﬁcation error. We present two equivalent ways of formalizing\\nthe SVM: the geometric view (Section 12.2.4) and the loss function view\\n(Section 12.2.5). We derive the dual version of the SVM using Lagrange\\nmultipliers (Section 7.2). The dual SVM allows us to observe a third way\\nof formalizing the SVM: in terms of the convex hulls of the examples of\\neach class (Section 12.3.2). We conclude by brieﬂy describing kernels and\\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\\n12.1 Separating Hyperplanes\\nGiven two examples represented as vectors xiandxj, one way to compute\\nthe similarity between them is using an inner product ⟨xi,xj⟩. Recall from\\nSection 3.2 that inner products are closely related to the angle between\\ntwo vectors. The value of the inner product between two vectors depends\\non the length (norm) of each vector. Furthermore, inner products allow\\nus to rigorously deﬁne geometric concepts such as orthogonality and pro-\\njections.\\nThe main idea behind many classiﬁcation algorithms is to represent\\ndata in RDand then partition this space, ideally in a way that examples\\nwith the same label (and no other examples) are in the same partition.\\nIn the case of binary classiﬁcation, the space would be divided into two\\nparts corresponding to the positive and negative classes, respectively. We\\nconsider a particularly convenient partition, which is to (linearly) split\\nthe space into two halves using a hyperplane. Let example x∈RDbe an\\nelement of the data space. Consider a function\\nf:RD→R (12.2a)\\nx↦→f(x) :=⟨w,x⟩+b, (12.2b)\\nparametrized by w∈RDandb∈R. Recall from Section 2.8 that hy-\\nperplanes are afﬁne subspaces. Therefore, we deﬁne the hyperplane that\\nseparates the two classes in our binary classiﬁcation problem as\\n{x∈RD:f(x) = 0}. (12.3)\\nAn illustration of the hyperplane is shown in Figure 12.2, where the\\nvectorwis a vector normal to the hyperplane and bthe intercept. We can\\nderive thatwis a normal vector to the hyperplane in (12.3) by choosing\\nany two examples xaandxbon the hyperplane and showing that the\\nvector between them is orthogonal to w. In the form of an equation,\\nf(xa)−f(xb) =⟨w,xa⟩+b−(⟨w,xb⟩+b) (12.4a)\\n=⟨w,xa−xb⟩, (12.4b)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3da8b982-2073-4a1d-bae8-3b1b7d51e125', embedding=None, metadata={'page_label': '373', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.1 Separating Hyperplanes 373\\nFigure 12.2\\nEquation of a\\nseparating\\nhyperplane (12.3).\\n(a) The standard\\nway of representing\\nthe equation in 3D.\\n(b) For ease of\\ndrawing, we look at\\nthe hyperplane edge\\non.w\\n(a) Separating hyperplane in 3Dw\\n.\\n0.Positive\\n.\\nNegativeb\\n(b) Projection of the setting in (a) onto\\na plane\\nwhere the second line is obtained by the linearity of the inner product\\n(Section 3.2). Since we have chosen xaandxbto be on the hyperplane,\\nthis implies that f(xa) = 0 andf(xb) = 0 and hence⟨w,xa−xb⟩= 0.\\nRecall that two vectors are orthogonal when their inner product is zero. wis orthogonal to\\nany vector on the\\nhyperplane.Therefore, we obtain that wis orthogonal to any vector on the hyperplane.\\nRemark. Recall from Chapter 2 that we can think of vectors in different\\nways. In this chapter, we think of the parameter vector was an arrow\\nindicating a direction, i.e., we consider wto be a geometric vector. In\\ncontrast, we think of the example vector xas a data point (as indicated\\nby its coordinates), i.e., we consider xto be the coordinates of a vector\\nwith respect to the standard basis. ♦\\nWhen presented with a test example, we classify the example as pos-\\nitive or negative depending on the side of the hyperplane on which it\\noccurs. Note that (12.3) not only deﬁnes a hyperplane; it additionally de-\\nﬁnes a direction. In other words, it deﬁnes the positive and negative side\\nof the hyperplane. Therefore, to classify a test example xtest, we calcu-\\nlate the value of the function f(xtest)and classify the example as +1if\\nf(xtest)⩾0and−1otherwise. Thinking geometrically, the positive ex-\\namples lie “above” the hyperplane and the negative examples “below” the\\nhyperplane.\\nWhen training the classiﬁer, we want to ensure that the examples with\\npositive labels are on the positive side of the hyperplane, i.e.,\\n⟨w,xn⟩+b⩾0 whenyn= +1 (12.5)\\nand the examples with negative labels are on the negative side, i.e.,\\n⟨w,xn⟩+b<0 whenyn=−1. (12.6)\\nRefer to Figure 12.2 for a geometric intuition of positive and negative\\nexamples. These two conditions are often presented in a single equation\\nyn(⟨w,xn⟩+b)⩾0. (12.7)\\nEquation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\\nsides of (12.5) and (12.6) with yn= 1andyn=−1, respectively.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fd9654e-aef1-4529-b789-09513b2ffc17', embedding=None, metadata={'page_label': '374', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='374 Classiﬁcation with Support Vector Machines\\nFigure 12.3\\nPossible separating\\nhyperplanes. There\\nare many linear\\nclassiﬁers (green\\nlines) that separate\\norange crosses from\\nblue discs.\\nx(1)x(2)\\n12.2 Primal Support Vector Machine\\nBased on the concept of distances from points to a hyperplane, we now\\nare in a position to discuss the support vector machine. For a dataset\\n{(x1,y1),..., (xN,yN)}that is linearly separable, we have inﬁnitely many\\ncandidate hyperplanes (refer to Figure 12.3), and therefore classiﬁers,\\nthat solve our classiﬁcation problem without any (training) errors. To ﬁnd\\na unique solution, one idea is to choose the separating hyperplane that\\nmaximizes the margin between the positive and negative examples. In\\nother words, we want the positive and negative examples to be separated\\nby a large margin (Section 12.2.1). In the following, we compute the dis- A classiﬁer with\\nlarge margin turns\\nout to generalize\\nwell (Steinwart and\\nChristmann, 2008).tance between an example and a hyperplane to derive the margin. Recall\\nthat the closest point on the hyperplane to a given point (example xn) is\\nobtained by the orthogonal projection (Section 3.8).\\n12.2.1 Concept of the Margin\\nThe concept of the margin is intuitively simple: It is the distance of the margin\\nseparating hyperplane to the closest examples in the dataset, assuming There could be two\\nor more closest\\nexamples to a\\nhyperplane.that the dataset is linearly separable. However, when trying to formalize\\nthis distance, there is a technical wrinkle that may be confusing. The tech-\\nnical wrinkle is that we need to deﬁne a scale at which to measure the\\ndistance. A potential scale is to consider the scale of the data, i.e., the raw\\nvalues ofxn. There are problems with this, as we could change the units\\nof measurement of xnand change the values in xn, and, hence, change\\nthe distance to the hyperplane. As we will see shortly, we deﬁne the scale\\nbased on the equation of the hyperplane (12.3) itself.\\nConsider a hyperplane ⟨w,x⟩+b, and an example xaas illustrated in\\nFigure 12.4. Without loss of generality, we can consider the example xa\\nto be on the positive side of the hyperplane, i.e., ⟨w,xa⟩+b > 0. We\\nwould like to compute the distance r >0ofxafrom the hyperplane. We\\ndo so by considering the orthogonal projection (Section 3.8) of xaonto\\nthe hyperplane, which we denote by x′\\na. Sincewis orthogonal to the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4eee74e6-c55c-460c-bd45-53335497ccef', embedding=None, metadata={'page_label': '375', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 Primal Support Vector Machine 375\\nFigure 12.4 Vector\\naddition to express\\ndistance to\\nhyperplane:\\nxa=x′\\na+rw\\n∥w∥.\\n.0.xa\\nw.x′\\nar\\nhyperplane, we know that the distance ris just a scaling of this vector w.\\nIf the length of wis known, then we can use this scaling factor rfactor\\nto work out the absolute distance between xaandx′\\na. For convenience,\\nwe choose to use a vector of unit length (its norm is 1) and obtain this\\nby dividingwby its norm,w\\n∥w∥. Using vector addition (Section 2.4), we\\nobtain\\nxa=x′\\na+rw\\n∥w∥. (12.8)\\nAnother way of thinking about ris that it is the coordinate of xain the\\nsubspace spanned by w/∥w∥. We have now expressed the distance of xa\\nfrom the hyperplane as r, and if we choose xato be the point closest to\\nthe hyperplane, this distance ris the margin.\\nRecall that we would like the positive examples to be further than r\\nfrom the hyperplane, and the negative examples to be further than dis-\\ntancer(in the negative direction) from the hyperplane. Analogously to\\nthe combination of (12.5) and (12.6) into (12.7), we formulate this ob-\\njective as\\nyn(⟨w,xn⟩+b)⩾r. (12.9)\\nIn other words, we combine the requirements that examples are at least\\nraway from the hyperplane (in the positive and negative direction) into\\none single inequality.\\nSince we are interested only in the direction, we add an assumption to\\nour model that the parameter vector wis of unit length, i.e., ∥w∥= 1,\\nwhere we use the Euclidean norm ∥w∥=√\\nw⊤w(Section 3.1). This We will see other\\nchoices of inner\\nproducts\\n(Section 3.2) in\\nSection 12.4.assumption also allows a more intuitive interpretation of the distance r\\n(12.8) since it is the scaling factor of a vector of length 1.\\nRemark. A reader familiar with other presentations of the margin would\\nnotice that our deﬁnition of ∥w∥= 1 is different from the standard\\npresentation if the SVM was the one provided by Sch ¨olkopf and Smola\\n(2002), for example. In Section 12.2.3, we will show the equivalence of\\nboth approaches. ♦\\nCollecting the three requirements into a single constrained optimization\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='08af7a25-1c09-42c9-a10c-49724e1a40d9', embedding=None, metadata={'page_label': '376', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='376 Classiﬁcation with Support Vector Machines\\nFigure 12.5\\nDerivation of the\\nmargin:r=1\\n∥w∥..xa\\nw\\n⟨w,x⟩+\\nb= 0⟨w,x⟩+\\nb= 1.x′\\nar\\nproblem, we obtain the objective\\nmax\\nw,b,rr\\ued19\\ued18\\ued17\\ued1a\\nmargin\\nsubject to yn(⟨w,xn⟩+b)⩾r\\ued19\\ued18\\ued17\\ued1a\\ndata ﬁtting,∥w∥= 1\\ued19\\ued18\\ued17\\ued1a\\nnormalization, r> 0,(12.10)\\nwhich says that we want to maximize the margin rwhile ensuring that\\nthe data lies on the correct side of the hyperplane.\\nRemark. The concept of the margin turns out to be highly pervasive in ma-\\nchine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\\nto show that when the margin is large, the “complexity” of the function\\nclass is low, and hence learning is possible (Vapnik, 2000). It turns out\\nthat the concept is useful for various different approaches for theoret-\\nically analyzing generalization error (Steinwart and Christmann, 2008;\\nShalev-Shwartz and Ben-David, 2014). ♦\\n12.2.2 Traditional Derivation of the Margin\\nIn the previous section, we derived (12.10) by making the observation that\\nwe are only interested in the direction of wand not its length, leading to\\nthe assumption that ∥w∥= 1. In this section, we derive the margin max-\\nimization problem by making a different assumption. Instead of choosing\\nthat the parameter vector is normalized, we choose a scale for the data.\\nWe choose this scale such that the value of the predictor ⟨w,x⟩+bis1at\\nthe closest example. Let us also denote the example in the dataset that is Recall that we\\ncurrently consider\\nlinearly separable\\ndata.closest to the hyperplane by xa.\\nFigure 12.5 is identical to Figure 12.4, except that now we rescaled the\\naxes, such that the example xalies exactly on the margin, i.e., ⟨w,xa⟩+\\nb= 1. Sincex′\\nais the orthogonal projection of xaonto the hyperplane, it\\nmust by deﬁnition lie on the hyperplane, i.e.,\\n⟨w,x′\\na⟩+b= 0. (12.11)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1248af50-154e-4bb6-8275-cf810fc07281', embedding=None, metadata={'page_label': '377', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 Primal Support Vector Machine 377\\nBy substituting (12.8) into (12.11), we obtain\\n⟨\\nw,xa−rw\\n∥w∥⟩\\n+b= 0. (12.12)\\nExploiting the bilinearity of the inner product (see Section 3.2), we get\\n⟨w,xa⟩+b−r⟨w,w⟩\\n∥w∥= 0. (12.13)\\nObserve that the ﬁrst term is 1by our assumption of scale, i.e., ⟨w,xa⟩+\\nb= 1. From (3.16) in Section 3.1, we know that ⟨w,w⟩=∥w∥2. Hence,\\nthe second term reduces to r∥w∥. Using these simpliﬁcations, we obtain\\nr=1\\n∥w∥. (12.14)\\nThis means we derived the distance rin terms of the normal vector w\\nof the hyperplane. At ﬁrst glance, this equation is counterintuitive as we We can also think of\\nthe distance as the\\nprojection error that\\nincurs when\\nprojectingxaonto\\nthe hyperplane.seem to have derived the distance from the hyperplane in terms of the\\nlength of the vector w, but we do not yet know this vector. One way to\\nthink about it is to consider the distance rto be a temporary variable\\nthat we only use for this derivation. Therefore, for the rest of this section\\nwe will denote the distance to the hyperplane by1\\n∥w∥. In Section 12.2.3,\\nwe will see that the choice that the margin equals 1is equivalent to our\\nprevious assumption of ∥w∥= 1in Section 12.2.1.\\nSimilar to the argument to obtain (12.9), we want the positive and\\nnegative examples to be at least 1away from the hyperplane, which yields\\nthe condition\\nyn(⟨w,xn⟩+b)⩾1. (12.15)\\nCombining the margin maximization with the fact that examples need to\\nbe on the correct side of the hyperplane (based on their labels) gives us\\nmax\\nw,b1\\n∥w∥(12.16)\\nsubject toyn(⟨w,xn⟩+b)⩾1 for all n= 1,...,N. (12.17)\\nInstead of maximizing the reciprocal of the norm as in (12.16), we often\\nminimize the squared norm. We also often include a constant1\\n2that does The squared norm\\nresults in a convex\\nquadratic\\nprogramming\\nproblem for the\\nSVM (Section 12.5).not affect the optimal w,bbut yields a tidier form when we compute the\\ngradient. Then, our objective becomes\\nmin\\nw,b1\\n2∥w∥2(12.18)\\nsubject toyn(⟨w,xn⟩+b)⩾1 for all n= 1,...,N. (12.19)\\nEquation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\\nexpression “hard” is because the formulation does not allow for any vi-\\nolations of the margin condition. We will see in Section 12.2.4 that this\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8c90c89a-9795-4e5a-bcda-076d12eca256', embedding=None, metadata={'page_label': '378', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='378 Classiﬁcation with Support Vector Machines\\n“hard” condition can be relaxed to accommodate violations if the data is\\nnot linearly separable.\\n12.2.3 Why We Can Set the Margin to 1\\nIn Section 12.2.1, we argued that we would like to maximize some value\\nr, which represents the distance of the closest example to the hyperplane.\\nIn Section 12.2.2, we scaled the data such that the closest example is of\\ndistance 1to the hyperplane. In this section, we relate the two derivations,\\nand show that they are equivalent.\\nTheorem 12.1. Maximizing the margin r, where we consider normalized\\nweights as in (12.10) ,\\nmax\\nw,b,rr\\ued19\\ued18\\ued17\\ued1a\\nmargin\\nsubject to yn(⟨w,xn⟩+b)⩾r\\ued19\\ued18\\ued17\\ued1a\\ndata ﬁtting,∥w∥= 1\\ued19\\ued18\\ued17\\ued1a\\nnormalization, r> 0,(12.20)\\nis equivalent to scaling the data, such that the margin is unity:\\nmin\\nw,b1\\n2∥w∥2\\n\\ued19\\ued18\\ued17\\ued1a\\nmargin\\nsubject to yn(⟨w,xn⟩+b)⩾1\\ued19\\ued18\\ued17\\ued1a\\ndata ﬁtting.(12.21)\\nProof Consider (12.20). Since the square is a strictly monotonic trans-\\nformation for non-negative arguments, the maximum stays the same if we\\nconsiderr2in the objective. Since ∥w∥= 1 we can reparametrize the\\nequation with a new weight vector w′that is not normalized by explicitly\\nusingw′\\n∥w′∥. We obtain\\nmax\\nw′,b,rr2\\nsubject to yn(⟨w′\\n∥w′∥,xn⟩\\n+b)\\n⩾r, r> 0.(12.22)\\nEquation (12.22) explicitly states that the distance ris positive. Therefore,\\nwe can divide the ﬁrst constraint by r, which yields Note thatr>0\\nbecause we\\nassumed linear\\nseparability, and\\nhence there is no\\nissue to divide by r.max\\nw′,b,rr2\\nsubject to yn\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ed⟨\\nw′\\n∥w′∥r\\ued19\\ued18\\ued17\\ued1a\\nw′′,xn⟩\\n+b\\nr\\ued19\\ued18\\ued17\\ued1a\\nb′′\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8⩾1, r> 0(12.23)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='31cb924e-7791-41c3-82ec-0b1674d28f1b', embedding=None, metadata={'page_label': '379', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 Primal Support Vector Machine 379\\nFigure 12.6\\n(a) Linearly\\nseparable and\\n(b) non-linearly\\nseparable data.\\nx(1)x(2)\\n(a) Linearly separable data, with a large\\nmargin\\nx(1)x(2)(b) Non-linearly separable data\\nrenaming the parameters to w′′andb′′. Sincew′′=w′\\n∥w′∥r, rearranging for\\nrgives\\n∥w′′∥=\\ued79\\ued79\\ued79\\ued79w′\\n∥w′∥r\\ued79\\ued79\\ued79\\ued79=1\\nr·\\ued79\\ued79\\ued79\\ued79w′\\n∥w′∥\\ued79\\ued79\\ued79\\ued79=1\\nr. (12.24)\\nBy substituting this result into (12.23), we obtain\\nmax\\nw′′,b′′1\\n∥w′′∥2\\nsubject to yn(⟨w′′,xn⟩+b′′)⩾1.(12.25)\\nThe ﬁnal step is to observe that maximizing1\\n∥w′′∥2yields the same solution\\nas minimizing1\\n2∥w′′∥2, which concludes the proof of Theorem 12.1.\\n12.2.4 Soft Margin SVM: Geometric View\\nIn the case where data is not linearly separable, we may wish to allow\\nsome examples to fall within the margin region, or even to be on the\\nwrong side of the hyperplane as illustrated in Figure 12.6.\\nThe model that allows for some classiﬁcation errors is called the soft soft margin SVM\\nmargin SVM . In this section, we derive the resulting optimization problem\\nusing geometric arguments. In Section 12.2.5, we will derive an equiv-\\nalent optimization problem using the idea of a loss function. Using La-\\ngrange multipliers (Section 7.2), we will derive the dual optimization\\nproblem of the SVM in Section 12.3. This dual optimization problem al-\\nlows us to observe a third interpretation of the SVM: as a hyperplane that\\nbisects the line between convex hulls corresponding to the positive and\\nnegative data examples (Section 12.3.2).\\nThe key geometric idea is to introduce a slack variable ξncorresponding slack variable\\nto each example–label pair (xn,yn)that allows a particular example to be\\nwithin the margin or even on the wrong side of the hyperplane (refer to\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ada545f1-fca2-43b8-878f-2ae305fa1259', embedding=None, metadata={'page_label': '380', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='380 Classiﬁcation with Support Vector Machines\\nFigure 12.7 Soft\\nmargin SVM allows\\nexamples to be\\nwithin the margin or\\non the wrong side of\\nthe hyperplane. The\\nslack variable ξ\\nmeasures the\\ndistance of a\\npositive example\\nx+to the positive\\nmargin hyperplane\\n⟨w,x⟩+b= 1\\nwhenx+is on the\\nwrong side..x+w\\n⟨w,x⟩+\\nb= 0⟨w,x⟩+\\nb= 1.\\nξ\\nFigure 12.7). We subtract the value of ξnfrom the margin, constraining\\nξnto be non-negative. To encourage correct classiﬁcation of the samples,\\nwe addξnto the objective\\nmin\\nw,b,ξ1\\n2∥w∥2+CN∑\\nn=1ξn (12.26a)\\nsubject to yn(⟨w,xn⟩+b)⩾1−ξn (12.26b)\\nξn⩾0 (12.26c)\\nforn= 1,...,N . In contrast to the optimization problem (12.18) for the\\nhard margin SVM, this one is called the soft margin SVM . The parameter soft margin SVM\\nC > 0trades off the size of the margin and the total amount of slack that\\nwe have. This parameter is called the regularization parameter since, as regularization\\nparameter we will see in the following section, the margin term in the objective func-\\ntion (12.26a) is a regularization term. The margin term ∥w∥2is called\\ntheregularizer , and in many books on numerical optimization, the reg- regularizer\\nularization parameter is multiplied with this term (Section 8.2.3). This\\nis in contrast to our formulation in this section. Here a large value of C\\nimplies low regularization, as we give the slack variables larger weight,\\nhence giving more priority to examples that do not lie on the correct side\\nof the margin. There are\\nalternative\\nparametrizations of\\nthis regularization,\\nwhich is\\nwhy (12.26a) is also\\noften referred to as\\ntheC-SVM.Remark. In the formulation of the soft margin SVM (12.26a) wis reg-\\nularized, but bis not regularized. We can see this by observing that the\\nregularization term does not contain b. The unregularized term bcom-\\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\\nand decreases computational efﬁciency (Fan et al., 2008). ♦\\n12.2.5 Soft Margin SVM: Loss Function View\\nLet us consider a different approach for deriving the SVM, following the\\nprinciple of empirical risk minimization (Section 8.2). For the SVM, we\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2eaff39c-b86f-4305-89b1-c97c2e762c0d', embedding=None, metadata={'page_label': '381', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.2 Primal Support Vector Machine 381\\nchoose hyperplanes as the hypothesis class, that is\\nf(x) =⟨w,x⟩+b. (12.27)\\nWe will see in this section that the margin corresponds to the regulariza-\\ntion term. The remaining question is, what is the loss function ? In con- loss function\\ntrast to Chapter 9, where we consider regression problems (the output\\nof the predictor is a real number), in this chapter, we consider binary\\nclassiﬁcation problems (the output of the predictor is one of two labels\\n{+1,−1}). Therefore, the error/loss function for each single example–\\nlabel pair needs to be appropriate for binary classiﬁcation. For example,\\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\\nnary classiﬁcation.\\nRemark. The ideal loss function between binary labels is to count the num-\\nber of mismatches between the prediction and the label. This means that\\nfor a predictor fapplied to an example xn, we compare the output f(xn)\\nwith the label yn. We deﬁne the loss to be zero if they match, and one if\\nthey do not match. This is denoted by 1(f(xn)̸=yn)and is called the\\nzero-one loss . Unfortunately, the zero-one loss results in a combinatorial zero-one loss\\noptimization problem for ﬁnding the best parameters w,b. Combinatorial\\noptimization problems (in contrast to continuous optimization problems\\ndiscussed in Chapter 7) are in general more challenging to solve. ♦\\nWhat is the loss function corresponding to the SVM? Consider the error\\nbetween the output of a predictor f(xn)and the label yn. The loss de-\\nscribes the error that is made on the training data. An equivalent way to\\nderive (12.26a) is to use the hinge loss hinge loss\\nℓ(t) = max{0,1−t}wheret=yf(x) =y(⟨w,x⟩+b).(12.28)\\nIff(x)is on the correct side (based on the corresponding label y) of the\\nhyperplane, and further than distance 1, this means that t⩾1and the\\nhinge loss returns a value of zero. If f(x)is on the correct side but too\\nclose to the hyperplane ( 0<t< 1), the example xis within the margin,\\nand the hinge loss returns a positive value. When the example is on the\\nwrong side of the hyperplane ( t<0), the hinge loss returns an even larger\\nvalue, which increases linearly. In other words, we pay a penalty once we\\nare closer than the margin to the hyperplane, even if the prediction is\\ncorrect, and the penalty increases linearly. An alternative way to express\\nthe hinge loss is by considering it as two linear pieces\\nℓ(t) ={\\n0 ift⩾1\\n1−tift<1, (12.29)\\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\\nSVM 12.18 is deﬁned as\\nℓ(t) ={\\n0 ift⩾1\\n∞ift<1. (12.30)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bece4c1b-6927-4063-8c58-6f6940574bf1', embedding=None, metadata={'page_label': '382', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='382 Classiﬁcation with Support Vector Machines\\nFigure 12.8 The\\nhinge loss is a\\nconvex upper bound\\nof zero-one loss.\\n−2 0 2\\nt024max{0,1−t}Zero-one loss\\nHinge loss\\nThis loss can be interpreted as never allowing any examples inside the\\nmargin.\\nFor a given training set {(x1,y1),..., (xN,yN)}, we seek to minimize\\nthe total loss, while regularizing the objective with ℓ2-regularization (see\\nSection 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\\noptimization problem\\nmin\\nw,b1\\n2∥w∥2\\n\\ued19\\ued18\\ued17\\ued1a\\nregularizer+CN∑\\nn=1max{0,1−yn(⟨w,xn⟩+b)}\\n\\ued19 \\ued18\\ued17 \\ued1a\\nerror term. (12.31)\\nThe ﬁrst term in (12.31) is called the regularization term or the regularizer regularizer\\n(see Section 8.2.3), and the second term is called the loss term or the error loss term\\nerror termterm. Recall from Section 12.2.4 that the term1\\n2∥w∥2arises directly from\\nthe margin. In other words, margin maximization can be interpreted as\\nregularization . regularization\\nIn principle, the unconstrained optimization problem in (12.31) can\\nbe directly solved with (sub-)gradient descent methods as described in\\nSection 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\\nthe hinge loss (12.28) essentially consists of two linear parts, as expressed\\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\\nWe can equivalently replace minimization of the hinge loss over twith a\\nminimization of a slack variable ξwith two constraints. In equation form,\\nmin\\ntmax{0,1−t} (12.32)\\nis equivalent to\\nmin\\nξ,tξ\\nsubject to ξ⩾0, ξ⩾1−t.(12.33)\\nBy substituting this expression into (12.31) and rearranging one of the\\nconstraints, we obtain exactly the soft margin SVM (12.26a).\\nRemark. Let us contrast our choice of the loss function in this section to the\\nloss function for linear regression in Chapter 9. Recall from Section 9.2.1\\nthat for ﬁnding maximum likelihood estimators, we usually minimize the\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='faa12273-0b1c-464d-b5be-d8be0a928596', embedding=None, metadata={'page_label': '383', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Dual Support Vector Machine 383\\nnegative log-likelihood. Furthermore, since the likelihood term for linear\\nregression with Gaussian noise is Gaussian, the negative log-likelihood for\\neach example is a squared error function. The squared error function is the\\nloss function that is minimized when looking for the maximum likelihood\\nsolution. ♦\\n12.3 Dual Support Vector Machine\\nThe description of the SVM in the previous sections, in terms of the vari-\\nableswandb, is known as the primal SVM. Recall that we consider inputs\\nx∈RDwithDfeatures. Since wis of the same dimension as x, this\\nmeans that the number of parameters (the dimension of w) of the opti-\\nmization problem grows linearly with the number of features.\\nIn the following, we consider an equivalent optimization problem (the\\nso-called dual view), which is independent of the number of features. In-\\nstead, the number of parameters increases with the number of examples\\nin the training set. We saw a similar idea appear in Chapter 10, where we\\nexpressed the learning problem in a way that does not scale with the num-\\nber of features. This is useful for problems where we have more features\\nthan the number of examples in the training dataset. The dual SVM also\\nhas the additional advantage that it easily allows kernels to be applied,\\nas we shall see at the end of this chapter. The word “dual” appears often\\nin mathematical literature, and in this particular case it refers to convex\\nduality. The following subsections are essentially an application of convex\\nduality, which we discussed in Section 7.2.\\n12.3.1 Convex Duality via Lagrange Multipliers\\nRecall the primal soft margin SVM (12.26a). We call the variables w,b,\\nandξcorresponding to the primal SVM the primal variables. We use αn⩾ In Chapter 7, we\\nusedλas Lagrange\\nmultipliers. In this\\nsection, we follow\\nthe notation\\ncommonly chosen in\\nSVM literature, and\\nuseαandγ.0as the Lagrange multiplier corresponding to the constraint (12.26b) that\\nthe examples are classiﬁed correctly and γn⩾0as the Lagrange multi-\\nplier corresponding to the non-negativity constraint of the slack variable;\\nsee (12.26c). The Lagrangian is then given by\\nL(w,b,ξ,α,γ ) =1\\n2∥w∥2+CN∑\\nn=1ξn (12.34)\\n−N∑\\nn=1αn(yn(⟨w,xn⟩+b)−1 +ξn)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nconstraint (12.26b)−N∑\\nn=1γnξn\\n\\ued19\\ued18\\ued17\\ued1a\\nconstraint (12.26c).\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f87b3c84-91ea-4ee7-8a82-a173290c4545', embedding=None, metadata={'page_label': '384', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='384 Classiﬁcation with Support Vector Machines\\nBy differentiating the Lagrangian (12.34) with respect to the three primal\\nvariablesw,b, andξrespectively, we obtain\\n∂L\\n∂w=w⊤−N∑\\nn=1αnynxn⊤, (12.35)\\n∂L\\n∂b=−N∑\\nn=1αnyn, (12.36)\\n∂L\\n∂ξn=C−αn−γn. (12.37)\\nWe now ﬁnd the maximum of the Lagrangian by setting each of these\\npartial derivatives to zero. By setting (12.35) to zero, we ﬁnd\\nw=N∑\\nn=1αnynxn, (12.38)\\nwhich is a particular instance of the representer theorem (Kimeldorf and representer theorem\\nWahba, 1970). Equation (12.38) states that the optimal weight vector in The representer\\ntheorem is actually\\na collection of\\ntheorems saying\\nthat the solution of\\nminimizing\\nempirical risk lies in\\nthe subspace\\n(Section 2.4.3)\\ndeﬁned by the\\nexamples.the primal is a linear combination of the examples xn. Recall from Sec-\\ntion 2.6.1 that this means that the solution of the optimization problem\\nlies in the span of training data. Additionally, the constraint obtained by\\nsetting (12.36) to zero implies that the optimal weight vector is an afﬁne\\ncombination of the examples. The representer theorem turns out to hold\\nfor very general settings of regularized empirical risk minimization (Hof-\\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\\ngeneral versions (Sch ¨olkopf et al., 2001), and necessary and sufﬁcient\\nconditions on its existence can be found in Yu et al. (2013).\\nRemark. The representer theorem (12.38) also provides an explanation\\nof the name “support vector machine.” The examples xn, for which the\\ncorresponding parameters αn= 0, do not contribute to the solution wat\\nall. The other examples, where αn>0, are called support vectors since support vector\\nthey “support” the hyperplane. ♦\\nBy substituting the expression for winto the Lagrangian (12.34), we\\nobtain the dual\\nD(ξ,α,γ ) =1\\n2N∑\\ni=1N∑\\nj=1yiyjαiαj⟨xi,xj⟩−N∑\\ni=1yiαi⟨N∑\\nj=1yjαjxj,xi⟩\\n+CN∑\\ni=1ξi−bN∑\\ni=1yiαi+N∑\\ni=1αi−N∑\\ni=1αiξi−N∑\\ni=1γiξi.\\n(12.39)\\nNote that there are no longer any terms involving the primal variable w.\\nBy setting (12.36) to zero, we obtain∑N\\nn=1ynαn= 0. Therefore, the term\\ninvolvingbalso vanishes. Recall that inner products are symmetric and\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bbfd594-cb23-473f-98a6-df64c7672f89', embedding=None, metadata={'page_label': '385', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Dual Support Vector Machine 385\\nbilinear (see Section 3.2). Therefore, the ﬁrst two terms in (12.39) are\\nover the same objects. These terms (colored blue) can be simpliﬁed, and\\nwe obtain the Lagrangian\\nD(ξ,α,γ ) =−1\\n2N∑\\ni=1N∑\\nj=1yiyjαiαj⟨xi,xj⟩+N∑\\ni=1αi+N∑\\ni=1(C−αi−γi)ξi.\\n(12.40)\\nThe last term in this equation is a collection of all terms that contain slack\\nvariablesξi. By setting (12.37) to zero, we see that the last term in (12.40)\\nis also zero. Furthermore, by using the same equation and recalling that\\nthe Lagrange multiplers γiare non-negative, we conclude that αi⩽C.\\nWe now obtain the dual optimization problem of the SVM, which is ex-\\npressed exclusively in terms of the Lagrange multipliers αi. Recall from\\nLagrangian duality (Deﬁnition 7.1) that we maximize the dual problem.\\nThis is equivalent to minimizing the negative dual problem, such that we\\nend up with the dual SVM dual SVM\\nmin\\nα1\\n2N∑\\ni=1N∑\\nj=1yiyjαiαj⟨xi,xj⟩−N∑\\ni=1αi\\nsubject toN∑\\ni=1yiαi= 0\\n0⩽αi⩽Cfor alli= 1,...,N.(12.41)\\nThe equality constraint in (12.41) is obtained from setting (12.36) to\\nzero. The inequality constraint αi⩾0is the condition imposed on La-\\ngrange multipliers of inequality constraints (Section 7.2). The inequality\\nconstraintαi⩽Cis discussed in the previous paragraph.\\nThe set of inequality constraints in the SVM are called “box constraints”\\nbecause they limit the vector α= [α1,···,αN]⊤∈RNof Lagrange mul-\\ntipliers to be inside the box deﬁned by 0andCon each axis. These\\naxis-aligned boxes are particularly efﬁcient to implement in numerical\\nsolvers (Dost ´al, 2009, chapter 5). It turns out that\\nexamples that lie\\nexactly on the\\nmargin are\\nexamples whose\\ndual parameters lie\\nstrictly inside the\\nbox constraints,\\n0<αi<C. This is\\nderived using the\\nKarush Kuhn Tucker\\nconditions, for\\nexample in\\nSch¨olkopf and\\nSmola (2002).Once we obtain the dual parameters α, we can recover the primal pa-\\nrameterswby using the representer theorem (12.38). Let us call the op-\\ntimal primal parameter w∗. However, there remains the question on how\\nto obtain the parameter b∗. Consider an example xnthat lies exactly on\\nthe margin’s boundary, i.e., ⟨w∗,xn⟩+b=yn. Recall that ynis either +1\\nor−1. Therefore, the only unknown is b, which can be computed by\\nb∗=yn−⟨w∗,xn⟩. (12.42)\\nRemark. In principle, there may be no examples that lie exactly on the\\nmargin. In this case, we should compute |yn−⟨w∗,xn⟩|for all support\\nvectors and take the median value of this absolute value difference to be\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b09a7234-db64-4f9d-91cf-ff9c9e2e641b', embedding=None, metadata={'page_label': '386', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='386 Classiﬁcation with Support Vector Machines\\nFigure 12.9 Convex\\nhulls. (a) Convex\\nhull of points, some\\nof which lie within\\nthe boundary;\\n(b) convex hulls\\naround positive and\\nnegative examples.\\n(a) Convex hull.\\nc\\nd (b) Convex hulls around positive (blue) and\\nnegative (orange) examples. The distance be-\\ntween the two convex sets is the length of the\\ndifference vector c−d.\\nthe value of b∗. A derivation of this can be found in http://fouryears.\\neu/2012/06/07/the-svm-bias-term-conspiracy/ .♦\\n12.3.2 Dual SVM: Convex Hull View\\nAnother approach to obtain the dual SVM is to consider an alternative\\ngeometric argument. Consider the set of examples xnwith the same label.\\nWe would like to build a convex set that contains all the examples such\\nthat it is the smallest possible set. This is called the convex hull and is\\nillustrated in Figure 12.9.\\nLet us ﬁrst build some intuition about a convex combination of points.\\nConsider two points x1andx2and corresponding non-negative weights\\nα1,α2⩾0such thatα1+α2= 1. The equation α1x1+α2x2describes each\\npoint on a line between x1andx2. Consider what happens when we add\\na third point x3along with a weight α3⩾0such that∑3\\nn=1αn= 1.\\nThe convex combination of these three points x1,x2,x3spans a two-\\ndimensional area. The convex hull of this area is the triangle formed by convex hull\\nthe edges corresponding to each pair of of points. As we add more points,\\nand the number of points becomes greater than the number of dimen-\\nsions, some of the points will be inside the convex hull, as we can see in\\nFigure 12.9(a).\\nIn general, building a convex convex hull can be done by introducing\\nnon-negative weights αn⩾0corresponding to each example xn. Then\\nthe convex hull can be described as the set\\nconv (X) ={N∑\\nn=1αnxn}\\nwithN∑\\nn=1αn= 1 andαn⩾0,(12.43)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48a160fa-e088-44ce-864d-e505060b3858', embedding=None, metadata={'page_label': '387', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Dual Support Vector Machine 387\\nfor alln= 1,...,N . If the two clouds of points corresponding to the\\npositive and negative classes are separated, then the convex hulls do not\\noverlap. Given the training data (x1,y1),..., (xN,yN), we form two con-\\nvex hulls, corresponding to the positive and negative classes respectively.\\nWe pick a point c, which is in the convex hull of the set of positive exam-\\nples, and is closest to the negative class distribution. Similarly, we pick a\\npointdin the convex hull of the set of negative examples and is closest to\\nthe positive class distribution; see Figure 12.9(b). We deﬁne a difference\\nvector between dandcas\\nw:=c−d. (12.44)\\nPicking the points canddas in the preceding cases, and requiring them\\nto be closest to each other is equivalent to minimizing the length/norm of\\nw, so that we end up with the corresponding optimization problem\\narg min\\nw∥w∥= arg min\\nw1\\n2∥w∥2. (12.45)\\nSincecmust be in the positive convex hull, it can be expressed as a convex\\ncombination of the positive examples, i.e., for non-negative coefﬁcients\\nα+\\nn\\nc=∑\\nn:yn=+1α+\\nnxn. (12.46)\\nIn (12.46), we use the notation n:yn= +1 to indicate the set of indices\\nnfor whichyn= +1 . Similarly, for the examples with negative labels, we\\nobtain\\nd=∑\\nn:yn=−1α−\\nnxn. (12.47)\\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\\nobjective\\nmin\\nα1\\n2\\ued79\\ued79\\ued79\\ued79\\ued79∑\\nn:yn=+1α+\\nnxn−∑\\nn:yn=−1α−\\nnxn\\ued79\\ued79\\ued79\\ued79\\ued792\\n. (12.48)\\nLetαbe the set of all coefﬁcients, i.e., the concatenation of α+andα−.\\nRecall that we require that for each convex hull that their coefﬁcients sum\\nto one,\\n∑\\nn:yn=+1α+\\nn= 1 and∑\\nn:yn=−1α−\\nn= 1. (12.49)\\nThis implies the constraint\\nN∑\\nn=1ynαn= 0. (12.50)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f7e5c41-578a-4779-aed1-390f2c117ba4', embedding=None, metadata={'page_label': '388', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='388 Classiﬁcation with Support Vector Machines\\nThis result can be seen by multiplying out the individual classes\\nN∑\\nn=1ynαn=∑\\nn:yn=+1(+1)α+\\nn+∑\\nn:yn=−1(−1)α−\\nn (12.51a)\\n=∑\\nn:yn=+1α+\\nn−∑\\nn:yn=−1α−\\nn= 1−1 = 0. (12.51b)\\nThe objective function (12.48) and the constraint (12.50), along with the\\nassumption that α⩾0, give us a constrained (convex) optimization prob-\\nlem. This optimization problem can be shown to be the same as that of\\nthe dual hard margin SVM (Bennett and Bredensteiner, 2000a).\\nRemark. To obtain the soft margin dual, we consider the reduced hull. The\\nreduced hull is similar to the convex hull but has an upper bound to the reduced hull\\nsize of the coefﬁcients α. The maximum possible value of the elements\\nofαrestricts the size that the convex hull can take. In other words, the\\nbound onαshrinks the convex hull to a smaller volume (Bennett and\\nBredensteiner, 2000b). ♦\\n12.4 Kernels\\nConsider the formulation of the dual SVM (12.41). Notice that the in-\\nner product in the objective occurs only between examples xiandxj.\\nThere are no inner products between the examples and the parameters.\\nTherefore, if we consider a set of features φ(xi)to representxi, the only\\nchange in the dual SVM will be to replace the inner product. This mod-\\nularity, where the choice of the classiﬁcation method (the SVM) and the\\nchoice of the feature representation φ(x)can be considered separately,\\nprovides ﬂexibility for us to explore the two problems independently. In\\nthis section, we discuss the representation φ(x)and brieﬂy introduce the\\nidea of kernels, but do not go into the technical details.\\nSinceφ(x)could be a non-linear function, we can use the SVM (which\\nassumes a linear classiﬁer) to construct classiﬁers that are nonlinear in\\nthe examples xn. This provides a second avenue, in addition to the soft\\nmargin, for users to deal with a dataset that is not linearly separable. It\\nturns out that there are many algorithms and statistical methods that have\\nthis property that we observed in the dual SVM: the only inner products\\nare those that occur between examples. Instead of explicitly deﬁning a\\nnon-linear feature map φ(·)and computing the resulting inner product\\nbetween examples xiandxj, we deﬁne a similarity function k(xi,xj)be-\\ntweenxiandxj. For a certain class of similarity functions, called kernels , kernel\\nthe similarity function implicitly deﬁnes a non-linear feature map φ(·).\\nKernels are by deﬁnition functions k:X×X→ Rfor which there exists The inputsXof the\\nkernel function can\\nbe very general and\\nare not necessarily\\nrestricted to RD.a Hilbert spaceHandφ:X→H a feature map such that\\nk(xi,xj) =⟨φ(xi),φ(xj)⟩H. (12.52)\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68f18639-8f26-4685-81c3-a00cb07b2a82', embedding=None, metadata={'page_label': '389', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.4 Kernels 389\\nFigure 12.10 SVM\\nwith different\\nkernels. Note that\\nwhile the decision\\nboundary is\\nnonlinear, the\\nunderlying problem\\nbeing solved is for a\\nlinear separating\\nhyperplane (albeit\\nwith a nonlinear\\nkernel).\\nFirst featureSecond feature\\n(a) SVM with linear kernel\\nFirst featureSecond feature (b) SVM with RBF kernel\\nFirst featureSecond feature\\n(c) SVM with polynomial (degree 2) kernel\\nFirst featureSecond feature (d) SVM with polynomial (degree 3) kernel\\nThere is a unique reproducing kernel Hilbert space associated with every\\nkernelk(Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\\nunique association, φ(x) =k(·,x)is called the canonical feature map .canonical feature\\nmap The generalization from an inner product to a kernel function (12.52) is\\nknown as the kernel trick (Sch¨olkopf and Smola, 2002; Shawe-Taylor and kernel trick\\nCristianini, 2004), as it hides away the explicit non-linear feature map.\\nThe matrixK∈RN×N, resulting from the inner products or the appli-\\ncation ofk(·,·)to a dataset, is called the Gram matrix , and is often just Gram matrix\\nreferred to as the kernel matrix . Kernels must be symmetric and positive kernel matrix\\nsemideﬁnite functions so that every kernel matrix Kis symmetric and\\npositive semideﬁnite (Section 3.2.3):\\n∀z∈RN:z⊤Kz⩾0. (12.53)\\nSome popular examples of kernels for multivariate real-valued data xi∈\\nRDare the polynomial kernel, the Gaussian radial basis function kernel,\\nand the rational quadratic kernel (Sch ¨olkopf and Smola, 2002; Rasmussen\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f78f568-c66e-47de-8c5c-0757ba72e6a2', embedding=None, metadata={'page_label': '390', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='390 Classiﬁcation with Support Vector Machines\\nand Williams, 2006). Figure 12.10 illustrates the effect of different kernels\\non separating hyperplanes on an example dataset. Note that we are still\\nsolving for hyperplanes, that is, the hypothesis class of functions are still\\nlinear. The non-linear surfaces are due to the kernel function.\\nRemark. Unfortunately for the ﬂedgling machine learner, there are mul-\\ntiple meanings of the word “kernel.” In this chapter, the word “kernel”\\ncomes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\\nszajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\\near algebra (Section 2.7.3), where the kernel is another word for the null\\nspace. The third common use of the word “kernel” in machine learning is\\nthe smoothing kernel in kernel density estimation (Section 11.5). ♦\\nSince the explicit representation φ(x)is mathematically equivalent to\\nthe kernel representation k(xi,xj), a practitioner will often design the\\nkernel function such that it can be computed more efﬁciently than the\\ninner product between explicit feature maps. For example, consider the\\npolynomial kernel (Sch ¨olkopf and Smola, 2002), where the number of\\nterms in the explicit expansion grows very quickly (even for polynomials\\nof low degree) when the input dimension is large. The kernel function\\nonly requires one multiplication per input dimension, which can provide\\nsigniﬁcant computational savings. Another example is the Gaussian ra-\\ndial basis function kernel (Sch ¨olkopf and Smola, 2002; Rasmussen and\\nWilliams, 2006), where the corresponding feature space is inﬁnite dimen-\\nsional. In this case, we cannot explicitly represent the feature space but\\ncan still compute similarities between a pair of examples using the kernel. The choice of\\nkernel, as well as\\nthe parameters of\\nthe kernel, is often\\nchosen using nested\\ncross-validation\\n(Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for\\nthe original data to be already represented as multivariate real-valued\\ndata. Note that the inner product is deﬁned on the output of the function\\nφ(·), but does not restrict the input to real numbers. Hence, the function\\nφ(·)and the kernel function k(·,·)can be deﬁned on any object, e.g.,\\nsets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\\nG¨artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\\net al., 2010).\\n12.5 Numerical Solution\\nWe conclude our discussion of SVMs by looking at how to express the\\nproblems derived in this chapter in terms of the concepts presented in\\nChapter 7. We consider two different approaches for ﬁnding the optimal\\nsolution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\\npress this as an unconstrained optimization problem. Then we express the\\nconstrained versions of the primal and dual SVMs as quadratic programs\\nin standard form 7.3.2.\\nConsider the loss function view of the SVM (12.31). This is a convex\\nunconstrained optimization problem, but the hinge loss (12.28) is not dif-\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7679ce3c-1bf9-4a6c-b4d4-d10672172a20', embedding=None, metadata={'page_label': '391', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5 Numerical Solution 391\\nferentiable. Therefore, we apply a subgradient approach for solving it.\\nHowever, the hinge loss is differentiable almost everywhere, except for\\none single point at the hinge t= 1. At this point, the gradient is a set of\\npossible values that lie between 0and−1. Therefore, the subgradient gof\\nthe hinge loss is given by\\ng(t) =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3−1t<1\\n[−1,0]t= 1\\n0t>1. (12.54)\\nUsing this subgradient, we can apply the optimization methods presented\\nin Section 7.1.\\nBoth the primal and the dual SVM result in a convex quadratic pro-\\ngramming problem (constrained optimization). Note that the primal SVM\\nin (12.26a) has optimization variables that have the size of the dimen-\\nsionDof the input examples. The dual SVM in (12.41) has optimization\\nvariables that have the size of the number Nof examples.\\nTo express the primal SVM in the standard form (7.45) for quadratic\\nprogramming, let us assume that we use the dot product (3.5) as the\\ninner product. We rearrange the equation for the primal SVM (12.26a), Recall from\\nSection 3.2 that we\\nuse the phrase dot\\nproduct to mean the\\ninner product on\\nEuclidean vector\\nspace.such that the optimization variables are all on the right and the inequality\\nof the constraint matches the standard form. This yields the optimization\\nmin\\nw,b,ξ1\\n2∥w∥2+CN∑\\nn=1ξn\\nsubject to−ynx⊤\\nnw−ynb−ξn⩽−1\\n−ξn⩽0(12.55)\\nn= 1,...,N . By concatenating the variables w,b,xninto a single vector,\\nand carefully collecting the terms, we obtain the following matrix form of\\nthe soft margin SVM:\\nmin\\nw,b,ξ1\\n2\\uf8ee\\n\\uf8f0w\\nb\\nξ\\uf8f9\\n\\uf8fb⊤[ID 0D,N+1\\n0N+1,D0N+1,N+1]\\uf8ee\\n\\uf8f0w\\nb\\nξ\\uf8f9\\n\\uf8fb+[0D+1,1C1N,1]⊤\\uf8ee\\n\\uf8f0w\\nb\\nξ\\uf8f9\\n\\uf8fb\\nsubject to[−YX−y−IN\\n0N,D+1−IN]\\uf8ee\\n\\uf8f0w\\nb\\nξ\\uf8f9\\n\\uf8fb⩽[−1N,1\\n0N,1]\\n.\\n(12.56)\\nIn the preceding optimization problem, the minimization is over the pa-\\nrameters [w⊤,b,ξ⊤]⊤∈RD+1+N, and we use the notation: Imto rep-\\nresent the identity matrix of size m×m,0m,nto represent the matrix\\nof zeros of size m×n, and 1m,nto represent the matrix of ones of size\\nm×n. In addition, yis the vector of labels [y1,···,yN]⊤,Y= diag(y)\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='069f080d-1de2-49e5-a3bb-8d1846b7de93', embedding=None, metadata={'page_label': '392', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='392 Classiﬁcation with Support Vector Machines\\nis anNbyNmatrix where the elements of the diagonal are from y, and\\nX∈RN×Dis the matrix obtained by concatenating all the examples.\\nWe can similarly perform a collection of terms for the dual version of the\\nSVM (12.41). To express the dual SVM in standard form, we ﬁrst have to\\nexpress the kernel matrix Ksuch that each entry is Kij=k(xi,xj). If we\\nhave an explicit feature representation xithen we deﬁne Kij=⟨xi,xj⟩.\\nFor convenience of notation we introduce a matrix with zeros everywhere\\nexcept on the diagonal, where we store the labels, that is, Y= diag(y).\\nThe dual SVM can be written as\\nmin\\nα1\\n2α⊤YKYα−1⊤\\nN,1α\\nsubject to\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0y⊤\\n−y⊤\\n−IN\\nIN\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fbα⩽[0N+2,1\\nC1N,1]\\n.(12.57)\\nRemark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\\nof the constraints to be inequality constraints. We will express the dual\\nSVM’s equality constraint as two inequality constraints, i.e.,\\nAx=bis replaced by Ax⩽bandAx⩾b. (12.58)\\nParticular software implementations of convex optimization methods may\\nprovide the ability to express equality constraints. ♦\\nSince there are many different possible views of the SVM, there are\\nmany approaches for solving the resulting optimization problem. The ap-\\nproach presented here, expressing the SVM problem in standard convex\\noptimization form, is not often used in practice. The two main implemen-\\ntations of SVM solvers are Chang and Lin (2011) (which is open source)\\nand Joachims (1999). Since SVMs have a clear and well-deﬁned optimiza-\\ntion problem, many approaches based on numerical optimization tech-\\nniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\\nSun, 2011).\\n12.6 Further Reading\\nThe SVM is one of many approaches for studying binary classiﬁcation.\\nOther approaches include the perceptron, logistic regression, Fisher dis-\\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\\nMurphy, 2012). A short tutorial on SVMs and kernels on discrete se-\\nquences can be found in Ben-Hur et al. (2008). The development of SVMs\\nis closely linked to empirical risk minimization, discussed in Section 8.2.\\nHence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\\nwart and Christmann, 2008). The book about kernel methods (Sch ¨olkopf\\nand Smola, 2002) includes many details of support vector machines and\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55b2013b-922c-4194-8cf2-18417dde1181', embedding=None, metadata={'page_label': '393', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.6 Further Reading 393\\nhow to optimize them. A broader book about kernel methods (Shawe-\\nTaylor and Cristianini, 2004) also includes many linear algebra approaches\\nfor different machine learning problems.\\nAn alternative derivation of the dual SVM can be obtained using the\\nidea of the Legendre–Fenchel transform (Section 7.3.3). The derivation\\nconsiders each term of the unconstrained formulation of the SVM (12.31)\\nseparately and calculates their convex conjugates (Rifkin and Lippert,\\n2007). Readers interested in the functional analysis view (also the reg-\\nularization methods view) of SVMs are referred to the work by Wahba\\n(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\\n1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\\ning in linear operators (Akhiezer and Glazman, 1993). The idea of kernels\\nhave been generalized to Banach spaces (Zhang et al., 2009) and Kre ˘ın\\nspaces (Ong et al., 2004; Loosli et al., 2016).\\nObserve that the hinge loss has three equivalent representations, as\\nshown in (12.28) and (12.29), as well as the constrained optimization\\nproblem in (12.33). The formulation (12.28) is often used when compar-\\ning the SVM loss function with other loss functions (Steinwart, 2007).\\nThe two-piece formulation (12.29) is convenient for computing subgra-\\ndients, as each piece is linear. The third formulation (12.33), as seen\\nin Section 12.5, enables the use of convex quadratic programming (Sec-\\ntion 7.3.2) tools.\\nSince binary classiﬁcation is a well-studied task in machine learning,\\nother words are also sometimes used, such as discrimination, separation,\\nand decision. Furthermore, there are three quantities that can be the out-\\nput of a binary classiﬁer. First is the output of the linear function itself\\n(often called the score), which can take any real value. This output can be\\nused for ranking the examples, and binary classiﬁcation can be thought\\nof as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\\ntianini, 2004). The second quantity that is often considered the output\\nof a binary classiﬁer is the output determined after it is passed through\\na non-linear function to constrain its value to a bounded range, for ex-\\nample in the interval [0,1]. A common non-linear function is the sigmoid\\nfunction (Bishop, 2006). When the non-linearity results in well-calibrated\\nprobabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),\\nthis is called class probability estimation. The third output of a binary\\nclassiﬁer is the ﬁnal binary decision {+1,−1}, which is the one most com-\\nmonly assumed to be the output of the classiﬁer.\\nThe SVM is a binary classiﬁer that does not naturally lend itself to a\\nprobabilistic interpretation. There are several approaches for converting\\nthe raw output of the linear function (the score) into a calibrated class\\nprobability estimate ( P(Y= 1|X=x)) that involve an additional cal-\\nibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).\\nFrom the training perspective, there are many related probabilistic ap-\\nproaches. We mentioned at the end of Section 12.2.5 that there is a re-\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dc14780-24c8-47c0-9c89-3dbe873ff717', embedding=None, metadata={'page_label': '394', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='394 Classiﬁcation with Support Vector Machines\\nlationship between loss function and the likelihood (also compare Sec-\\ntions 8.2 and 8.3). The maximum likelihood approach corresponding to\\na well-calibrated transformation during training is called logistic regres-\\nsion, which comes from a class of methods called generalized linear mod-\\nels. Details of logistic regression from this point of view can be found in\\nAgresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).\\nNaturally, one could take a more Bayesian view of the classiﬁer output by\\nestimating a posterior distribution using Bayesian logistic regression. The\\nBayesian view also includes the speciﬁcation of the prior, which includes\\ndesign choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\\nditionally, one could consider latent functions as priors, which results in\\nGaussian process classiﬁcation (Rasmussen and Williams, 2006, chapter\\n3).\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2f980dc-7a05-40d2-a32d-6458b6e4b0c6', embedding=None, metadata={'page_label': '395', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\nAbel, Niels H. 1826. D´emonstration de l’Impossibilit ´e de la R ´esolution Alg ´ebrique des\\n´Equations G ´en´erales qui Passent le Quatri `eme Degr ´e. Grøndahl and Søn.\\nAdhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The\\nFoundations of Data Science . Gitbooks.\\nAgarwal, Arvind, and Daum ´e III, Hal. 2010. A Geometric View of Conjugate Priors.\\nMachine Learning ,81(1), 99–113.\\nAgresti, A. 2002. Categorical Data Analysis . Wiley.\\nAkaike, Hirotugu. 1974. A New Look at the Statistical Model Identiﬁcation. IEEE\\nTransactions on Automatic Control ,19(6), 716–723.\\nAkhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\\nSpace . Dover Publications.\\nAlpaydin, Ethem. 2010. Introduction to Machine Learning . MIT Press.\\nAmari, Shun-ichi. 2016. Information Geometry and Its Applications . Springer.\\nArgyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer\\nTheorems. In: Proceedings of the International Conference on Machine Learning .\\nAronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the Amer-\\nican Mathematical Society ,68, 337–404.\\nAxler, Sheldon. 2015. Linear Algebra Done Right . Springer.\\nBakir, G ¨okhan, Hofmann, Thomas, Sch ¨olkopf, Bernhard, Smola, Alexander J., Taskar,\\nBen, and Vishwanathan, S. V. N. (eds). 2007. Predicting Structured Data . MIT Press.\\nBarber, David. 2012. Bayesian Reasoning and Machine Learning . Cambridge University\\nPress.\\nBarndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical The-\\nory. Wiley.\\nBartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models\\nand Factor Analysis: A Uniﬁed Approach . Wiley.\\nBaydin, Atılım G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.\\n2018. Automatic Differentiation in Machine Learning: A Survey. Journal of Machine\\nLearning Research ,18, 1–43.\\nBeck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\\ndient Methods for Convex Optimization. Operations Research Letters ,31(3), 167–\\n175.\\nBelabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine\\nLearning and New Strategies for Very Large Datasets. Proceedings of the National\\nAcademy of Sciences , 0810600105.\\nBelkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality\\nReduction and Data Representation. Neural Computation ,15(6), 1373–1396.\\nBen-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S ¨oren, Sch ¨olkopf, Bernhard, and R ¨atsch,\\nGunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\\nPLoS Computational Biology ,4(10), e1000173.\\n395\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='737657ec-1afb-4651-89b8-1c81a7892e99', embedding=None, metadata={'page_label': '396', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='396 References\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM\\nClassiﬁers. In: Proceedings of the International Conference on Machine Learning .\\nBennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages\\n132–145 of: Geometry at Work . Mathematical Association of America.\\nBerlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces\\nin Probability and Statistics . Springer.\\nBertsekas, Dimitri P. 1999. Nonlinear Programming . Athena Scientiﬁc.\\nBertsekas, Dimitri P. 2009. Convex Optimization Theory . Athena Scientiﬁc.\\nBickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\\nSelected Topics . Vol. 1. Prentice Hall.\\nBickson, Danny, Dolev, Danny, Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.\\nLinear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Con-\\nference on Communication, Control, and Computing .\\nBillingsley, Patrick. 1995. Probability and Measure . Wiley.\\nBishop, Christopher M. 1995. Neural Networks for Pattern Recognition . Clarendon\\nPress.\\nBishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Pro-\\ncessing Systems .\\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning . Springer.\\nBlei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\\nReview for Statisticians. Journal of the American Statistical Association ,112(518),\\n859–877.\\nBlum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Ma-\\nchine Learning Competitions. In: International Conference on Machine Learning .\\nBonnans, J. Fr ´ed´eric, Gilbert, J. Charles, Lemar ´echal, Claude, and Sagastiz ´abal, Clau-\\ndia A. 2006. Numerical Optimization: Theoretical and Practical Aspects . Springer.\\nBorwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\\nOptimization . 2nd edn. Canadian Mathematical Society.\\nBottou, L ´eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9–42\\nof:Online Learning and Neural Networks . Cambridge University Press.\\nBottou, L ´eon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for\\nLarge-Scale Machine Learning. SIAM Review ,60(2), 223–311.\\nBoucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-\\nequalities: A Nonasymptotic Theory of Independence. Oxford University Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization . Cambridge\\nUniversity Press.\\nBoyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\\nbra. Cambridge University Press.\\nBrochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian\\nOptimization of Expensive Cost Functions, with Application to Active User Modeling\\nand Hierarchical Reinforcement Learning . Tech. rept. TR-2009-023. Department of\\nComputer Science, University of British Columbia.\\nBrooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011. Hand-\\nbook of Markov Chain Monte Carlo . Chapman and Hall/CRC.\\nBrown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\\nplications in Statistical Decision Theory . Institute of Mathematical Statistics.\\nBryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation\\nProcesses. In: Proceedings of the Harvard University Symposium on Digital Computers\\nand Their Applications .\\nBubeck, S ´ebastien. 2015. Convex Optimization: Algorithms and Complexity. Founda-\\ntions and Trends in Machine Learning ,8(3-4), 231–357.\\nB¨uhlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data .\\nSpringer.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14d40718-a7d8-4741-b4a6-4e62614d74c0', embedding=None, metadata={'page_label': '397', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 397\\nBurges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and\\nTrends in Machine Learning ,2(4), 275–365.\\nCarroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in\\nMultidimensional Scaling via an N-Way Generalization of “Eckart-Young” Decom-\\nposition. Psychometrika ,35(3), 283–319.\\nCasella, George, and Berger, Roger L. 2002. Statistical Inference . Duxbury.\\nC ¸inlar, Erhan. 2011. Probability and Stochastics . Springer.\\nChang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector\\nMachines. ACM Transactions on Intelligent Systems and Technology ,2, 27:1–27:27.\\nCheeseman, Peter. 1985. In Defense of Probability. In: Proceedings of the International\\nJoint Conference on Artiﬁcial Intelligence .\\nChollet, Francois, and Allaire, J. J. 2018. Deep Learning with R . Manning Publications.\\nCodd, Edgar F. 1990. The Relational Model for Database Management . Addison-Wesley\\nLongman Publishing.\\nCunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Reduc-\\ntion: Survey, Insights, and Generalizations. Journal of Machine Learning Research ,\\n16, 2859–2900.\\nDatta, Biswa N. 2010. Numerical Linear Algebra and Applications . SIAM.\\nDavidson, Anthony C., and Hinkley, David V. 1997. Bootstrap Methods and Their Appli-\\ncation . Cambridge University Press.\\nDean, Jeffrey, Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale\\nDistributed Deep Networks. In: Advances in Neural Information Processing Systems .\\nDeisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus-\\nsian Process Dynamical Systems. Pages 2618–2626 of: Advances in Neural Informa-\\ntion Processing Systems .\\nDeisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian\\nFiltering and Smoothing: Explaining Current and Deriving New Algorithms. In:\\nProceedings of the American Control Conference .\\nDeisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\\nfor Data-Efﬁcient Learning in Robotics and Control. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence ,37(2), 408–423.\\nDempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood\\nfrom Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society ,\\n39(1), 1–38.\\nDeng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and\\nHinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep\\nAuto-Encoder. In: Proceedings of Interspeech .\\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation . Springer.\\nDonoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear\\nEmbedding Techniques for High-Dimensional Data. Proceedings of the National\\nAcademy of Sciences ,100(10), 5591–5596.\\nDost´al, Zden ˘ek. 2009. Optimal Quadratic Programming Algorithms: With Applications\\nto Variational Inequalities . Springer.\\nDouven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy . Meta-\\nphysics Research Lab, Stanford University.\\nDowney, Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O’Reilly\\nMedia.\\nDreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\\nMathematical Analysis and Applications ,5(1), 30–45.\\nDrumm, Volker, and Weil, Wolfgang. 2001. Lineare Algebra und Analytische Geometrie .\\nLecture Notes, Universit ¨at Karlsruhe (TH).\\nDudley, Richard M. 2002. Real Analysis and Probability . Cambridge University Press.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad3cdfa2-a254-4c50-a46c-0151dcdc0942', embedding=None, metadata={'page_label': '398', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='398 References\\nEaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach . Institute of\\nMathematical Statistics Lecture Notes.\\nEckart, Carl, and Young, Gale. 1936. The Approximation of One Matrix by Another of\\nLower Rank. Psychometrika ,1(3), 211–218.\\nEfron, Bradley, and Hastie, Trevor. 2016. Computer Age Statistical Inference: Algorithms,\\nEvidence and Data Science . Cambridge University Press.\\nEfron, Bradley, and Tibshirani, Robert J. 1993. An Introduction to the Bootstrap . Chap-\\nman and Hall/CRC.\\nElliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-\\ntional Programming .\\nEvgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\\nLearning Theory: A Primer. International Journal of Computer Vision ,38(1), 9–13.\\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.\\n2008. LIBLINEAR: A Library for Large Linear Classiﬁcation. Journal of Machine\\nLearning Research ,9, 1871–1874.\\nGal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variational\\nInference in Sparse Gaussian Process Regression and Latent Variable Models. In:\\nAdvances in Neural Information Processing Systems .\\nG¨artner, Thomas. 2008. Kernels for Structured Data . World Scientiﬁc.\\nGavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\\nValues is 4√\\n3.IEEE Transactions on Information Theory ,60(8), 5040–5053.\\nGelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Bayesian\\nData Analysis . Chapman and Hall/CRC.\\nGentle, James E. 2004. Random Number Generation and Monte Carlo Methods .\\nSpringer.\\nGhahramani, Zoubin. 2015. Probabilistic Machine Learning and Artiﬁcial Intelligence.\\nNature ,521, 452–459.\\nGhahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Sys-\\ntems Using an EM Algorithm. In: Advances in Neural Information Processing Systems .\\nMIT Press.\\nGilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\\nMonte Carlo in Practice . Chapman and Hall/CRC.\\nGneiting, Tilmann, and Raftery, Adrian E. 2007. Strictly Proper Scoring Rules, Pre-\\ndiction, and Estimation. Journal of the American Statistical Association ,102(477),\\n359–378.\\nGoh, Gabriel. 2017. Why Momentum Really Works. Distill .\\nGohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Determi-\\nnants of Linear Operators . Birkh ¨auser.\\nGolan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\\nKnow . Springer.\\nGolub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations . JHU Press.\\nGoodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning . MIT\\nPress.\\nGraepel, Thore, Candela, Joaquin Qui ˜nonero-Candela, Borchert, Thomas, and Her-\\nbrich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Sponsored\\nSearch Advertising in Microsoft’s Bing Search Engine. In: Proceedings of the Interna-\\ntional Conference on Machine Learning .\\nGriewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Differenti-\\nation. In: Proceedings in Applied Mathematics and Mechanics .\\nGriewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\\nTechniques of Algorithmic Differentiation . SIAM.\\nGrimmett, Geoffrey R., and Welsh, Dominic. 2014. Probability: An Introduction . Oxford\\nUniversity Press.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d190a647-df82-47b6-a07f-d13d47fe245f', embedding=None, metadata={'page_label': '399', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 399\\nGrinstead, Charles M., and Snell, J. Laurie. 1997. Introduction to Probability . American\\nMathematical Society.\\nHacking, Ian. 2001. Probability and Inductive Logic . Cambridge University Press.\\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion . Springer.\\nHallin, Marc, Paindaveine, Davy, and ˇSiman, Miroslav. 2010. Multivariate Quan-\\ntiles and Multiple-Output Regression Quantiles: From ℓ1Optimization to Halfspace\\nDepth. Annals of Statistics ,38, 635–669.\\nHasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a\\nPanorama of Recent Developments . Cambridge University Press.\\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\\ntistical Learning – Data Mining, Inference, and Prediction . Springer.\\nHausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,\\nMartin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:\\nProceedings of the International Conference on Learning Representations .\\nHazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and\\nTrends in Optimization ,2(3–4), 157–325.\\nHensman, James, Fusi, Nicol `o, and Lawrence, Neil D. 2013. Gaussian Processes for\\nBig Data. In: Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence .\\nHerbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\\nSkill Rating System. In: Advances in Neural Information Processing Systems .\\nHiriart-Urruty, Jean-Baptiste, and Lemar ´echal, Claude. 2001. Fundamentals of Convex\\nAnalysis . Springer.\\nHoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for\\nLatent Dirichlet Allocation. Advances in Neural Information Processing Systems .\\nHoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley, John. 2013. Stochas-\\ntic Variational Inference. Journal of Machine Learning Research ,14(1), 1303–1347.\\nHofmann, Thomas, Sch ¨olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\\nods in Machine Learning. Annals of Statistics ,36(3), 1171–1220.\\nHogben, Leslie. 2013. Handbook of Linear Algebra . Chapman and Hall/CRC.\\nHorn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis . Cambridge University\\nPress.\\nHotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal\\nComponents. Journal of Educational Psychology ,24, 417–441.\\nHyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001. Independent Component Anal-\\nysis. Wiley.\\nImbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\\nand Biomedical Sciences . Cambridge University Press.\\nJacod, Jean, and Protter, Philip. 2004. Probability Essentials . Springer.\\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science . Cambridge University\\nPress.\\nJefferys, William H., and Berger, James O. 1992. Ockham’s Razor and Bayesian Anal-\\nysis. American Scientist ,80, 64–72.\\nJeffreys, Harold. 1961. Theory of Probability . Oxford University Press.\\nJimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Nor-\\nmalizing Flows. In: Proceedings of the International Conference on Machine Learning .\\nJimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\\nBackpropagation and Approximate Inference in Deep Generative Models. In: Pro-\\nceedings of the International Conference on Machine Learning .\\nJoachims, Thorsten. 1999. Advances in Kernel Methods – Support Vector Learning . MIT\\nPress. Chap. Making Large-Scale SVM Learning Practical, pages 169–184.\\nJordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K.\\n1999. An Introduction to Variational Methods for Graphical Models. Machine Learn-\\ning,37, 183–233.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53b203f2-146e-4302-94f3-c280577447e1', embedding=None, metadata={'page_label': '400', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='400 References\\nJulier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter\\nto Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense\\nSensing, Simulation and Controls .\\nKaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but\\nShort Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS\\nComputational Biology ,2(7), e95.\\nKalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. Col-\\nlege Mathematics Journal ,27(1), 2–23.\\nKalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Problems.\\nTransactions of the ASME – Journal of Basic Engineering ,82(Series D), 35–45.\\nKamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efﬁcient Reinforcement Learning\\nwith Probabilistic Model Predictive Control. In: Proceedings of the International\\nConference on Artiﬁcial Intelligence and Statistics .\\nKatz, Victor J. 2004. A History of Mathematics . Pearson/Addison-Wesley.\\nKelley, Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal ,30(10),\\n947–954.\\nKimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayesian\\nEstimation on Stochastic Processes and Smoothing by Splines. Annals of Mathemat-\\nical Statistics ,41(2), 495–502.\\nKingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\\nProceedings of the International Conference on Learning Representations .\\nKittler, Josef, and F ¨oglein, Janos. 1984. Contextual Classiﬁcation of Multispectral Pixel\\nData. Image and Vision Computing ,2(1), 13–29.\\nKolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications.\\nSIAM Review ,51(3), 455–500.\\nKoller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models . MIT Press.\\nKong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with\\nMultivariate Data. Statistica Sinica ,22, 1598–1610.\\nLang, Serge. 1987. Linear Algebra . Springer.\\nLawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with\\nGaussian Process Latent Variable Models. Journal of Machine Learning Research ,\\n6(Nov.), 1783–1816.\\nLeemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution\\nRelationships. American Statistician ,62(1), 45–53.\\nLehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses .\\nSpringer.\\nLehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation . Springer.\\nLiesen, J ¨org, and Mehrmann, Volker. 2015. Linear Algebra . Springer.\\nLin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt’s Probabilistic\\nOutputs for Support Vector Machines. Machine Learning ,68, 267–276.\\nLjung, Lennart. 1999. System Identiﬁcation: Theory for the User . Prentice Hall.\\nLoosli, Ga ¨elle, Canu, St ´ephane, and Ong, Cheng Soon. 2016. Learning SVM in Kre ˘ın\\nSpaces. IEEE Transactions of Pattern Analysis and Machine Intelligence ,38(6), 1204–\\n1216.\\nLuenberger, David G. 1969. Optimization by Vector Space Methods . Wiley.\\nMacKay, David J. C. 1992. Bayesian Interpolation. Neural Computation ,4, 415–447.\\nMacKay, David J. C. 1998. Introduction to Gaussian Processes. Pages 133–165 of:\\nBishop, C. M. (ed), Neural Networks and Machine Learning . Springer.\\nMacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms .\\nCambridge University Press.\\nMagnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Appli-\\ncations in Statistics and Econometrics . Wiley.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d20666bd-ba55-4e17-b911-e28ac4cfc365', embedding=None, metadata={'page_label': '401', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 401\\nManton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing\\nKernel Hilbert Spaces. Foundations and Trends in Signal Processing ,8(1–2), 1–126.\\nMarkovsky, Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-\\ncations . Springer.\\nMaybeck, Peter S. 1979. Stochastic Models, Estimation, and Control . Academic Press.\\nMcCullagh, Peter, and Nelder, John A. 1989. Generalized Linear Models . CRC Press.\\nMcEliece, Robert J., MacKay, David J. C., and Cheng, Jung-Fu. 1998. Turbo Decoding\\nas an Instance of Pearl’s “Belief Propagation” Algorithm. IEEE Journal on Selected\\nAreas in Communications ,16(2), 140–152.\\nMika, Sebastian, R ¨atsch, Gunnar, Weston, Jason, Sch ¨olkopf, Bernhard, and M ¨uller,\\nKlaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41–48 of:\\nProceedings of the Workshop on Neural Networks for Signal Processing .\\nMinka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference .\\nPh.D. thesis, Massachusetts Institute of Technology.\\nMinka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in\\nNeural Information Processing Systems .\\nMitchell, Tom. 1997. Machine Learning . McGraw-Hill.\\nMnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David, et al. 2015. Human-Level\\nControl through Deep Reinforcement Learning. Nature ,518, 529–533.\\nMoonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,\\nArchitectures and Applications . Elsevier.\\nMoustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Mod-\\neling. American Cancer Society. Pages 1–10.\\nM¨uller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with\\nPython: A Guide for Data Scientists . O’Reilly Publishing.\\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective . MIT Press.\\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks . Ph.D. thesis, Depart-\\nment of Computer Science, University of Toronto.\\nNeal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that\\nJustiﬁes Incremental, Sparse, and Other Variants. Pages 355–368 of: Learning in\\nGraphical Models . MIT Press.\\nNelsen, Roger. 2006. An Introduction to Copulas . Springer.\\nNesterov, Yuri. 2018. Lectures on Convex Optimization . Springer.\\nNeumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tu-\\ntorial on Regularization. SIAM Review ,40, 636–666.\\nNocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization . Springer.\\nNowozin, Sebastian, Gehler, Peter V., Jancsary, Jeremy, and Lampert, Christoph H.\\n(eds). 2014. Advanced Structured Prediction . MIT Press.\\nO’Hagan, Anthony. 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning\\nand Inference ,29, 245–260.\\nOng, Cheng Soon, Mary, Xavier, Canu, St ´ephane, and Smola, Alexander J. 2004. Learn-\\ning with Non-Positive Kernels. In: Proceedings of the International Conference on\\nMachine Learning .\\nOrmoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.\\nLearning and Tracking Cyclic Human Motion. In: Advances in Neural Information\\nProcessing Systems .\\nPage, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd, Terry. 1999. The\\nPageRank Citation Ranking: Bringing Order to the Web . Tech. rept. Stanford Info-\\nLab.\\nPaquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models . Ph.D. thesis, Uni-\\nversity of Cambridge.\\nParzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.\\nAnnals of Mathematical Statistics ,33(3), 1065–1076.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b227a015-975f-4557-8b32-1b568fa4e56b', embedding=None, metadata={'page_label': '402', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='402 References\\nPearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\\nInference . Morgan Kaufmann.\\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference . 2nd edn. Cambridge\\nUniversity Press.\\nPearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew\\nVariation in Homogeneous Material. Philosophical Transactions of the Royal Society\\nA: Mathematical, Physical and Engineering Sciences ,186, 343–414.\\nPearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space.\\nPhilosophical Magazine ,2(11), 559–572.\\nPeters, Jonas, Janzing, Dominik, and Sch ¨olkopf, Bernhard. 2017. Elements of Causal\\nInference: Foundations and Learning Algorithms . MIT Press.\\nPetersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook . Tech. rept.\\nTechnical University of Denmark.\\nPlatt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Compar-\\nisons to Regularized Likelihood Methods. In: Advances in Large Margin Classiﬁers .\\nPollard, David. 2002. A User’s Guide to Measure Theoretic Probability . Cambridge\\nUniversity Press.\\nPolyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages\\n437–507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\\nData Sciences . Springer.\\nPress, William H., Teukolsky, Saul A., Vetterling, William T., and Flannery, Brian P.\\n2007. Numerical Recipes: The Art of Scientiﬁc Computing . Cambridge University\\nPress.\\nProschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Condi-\\ntional Expectation. American Statistician ,52(3), 248–252.\\nRaschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine\\nLearning and Deep Learning with Python, scikit-learn, and TensorFlow . Packt Publish-\\ning.\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam’s Razor. In: Advances in\\nNeural Information Processing Systems .\\nRasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-\\nvances in Neural Information Processing Systems .\\nRasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Ma-\\nchine Learning . MIT Press.\\nReid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for\\nBinary Experiments. Journal of Machine Learning Research ,12, 731–817.\\nRifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Duality.\\nJournal of Machine Learning Research ,8, 441–479.\\nRockafellar, Ralph T. 1970. Convex Analysis . Princeton University Press.\\nRogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning . Chap-\\nman and Hall/CRC.\\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal\\nInference . Harvard University Press.\\nRosenblatt, Murray. 1956. Remarks on Some Nonparametric Estimates of a Density\\nFunction. Annals of Mathematical Statistics ,27(3), 832–837.\\nRoweis, Sam T. 1998. EM Algorithms for PCA and SPCA. Pages 626–632 of: Advances\\nin Neural Information Processing Systems .\\nRoweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\\nModels. Neural Computation ,11(2), 305–345.\\nRoy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for\\nStatistics . Chapman and Hall/CRC.\\nRubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo\\nMethod . Wiley.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b932ef3f-e768-4458-85fa-c06ea80e7532', embedding=None, metadata={'page_label': '403', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 403\\nRufﬁni, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la\\nSoluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto . Stampe-\\nria di S. Tommaso d’Aquino.\\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning\\nRepresentations by Back-Propagating Errors. Nature ,323(6088), 533–536.\\nSæmundsson, Steind ´or, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-\\nforcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the\\nConference on Uncertainty in Artiﬁcial Intelligence .\\nSaitoh, Saburou. 1988. Theory of Reproducing Kernels and its Applications . Longman\\nScientiﬁc and Technical.\\nS¨arkk¨a, Simo. 2013. Bayesian Filtering and Smoothing . Cambridge University Press.\\nSch¨olkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels – Support\\nVector Machines, Regularization, Optimization, and Beyond . MIT Press.\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1997. Kernel\\nPrincipal Component Analysis. In: Proceedings of the International Conference on\\nArtiﬁcial Neural Networks .\\nSch¨olkopf, Bernhard, Smola, Alexander J., and M ¨uller, Klaus-Robert. 1998. Nonlinear\\nComponent Analysis as a Kernel Eigenvalue Problem. Neural Computation ,10(5),\\n1299–1319.\\nSch¨olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\\nRepresenter Theorem. In: Proceedings of the International Conference on Computa-\\ntional Learning Theory .\\nSchwartz, Laurent. 1964. Sous Espaces Hilbertiens d’Espaces Vectoriels Topologiques\\net Noyaux Associ ´es.Journal d’Analyse Math ´ematique ,13, 115–256.\\nSchwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics ,\\n6(2), 461–464.\\nShahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando.\\n2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\\nProceedings of the IEEE ,104(1), 148–175.\\nShalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:\\nFrom Theory to Algorithms . Cambridge University Press.\\nShawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Analysis .\\nCambridge University Press.\\nShawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodologies\\nin Support Vector Machines. Neurocomputing ,74(17), 3609–3618.\\nShental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny, and Dolev, Danny. 2008.\\nGaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863–\\n1867 of: Proceedings of the International Symposium on Information Theory .\\nShewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method with-\\nout the Agonizing Pain .\\nShi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence ,22(8), 888–905.\\nShi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,\\nand Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of\\nMachine Learning Research , 2615–2637.\\nShiryayev, Albert N. 1984. Probability . Springer.\\nShor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions . Springer.\\nShotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-\\nBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recog-\\nnition and Segmentation. In: Proceedings of the European Conference on Computer\\nVision .\\nSmith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Criteria\\nfor Linear Models. Journal of the Royal Statistical Society B ,42(2), 213–220.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7e22909-54b6-4940-8d47-38d02a158755', embedding=None, metadata={'page_label': '404', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='404 References\\nSnoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-\\ntimization of Machine Learning Algorithms. In: Advances in Neural Information\\nProcessing Systems .\\nSpearman, Charles. 1904. “General Intelligence,” Objectively Determined and Mea-\\nsured. American Journal of Psychology ,15(2), 201–292.\\nSriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch ¨olkopf, Bernhard,\\nand Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Proba-\\nbility Measures. Journal of Machine Learning Research ,11, 1517–1561.\\nSteinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.\\nConstructive Approximation ,26, 225–287.\\nSteinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines . Springer.\\nStoer, Josef, and Burlirsch, Roland. 2002. Introduction to Numerical Analysis . Springer.\\nStrang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American\\nMathematical Monthly ,100(9), 848–855.\\nStrang, Gilbert. 2003. Introduction to Linear Algebra . Wellesley-Cambridge Press.\\nStray, Jonathan. 2016. The Curious Journalist’s Guide to Data . Tow Center for Digital\\nJournalism at Columbia’s Graduate School of Journalism.\\nStrogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\\nNotices of the American Mathematical Society ,61(3), 286–291.\\nSucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level\\nVision. Image and Vision Computing ,12(1), 42–60.\\nSzeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-\\native Study of Energy Minimization Methods for Markov Random Fields with\\nSmoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-\\ntelligence ,30(6), 1068–1080.\\nTandra, Haryono. 2014. The Relationship between the Change of Variable Theorem\\nand the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\\nMathematics ,17(2), 76–83.\\nTenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geometric\\nFramework for Nonlinear Dimensionality Reduction. Science ,290(5500), 2319–\\n2323.\\nTibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal\\nof the Royal Statistical Society B ,58(1), 267–288.\\nTipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Compo-\\nnent Analysis. Journal of the Royal Statistical Society: Series B ,61(3), 611–622.\\nTitsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\\nVariable Model. In: Proceedings of the International Conference on Artiﬁcial Intelli-\\ngence and Statistics .\\nToussaint, Marc. 2012. Some Notes on Gradient Descent . https://ipvs.informatik.uni-\\nstuttgart.de/mlr/marc/notes/gradientDescent.pdf.\\nTrefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra . SIAM.\\nTucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analysis.\\nPsychometrika ,31(3), 279–311.\\nVapnik, Vladimir N. 1998. Statistical Learning Theory . Wiley.\\nVapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory. IEEE Transac-\\ntions on Neural Networks ,10(5), 988–999.\\nVapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory . Springer.\\nVishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,\\nKarsten M. 2010. Graph Kernels. Journal of Machine Learning Research ,11, 1201–\\n1242.\\nvon Luxburg, Ulrike, and Sch ¨olkopf, Bernhard. 2011. Statistical Learning Theory:\\nModels, Concepts, and Results. Pages 651–706 of: D. M. Gabbay, S. Hartmann,\\nJ. Woods (ed), Handbook of the History of Logic , vol. 10. Elsevier.\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5202011d-5a89-4084-9db7-67ae8e0af5bf', embedding=None, metadata={'page_label': '405', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 405\\nWahba, Grace. 1990. Spline Models for Observational Data . Society for Industrial and\\nApplied Mathematics.\\nWalpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.\\nProbability and Statistics for Engineers and Scientists . Prentice Hall.\\nWasserman, Larry. 2004. All of Statistics . Springer.\\nWasserman, Larry. 2007. All of Nonparametric Statistics . Springer.\\nWhittle, Peter. 2000. Probability via Expectation . Springer.\\nWickham, Hadley. 2014. Tidy Data. Journal of Statistical Software ,59, 1–23.\\nWilliams, Christopher K. I. 1997. Computing with Inﬁnite Networks. In: Advances in\\nNeural Information Processing Systems .\\nYu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv ´ari, Csaba. 2013. Charac-\\nterizing the Representer Theorem. In: Proceedings of the International Conference on\\nMachine Learning .\\nZadrozny, Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-\\nmates from Decision Trees and Naive Bayesian Classiﬁers. In: Proceedings of the\\nInternational Conference on Machine Learning .\\nZhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach\\nSpaces for Machine Learning. Journal of Machine Learning Research ,10, 2741–2775.\\nZia, Royce K. P., Redish, Edward F., and McKay, Susan R. 2009. Making Sense of the\\nLegendre Transform. American Journal of Physics ,77(614), 614–622.\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb5c579f-fbee-4e3f-be95-0af48dc59dfd', embedding=None, metadata={'page_label': '406', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5d30063-b485-4913-9fc5-9140f220d7d5', embedding=None, metadata={'page_label': '407', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index\\n1-of-Krepresentation, 364\\nℓ1norm, 71\\nℓ2norm, 72\\nabduction, 258\\nAbel-Rufﬁni theorem, 334\\nAbelian group, 36\\nabsolutely homogeneous, 71\\nactivation function, 315\\nafﬁne mapping, 63\\nafﬁne subspace, 61\\nAkaike information criterion, 288\\nalgebra, 17\\nalgebraic multiplicity, 106\\nanalytic, 143\\nancestral sampling, 340, 364\\nangle, 76\\nassociativity, 24, 25, 36\\nattribute, 253\\naugmented matrix, 29\\nauto-encoder, 343\\nautomatic differentiation, 161\\nautomorphism, 49\\nbackpropagation, 159\\nbasic variable, 30\\nbasis, 44\\nbasis vector, 45\\nBayes factor, 287\\nBayes’ law, 185\\nBayes’ rule, 185\\nBayes’ theorem, 185\\nBayesian GP-LVM, 347\\nBayesian inference, 274\\nBayesian information criterion, 288\\nBayesian linear regression, 303\\nBayesian model selection, 286\\nBayesian network, 278, 283\\nBayesian PCA, 346\\nBernoulli distribution, 205\\nBeta distribution, 206\\nbilinear mapping, 72\\nbijective, 48\\nbinary classiﬁcation, 370\\nBinomial distribution, 206\\nblind-source separation, 346\\nBorelσ-algebra, 180canonical basis, 45\\ncanonical feature map, 389\\ncanonical link function, 315\\ncategorical variable, 180\\nCauchy-Schwarz inequality, 75\\nchange-of-variable technique, 219\\ncharacteristic polynomial, 104\\nCholesky decomposition, 114\\nCholesky factor, 114\\nCholesky factorization, 114\\nclass, 370\\nclassiﬁcation, 315\\nclosure, 36\\ncode, 343\\ncodirected, 105\\ncodomain, 58, 139\\ncollinear, 105\\ncolumn, 22\\ncolumn space, 59\\ncolumn vector, 22, 38\\ncompleting the squares, 307\\nconcave function, 236\\ncondition number, 230\\nconditional probability, 179\\nconditionally independent, 195\\nconjugate, 208\\nconjugate prior, 208\\nconvex conjugate, 242\\nconvex function, 236\\nconvex hull, 386\\nconvex optimization problem, 236, 239\\nconvex set, 236\\ncoordinate, 50\\ncoordinate representation, 50\\ncoordinate vector, 50\\ncorrelation, 191\\ncovariance, 190\\ncovariance matrix, 190, 198\\ncovariate, 253\\nCP decomposition, 136\\ncross-covariance, 191\\ncross-validation, 258, 263\\ncumulative distribution function, 178,\\n181\\nd-separation, 281\\n407\\nThis material will be published by Cambridge University Press as Mathematics for Machine Learn-\\ningby Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. This pre-publication version is\\nfree to view and download for personal use only. Not for re-distribution, re-sale or use in deriva-\\ntive works. c⃝by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2020. https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3fb847bb-f3c3-4267-9a43-52d3ddb979fd', embedding=None, metadata={'page_label': '408', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='408 Index\\ndata covariance matrix, 318\\ndata point, 253\\ndata-ﬁt term, 302\\ndecoder, 343\\ndeep auto-encoder, 347\\ndefective, 111\\ndenominator layout, 151\\nderivative, 141\\ndesign matrix, 294, 296\\ndeterminant, 99\\ndiagonal matrix, 115\\ndiagonalizable, 116\\ndiagonalization, 116\\ndifference quotient, 141\\ndimension, 45\\ndimensionality reduction, 317\\ndirected graphical model, 278, 283\\ndirection, 61\\ndirection space, 61\\ndistance, 75\\ndistribution, 177\\ndistributivity, 24, 26\\ndomain, 58, 139\\ndot product, 72\\ndual SVM, 385\\nEckart-Young theorem, 131, 334\\neigendecomposition, 116\\neigenspace, 106\\neigenspectrum, 106\\neigenvalue, 105\\neigenvalue equation, 105\\neigenvector, 105\\nelementary transformations, 28\\nEM algorithm, 360\\nembarrassingly parallel, 264\\nempirical covariance, 192\\nempirical mean, 192\\nempirical risk, 260\\nempirical risk minimization, 257, 260\\nencoder, 343\\nendomorphism, 49\\nepigraph, 236\\nequivalent, 56\\nerror function, 294\\nerror term, 382\\nEuclidean distance, 72, 75\\nEuclidean norm, 72\\nEuclidean vector space, 73\\nevent space, 175\\nevidence, 186, 285, 306\\nexample, 253\\nexpected risk, 261\\nexpected value, 187\\nexponential family, 205, 211\\nextended Kalman ﬁlter, 170\\nfactor analysis, 346\\nfactor graph, 283feature, 253\\nfeature map, 254\\nfeature matrix, 296\\nfeature vector, 295\\nFisher discriminant analysis, 136\\nFisher-Neyman theorem, 210\\nforward mode, 161\\nfree variable, 30\\nfull rank, 47\\nfull SVD, 128\\nfundamental theorem of linear\\nmappings, 60\\nGaussian elimination, 31\\nGaussian mixture model, 349\\nGaussian process, 316\\nGaussian process latent-variable model,\\n347\\ngeneral linear group, 37\\ngeneral solution, 28, 30\\ngeneralized linear model, 272, 315\\ngenerating set, 44\\ngenerative process, 272, 286\\ngenerator, 344\\ngeometric multiplicity, 108\\nGivens rotation, 94\\nglobal minimum, 225\\nGP-LVM, 347\\ngradient, 146\\nGram matrix, 389\\nGram-Schmidt orthogonalization, 89\\ngraphical model, 278\\ngroup, 36\\nHadamard product, 23\\nhard margin SVM, 377\\nHessian, 164\\nHessian eigenmaps, 136\\nHessian matrix, 165\\nhinge loss, 381\\nhistogram, 369\\nhyperparameter, 258\\nhyperplane, 61, 62\\nhyperprior, 281\\ni.i.d., 195\\nICA, 346\\nidentity automorphism, 49\\nidentity mapping, 49\\nidentity matrix, 23\\nimage, 58, 139\\nindependent and identically distributed,\\n195, 260, 266\\nindependent component analysis, 346\\ninference network, 344\\ninjective, 48\\ninner product, 73\\ninner product space, 73\\nintermediate variables, 162\\ninverse, 24\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d168b48f-8ff9-4eed-b426-3b6a860fe319', embedding=None, metadata={'page_label': '409', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 409\\ninverse element, 36\\ninvertible, 24\\nIsomap, 136\\nisomorphism, 49\\nJacobian, 146, 150\\nJacobian determinant, 152\\nJeffreys-Lindley paradox, 287\\nJensen’s inequality, 239\\njoint probability, 178\\nKarhunen-Lo `eve transform, 318\\nkernel, 33, 47, 58, 254, 388\\nkernel density estimation, 369\\nkernel matrix, 389\\nkernel PCA, 347\\nkernel trick, 316, 347, 389\\nlabel, 253\\nLagrange multiplier, 234\\nLagrangian, 234\\nLagrangian dual problem, 234\\nLaplace approximation, 170\\nLaplace expansion, 102\\nLaplacian eigenmaps, 136\\nLASSO, 303, 316\\nlatent variable, 275\\nlaw, 177, 181\\nlaw of total variance, 203\\nleading coefﬁcient, 30\\nleast-squares loss, 154\\nleast-squares problem, 261\\nleast-squares solution, 88\\nleft-singular vectors, 119\\nLegendre transform, 242\\nLegendre–Fenchel transform, 242\\nlength, 71\\nlikelihood, 185, 265, 269, 291\\nline, 61, 82\\nlinear combination, 40\\nlinear manifold, 61\\nlinear mapping, 48\\nlinear program, 239\\nlinear subspace, 39\\nlinear transformation, 48\\nlinearly dependent, 40\\nlinearly independent, 40\\nlink function, 272\\nloading, 322\\nlocal minimum, 225\\nlog-partition function, 211\\nlogistic regression, 315\\nlogistic sigmoid, 315\\nloss function, 260, 381\\nloss term, 382\\nlower-triangular matrix, 101\\nMaclaurin series, 143\\nManhattan norm, 71\\nMAP, 300\\nMAP estimation, 269margin, 374\\nmarginal, 190\\nmarginal likelihood, 186, 286, 306\\nmarginal probability, 179\\nmarginalization property, 184\\nMarkov random ﬁeld, 283\\nmatrix, 22\\nmatrix factorization, 98\\nmaximum a posteriori, 300\\nmaximum a posteriori estimation, 269\\nmaximum likelihood, 257\\nmaximum likelihood estimate, 296\\nmaximum likelihood estimation, 265,\\n293\\nmean, 187\\nmean function, 309\\nmean vector, 198\\nmeasure, 180\\nmedian, 188\\nmetric, 76\\nminimal, 44\\nminimax inequality, 234\\nmisﬁt term, 302\\nmixture model, 349\\nmixture weight, 349\\nmode, 188\\nmodel, 251\\nmodel evidence, 286\\nmodel selection, 258\\nMoore-Penrose pseudo-inverse, 35\\nmultidimensional scaling, 136\\nmultiplication by scalars, 37\\nmultivariate, 178\\nmultivariate Gaussian distribution, 198\\nmultivariate Taylor series, 166\\nnatural parameters, 212\\nnegative log-likelihood, 265\\nnested cross-validation, 258, 284\\nneutral element, 36\\nnoninvertible, 24\\nnonsingular, 24\\nnorm, 71\\nnormal distribution, 197\\nnormal equation, 86\\nnormal vector, 80\\nnull space, 33, 47, 58\\nnumerator layout, 150\\nOccam’s razor, 285\\nONB, 79\\none-hot encoding, 364\\nordered basis, 50\\northogonal, 77\\northogonal basis, 79\\northogonal complement, 79\\northogonal matrix, 78\\northonormal, 77\\northonormal basis, 79\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e370c385-ca70-46e4-98b4-22fba6a40a22', embedding=None, metadata={'page_label': '410', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='410 Index\\nouter product, 38\\noverﬁtting, 262, 271, 299\\nPageRank, 114\\nparameters, 61\\nparametric equation, 61\\npartial derivative, 146\\nparticular solution, 27, 30\\nPCA, 317\\npdf, 181\\npenalty term, 263\\npivot, 30\\nplane, 62\\nplate, 281\\npopulation mean and covariance, 191\\npositive deﬁnite, 71, 73, 74, 76\\nposterior, 185, 269\\nposterior odds, 287\\npower iteration, 334\\npower series representation, 145\\nPPCA, 340\\npreconditioner, 230\\npredictor, 12, 255\\nprimal problem, 234\\nprincipal component, 322\\nprincipal component analysis, 136, 317\\nprincipal subspace, 327\\nprior, 185, 269\\nprior odds, 287\\nprobabilistic inverse, 186\\nprobabilistic PCA, 340\\nprobabilistic programming, 278\\nprobability, 175\\nprobability density function, 181\\nprobability distribution, 172\\nprobability integral transform, 217\\nprobability mass function, 178\\nproduct rule, 184\\nprojection, 82\\nprojection error, 88\\nprojection matrix, 82\\npseudo-inverse, 86\\nrandom variable, 172, 175\\nrange, 58\\nrank, 47\\nrank deﬁcient, 47\\nrank-kapproximation, 130\\nrank-nullity theorem, 60\\nraw-score formula for variance, 193\\nrecognition network, 344\\nreconstruction error, 88, 327\\nreduced hull, 388\\nreduced row-echelon form, 31\\nreduced SVD, 129\\nREF, 30\\nregression, 289\\nregular, 24\\nregularization, 262, 302, 382regularization parameter, 263, 302, 380\\nregularized least squares, 302\\nregularizer, 263, 302, 380, 382\\nrepresenter theorem, 384\\nresponsibility, 352\\nreverse mode, 161\\nright-singular vectors, 119\\nRMSE, 298\\nroot mean square error, 298\\nrotation, 91\\nrotation matrix, 92\\nrow, 22\\nrow vector, 22, 38\\nrow-echelon form, 30\\nsample mean, 192\\nsample space, 175\\nscalar, 37\\nscalar product, 72\\nsigmoid, 213\\nsimilar, 56\\nsingular, 24\\nsingular value decomposition, 119\\nsingular value equation, 124\\nsingular value matrix, 119\\nsingular values, 119\\nslack variable, 379\\nsoft margin SVM, 379, 380\\nsolution, 20\\nspan, 44\\nspecial solution, 27\\nspectral clustering, 136\\nspectral norm, 131\\nspectral theorem, 111\\nspectrum, 106\\nsquare matrix, 25\\nstandard basis, 45\\nstandard deviation, 190\\nstandard normal distribution, 198\\nstandardization, 336\\nstatistical independence, 194\\nstatistical learning theory, 265\\nstochastic gradient descent, 231\\nstrong duality, 236\\nsufﬁcient statistics, 210\\nsum rule, 184\\nsupport point, 61\\nsupport vector, 384\\nsupporting hyperplane, 242\\nsurjective, 48\\nSVD, 119\\nSVD theorem, 119\\nsymmetric, 73, 76\\nsymmetric matrix, 25\\nsymmetric, positive deﬁnite, 74\\nsymmetric, positive semideﬁnite, 74\\nsystem of linear equations, 20\\ntarget space, 175\\nDraft (2020-08-16) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com .', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbf46e85-14c3-4682-be0e-6f19291c6b85', embedding=None, metadata={'page_label': '411', 'file_name': 'mathematics_for_ml.pdf', 'file_path': '/content/pdf/mathematics_for_ml.pdf', 'file_type': 'application/pdf', 'file_size': 17392364, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 411\\nTaylor polynomial, 142, 166\\nTaylor series, 142\\ntest error, 300\\ntest set, 262, 284\\nTikhonov regularization, 265\\ntrace, 103\\ntraining, 12\\ntraining error, 300\\ntraining set, 260, 292\\ntransfer function, 315\\ntransformation matrix, 51\\ntranslation vector, 63\\ntranspose, 25, 38\\ntriangle inequality, 71, 76\\ntruncated SVD, 129\\nTucker decomposition, 136\\nunderﬁtting, 271\\nundirected graphical model, 283\\nuniform distribution, 182\\nunivariate, 178\\nunscented transform, 170\\nupper-triangular matrix, 101\\nvalidation set, 263, 284\\nvariable selection, 316\\nvariance, 190\\nvector, 37\\nvector addition, 37\\nvector space, 37\\nvector space homomorphism, 48\\nvector space with inner product, 73\\nvector subspace, 39\\nweak duality, 235\\nzero-one loss, 381\\nc⃝2020 M. P. Deisenroth, A. A. Faisal, C. S. Ong. To be published by Cambridge University Press.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\"\"\"\n",
        "\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "uL4_S50P5P1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95tUPlp853Tb",
        "outputId": "02e73424-94e5-4c4c-8c10-99ef8399a9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "8cce75581b9442e484f1f467c4483117",
            "b1c7eab3edba46f9a1094f6641050521",
            "a83e939c1c46409ead9611d498bc4bcc",
            "a52036de001543c7bfe20c4bcd47f90e",
            "4ff36f6a3cbe4a86bd28204641409b8b",
            "11d3eeec391c41d7826f63248d599ab7",
            "9a869d1b49344d25a5fa61c6d63be7aa",
            "4577344977a34c4db0a0976ed5c9cdae",
            "3953a4ab9e3d4816a0a3cba28e009335",
            "c12de6f56f8c43709d9e5a861b58862b",
            "1ed535f109504beea9fe5e059e50177d",
            "3783b2a7e5874a88b22dbe25734cb163",
            "6772f12a121b4b23bea4f93141124f52",
            "8d28b142986a4607b9240e34504dca84",
            "4abc3bd1530f40659320c51199e3f89c",
            "64d009628d164de78ce7d34c7551739c",
            "162a6d5553d54beb9628e69c79b20666",
            "ee25138d2c02403785253a6a94cb1bc5",
            "3d11a3ba7ab94f7b9fdfd1c0bf8e393f",
            "ca54a6d8057b41b1896526af1d506424",
            "8d54e5ca30874d0f8007a2e789aab186",
            "7bdb4bce9f8a4acb890111c5d45f6999",
            "82e66c4e9e784d5d9599c5e345608c36",
            "50b948efe86f4a25abedafe3e6350bae",
            "1a0bec2554eb4ef49ec41e171d1e5232",
            "8ad82d8892214941a7efbabb0bee97a3",
            "74b1fc00dffb4c949aa22a8076d7e190",
            "8387499768e24d11bee94b7fae479f6f",
            "d5fbbd23a9364c94ab37ee232ba886f3",
            "f35cb5a2a91f4cd68e63013aa61f8100",
            "5fc7017c3daa4a44b1eb384e64a17cac",
            "0fa58ce7a3c84e9f82e759d24e5efbeb",
            "3a5a2ea12e4e45e282911ccde71ec1ce",
            "8d85722344dc4e40858191462a9fced4",
            "37f9764f7a194b2cb49c44ca0a8ff7c5",
            "68262dcf352f441284fbd28fc5582a94",
            "14c7bf42e30547cfad7e423536d0a9e3",
            "da8c6495b3424503b0519e6586103cb5",
            "5a758a4649314334afebd13411921a84",
            "64fedf40dc4e44bcac734474c69e3622",
            "213f96af7d8b434a81226b4327ba31fb",
            "eb18f061e25c48b681394c0a5f3beaa6",
            "6098bb4cedb04156878b96b1a8469b89",
            "76cca771a8a54eb784218ce71877594a",
            "4a0b293e50634131ae58c4089bef56fc",
            "c2a1978144f64b5f9e19a5c41617f15f",
            "7795cde88bc8468d9633a9e47bca52bd",
            "97adbc9a0d7f46ca95b88983300c0436",
            "944790a28a3c443ead377c010ccaef17",
            "c91902d3d4d1489c95290d9b949335f4",
            "7875996a298f4dc78925c5c6bb558371",
            "e438239089ee4d1e85ab135bf614f8a6",
            "73ef4e8d9d614f72bf0dac3e875f948c",
            "2314f3b9170b461faa0850ec7b4b5db1",
            "8bf7404cf96d472388ce64cc293a0728",
            "d5dea3ad0c074123aa0daff377f12339",
            "70d4f8e073d7437889694729bb9147ee",
            "35fee16af81646bb94f7509ec4932074",
            "012263ffad3d4a6a94339ff01ab1f0e2",
            "b9136f5fe4cc4a4f969c5441badda5c3",
            "d59c8956a1a841d3b90b71fc324dfd1e",
            "618cebc6b3854f28b0a38af78b2c1cfd",
            "6c1a2e40d64a48ea8f3e9a28ac4a99dc",
            "3f6a9af688de4f7485de5fdbb131173e",
            "bf0337dc39c6499c953cb8c46c5ecbc5",
            "7d99a3d6734a43e39ef3b5f1f641756d",
            "c39a27f49a4e45a69e12c93b34e2d5cc",
            "d4e6f6550d8641478eb04a7a46255804",
            "f4f3782e18b04220bdae397d3d8dbb43",
            "cbfdfd695ec0445fb00440b8d19c9c2b",
            "7fd1e94ecbab4fde853d5fcf2e43a017",
            "0dd94460b6d24cd184656f2f812c4509",
            "574480f7eca941318a27feec2a3e7083",
            "3a1cc6ea33cf4a269574aad929db4a13",
            "946db45e6da545da8db8e6f8e2ec17ac",
            "6a132d1f2f904a6080c5d36aee40c009",
            "58172ee191db45b2880e1ceb80ca95a6",
            "a03260f8db53424b95e5e0f8838560ce",
            "53ef9c1c1cf74c83b9b4f74752f84a49",
            "ddb696912dfb4ff2a0b6aaeb359f8ee7",
            "6f1e59f341bf43d490b184dc7507d186",
            "a844938a59f74935a93a5c1bacb8e020",
            "a1e4077a5eca495aa0df047adac510d2",
            "fa78cd882c2b40eab55698981968299c",
            "26bd0feeae414c188416fe5a75fe8e99",
            "986a2e3084f5445597ac22ae45d3562b",
            "9b1f1e15d48e44aeb0ca69523706afdb",
            "f28849387d3c4866822dc182c2361b73",
            "1e8a78f75fb645a9bf4297f09a8883ed",
            "c8ded96fa7ad4ac8983daadf2866f266",
            "4b72d2984b0d4a2e93fc426524e69c2b",
            "e9ec9527cf59485baf07ce867eeb4cf5",
            "c124637edd7f48b5ae0d0b3a93bd8418",
            "a34a026afe0644c49fc50e1850f9afa2",
            "3db996a7f93f47e4b1d1b1072282846d",
            "71f71309562344458a50e72fc2c00711",
            "b55766ec80aa4f6fa285276e438a5db8",
            "75257e71a7e346398b7093d707fececd",
            "e8527b5c6ac84a919653c716120faf7b",
            "1b08f3f1dd20475582404a9c15ac9588",
            "eb297a0215c643a9806d1819e6f311b1",
            "0a0c04287a6f4c42b1a5a181c3d1e055",
            "20706b2e97b144fcbb050f8dd74ce618",
            "dd7f872a94fc478abf5036a7cdb9999f",
            "b190cf64cd0247208bfdb15ad5440dfb",
            "c6322b4ce7fd4ac09680c3943bec0152",
            "ddb0631060ea484faf18199a228879c8",
            "5d332b725a834a4ba3487743b46659a0",
            "1bc997d0e48a4aed8ab38ecfcb18d642",
            "ecc9970f033d46feb0fff9ebd717bddf",
            "9cb52f5bdfe544c4998336eee9d3844c",
            "f82a3fc45c004c95b062dbf610c1ddd6",
            "b88555f787014e8589187e60dc04b98d",
            "8cd85dfa6cd7465596f2809cc2130676",
            "eed131eba1dc44cd810f6399e23539e6",
            "603c578828d54f4f87be550b5bf17994",
            "e8d6fd5015d448a1b55191d8386cb8c2",
            "af2855a8ccf34d1ab27c2c6823e0dcec",
            "a6372e5ca4f44272820e4bf772c2a3ae",
            "69c4c74a4a2d414e8b996711fdddc9c1",
            "7de8a9827f8a4dc1b054bbd063dc3298"
          ]
        },
        "id": "L7lGQZsH5upl",
        "outputId": "ce90b462-3fb0-4bb9-d092-98ae203bd3bd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cce75581b9442e484f1f467c4483117"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3783b2a7e5874a88b22dbe25734cb163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82e66c4e9e784d5d9599c5e345608c36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d85722344dc4e40858191462a9fced4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a0b293e50634131ae58c4089bef56fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5dea3ad0c074123aa0daff377f12339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c39a27f49a4e45a69e12c93b34e2d5cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a03260f8db53424b95e5e0f8838560ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e8a78f75fb645a9bf4297f09a8883ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b08f3f1dd20475582404a9c15ac9588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cb52f5bdfe544c4998336eee9d3844c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgH7rkITwuuB",
        "outputId": "d060266a-fbe6-430c-d4e3-a87abfe362dd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.7)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.13)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.8.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.20.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Installing collected packages: langchain-community\n",
            "Successfully installed langchain-community-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y07-1NVrxJ0L",
        "outputId": "ed386e6e-7371-4644-94bc-8c10defceb0a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-core in /usr/local/lib/python3.10/dist-packages (0.10.54)\n",
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.10/dist-packages (0.1.25)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.35.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2TLEstxxiSo",
        "outputId": "a06736b3-597a-4319-fcbc-c2bab37953d2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-embeddings-langchain\n",
            "  Downloading llama_index_embeddings_langchain-0.1.2-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-langchain) (0.10.54)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.35.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-langchain) (1.16.0)\n",
            "Installing collected packages: llama-index-embeddings-langchain\n",
            "Successfully installed llama-index-embeddings-langchain-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ],
      "metadata": {
        "id": "29H5AfQk5z8j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "335258a2365340a9aa540d353c92d355",
            "304138af4b874e91976875c168ad0342",
            "89859ed4d7cf4c8abcc5d3b3829dacca",
            "a04fb3d1a4b14f0e8c133c64f122783b",
            "b6d9c4ee969145cda249d16139070a80",
            "63a0af0c729a44128b74c6eb9f0e9284",
            "75b370cca5214751878db98e020e49d6",
            "237be1a331bc4bff85619f271acfa002",
            "89abd83007924204a89e09d7a28d2b30",
            "6547b1a65acf465fb6ef560f561c5a8c",
            "143c16cbc77f44bd9140ab6a6d2c5d67",
            "0a5f18a5ddcd48afa9d51e8a3abd2269",
            "687042e2bde44b528da342369460b4f2",
            "36ef1f1d15ee4728b185607c70fc3774",
            "456ef102fd1e4b4a9fead89faa67368e",
            "5487b2527c4b4ac08c6a835fbc88b147",
            "1e025ac6355046f9a744058c4edb1adb",
            "af2bff5e5d29425ab6293c78caca7676",
            "9e7df71550644a9bb2f22e4b45442931",
            "17f08c6d5ec647829e223b4740d49a95",
            "cdaf66f454b349698a550b25a241fb02",
            "a7fe5827f5f64d27b8c0a2d07ffbfcf9",
            "15100addfdec4b72b4275ef08f488c99",
            "c24c00186cca41f885727f0be6bd7806",
            "8aab4586c7db407fbd268dd89133c2d4",
            "3e5c32e1901441eab45a0627c30e7bca",
            "4202b757c305496cac860d8fda7f71bc",
            "3cc24ef422cd408abbbd28d87fc30e0e",
            "df53dc8b46554622bbe90523a95bba5a",
            "5154f3cb62294f9a8d0c9592c9d68b4b",
            "d476df15783446d5b46795388eae1620",
            "24646d0961c34537be104fe3b9b70a18",
            "f28c5d46db6b4602ab94917e505eae81",
            "da3c923801334fa98506609efff84106",
            "48a06348493e4fd0804430afd055acdc",
            "840210d43ea347ccb69f3239c05dcf68",
            "1f43faf2d868481fbc9da7d16c20772a",
            "8d3fb82852a14ec9bc04b9eb82722da1",
            "51ec5e5e1e3c40a7beb16e57c6f81bcb",
            "30ed7beeebfb4365bcd9f3efeebe6b5b",
            "56ebab556dea4eb9919cbca593b39012",
            "65a79ebd80a340cc87ea41fa005ff799",
            "10a1c36225cf4cbfab35fce25636b56c",
            "fdbc6992d8d743da98f8c249421926a6",
            "b29537128a304b11a630351f5f9ee514",
            "16670f57b1bc4b05a445a99fd0d264bd",
            "df53ae5397874b0c8f2695c78301b033",
            "da9d498977f742e8a98c50c717e37cc1",
            "eb82fd0010e946889643ab8e2cb2619a",
            "6dfcaa8596ca44d0ac157ff1671b4ece",
            "e5c852952c834c7b835cbf98abe32a87",
            "c82d78a6a43141da9232f021aedbe0a1",
            "2eeb64e3f64341d0b7499f009688d100",
            "eca0b6c390bf4879ba44f492cb17506e",
            "0a92c678e0a243f495da29ca97f74b59",
            "103c2dc08b78468c866943f6f06d0722",
            "1f2b00b86f094ab49e78324731c40b94",
            "bab255a370fa44be8ec40417f8122d33",
            "cab1c8a52c894d968558e332e8918b8f",
            "c7ba0da5e8ff4e4282f937e3f99c727c",
            "092d52ebbf594dcfb16b15fbebdeb240",
            "db490f4208bb4ecfb4c5ea9595dac6fa",
            "808945c0f329415fb3cdd96d5e5efe56",
            "5931beb4fd5a473d8a128ace4b87f7d9",
            "9b76aa3dba51459da9ecf31ba0224e33",
            "16c76cfc091240cc8045a786c9d78d9a",
            "8e5b90b6b67045d2be02aaeb4f53263f",
            "cf62bb2c97974e46b988a7b03f59b297",
            "0b65e6c4e4fb4023b5c00bc20d8377c7",
            "4f1725a0abf04e499819d59df4bd7182",
            "f0fe78705f2f4dc8a90baf1ac5363b6d",
            "2c0109e77a7b4399ad10e34a046bb5bf",
            "1a0cf724d3e049069e300e15e8b09241",
            "2b00faf51ba044b0be55d6f70964f042",
            "8dbc1cf4b90d44ffa8c4d13129884cda",
            "97f5f00ef85c45e69881e61df56ce86b",
            "a7a4f346396347b39c6c6b33a4c73010",
            "990bc0042c1a4f9f8e15fb98be347090",
            "622e3b10d8aa4759b55c7f41d49f8039",
            "92f3cf98e9224063a154f063a1bca6c0",
            "b9c9e625fbaa4bfeb46995557d0bf713",
            "624ec952dabb450684d617b5502364cb",
            "badbd1d7647f414cbd8fae47c817897d",
            "74953d868bb347ca9128fad4e499e1b3",
            "1bad666995c047e797607d207c9278b0",
            "f58c096ebdf645eaa35f0b33cccc1eed",
            "b058abfaade0433186ed69b1d1d82bee",
            "db175c591dc34c29bace049696642393",
            "cf54f0c242974975bb3ab621ed679d20",
            "db269263495d4fbfb5019f41e1e90465",
            "ea7c600f06574155a449a92bcf098558",
            "bcaaa94a39c1454b9ca78a0f0ba80473",
            "9e2672f471564acd9de4c0b6b87a768f",
            "5a708ff82f1b4ca08734bdc2eee65fc1",
            "afe88b8e33704495ba4a4421a5a04206",
            "1d8430c90db4423b8ebe564c70876e29",
            "139280f7856e4ffab7720b11d7d9e62c",
            "0a674bec15464d419f43846c8c7dc32b",
            "65d14c4843d140caac9985f819a1360e",
            "1d2d3ba2174e4942909464f099b8580e",
            "c22077e1c3a64f7f84494978e7341194",
            "355776f78d164837bb2a3355150971c4",
            "c87cbe993be34ced905ba44ab74eb29c",
            "66aa2a60919e43868f6f46cf0e5c5ab2",
            "c5f244c1654b48328985451913b047f2",
            "6d302273b1934a1fabfd32fdfc31bcd2",
            "842541c522e547db8caf61533f225a47",
            "ca17ea1970574d9793109eae5fcb3ba8",
            "47d5285f92e94ff882921adef87d6564",
            "6fd79eff17534d328f66cd0b44e986cf",
            "92d2b908b55747f4ac8871240149008e",
            "e2989a2d728c452db3640a51e8c969e6",
            "121417b5d3bb46e49459762dac4c8bef",
            "a8be24b38613463f977d56fa4e7ba2a6",
            "cb04593fb9404e26ab113aafa0cbf160",
            "bb7160d0700e439cbd56e6351f4ddff5",
            "492634a0e764462b9381565446643ee3",
            "ca8a889d501147c1b704294ee9c2beb1",
            "2074921a754045e3972ededfa0b2f08c",
            "db9cacbdb06c44db8cafc91ff5105283",
            "545f0015725d47deb1e62db74c724976"
          ]
        },
        "outputId": "0f78e51a-111e-454a-bc35-ab37ae856b62",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "335258a2365340a9aa540d353c92d355"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a5f18a5ddcd48afa9d51e8a3abd2269"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15100addfdec4b72b4275ef08f488c99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da3c923801334fa98506609efff84106"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b29537128a304b11a630351f5f9ee514"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "103c2dc08b78468c866943f6f06d0722"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e5b90b6b67045d2be02aaeb4f53263f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "990bc0042c1a4f9f8e15fb98be347090"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf54f0c242974975bb3ab621ed679d20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d2d3ba2174e4942909464f099b8580e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92d2b908b55747f4ac8871240149008e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "57WSXT48689o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5932fe-da3e-460f-c291-2c05502619c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f6dd2f399444>:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context=ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "id": "MGRJQhPx7ArK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0dd09b-7a65-4b30-bb80-998583c54454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7d8d881c50f0>, num_workers=None), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7d8d881c50f0>, id_func=<function default_id_func at 0x7d8e43515d80>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.core.service_context_elements.llama_logger.LlamaLogger object at 0x7d8cf75b7eb0>, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7d8d881c50f0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
      ],
      "metadata": {
        "id": "FDnOSwBs7ENN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "XQzGtnRu7GxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8466ccb-58ff-455f-f707-a43854c9e7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7d8d881c7370>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine()"
      ],
      "metadata": {
        "id": "IPdqBTWh7KC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"what is attention is all you need?\")"
      ],
      "metadata": {
        "id": "fw6lhAOH7KoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76550b30-c7cd-40c0-9308-5a81b2715135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "neBDJeWA7M_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4c9328-3702-499d-c588-8c3a10fb3614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention is a crucial component in transformer-based models, including BERT. It allows the model to focus on specific parts of the input sequence when generating the output. In the context of BERT, attention is used to compute a weighted sum of the input tokens' representations, where the weights are learned during training. The attention mechanism helps the model to selectively focus on the most relevant tokens in the input sequence, which improves the model's performance on downstream NLP tasks. However, attention is not the only thing you need to build a successful transformer-based model. Other important components include the pre-training task, the model architecture, and the choice of hyperparameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"how to approach any machine learning problem?\")"
      ],
      "metadata": {
        "id": "T7TiYNYt7OtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "YQuqIGVB7RH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2bb3e1-6e18-4320-fd5a-489bc50ac1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are two ways to approach any machine learning problem, as described in the book \"Mathematics for Machine Learning\" by Deisenroth et al. (2020):\n",
            "\n",
            "1. Bottom-up approach: This approach involves building up the concepts from foundational to more advanced. This is often the preferred approach in more technical fields, such as mathematics. The advantage of this approach is that the reader can rely on previously learned concepts at all times. However, the downside is that the foundational concepts may not be particularly interesting on their own, and the lack of motivation means that most foundational definitions are quickly forgotten.\n",
            "2. Top-down approach: This approach involves drilling down from practical needs to more basic requirements. This goal-driven approach has the advantage that the reader knows at all times why they need to work on a particular concept, and there is a clear path of required knowledge. However, the downside is that the knowledge is built on potentially shaky foundations, and the readers have to remember a set of words that they do not have any way of understanding.\n",
            "\n",
            "Based on the context information provided, it seems that the authors of the book recommend using a combination of both approaches, depending\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"who is abhishek thakur?\")"
      ],
      "metadata": {
        "id": "ugC9XuNZyi2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-D9jGyczbW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}